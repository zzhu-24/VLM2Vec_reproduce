# VLM2Vec Environment Requirements
# Generated from conda environment: vlm2vec_update
# Python version: 3.10.18

# Core ML/DL frameworks
torch==2.4.0+cu121
torchvision==0.19.0+cu121
transformers==4.57.0
accelerate==1.12.0
peft==0.17.1
datasets==4.1.1
safetensors==0.7.0
tokenizers==0.22.1

# Qwen VL specific
qwen-vl-utils==0.0.8

# Flash Attention (may need to compile from source on target cluster)
flash-attn==2.8.3

# Computer Vision
opencv-python==4.11.0.86
opencv-contrib-python==4.11.0.86
pillow==11.3.0
timm==1.0.20
scikit-image==0.25.2
imageio==2.37.0

# Video processing
decord==0.6.0
av==15.1.0

# Scientific computing
numpy==1.26.4
scipy==1.15.3
scikit-learn==1.7.2
pandas==2.3.3

# Utilities
einops==0.8.1
hnswlib==0.8.0
sentencepiece==0.2.1
tqdm==4.67.1
ninja==1.13.0
wrapt==1.17.3
hjson==3.1.0
py-cpuinfo==9.0.0

# Distributed computing
ray==2.49.2

# Monitoring and logging
wandb==0.22.1

# Hugging Face ecosystem
huggingface-hub==0.36.0
hf-xet==1.2.0

# NVIDIA CUDA libraries (automatically installed with torch, but listed for reference)
# nvidia-cublas-cu12==12.1.3.1
# nvidia-cudnn-cu12==9.1.0.70
# ... (other nvidia packages will be installed automatically with torch)

# Additional dependencies (all transitive dependencies)
aiohappyeyeballs==2.6.1
aiohttp==3.12.15
aiosignal==1.4.0
annotated-types==0.7.0
anyio==4.10.0
argon2-cffi==21.3.0
argon2-cffi-bindings==25.1.0
async-lru==2.0.4
async-timeout==5.0.1
attrs==24.3.0
babel==2.16.0
beautifulsoup4==4.13.5
brotlicffi==1.0.9.2
certifi==2025.8.3
cffi==1.17.1
charset-normalizer==3.3.2
click==8.3.0
contourpy==1.3.2
cycler==0.12.1
dill==0.4.0
exceptiongroup==1.2.0
fastjsonschema==2.20.0
filelock==3.19.1
fonttools==4.60.1
frozenlist==1.7.0
fsspec==2025.9.0
gitdb==4.0.12
GitPython==3.1.45
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.10
Jinja2==3.1.6
joblib==1.5.2
json5==0.9.25
jsonschema==4.25.0
jsonschema-specifications==2023.7.1
kiwisolver==1.4.9
lazy_loader==0.4
MarkupSafe==3.0.2
matplotlib==3.10.6
mpmath==1.3.0
msgpack==1.1.1
multidict==6.6.4
multiprocess==0.70.16
networkx==3.4.2
overrides==7.4.0
packaging==25.0
platformdirs==4.3.7
propcache==0.3.2
protobuf==6.32.1
pyarrow==21.0.0
pycparser==2.23
pydantic==2.11.10
pydantic_core==2.33.2
pyparsing==3.2.5
PySocks==1.7.1
python-dateutil==2.9.0.post0
python-json-logger==3.2.1
pytrec_eval==0.5
pytz==2025.2
PyYAML==6.0.2
QtPy==2.4.3
referencing==0.30.2
regex==2025.11.3
requests==2.32.5
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rpds-py==0.22.3
sentry-sdk==2.39.0
six==1.17.0
smmap==5.0.2
sniffio==1.3.0
soupsieve==2.5
sympy==1.14.0
threadpoolctl==3.6.0
tifffile==2025.5.10
tinycss2==1.4.0
tomli==2.2.1
triton==3.0.0
typing_extensions==4.15.0
typing-inspection==0.4.2
tzdata==2025.2
urllib3==2.5.0
websocket-client==1.8.0
xxhash==3.6.0
yarl==1.20.1

# Note: Jupyter-related packages are excluded. If you need them, install separately:
# jupyter jupyterlab ipython ipykernel

# Note: flash-attn may need to be compiled from source on the target cluster
# See: https://github.com/Dao-AILab/flash-attention

# Note: torch with CUDA 12.1 is specified. For different CUDA versions, adjust accordingly:
# For CUDA 11.8: torch==2.4.0+cu118
# For CPU only: torch==2.4.0
