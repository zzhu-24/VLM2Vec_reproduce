#!/bin/bash
# NOTE: replace ... with actual paths
# export LD_LIBRARY_PATH=...
# export PATH=...
# echo "conda location: $(which conda)"
# echo "Python location: $(which python)"
# echo "Python version: $(python --version)"

set -e  # Stop on error
trap 'echo "âŒ Script failed at line $LINENO"; exit 1' ERR

echo "==> Environment"
echo "Python: $(which python)"
echo "Version: $(python --version)"
echo ""


# export HF_DATASETS_CACHE=...
# export HF_HOME=...
export WANDB_DISABLED=false
export WANDB_PROJECT=vlm2vec_train
export WANDB_API_KEY=151b985aec8f2669c89875abb20b1c822ecdb9ad
# export HUGGING_FACE_HUB_TOKEN=...
# export WANDB_PROJECT=...
export WANDB_RUN_GROUP=3Feb_Qwen3VL4B_rmv_30_23_lora24
export MODEL_NAME=Qwen/Qwen3-VL-4B-Instruct
export DATASET_CONFIG_PATH=/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/train_image.yaml

# export MODEL_NAME=Alibaba-NLP/gme-Qwen3-VL-4B-Instruct
export WANDB_NAME="${WANDB_RUN_GROUP}-${MODEL_NAME}"
export EXP_NAME=$WANDB_NAME
export EXP_DIR=/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/$WANDB_NAME
export WANDB_DIR=$EXP_DIR
echo $EXP_DIR

export DATA_BASEDIR=/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train

# --- Head Pruning Config ---
# Option A: Provide a JSON file with per-layer KV group pruning spec
#   (generated by compute_head_importance.py or manually created)
#   Format: {"0": [2, 5], "3": [1, 7]}  means prune groups 2,5 from layer 0, groups 1,7 from layer 3
# export HEAD_PRUNE_CONFIG=/path/to/head_importance.json
#
# Option B: Uniform pruning - prune N least-important KV groups from every layer
# export HEAD_PRUNE_N=2

# --- MLP Pruning Config ---
# Ratio of MLP intermediate neurons to prune per layer (0.0 = no pruning)
# export MLP_PRUNE_RATIO=0.25
#
# Path to mlp_importance.json (generated by compute_head_importance.py)
# Required when MLP_PRUNE_RATIO > 0 for importance-based pruning
# export MLP_IMPORTANCE_PATH=/path/to/mlp_importance.json

mkdir -p $EXP_DIR/wandb
rm -rf $EXP_DIR/wandb/*

cd /home/infres/zzhu-24/PRIM/VLM2Vec/

# --- Optional: Step 0 - Compute head + MLP importance scores ---
# Run this once before training to generate head_importance.json, mlp_importance.json, and heatmaps:
#
# python compute_head_importance.py \
#     --model_name $MODEL_NAME \
#     --dataset_config "$DATASET_CONFIG_PATH" \
#     --data_basedir "$DATA_BASEDIR" \
#     --output_dir $EXP_DIR/importance \
#     --delete_L 30 \
#     --delete_n 23 \
#     --num_calibration_batches 32 \
#     --batch_size 4 \
#     --bf16
# Then set:
#   export MLP_IMPORTANCE_PATH=$EXP_DIR/importance/mlp_importance.json

# --- Build head_prune arg ---
HEAD_PRUNE_ARG=""
if [ -n "${HEAD_PRUNE_CONFIG:-}" ]; then
    HEAD_PRUNE_ARG="--head_prune_config $HEAD_PRUNE_CONFIG"
elif [ -n "${HEAD_PRUNE_N:-}" ]; then
    HEAD_PRUNE_ARG="--head_prune_n $HEAD_PRUNE_N"
fi

# --- Build mlp_prune arg ---
MLP_PRUNE_ARG=""
if [ -n "${MLP_PRUNE_RATIO:-}" ]; then
    MLP_PRUNE_ARG="--mlp_prune_ratio $MLP_PRUNE_RATIO"
    if [ -n "${MLP_IMPORTANCE_PATH:-}" ]; then
        MLP_PRUNE_ARG="$MLP_PRUNE_ARG --mlp_importance_path $MLP_IMPORTANCE_PATH"
    fi
fi

cmd="CUDA_VISIBLE_DEVICES=0,1,2,3 torchrun --nproc_per_node=2 --master_port=2208 --max_restarts=0 train.py 
    --lora
    --lora_r 24
    --model_name $MODEL_NAME
    --bf16
    --pooling eos
    --normalize True
    --temperature 0.02
    --dataloader_num_workers 2
    --dataset_config "$DATASET_CONFIG_PATH"
    --data_basedir "$DATA_BASEDIR"
    --run_name $EXP_NAME
    --output_dir $EXP_DIR
    --grad_cache True
    --per_device_train_batch_size 16
    --gc_q_chunk_size 4
    --gc_p_chunk_size 4
    --interleave_batch_size 0
    --lr_scheduler_type linear
    --learning_rate 1e-5 
    --max_steps 14700
    --warmup_steps 100
    --save_steps 500
    --logging_steps 1
    --save_safetensors True
    --remove_unused_columns False
    --delete_L 30
    --delete_n 23
    --joint_training_layers -1
    --eval_layers -1
    $HEAD_PRUNE_ARG
    $MLP_PRUNE_ARG
    --report_to wandb 2>&1 | tee $EXP_DIR/train.log"

echo $cmd
eval $cmd