/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
[2026-01-21 01:54:04,121] INFO [src.utils:21] Distributed init debug info:
[2026-01-21 01:54:04,121] INFO [src.utils:21] RANK: None
[2026-01-21 01:54:04,121] INFO [src.utils:21] LOCAL_RANK: None
[2026-01-21 01:54:04,121] INFO [src.utils:21] WORLD_SIZE: None
[2026-01-21 01:54:04,121] INFO [src.utils:21] MASTER_ADDR: None
[2026-01-21 01:54:04,121] INFO [src.utils:21] MASTER_PORT: None
[2026-01-21 01:54:05,667] INFO [src.utils:21] Model Backbone: qwen2_vl
[2026-01-21 01:54:05,667] INFO [src.utils:21] Loading processor from: /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/original-Qwen/Qwen2-VL-2B-Instruct/checkpoint-5000
[2026-01-21 01:54:08,017] INFO [src.utils:21] Loading backbone [qwen2_vl] from /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/original-Qwen/Qwen2-VL-2B-Instruct/checkpoint-5000
`torch_dtype` is deprecated! Use `dtype` instead!
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 22.79it/s]
[2026-01-21 01:54:08,503] INFO [src.utils:21] Loading LoRA from /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/original-Qwen/Qwen2-VL-2B-Instruct/checkpoint-5000
[2026-01-21 01:54:24,413] INFO [src.utils:21] [rank=0] Loading the model from Huggingface: Qwen/Qwen2-VL-2B-Instruct...
[2026-01-21 01:54:25,263] INFO [src.utils:21] --- Evaluating N24News ---
Map (num_proc=4): 100%|██████████| 1000/1000 [00:00<?, ? examples/s]Map (num_proc=4): 1250 examples [00:00, 1047.11 examples/s]         Map (num_proc=4): 2000 examples [00:00, 2278.93 examples/s]
[2026-01-21 01:54:27,646] INFO [src.utils:21] Encoding queries...
Queries for N24News (rank 0):   0%|          | 0/125 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Queries for N24News (rank 0):   0%|          | 0/125 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/eval.py", line 390, in <module>
    main()
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/eval.py", line 253, in main
    query_embeds_dict, gt_infos = encode_embeddings(model, eval_qry_loader, training_args, model_args, padded_qry_dataset, encode_side="qry", description=f"Queries for {dataset_name}")
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/eval.py", line 78, in encode_embeddings
    output = model(qry=inputs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 475, in forward
    qry_reps_list = self.encode_input(qry, self.eval_layers) if qry else None  # list of (bsz_per_device, dim)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 188, in encode_input
    hidden_states = self.encoder(**input, return_dict=True, output_hidden_states=True)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/vlm_backbone/qwen2_vl/modeling_qwen2_vl.py", line 1692, in forward
    image_embeds = self.visual(valid_pixel_values, grid_thw=valid_grid_thw)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/vlm_backbone/qwen2_vl/modeling_qwen2_vl.py", line 407, in forward
    hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/vlm_backbone/qwen2_vl/modeling_qwen2_vl.py", line 275, in forward
    hidden_states = hidden_states + self.attn(
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/vlm_backbone/qwen2_vl/modeling_qwen2_vl.py", line 249, in forward
    attn_output = F.scaled_dot_product_attention(q, k, v, attention_mask, dropout_p=0.0)
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 17.63 GiB. GPU 0 has a total capacity of 23.56 GiB of which 16.38 GiB is free. Including non-PyTorch memory, this process has 7.17 GiB memory in use. Of the allocated memory 6.45 GiB is allocated by PyTorch, and 423.42 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
