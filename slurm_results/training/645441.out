==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test1-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/train.log
W1022 17:26:18.227000 139726608017216 torch/distributed/run.py:779] 
W1022 17:26:18.227000 139726608017216 torch/distributed/run.py:779] *****************************************
W1022 17:26:18.227000 139726608017216 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 17:26:18.227000 139726608017216 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-22 17:26:28,443] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.09it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.89it/s]
wandb: setting up run cghjwuab
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251022_172628-cghjwuab
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test1-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/cghjwuab
[2025-10-22 17:26:30,015] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.21it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.08it/s]
[2025-10-22 17:26:30,641] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-22 17:26:30,644] INFO [src.utils:19] Loading lora adapter from TailTokenDetachPrefixWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-22 17:26:39,483] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-22 17:26:40,795] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-22 17:26:40,796] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-22 17:26:45,125] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-22 17:26:45,125] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-22 17:26:45,980] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-22 17:26:45,980] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-22 17:26:45,981] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-22 17:26:45,982] INFO [src.utils:19] ==================================================
[2025-10-22 17:26:45,982] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-22 17:26:45,983] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 17:26:45,983] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 17:26:45,984] INFO [src.utils:19] ==================================================
[2025-10-22 17:26:47,735] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 17:26:47,735] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 17:26:47,735] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 17:26:47,735] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 17:26:47,735] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 17:26:47,735] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 17:26:47,735] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 17:26:47,735] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 17:26:47,736] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 17:26:47,736] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 17:26:47,736] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 17:26:47,737] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 17:26:47,737] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 17:26:47,737] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 17:26:47,746] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-22 17:26:47,749] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1022 17:26:50.362353259 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1022 17:26:50.389338831 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:03<6:37:06,  3.97s/it]                                                  {'loss': 12.3501, 'grad_norm': 1116.3026123046875, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<6:37:06,  3.97s/it]  0%|          | 2/6000 [00:06<5:16:11,  3.16s/it]                                                  {'loss': 10.9682, 'grad_norm': 1189.1739501953125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:16:11,  3.16s/it]  0%|          | 3/6000 [00:09<4:53:37,  2.94s/it]                                                  {'loss': 10.7074, 'grad_norm': 1185.656982421875, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:53:37,  2.94s/it]  0%|          | 4/6000 [00:11<4:42:34,  2.83s/it]                                                  {'loss': 10.7951, 'grad_norm': 1136.5447998046875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:11<4:42:34,  2.83s/it]  0%|          | 5/6000 [00:14<4:34:42,  2.75s/it]                                                  {'loss': 10.7727, 'grad_norm': 1081.3226318359375, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:34:42,  2.75s/it]  0%|          | 6/6000 [00:17<4:31:27,  2.72s/it]                                                  {'loss': 9.9614, 'grad_norm': 970.348876953125, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:31:27,  2.72s/it]  0%|          | 7/6000 [00:19<4:29:18,  2.70s/it]                                                  {'loss': 9.2634, 'grad_norm': 935.3402099609375, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:19<4:29:18,  2.70s/it]  0%|          | 8/6000 [00:22<4:25:09,  2.66s/it]                                                  {'loss': 8.8562, 'grad_norm': 770.409912109375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:25:09,  2.66s/it]  0%|          | 9/6000 [00:25<4:24:20,  2.65s/it]                                                  {'loss': 6.1891, 'grad_norm': 456.8663024902344, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:24:20,  2.65s/it]  0%|          | 10/6000 [00:27<4:22:51,  2.63s/it]                                                   {'loss': 6.9485, 'grad_norm': 442.69580078125, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:27<4:22:51,  2.63s/it]  0%|          | 11/6000 [00:30<4:31:57,  2.72s/it]                                                   {'loss': 7.0163, 'grad_norm': 397.5359802246094, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:30<4:31:57,  2.72s/it]  0%|          | 12/6000 [00:33<4:33:26,  2.74s/it]                                                   {'loss': 5.7796, 'grad_norm': 301.2615051269531, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:33<4:33:26,  2.74s/it]  0%|          | 13/6000 [00:35<4:30:24,  2.71s/it]                                                   {'loss': 5.3786, 'grad_norm': 228.75253295898438, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:35<4:30:24,  2.71s/it]  0%|          | 14/6000 [00:38<4:31:18,  2.72s/it]                                                   {'loss': 5.0299, 'grad_norm': 222.74929809570312, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:38<4:31:18,  2.72s/it]  0%|          | 15/6000 [00:41<4:28:02,  2.69s/it]                                                   {'loss': 4.4316, 'grad_norm': 224.43409729003906, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:41<4:28:02,  2.69s/it]  0%|          | 16/6000 [00:43<4:25:07,  2.66s/it]                                                   {'loss': 4.3238, 'grad_norm': 263.592041015625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:43<4:25:07,  2.66s/it]  0%|          | 17/6000 [00:46<4:25:21,  2.66s/it]                                                   {'loss': 3.8661, 'grad_norm': 250.82257080078125, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:46<4:25:21,  2.66s/it]