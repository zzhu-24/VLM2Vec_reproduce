==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 16:07:54.861000 137927590500160 torch/distributed/run.py:779] 
W1103 16:07:54.861000 137927590500160 torch/distributed/run.py:779] *****************************************
W1103 16:07:54.861000 137927590500160 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 16:07:54.861000 137927590500160 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 16:08:04,768] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.83it/s]
wandb: setting up run hfofp648
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_160805-hfofp648
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/hfofp648
[2025-11-03 16:08:06,384] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.18it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.00it/s]
[2025-11-03 16:08:07,001] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 16:08:15,995] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 16:08:17,046] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 16:08:17,047] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 16:08:21,359] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 16:08:21,360] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 16:08:22,344] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 16:08:22,345] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 16:08:22,345] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 16:08:22,346] INFO [src.utils:19] ==================================================
[2025-11-03 16:08:22,347] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 16:08:22,347] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 16:08:22,348] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 16:08:22,348] INFO [src.utils:19] ==================================================
[2025-11-03 16:08:24,118] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 16:08:24,118] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 16:08:24,118] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 16:08:24,118] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 16:08:24,118] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 16:08:24,118] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 16:08:24,118] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 16:08:24,118] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 16:08:24,119] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 16:08:24,119] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 16:08:24,119] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 16:08:24,119] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 16:08:24,120] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 16:08:24,120] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 16:08:24,126] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 16:08:24,128] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 16:08:24,133] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 16:08:24,135] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:04<6:49:31,  4.10s/it]                                                  {'loss': 21.6478, 'grad_norm': 672.0105590820312, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:49:31,  4.10s/it]  0%|          | 2/6000 [00:06<5:24:34,  3.25s/it]                                                  {'loss': 18.9604, 'grad_norm': 635.9318237304688, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:24:34,  3.25s/it]  0%|          | 3/6000 [00:09<5:00:54,  3.01s/it]                                                  {'loss': 18.7063, 'grad_norm': 665.1550903320312, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:00:54,  3.01s/it]  0%|          | 4/6000 [00:12<4:49:09,  2.89s/it]                                                  {'loss': 19.6582, 'grad_norm': 643.5559692382812, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:49:09,  2.89s/it]  0%|          | 5/6000 [00:14<4:42:21,  2.83s/it]                                                  {'loss': 20.5834, 'grad_norm': 567.0276489257812, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:42:21,  2.83s/it]  0%|          | 6/6000 [00:17<4:37:43,  2.78s/it]                                                  {'loss': 20.3418, 'grad_norm': 605.24462890625, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:37:43,  2.78s/it]  0%|          | 7/6000 [00:20<4:33:48,  2.74s/it]                                                  {'loss': 20.4439, 'grad_norm': 641.0663452148438, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:33:48,  2.74s/it]  0%|          | 8/6000 [00:22<4:30:21,  2.71s/it]                                                  {'loss': 19.6416, 'grad_norm': 606.5530395507812, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:30:21,  2.71s/it]  0%|          | 9/6000 [00:25<4:30:39,  2.71s/it]                                                  {'loss': 14.5173, 'grad_norm': 549.986083984375, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:30:39,  2.71s/it]  0%|          | 10/6000 [00:28<4:27:30,  2.68s/it]                                                   {'loss': 18.8628, 'grad_norm': 573.77490234375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:27:30,  2.68s/it]  0%|          | 11/6000 [00:31<4:38:26,  2.79s/it]                                                   {'loss': 23.0675, 'grad_norm': 603.9774780273438, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:38:26,  2.79s/it]  0%|          | 12/6000 [00:34<4:41:28,  2.82s/it]                                                   {'loss': 19.5244, 'grad_norm': 539.0565185546875, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:41:28,  2.82s/it]  0%|          | 13/6000 [00:36<4:38:10,  2.79s/it]                                                   {'loss': 19.2242, 'grad_norm': 529.1542358398438, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:38:10,  2.79s/it]  0%|          | 14/6000 [00:39<4:38:54,  2.80s/it]                                                   {'loss': 21.7909, 'grad_norm': 551.8148193359375, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:39<4:38:54,  2.80s/it]  0%|          | 15/6000 [00:42<4:35:45,  2.76s/it]                                                   {'loss': 17.6596, 'grad_norm': 503.120849609375, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:35:45,  2.76s/it]  0%|          | 16/6000 [00:45<4:32:26,  2.73s/it]                                                   {'loss': 18.188, 'grad_norm': 505.1995544433594, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:32:26,  2.73s/it]  0%|          | 17/6000 [00:47<4:33:20,  2.74s/it]                                                   {'loss': 18.0886, 'grad_norm': 483.40216064453125, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:47<4:33:20,  2.74s/it]  0%|          | 18/6000 [00:50<4:33:15,  2.74s/it]                                                   {'loss': 15.6292, 'grad_norm': 400.7611083984375, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:33:15,  2.74s/it]  0%|          | 19/6000 [00:53<4:31:17,  2.72s/it]                                                   {'loss': 17.967, 'grad_norm': 457.5495910644531, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:31:17,  2.72s/it]  0%|          | 20/6000 [00:55<4:29:20,  2.70s/it]                                                   {'loss': 18.8542, 'grad_norm': 502.8291015625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:55<4:29:20,  2.70s/it]  0%|          | 21/6000 [00:58<4:32:20,  2.73s/it]                                                   {'loss': 15.6805, 'grad_norm': 434.5492248535156, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:58<4:32:20,  2.73s/it]  0%|          | 22/6000 [01:01<4:33:39,  2.75s/it]                                                   {'loss': 15.7238, 'grad_norm': 388.55072021484375, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:33:39,  2.75s/it]  0%|          | 23/6000 [01:04<4:31:32,  2.73s/it]                                                   {'loss': 14.3235, 'grad_norm': 405.8369140625, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:31:32,  2.73s/it]  0%|          | 24/6000 [01:06<4:32:36,  2.74s/it]                                                   {'loss': 11.6462, 'grad_norm': 386.958740234375, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:06<4:32:36,  2.74s/it]  0%|          | 25/6000 [01:09<4:31:04,  2.72s/it]                                                   {'loss': 14.0818, 'grad_norm': 467.2899475097656, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:09<4:31:04,  2.72s/it]  0%|          | 26/6000 [01:12<4:31:56,  2.73s/it]                                                   {'loss': 15.2607, 'grad_norm': 572.5853881835938, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:12<4:31:56,  2.73s/it]  0%|          | 27/6000 [01:15<4:31:04,  2.72s/it]                                                   {'loss': 13.8256, 'grad_norm': 644.233642578125, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:31:04,  2.72s/it]  0%|          | 28/6000 [01:18<4:58:45,  3.00s/it]                                                   {'loss': 12.7046, 'grad_norm': 463.65081787109375, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:18<4:58:45,  3.00s/it]  0%|          | 29/6000 [01:21<4:48:13,  2.90s/it]                                                   {'loss': 12.062, 'grad_norm': 474.2420959472656, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:21<4:48:13,  2.90s/it]  0%|          | 30/6000 [01:24<4:43:00,  2.84s/it]                                                   {'loss': 11.9287, 'grad_norm': 550.891357421875, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:43:00,  2.84s/it]  1%|          | 31/6000 [01:26<4:39:03,  2.81s/it]                                                   {'loss': 10.659, 'grad_norm': 349.5003967285156, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:26<4:39:03,  2.81s/it]  1%|          | 32/6000 [01:29<4:35:10,  2.77s/it]                                                   {'loss': 10.4251, 'grad_norm': 467.6859130859375, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:29<4:35:10,  2.77s/it]  1%|          | 33/6000 [01:32<4:34:36,  2.76s/it]                                                   {'loss': 10.8305, 'grad_norm': 428.3185729980469, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:32<4:34:36,  2.76s/it]  1%|          | 34/6000 [01:34<4:33:09,  2.75s/it]                                                   {'loss': 9.8016, 'grad_norm': 324.80224609375, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:34<4:33:09,  2.75s/it]  1%|          | 35/6000 [01:37<4:33:09,  2.75s/it]                                                   {'loss': 8.6666, 'grad_norm': 314.6619567871094, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:37<4:33:09,  2.75s/it]  1%|          | 36/6000 [01:40<4:31:01,  2.73s/it]                                                   {'loss': 8.3265, 'grad_norm': 454.0018005371094, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:40<4:31:01,  2.73s/it]  1%|          | 37/6000 [01:42<4:29:21,  2.71s/it]                                                   {'loss': 7.6896, 'grad_norm': 458.5365905761719, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:29:21,  2.71s/it]  1%|          | 38/6000 [01:45<4:27:36,  2.69s/it]                                                   {'loss': 7.3666, 'grad_norm': 343.5238952636719, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:45<4:27:36,  2.69s/it]  1%|          | 39/6000 [01:48<4:27:40,  2.69s/it]                                                   {'loss': 6.973, 'grad_norm': 299.97833251953125, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:48<4:27:40,  2.69s/it]  1%|          | 40/6000 [01:51<4:27:30,  2.69s/it]                                                   {'loss': 6.9782, 'grad_norm': 325.2816467285156, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:51<4:27:30,  2.69s/it]  1%|          | 41/6000 [01:53<4:28:22,  2.70s/it]                                                   {'loss': 7.9353, 'grad_norm': 434.39288330078125, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:53<4:28:22,  2.70s/it]  1%|          | 42/6000 [01:56<4:28:54,  2.71s/it]                                                   {'loss': 7.4463, 'grad_norm': 283.5238037109375, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:56<4:28:54,  2.71s/it]  1%|          | 43/6000 [02:00<5:00:20,  3.03s/it]                                                   {'loss': 6.2524, 'grad_norm': 238.63021850585938, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:00<5:00:20,  3.03s/it]  1%|          | 44/6000 [02:03<5:06:03,  3.08s/it]                                                   {'loss': 6.0502, 'grad_norm': 249.03025817871094, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:03<5:06:03,  3.08s/it]  1%|          | 45/6000 [02:06<4:55:33,  2.98s/it]                                                   {'loss': 6.7117, 'grad_norm': 307.2523498535156, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:06<4:55:33,  2.98s/it]  1%|          | 46/6000 [02:09<4:52:27,  2.95s/it]                                                   {'loss': 5.8077, 'grad_norm': 272.3092346191406, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:09<4:52:27,  2.95s/it]  1%|          | 47/6000 [02:11<4:45:15,  2.88s/it]                                                   {'loss': 5.6526, 'grad_norm': 359.5151672363281, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:11<4:45:15,  2.88s/it]  1%|          | 48/6000 [02:14<4:42:53,  2.85s/it]                                                   {'loss': 5.5404, 'grad_norm': 158.24600219726562, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:14<4:42:53,  2.85s/it]  1%|          | 49/6000 [02:17<4:37:54,  2.80s/it]                                                   {'loss': 5.5654, 'grad_norm': 484.1002502441406, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:17<4:37:54,  2.80s/it]  1%|          | 50/6000 [02:20<4:41:26,  2.84s/it]                                                   {'loss': 4.3973, 'grad_norm': 205.38169860839844, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:20<4:41:26,  2.84s/it][2025-11-03 16:10:44,522] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-03 16:10:44,539] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 16:10:45,150] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:25<5:42:37,  3.46s/it]                                                   {'loss': 5.2065, 'grad_norm': 176.1419219970703, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:25<5:42:37,  3.46s/it]  1%|          | 52/6000 [02:27<5:19:37,  3.22s/it]                                                   {'loss': 4.4771, 'grad_norm': 349.0827941894531, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:27<5:19:37,  3.22s/it]  1%|          | 53/6000 [02:30<5:07:15,  3.10s/it]                                                   {'loss': 5.8277, 'grad_norm': 234.52626037597656, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:30<5:07:15,  3.10s/it]  1%|          | 54/6000 [02:33<4:54:03,  2.97s/it]                                                   {'loss': 4.5364, 'grad_norm': 149.01255798339844, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:33<4:54:03,  2.97s/it]  1%|          | 55/6000 [02:35<4:45:23,  2.88s/it]                                                   {'loss': 4.7035, 'grad_norm': 151.7540283203125, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:35<4:45:23,  2.88s/it]  1%|          | 56/6000 [02:38<4:43:01,  2.86s/it]                                                   {'loss': 3.7131, 'grad_norm': 95.49444580078125, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:38<4:43:01,  2.86s/it]  1%|          | 57/6000 [02:41<4:38:46,  2.81s/it]                                                   {'loss': 3.9953, 'grad_norm': 121.71159362792969, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:41<4:38:46,  2.81s/it]  1%|          | 58/6000 [02:44<4:35:28,  2.78s/it]                                                   {'loss': 3.4319, 'grad_norm': 82.45791625976562, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:44<4:35:28,  2.78s/it]  1%|          | 59/6000 [02:46<4:31:20,  2.74s/it]                                                   {'loss': 4.5078, 'grad_norm': 159.81967163085938, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [02:46<4:31:20,  2.74s/it]  1%|          | 60/6000 [02:49<4:28:41,  2.71s/it]                                                   {'loss': 3.9883, 'grad_norm': 103.0674057006836, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:49<4:28:41,  2.71s/it]  1%|          | 61/6000 [02:52<4:26:22,  2.69s/it]                                                   {'loss': 3.1635, 'grad_norm': 64.38276672363281, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:52<4:26:22,  2.69s/it]  1%|          | 62/6000 [02:54<4:28:00,  2.71s/it]                                                   {'loss': 3.6616, 'grad_norm': 127.29978942871094, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:54<4:28:00,  2.71s/it]  1%|          | 63/6000 [02:57<4:28:24,  2.71s/it]                                                   {'loss': 3.4228, 'grad_norm': 105.24329376220703, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:57<4:28:24,  2.71s/it]  1%|          | 64/6000 [03:00<4:25:36,  2.68s/it]                                                   {'loss': 3.1492, 'grad_norm': 58.906639099121094, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:00<4:25:36,  2.68s/it]  1%|          | 65/6000 [03:02<4:24:39,  2.68s/it]                                                   {'loss': 3.5578, 'grad_norm': 136.83265686035156, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:02<4:24:39,  2.68s/it]  1%|          | 66/6000 [03:05<4:38:23,  2.81s/it]                                                   {'loss': 3.4358, 'grad_norm': 112.21499633789062, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:05<4:38:23,  2.81s/it]  1%|          | 67/6000 [03:08<4:35:11,  2.78s/it]                                                   {'loss': 3.4709, 'grad_norm': 240.4877166748047, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:08<4:35:11,  2.78s/it]  1%|          | 68/6000 [03:11<4:32:17,  2.75s/it]                                                   {'loss': 3.1177, 'grad_norm': 75.99966430664062, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:11<4:32:17,  2.75s/it]  1%|          | 69/6000 [03:14<4:31:06,  2.74s/it]                                                   {'loss': 3.0851, 'grad_norm': 87.27710723876953, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:14<4:31:06,  2.74s/it]  1%|          | 70/6000 [03:16<4:32:14,  2.75s/it]                                                   {'loss': 2.9662, 'grad_norm': 46.79401779174805, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:16<4:32:14,  2.75s/it]  1%|          | 71/6000 [03:19<4:37:22,  2.81s/it]                                                   {'loss': 3.0834, 'grad_norm': 62.97570037841797, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:19<4:37:22,  2.81s/it]  1%|          | 72/6000 [03:22<4:46:54,  2.90s/it]                                                   {'loss': 3.1573, 'grad_norm': 100.4831314086914, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:22<4:46:54,  2.90s/it]  1%|          | 73/6000 [03:25<4:42:24,  2.86s/it]                                                   {'loss': 2.9002, 'grad_norm': 29.698144912719727, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:25<4:42:24,  2.86s/it]  1%|          | 74/6000 [03:28<4:36:58,  2.80s/it]                                                   {'loss': 3.0482, 'grad_norm': 44.689456939697266, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:28<4:36:58,  2.80s/it]  1%|â–         | 75/6000 [03:30<4:32:24,  2.76s/it]                                                   {'loss': 2.9344, 'grad_norm': 54.129581451416016, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:30<4:32:24,  2.76s/it]  1%|â–         | 76/6000 [03:33<4:34:17,  2.78s/it]                                                   {'loss': 3.1346, 'grad_norm': 55.78152084350586, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:33<4:34:17,  2.78s/it]  1%|â–         | 77/6000 [03:36<4:33:22,  2.77s/it]                                                   {'loss': 3.4108, 'grad_norm': 79.93425750732422, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:36<4:33:22,  2.77s/it]  1%|â–         | 78/6000 [03:39<4:43:30,  2.87s/it]                                                   {'loss': 3.3821, 'grad_norm': 98.0641860961914, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:39<4:43:30,  2.87s/it]  1%|â–         | 79/6000 [03:42<4:45:30,  2.89s/it]                                                   {'loss': 2.9504, 'grad_norm': 44.27396011352539, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:42<4:45:30,  2.89s/it]  1%|â–         | 80/6000 [03:45<4:40:52,  2.85s/it]                                                   {'loss': 3.0287, 'grad_norm': 60.97098922729492, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:45<4:40:52,  2.85s/it]  1%|â–         | 81/6000 [03:48<4:36:36,  2.80s/it]                                                   {'loss': 2.9111, 'grad_norm': 30.913265228271484, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:48<4:36:36,  2.80s/it]  1%|â–         | 82/6000 [03:50<4:35:48,  2.80s/it]                                                   {'loss': 2.966, 'grad_norm': 42.83852005004883, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:50<4:35:48,  2.80s/it]  1%|â–         | 83/6000 [03:53<4:32:16,  2.76s/it]                                                   {'loss': 3.2231, 'grad_norm': 114.76372528076172, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:53<4:32:16,  2.76s/it]  1%|â–         | 84/6000 [03:56<4:34:35,  2.78s/it]                                                   {'loss': 3.0876, 'grad_norm': 120.87359619140625, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:56<4:34:35,  2.78s/it]  1%|â–         | 85/6000 [03:59<4:43:38,  2.88s/it]                                                   {'loss': 3.0398, 'grad_norm': 44.270912170410156, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:59<4:43:38,  2.88s/it]  1%|â–         | 86/6000 [04:02<4:38:41,  2.83s/it]                                                   {'loss': 3.1807, 'grad_norm': 63.18205642700195, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:02<4:38:41,  2.83s/it]  1%|â–         | 87/6000 [04:04<4:35:13,  2.79s/it]                                                   {'loss': 3.1959, 'grad_norm': 106.29464721679688, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:04<4:35:13,  2.79s/it]  1%|â–         | 88/6000 [04:07<4:32:33,  2.77s/it]                                                   {'loss': 3.3522, 'grad_norm': 72.03974914550781, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:07<4:32:33,  2.77s/it]  1%|â–         | 89/6000 [04:10<4:28:30,  2.73s/it]                                                   {'loss': 3.0135, 'grad_norm': 63.20963668823242, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:10<4:28:30,  2.73s/it]  2%|â–         | 90/6000 [04:12<4:29:08,  2.73s/it]                                                   {'loss': 2.8936, 'grad_norm': 41.06595993041992, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:12<4:29:08,  2.73s/it]  2%|â–         | 91/6000 [04:15<4:29:50,  2.74s/it]                                                   {'loss': 2.8886, 'grad_norm': 34.456233978271484, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:15<4:29:50,  2.74s/it]  2%|â–         | 92/6000 [04:18<4:41:05,  2.85s/it]                                                   {'loss': 3.0139, 'grad_norm': 103.05677795410156, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:18<4:41:05,  2.85s/it]  2%|â–         | 93/6000 [04:21<4:42:46,  2.87s/it]                                                   {'loss': 2.8906, 'grad_norm': 47.65178680419922, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:21<4:42:46,  2.87s/it]  2%|â–         | 94/6000 [04:24<4:37:12,  2.82s/it]                                                   {'loss': 2.9489, 'grad_norm': 33.27722930908203, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:24<4:37:12,  2.82s/it]  2%|â–         | 95/6000 [04:27<4:33:54,  2.78s/it]                                                   {'loss': 2.8976, 'grad_norm': 32.53230667114258, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:27<4:33:54,  2.78s/it]  2%|â–         | 96/6000 [04:29<4:32:04,  2.77s/it]                                                   {'loss': 2.8985, 'grad_norm': 22.45139503479004, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:29<4:32:04,  2.77s/it]  2%|â–         | 97/6000 [04:32<4:31:24,  2.76s/it]                                                   {'loss': 2.9879, 'grad_norm': 55.787574768066406, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:32<4:31:24,  2.76s/it]  2%|â–         | 98/6000 [04:35<4:28:40,  2.73s/it]                                                   {'loss': 2.8508, 'grad_norm': 23.996410369873047, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:35<4:28:40,  2.73s/it]  2%|â–         | 99/6000 [04:37<4:27:54,  2.72s/it]                                                   {'loss': 2.8023, 'grad_norm': 18.709548950195312, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:37<4:27:54,  2.72s/it]  2%|â–         | 100/6000 [04:40<4:26:35,  2.71s/it]                                                    {'loss': 2.8747, 'grad_norm': 23.19597625732422, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:40<4:26:35,  2.71s/it][2025-11-03 16:13:04,999] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-03 16:13:05,010] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 16:13:05,801] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:46<6:03:38,  3.70s/it]                                                    {'loss': 2.968, 'grad_norm': 71.02288055419922, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:46<6:03:38,  3.70s/it]  2%|â–         | 102/6000 [04:49<5:33:37,  3.39s/it]                                                    {'loss': 2.8862, 'grad_norm': 26.89806365966797, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:49<5:33:37,  3.39s/it]  2%|â–         | 103/6000 [04:52<5:15:26,  3.21s/it]                                                    {'loss': 2.8496, 'grad_norm': 35.306617736816406, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:52<5:15:26,  3.21s/it]  2%|â–         | 104/6000 [04:55<5:08:39,  3.14s/it]                                                    {'loss': 2.8648, 'grad_norm': 20.402536392211914, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:55<5:08:39,  3.14s/it]  2%|â–         | 105/6000 [04:57<4:58:45,  3.04s/it]                                                    {'loss': 2.988, 'grad_norm': 102.10181427001953, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:57<4:58:45,  3.04s/it]  2%|â–         | 106/6000 [05:00<4:54:04,  2.99s/it]                                                    {'loss': 2.8061, 'grad_norm': 16.501562118530273, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:00<4:54:04,  2.99s/it]  2%|â–         | 107/6000 [05:03<4:45:03,  2.90s/it]                                                    {'loss': 2.8724, 'grad_norm': 24.923730850219727, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:03<4:45:03,  2.90s/it]  2%|â–         | 108/6000 [05:06<4:43:56,  2.89s/it]                                                    {'loss': 2.8206, 'grad_norm': 26.445749282836914, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:06<4:43:56,  2.89s/it]  2%|â–         | 109/6000 [05:09<4:51:19,  2.97s/it]                                                    {'loss': 2.8999, 'grad_norm': 31.36284637451172, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:09<4:51:19,  2.97s/it]  2%|â–         | 110/6000 [05:12<4:43:34,  2.89s/it]                                                    {'loss': 2.929, 'grad_norm': 22.296092987060547, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:12<4:43:34,  2.89s/it]  2%|â–         | 111/6000 [05:15<4:42:43,  2.88s/it]                                                    {'loss': 2.8218, 'grad_norm': 22.443124771118164, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:15<4:42:43,  2.88s/it]  2%|â–         | 112/6000 [05:18<4:54:06,  3.00s/it]                                                    {'loss': 3.1214, 'grad_norm': 58.55652618408203, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:18<4:54:06,  3.00s/it]  2%|â–         | 113/6000 [05:21<4:46:14,  2.92s/it]                                                    {'loss': 2.8768, 'grad_norm': 41.13614273071289, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:21<4:46:14,  2.92s/it]  2%|â–         | 114/6000 [05:23<4:42:07,  2.88s/it]                                                    {'loss': 2.8684, 'grad_norm': 34.36763000488281, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:23<4:42:07,  2.88s/it]  2%|â–         | 115/6000 [05:26<4:37:40,  2.83s/it]                                                    {'loss': 2.8729, 'grad_norm': 18.516260147094727, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:26<4:37:40,  2.83s/it]  2%|â–         | 116/6000 [05:29<4:34:44,  2.80s/it]                                                    {'loss': 2.8907, 'grad_norm': 39.36522674560547, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:29<4:34:44,  2.80s/it]  2%|â–         | 117/6000 [05:32<4:34:41,  2.80s/it]                                                    {'loss': 3.1413, 'grad_norm': 54.82988739013672, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:32<4:34:41,  2.80s/it]  2%|â–         | 118/6000 [05:35<4:50:38,  2.96s/it]                                                    {'loss': 2.8411, 'grad_norm': 26.815185546875, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:35<4:50:38,  2.96s/it]  2%|â–         | 119/6000 [05:38<4:41:57,  2.88s/it]                                                    {'loss': 2.8288, 'grad_norm': 22.392940521240234, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:38<4:41:57,  2.88s/it]  2%|â–         | 120/6000 [05:40<4:37:29,  2.83s/it]                                                    {'loss': 2.8283, 'grad_norm': 19.385456085205078, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:40<4:37:29,  2.83s/it]  2%|â–         | 121/6000 [05:43<4:35:48,  2.81s/it]                                                    {'loss': 2.8485, 'grad_norm': 18.17384147644043, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:43<4:35:48,  2.81s/it]  2%|â–         | 122/6000 [05:46<4:37:12,  2.83s/it]                                                    {'loss': 2.8231, 'grad_norm': 25.086286544799805, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:46<4:37:12,  2.83s/it]  2%|â–         | 123/6000 [05:49<4:33:19,  2.79s/it]                                                    {'loss': 2.9586, 'grad_norm': 39.25376892089844, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:49<4:33:19,  2.79s/it]  2%|â–         | 124/6000 [05:51<4:29:27,  2.75s/it]                                                    {'loss': 2.8479, 'grad_norm': 18.127300262451172, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:51<4:29:27,  2.75s/it]  2%|â–         | 125/6000 [05:54<4:35:32,  2.81s/it]                                                    {'loss': 2.9724, 'grad_norm': 12.874923706054688, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:54<4:35:32,  2.81s/it]  2%|â–         | 126/6000 [05:57<4:36:27,  2.82s/it]                                                    {'loss': 2.8744, 'grad_norm': 46.2256965637207, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:57<4:36:27,  2.82s/it]  2%|â–         | 127/6000 [06:00<4:38:00,  2.84s/it]                                                    {'loss': 2.8566, 'grad_norm': 16.83426284790039, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:00<4:38:00,  2.84s/it]  2%|â–         | 128/6000 [06:03<4:34:17,  2.80s/it]                                                    {'loss': 2.8067, 'grad_norm': 18.05267333984375, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:03<4:34:17,  2.80s/it]  2%|â–         | 129/6000 [06:06<4:32:52,  2.79s/it]                                                    {'loss': 2.9843, 'grad_norm': 27.734277725219727, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:06<4:32:52,  2.79s/it]  2%|â–         | 130/6000 [06:08<4:30:11,  2.76s/it]                                                    {'loss': 2.8761, 'grad_norm': 33.201576232910156, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:08<4:30:11,  2.76s/it]  2%|â–         | 131/6000 [06:11<4:28:21,  2.74s/it]                                                    {'loss': 2.8025, 'grad_norm': 24.285911560058594, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:11<4:28:21,  2.74s/it]  2%|â–         | 132/6000 [06:14<4:25:52,  2.72s/it]                                                    {'loss': 2.7849, 'grad_norm': 20.9757022857666, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:14<4:25:52,  2.72s/it]  2%|â–         | 133/6000 [06:16<4:25:30,  2.72s/it]                                                    {'loss': 2.829, 'grad_norm': 25.226409912109375, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:16<4:25:30,  2.72s/it]  2%|â–         | 134/6000 [06:19<4:30:59,  2.77s/it]                                                    {'loss': 2.824, 'grad_norm': 23.707298278808594, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:19<4:30:59,  2.77s/it]  2%|â–         | 135/6000 [06:22<4:29:11,  2.75s/it]                                                    {'loss': 2.8407, 'grad_norm': 18.471961975097656, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:22<4:29:11,  2.75s/it]  2%|â–         | 136/6000 [06:25<4:27:15,  2.73s/it]                                                    {'loss': 2.8505, 'grad_norm': 23.027727127075195, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:25<4:27:15,  2.73s/it]  2%|â–         | 137/6000 [06:28<4:42:24,  2.89s/it]                                                    {'loss': 2.8449, 'grad_norm': 28.365476608276367, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:28<4:42:24,  2.89s/it]  2%|â–         | 138/6000 [06:30<4:35:14,  2.82s/it]                                                    {'loss': 2.7995, 'grad_norm': 14.752617835998535, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:30<4:35:14,  2.82s/it]  2%|â–         | 139/6000 [06:33<4:35:49,  2.82s/it]                                                    {'loss': 2.8791, 'grad_norm': 15.814602851867676, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:33<4:35:49,  2.82s/it]  2%|â–         | 140/6000 [06:36<4:46:14,  2.93s/it]                                                    {'loss': 2.8439, 'grad_norm': 28.601911544799805, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:37<4:46:14,  2.93s/it]  2%|â–         | 141/6000 [06:39<4:42:04,  2.89s/it]                                                    {'loss': 2.8418, 'grad_norm': 25.203088760375977, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:39<4:42:04,  2.89s/it]  2%|â–         | 142/6000 [06:42<4:35:53,  2.83s/it]                                                    {'loss': 2.8234, 'grad_norm': 22.27461814880371, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:42<4:35:53,  2.83s/it]  2%|â–         | 143/6000 [06:45<4:31:10,  2.78s/it]                                                    {'loss': 2.8294, 'grad_norm': 21.217838287353516, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:45<4:31:10,  2.78s/it]  2%|â–         | 144/6000 [06:47<4:29:03,  2.76s/it]                                                    {'loss': 2.84, 'grad_norm': 24.51971435546875, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:47<4:29:03,  2.76s/it]  2%|â–         | 145/6000 [06:50<4:24:53,  2.71s/it]                                                    {'loss': 2.8429, 'grad_norm': 14.503085136413574, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:50<4:24:53,  2.71s/it]  2%|â–         | 146/6000 [06:53<4:25:36,  2.72s/it]                                                    {'loss': 2.8652, 'grad_norm': 19.24127197265625, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:53<4:25:36,  2.72s/it]  2%|â–         | 147/6000 [06:55<4:23:41,  2.70s/it]                                                    {'loss': 2.8073, 'grad_norm': 14.079499244689941, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:55<4:23:41,  2.70s/it]  2%|â–         | 148/6000 [06:58<4:25:24,  2.72s/it]                                                    {'loss': 2.8288, 'grad_norm': 19.222379684448242, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:58<4:25:24,  2.72s/it]  2%|â–         | 149/6000 [07:01<4:25:18,  2.72s/it]                                                    {'loss': 2.8003, 'grad_norm': 17.42148208618164, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:01<4:25:18,  2.72s/it]  2%|â–Ž         | 150/6000 [07:04<4:24:30,  2.71s/it]                                                    {'loss': 2.8429, 'grad_norm': 16.082609176635742, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:04<4:24:30,  2.71s/it][2025-11-03 16:15:28,369] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-03 16:15:28,381] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 16:15:29,026] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:09<5:35:00,  3.44s/it]                                                    {'loss': 2.8134, 'grad_norm': 17.257658004760742, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:09<5:35:00,  3.44s/it]  3%|â–Ž         | 152/6000 [07:11<5:15:31,  3.24s/it]                                                    {'loss': 2.8721, 'grad_norm': 40.43106460571289, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:11<5:15:31,  3.24s/it]  3%|â–Ž         | 153/6000 [07:14<4:59:18,  3.07s/it]                                                    {'loss': 2.8156, 'grad_norm': 16.683942794799805, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:14<4:59:18,  3.07s/it]  3%|â–Ž         | 154/6000 [07:17<4:51:59,  3.00s/it]                                                    {'loss': 2.8218, 'grad_norm': 20.454425811767578, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:17<4:51:59,  3.00s/it]  3%|â–Ž         | 155/6000 [07:20<4:45:01,  2.93s/it]                                                    {'loss': 2.834, 'grad_norm': 18.274559020996094, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:20<4:45:01,  2.93s/it]  3%|â–Ž         | 156/6000 [07:23<4:41:33,  2.89s/it]                                                    {'loss': 2.8936, 'grad_norm': 16.58502197265625, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:23<4:41:33,  2.89s/it]  3%|â–Ž         | 157/6000 [07:26<4:51:28,  2.99s/it]                                                    {'loss': 2.8049, 'grad_norm': 15.771312713623047, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:26<4:51:28,  2.99s/it]  3%|â–Ž         | 158/6000 [07:28<4:42:53,  2.91s/it]                                                    {'loss': 2.821, 'grad_norm': 15.298596382141113, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:28<4:42:53,  2.91s/it]  3%|â–Ž         | 159/6000 [07:31<4:36:31,  2.84s/it]                                                    {'loss': 2.7927, 'grad_norm': 13.431812286376953, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:31<4:36:31,  2.84s/it]  3%|â–Ž         | 160/6000 [07:34<4:49:59,  2.98s/it]                                                    {'loss': 2.8019, 'grad_norm': 20.497835159301758, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:34<4:49:59,  2.98s/it]  3%|â–Ž         | 161/6000 [07:37<4:48:25,  2.96s/it]                                                    {'loss': 2.801, 'grad_norm': 23.059999465942383, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:37<4:48:25,  2.96s/it]  3%|â–Ž         | 162/6000 [07:40<4:40:40,  2.88s/it]                                                    {'loss': 2.7915, 'grad_norm': 14.584803581237793, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:40<4:40:40,  2.88s/it]  3%|â–Ž         | 163/6000 [07:43<4:52:30,  3.01s/it]                                                    {'loss': 2.8065, 'grad_norm': 18.290124893188477, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:43<4:52:30,  3.01s/it]  3%|â–Ž         | 164/6000 [07:46<4:44:07,  2.92s/it]                                                    {'loss': 2.929, 'grad_norm': 16.879152297973633, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:46<4:44:07,  2.92s/it]  3%|â–Ž         | 165/6000 [07:49<4:41:43,  2.90s/it]                                                    {'loss': 2.8211, 'grad_norm': 21.6744384765625, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:49<4:41:43,  2.90s/it]  3%|â–Ž         | 166/6000 [07:52<4:36:14,  2.84s/it]                                                    {'loss': 2.8193, 'grad_norm': 13.670249938964844, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:52<4:36:14,  2.84s/it]  3%|â–Ž         | 167/6000 [07:54<4:33:54,  2.82s/it]                                                    {'loss': 2.8142, 'grad_norm': 15.115416526794434, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:54<4:33:54,  2.82s/it]  3%|â–Ž         | 168/6000 [07:57<4:33:35,  2.81s/it]                                                    {'loss': 2.9195, 'grad_norm': 24.610313415527344, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:57<4:33:35,  2.81s/it]  3%|â–Ž         | 169/6000 [08:00<4:44:22,  2.93s/it]                                                    {'loss': 2.8351, 'grad_norm': 14.356475830078125, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:00<4:44:22,  2.93s/it]  3%|â–Ž         | 170/6000 [08:03<4:36:57,  2.85s/it]                                                    {'loss': 2.8262, 'grad_norm': 14.43274211883545, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:03<4:36:57,  2.85s/it]  3%|â–Ž         | 171/6000 [08:06<4:31:54,  2.80s/it]                                                    {'loss': 2.8075, 'grad_norm': 18.406005859375, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:06<4:31:54,  2.80s/it]  3%|â–Ž         | 172/6000 [08:09<4:33:09,  2.81s/it]                                                    {'loss': 2.8367, 'grad_norm': 31.59421730041504, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:09<4:33:09,  2.81s/it]  3%|â–Ž         | 173/6000 [08:11<4:31:26,  2.79s/it]                                                    {'loss': 2.8223, 'grad_norm': 17.402481079101562, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:11<4:31:26,  2.79s/it]  3%|â–Ž         | 174/6000 [08:15<4:49:03,  2.98s/it]                                                    {'loss': 2.8159, 'grad_norm': 19.31719398498535, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:15<4:49:03,  2.98s/it]  3%|â–Ž         | 175/6000 [08:18<4:44:05,  2.93s/it]                                                    {'loss': 2.8434, 'grad_norm': 21.432662963867188, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:18<4:44:05,  2.93s/it]  3%|â–Ž         | 176/6000 [08:21<4:47:36,  2.96s/it]                                                    {'loss': 2.8054, 'grad_norm': 18.355022430419922, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:21<4:47:36,  2.96s/it]  3%|â–Ž         | 177/6000 [08:23<4:39:57,  2.88s/it]                                                    {'loss': 2.821, 'grad_norm': 14.654231071472168, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:23<4:39:57,  2.88s/it]  3%|â–Ž         | 178/6000 [08:26<4:36:10,  2.85s/it]                                                    {'loss': 2.8006, 'grad_norm': 18.430526733398438, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:26<4:36:10,  2.85s/it]  3%|â–Ž         | 179/6000 [08:29<4:33:04,  2.81s/it]                                                    {'loss': 2.8076, 'grad_norm': 13.054485321044922, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:29<4:33:04,  2.81s/it]  3%|â–Ž         | 180/6000 [08:32<4:34:14,  2.83s/it]                                                    {'loss': 2.8652, 'grad_norm': 19.889543533325195, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:32<4:34:14,  2.83s/it]  3%|â–Ž         | 181/6000 [08:34<4:32:27,  2.81s/it]                                                    {'loss': 2.8581, 'grad_norm': 19.46210289001465, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:34<4:32:27,  2.81s/it]  3%|â–Ž         | 182/6000 [08:37<4:29:52,  2.78s/it]                                                    {'loss': 2.8482, 'grad_norm': 18.769859313964844, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:37<4:29:52,  2.78s/it]  3%|â–Ž         | 183/6000 [08:40<4:26:08,  2.75s/it]                                                    {'loss': 2.799, 'grad_norm': 12.058523178100586, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:40<4:26:08,  2.75s/it]  3%|â–Ž         | 184/6000 [08:43<4:26:20,  2.75s/it]                                                    {'loss': 2.8547, 'grad_norm': 44.535240173339844, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:43<4:26:20,  2.75s/it]  3%|â–Ž         | 185/6000 [08:45<4:26:08,  2.75s/it]                                                    {'loss': 2.8311, 'grad_norm': 19.0175838470459, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:45<4:26:08,  2.75s/it]  3%|â–Ž         | 186/6000 [08:48<4:25:37,  2.74s/it]                                                    {'loss': 2.8532, 'grad_norm': 18.012432098388672, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:48<4:25:37,  2.74s/it]  3%|â–Ž         | 187/6000 [08:51<4:23:40,  2.72s/it]                                                    {'loss': 2.7901, 'grad_norm': 14.402413368225098, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:51<4:23:40,  2.72s/it]