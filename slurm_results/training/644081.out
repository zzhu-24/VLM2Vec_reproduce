==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailToken-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 4 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 5000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/train.log
W1019 17:12:51.393000 129284929341248 torch/distributed/run.py:779] 
W1019 17:12:51.393000 129284929341248 torch/distributed/run.py:779] *****************************************
W1019 17:12:51.393000 129284929341248 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1019 17:12:51.393000 129284929341248 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-19 17:13:01,513] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.67it/s]
wandb: setting up run gzf6mrs4
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_171301-gzf6mrs4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailToken-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/gzf6mrs4
[2025-10-19 17:13:03,072] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.26it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.16it/s]
[2025-10-19 17:13:03,693] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-19 17:13:03,703] INFO [src.utils:19] Loading lora adapter from TailTokenWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-19 17:13:12,770] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-19 17:13:13,879] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-19 17:13:13,880] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-19 17:13:18,264] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-19 17:13:18,265] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-19 17:13:19,098] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-19 17:13:19,099] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-19 17:13:19,099] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=8
		estimated num step per epoch=26660.875
		interleave_batch_size=0.0
[2025-10-19 17:13:19,101] INFO [src.utils:19] ==================================================
[2025-10-19 17:13:19,101] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-19 17:13:19,102] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:13:19,103] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:13:19,104] INFO [src.utils:19] ==================================================
[2025-10-19 17:13:20,375] INFO [src.utils:19] âœ… Custom optimizer (gme.learnable_token only) enabled
[2025-10-19 17:13:20,831] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:13:20,831] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:13:20,831] INFO [src.trainer:343]   Num examples = 40,000
[2025-10-19 17:13:20,831] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:13:20,831] INFO [src.trainer:345]   Instantaneous batch size per device = 4
[2025-10-19 17:13:20,831] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 8
[2025-10-19 17:13:20,831] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:13:20,831] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:13:20,832] INFO [src.trainer:343]   Num examples = 40,000
[2025-10-19 17:13:20,832] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:13:20,833] INFO [src.trainer:345]   Instantaneous batch size per device = 4
[2025-10-19 17:13:20,833] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 8
[2025-10-19 17:13:20,833] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:13:20,834] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:13:20,843] INFO [src.trainer:351]   Number of trainable parameters = 1,536
[2025-10-19 17:13:20,845] INFO [src.trainer:351]   Number of trainable parameters = 1,536
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1019 17:13:22.429056546 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1019 17:13:22.436230743 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/5000 [00:02<2:49:42,  2.04s/it]                                                  {'loss': 10.2008, 'grad_norm': 6.340056896209717, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 1/5000 [00:02<2:49:42,  2.04s/it]  0%|          | 2/5000 [00:03<2:00:19,  1.44s/it]                                                  {'loss': 8.278, 'grad_norm': 5.270293712615967, 'learning_rate': 2e-05, 'epoch': 0.0}
  0%|          | 2/5000 [00:03<2:00:19,  1.44s/it]  0%|          | 3/5000 [00:04<1:43:07,  1.24s/it]                                                  {'loss': 11.4708, 'grad_norm': 6.842334270477295, 'learning_rate': 3e-05, 'epoch': 0.0}
  0%|          | 3/5000 [00:04<1:43:07,  1.24s/it]  0%|          | 4/5000 [00:05<1:35:06,  1.14s/it]                                                  {'loss': 9.2529, 'grad_norm': 5.925718307495117, 'learning_rate': 4e-05, 'epoch': 0.0}
  0%|          | 4/5000 [00:05<1:35:06,  1.14s/it]  0%|          | 5/5000 [00:06<1:29:39,  1.08s/it]                                                  {'loss': 4.4863, 'grad_norm': 7.478423118591309, 'learning_rate': 5e-05, 'epoch': 0.0}
  0%|          | 5/5000 [00:06<1:29:39,  1.08s/it]  0%|          | 6/5000 [00:07<1:27:34,  1.05s/it]                                                  {'loss': 9.3156, 'grad_norm': 6.994172096252441, 'learning_rate': 6e-05, 'epoch': 0.0}
  0%|          | 6/5000 [00:07<1:27:34,  1.05s/it]  0%|          | 7/5000 [00:08<1:26:00,  1.03s/it]                                                  {'loss': 9.2713, 'grad_norm': 6.375422954559326, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.0}
  0%|          | 7/5000 [00:08<1:26:00,  1.03s/it]  0%|          | 8/5000 [00:09<1:24:49,  1.02s/it]                                                  {'loss': 10.9239, 'grad_norm': 7.341486930847168, 'learning_rate': 8e-05, 'epoch': 0.0}
  0%|          | 8/5000 [00:09<1:24:49,  1.02s/it]  0%|          | 9/5000 [00:09<1:23:54,  1.01s/it]                                                  {'loss': 7.7703, 'grad_norm': 6.284546375274658, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.0}
  0%|          | 9/5000 [00:09<1:23:54,  1.01s/it]  0%|          | 10/5000 [00:11<1:24:47,  1.02s/it]                                                   {'loss': 9.1826, 'grad_norm': 5.666347980499268, 'learning_rate': 0.0001, 'epoch': 0.0}
  0%|          | 10/5000 [00:11<1:24:47,  1.02s/it]  0%|          | 11/5000 [00:12<1:24:23,  1.01s/it]                                                   {'loss': 7.6151, 'grad_norm': 3.949544668197632, 'learning_rate': 0.00011, 'epoch': 0.0}
  0%|          | 11/5000 [00:12<1:24:23,  1.01s/it]  0%|          | 12/5000 [00:13<1:23:45,  1.01s/it]                                                   {'loss': 7.5837, 'grad_norm': 3.8732898235321045, 'learning_rate': 0.00012, 'epoch': 0.0}
  0%|          | 12/5000 [00:13<1:23:45,  1.01s/it]  0%|          | 13/5000 [00:14<1:23:33,  1.01s/it]                                                   {'loss': 9.7752, 'grad_norm': 6.250244617462158, 'learning_rate': 0.00013000000000000002, 'epoch': 0.0}
  0%|          | 13/5000 [00:14<1:23:33,  1.01s/it]  0%|          | 14/5000 [00:15<1:23:26,  1.00s/it]                                                   {'loss': 7.4648, 'grad_norm': 5.115225791931152, 'learning_rate': 0.00014000000000000001, 'epoch': 0.0}
  0%|          | 14/5000 [00:15<1:23:26,  1.00s/it]  0%|          | 15/5000 [00:16<1:23:45,  1.01s/it]                                                   {'loss': 9.5021, 'grad_norm': 6.986490726470947, 'learning_rate': 0.00015, 'epoch': 0.0}
  0%|          | 15/5000 [00:16<1:23:45,  1.01s/it]  0%|          | 16/5000 [00:17<1:23:31,  1.01s/it]                                                   {'loss': 5.69, 'grad_norm': 3.782611608505249, 'learning_rate': 0.00016, 'epoch': 0.0}
  0%|          | 16/5000 [00:17<1:23:31,  1.01s/it]  0%|          | 17/5000 [00:18<1:22:53,  1.00it/s]                                                   {'loss': 8.7475, 'grad_norm': 5.995516777038574, 'learning_rate': 0.00017, 'epoch': 0.0}
  0%|          | 17/5000 [00:18<1:22:53,  1.00it/s]  0%|          | 18/5000 [00:19<1:22:32,  1.01it/s]                                                   {'loss': 7.777, 'grad_norm': 5.776442527770996, 'learning_rate': 0.00017999999999999998, 'epoch': 0.0}
  0%|          | 18/5000 [00:19<1:22:32,  1.01it/s]  0%|          | 19/5000 [00:20<1:23:15,  1.00s/it]                                                   {'loss': 8.8561, 'grad_norm': 8.65799331665039, 'learning_rate': 0.00019, 'epoch': 0.0}
  0%|          | 19/5000 [00:20<1:23:15,  1.00s/it]  0%|          | 20/5000 [00:21<1:22:50,  1.00it/s]                                                   {'loss': 10.9292, 'grad_norm': 6.2083821296691895, 'learning_rate': 0.0002, 'epoch': 0.0}
  0%|          | 20/5000 [00:21<1:22:50,  1.00it/s]  0%|          | 21/5000 [00:22<1:22:48,  1.00it/s]                                                   {'loss': 8.3867, 'grad_norm': 5.911558151245117, 'learning_rate': 0.00021, 'epoch': 0.0}
  0%|          | 21/5000 [00:22<1:22:48,  1.00it/s]  0%|          | 22/5000 [00:23<1:22:52,  1.00it/s]                                                   {'loss': 7.274, 'grad_norm': 4.5937676429748535, 'learning_rate': 0.00022, 'epoch': 0.0}
  0%|          | 22/5000 [00:23<1:22:52,  1.00it/s]  0%|          | 23/5000 [00:23<1:22:20,  1.01it/s]                                                   {'loss': 12.5161, 'grad_norm': 5.729051113128662, 'learning_rate': 0.00023, 'epoch': 0.0}
  0%|          | 23/5000 [00:24<1:22:20,  1.01it/s]  0%|          | 24/5000 [00:24<1:22:24,  1.01it/s]                                                   {'loss': 8.3533, 'grad_norm': 5.854227066040039, 'learning_rate': 0.00024, 'epoch': 0.0}
  0%|          | 24/5000 [00:25<1:22:24,  1.01it/s]  0%|          | 25/5000 [00:26<1:22:55,  1.00s/it]                                                   {'loss': 8.8493, 'grad_norm': 4.858150959014893, 'learning_rate': 0.00025, 'epoch': 0.01}
  0%|          | 25/5000 [00:26<1:22:55,  1.00s/it]  1%|          | 26/5000 [00:27<1:23:41,  1.01s/it]                                                   {'loss': 9.5767, 'grad_norm': 6.445367336273193, 'learning_rate': 0.00026000000000000003, 'epoch': 0.01}
  1%|          | 26/5000 [00:27<1:23:41,  1.01s/it]  1%|          | 27/5000 [00:28<1:24:02,  1.01s/it]                                                   {'loss': 12.176, 'grad_norm': 6.314594745635986, 'learning_rate': 0.00027, 'epoch': 0.01}
  1%|          | 27/5000 [00:28<1:24:02,  1.01s/it]  1%|          | 28/5000 [00:29<1:23:49,  1.01s/it]                                                   {'loss': 8.1703, 'grad_norm': 6.851759910583496, 'learning_rate': 0.00028000000000000003, 'epoch': 0.01}
  1%|          | 28/5000 [00:29<1:23:49,  1.01s/it]  1%|          | 29/5000 [00:30<1:24:00,  1.01s/it]                                                   {'loss': 6.7779, 'grad_norm': 5.623631000518799, 'learning_rate': 0.00029, 'epoch': 0.01}
  1%|          | 29/5000 [00:30<1:24:00,  1.01s/it]  1%|          | 30/5000 [00:31<1:23:32,  1.01s/it]                                                   {'loss': 11.0325, 'grad_norm': 8.416708946228027, 'learning_rate': 0.0003, 'epoch': 0.01}
  1%|          | 30/5000 [00:31<1:23:32,  1.01s/it]  1%|          | 31/5000 [00:32<1:23:44,  1.01s/it]                                                   {'loss': 11.7182, 'grad_norm': 6.773676872253418, 'learning_rate': 0.00031, 'epoch': 0.01}
  1%|          | 31/5000 [00:32<1:23:44,  1.01s/it]  1%|          | 32/5000 [00:33<1:24:01,  1.01s/it]                                                   {'loss': 6.964, 'grad_norm': 5.6243414878845215, 'learning_rate': 0.00032, 'epoch': 0.01}
  1%|          | 32/5000 [00:33<1:24:01,  1.01s/it]  1%|          | 33/5000 [00:34<1:24:47,  1.02s/it]                                                   {'loss': 6.88, 'grad_norm': 3.780224323272705, 'learning_rate': 0.00033, 'epoch': 0.01}
  1%|          | 33/5000 [00:34<1:24:47,  1.02s/it]  1%|          | 34/5000 [00:35<1:24:24,  1.02s/it]                                                   {'loss': 3.6928, 'grad_norm': 3.680914878845215, 'learning_rate': 0.00034, 'epoch': 0.01}
  1%|          | 34/5000 [00:35<1:24:24,  1.02s/it]  1%|          | 35/5000 [00:36<1:26:29,  1.05s/it]                                                   {'loss': 9.9107, 'grad_norm': 4.884860038757324, 'learning_rate': 0.00035, 'epoch': 0.01}
  1%|          | 35/5000 [00:36<1:26:29,  1.05s/it]  1%|          | 36/5000 [00:37<1:25:51,  1.04s/it]                                                   {'loss': 4.3329, 'grad_norm': 1.5295741558074951, 'learning_rate': 0.00035999999999999997, 'epoch': 0.01}
  1%|          | 36/5000 [00:37<1:25:51,  1.04s/it]  1%|          | 37/5000 [00:38<1:24:26,  1.02s/it]                                                   {'loss': 10.4984, 'grad_norm': 6.455719947814941, 'learning_rate': 0.00037, 'epoch': 0.01}
  1%|          | 37/5000 [00:38<1:24:26,  1.02s/it]  1%|          | 38/5000 [00:39<1:23:13,  1.01s/it]                                                   {'loss': 13.2975, 'grad_norm': 7.301984786987305, 'learning_rate': 0.00038, 'epoch': 0.01}
  1%|          | 38/5000 [00:39<1:23:13,  1.01s/it]  1%|          | 39/5000 [00:40<1:23:00,  1.00s/it]                                                   {'loss': 5.2513, 'grad_norm': 3.8889570236206055, 'learning_rate': 0.00039000000000000005, 'epoch': 0.01}
  1%|          | 39/5000 [00:40<1:23:00,  1.00s/it]  1%|          | 40/5000 [00:41<1:22:44,  1.00s/it]                                                   {'loss': 5.8892, 'grad_norm': 3.318901538848877, 'learning_rate': 0.0004, 'epoch': 0.01}
  1%|          | 40/5000 [00:41<1:22:44,  1.00s/it]  1%|          | 41/5000 [00:42<1:22:41,  1.00s/it]                                                   {'loss': 9.8696, 'grad_norm': 5.101047992706299, 'learning_rate': 0.00041, 'epoch': 0.01}
  1%|          | 41/5000 [00:42<1:22:41,  1.00s/it]  1%|          | 42/5000 [00:43<1:23:16,  1.01s/it]                                                   {'loss': 10.419, 'grad_norm': 6.964803695678711, 'learning_rate': 0.00042, 'epoch': 0.01}
  1%|          | 42/5000 [00:43<1:23:16,  1.01s/it]  1%|          | 43/5000 [00:44<1:23:22,  1.01s/it]                                                   {'loss': 10.813, 'grad_norm': 7.066532135009766, 'learning_rate': 0.00043, 'epoch': 0.01}
  1%|          | 43/5000 [00:44<1:23:22,  1.01s/it]  1%|          | 44/5000 [00:45<1:23:14,  1.01s/it]                                                   {'loss': 9.5174, 'grad_norm': 6.060997009277344, 'learning_rate': 0.00044, 'epoch': 0.01}
  1%|          | 44/5000 [00:45<1:23:14,  1.01s/it]  1%|          | 45/5000 [00:46<1:23:23,  1.01s/it]                                                   {'loss': 10.1661, 'grad_norm': 5.544106960296631, 'learning_rate': 0.00045000000000000004, 'epoch': 0.01}
  1%|          | 45/5000 [00:46<1:23:23,  1.01s/it]  1%|          | 46/5000 [00:47<1:23:58,  1.02s/it]                                                   {'loss': 8.0327, 'grad_norm': 5.896103382110596, 'learning_rate': 0.00046, 'epoch': 0.01}
  1%|          | 46/5000 [00:47<1:23:58,  1.02s/it]  1%|          | 47/5000 [00:48<1:24:00,  1.02s/it]                                                   {'loss': 5.6315, 'grad_norm': 3.3996710777282715, 'learning_rate': 0.00047, 'epoch': 0.01}
  1%|          | 47/5000 [00:48<1:24:00,  1.02s/it]  1%|          | 48/5000 [00:49<1:23:24,  1.01s/it]                                                   {'loss': 8.5759, 'grad_norm': 6.429058074951172, 'learning_rate': 0.00048, 'epoch': 0.01}
  1%|          | 48/5000 [00:49<1:23:24,  1.01s/it]  1%|          | 49/5000 [00:50<1:23:11,  1.01s/it]                                                   {'loss': 11.3796, 'grad_norm': 7.100161552429199, 'learning_rate': 0.00049, 'epoch': 0.01}
  1%|          | 49/5000 [00:50<1:23:11,  1.01s/it]  1%|          | 50/5000 [00:51<1:23:32,  1.01s/it]                                                   {'loss': 6.7087, 'grad_norm': 5.522042274475098, 'learning_rate': 0.0005, 'epoch': 0.01}
  1%|          | 50/5000 [00:51<1:23:32,  1.01s/it][2025-10-19 17:14:12,440] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/5000 [00:53<1:55:08,  1.40s/it]                                                   {'loss': 8.7462, 'grad_norm': 4.315682888031006, 'learning_rate': 0.00051, 'epoch': 0.01}
  1%|          | 51/5000 [00:53<1:55:08,  1.40s/it]  1%|          | 52/5000 [00:54<1:45:13,  1.28s/it]                                                   {'loss': 8.6911, 'grad_norm': 5.56367826461792, 'learning_rate': 0.0005200000000000001, 'epoch': 0.01}
  1%|          | 52/5000 [00:54<1:45:13,  1.28s/it]  1%|          | 53/5000 [00:55<1:38:35,  1.20s/it]                                                   {'loss': 6.1369, 'grad_norm': 3.154996395111084, 'learning_rate': 0.0005300000000000001, 'epoch': 0.01}
  1%|          | 53/5000 [00:55<1:38:35,  1.20s/it]  1%|          | 54/5000 [00:56<1:33:43,  1.14s/it]                                                   {'loss': 9.5383, 'grad_norm': 5.897871494293213, 'learning_rate': 0.00054, 'epoch': 0.01}
  1%|          | 54/5000 [00:56<1:33:43,  1.14s/it]  1%|          | 55/5000 [00:57<1:30:50,  1.10s/it]                                                   {'loss': 8.9842, 'grad_norm': 5.20894718170166, 'learning_rate': 0.00055, 'epoch': 0.01}
  1%|          | 55/5000 [00:57<1:30:50,  1.10s/it]  1%|          | 56/5000 [00:58<1:28:13,  1.07s/it]                                                   {'loss': 8.1384, 'grad_norm': 4.180531978607178, 'learning_rate': 0.0005600000000000001, 'epoch': 0.01}
  1%|          | 56/5000 [00:58<1:28:13,  1.07s/it]  1%|          | 57/5000 [00:59<1:26:36,  1.05s/it]                                                   {'loss': 6.647, 'grad_norm': 4.342245578765869, 'learning_rate': 0.00057, 'epoch': 0.01}
  1%|          | 57/5000 [00:59<1:26:36,  1.05s/it]  1%|          | 58/5000 [01:00<1:24:59,  1.03s/it]                                                   {'loss': 8.3299, 'grad_norm': 4.782317161560059, 'learning_rate': 0.00058, 'epoch': 0.01}
  1%|          | 58/5000 [01:00<1:24:59,  1.03s/it]  1%|          | 59/5000 [01:01<1:24:05,  1.02s/it]                                                   {'loss': 8.6242, 'grad_norm': 5.952569484710693, 'learning_rate': 0.00059, 'epoch': 0.01}
  1%|          | 59/5000 [01:01<1:24:05,  1.02s/it]  1%|          | 60/5000 [01:02<1:23:42,  1.02s/it]                                                   {'loss': 5.1356, 'grad_norm': 3.2385711669921875, 'learning_rate': 0.0006, 'epoch': 0.01}
  1%|          | 60/5000 [01:02<1:23:42,  1.02s/it]  1%|          | 61/5000 [01:03<1:23:03,  1.01s/it]                                                   {'loss': 3.7868, 'grad_norm': 4.154112339019775, 'learning_rate': 0.00061, 'epoch': 0.01}
  1%|          | 61/5000 [01:03<1:23:03,  1.01s/it]  1%|          | 62/5000 [01:04<1:22:57,  1.01s/it]                                                   {'loss': 9.9189, 'grad_norm': 6.111857891082764, 'learning_rate': 0.00062, 'epoch': 0.01}
  1%|          | 62/5000 [01:04<1:22:57,  1.01s/it]  1%|â–         | 63/5000 [01:05<1:22:16,  1.00it/s]                                                   {'loss': 8.882, 'grad_norm': 4.589949607849121, 'learning_rate': 0.00063, 'epoch': 0.01}
  1%|â–         | 63/5000 [01:05<1:22:16,  1.00it/s]  1%|â–         | 64/5000 [01:06<1:21:41,  1.01it/s]                                                   {'loss': 7.4422, 'grad_norm': 5.129567623138428, 'learning_rate': 0.00064, 'epoch': 0.01}
  1%|â–         | 64/5000 [01:06<1:21:41,  1.01it/s]  1%|â–         | 65/5000 [01:07<1:22:27,  1.00s/it]                                                   {'loss': 5.6584, 'grad_norm': 5.6890459060668945, 'learning_rate': 0.0006500000000000001, 'epoch': 0.01}
  1%|â–         | 65/5000 [01:07<1:22:27,  1.00s/it]  1%|â–         | 66/5000 [01:08<1:22:17,  1.00s/it]                                                   {'loss': 7.0456, 'grad_norm': 3.636080503463745, 'learning_rate': 0.00066, 'epoch': 0.01}
  1%|â–         | 66/5000 [01:08<1:22:17,  1.00s/it]  1%|â–         | 67/5000 [01:09<1:22:26,  1.00s/it]                                                   {'loss': 8.2678, 'grad_norm': 4.299968242645264, 'learning_rate': 0.00067, 'epoch': 0.01}
  1%|â–         | 67/5000 [01:09<1:22:26,  1.00s/it]  1%|â–         | 68/5000 [01:10<1:21:51,  1.00it/s]                                                   {'loss': 8.6634, 'grad_norm': 5.358226776123047, 'learning_rate': 0.00068, 'epoch': 0.01}
  1%|â–         | 68/5000 [01:10<1:21:51,  1.00it/s]  1%|â–         | 69/5000 [01:11<1:22:18,  1.00s/it]                                                   {'loss': 3.2336, 'grad_norm': 0.972138524055481, 'learning_rate': 0.00069, 'epoch': 0.01}
  1%|â–         | 69/5000 [01:11<1:22:18,  1.00s/it]  1%|â–         | 70/5000 [01:12<1:22:07,  1.00it/s]                                                   {'loss': 4.2632, 'grad_norm': 2.7359559535980225, 'learning_rate': 0.0007, 'epoch': 0.01}
  1%|â–         | 70/5000 [01:12<1:22:07,  1.00it/s]  1%|â–         | 71/5000 [01:13<1:22:42,  1.01s/it]                                                   {'loss': 7.5814, 'grad_norm': 6.6132121086120605, 'learning_rate': 0.00071, 'epoch': 0.01}
  1%|â–         | 71/5000 [01:13<1:22:42,  1.01s/it]  1%|â–         | 72/5000 [01:14<1:22:31,  1.00s/it]                                                   {'loss': 7.0177, 'grad_norm': 3.984315872192383, 'learning_rate': 0.0007199999999999999, 'epoch': 0.01}
  1%|â–         | 72/5000 [01:14<1:22:31,  1.00s/it]  1%|â–         | 73/5000 [01:15<1:22:23,  1.00s/it]                                                   {'loss': 7.0374, 'grad_norm': 4.027950763702393, 'learning_rate': 0.00073, 'epoch': 0.01}
  1%|â–         | 73/5000 [01:15<1:22:23,  1.00s/it]  1%|â–         | 74/5000 [01:16<1:21:50,  1.00it/s]                                                   {'loss': 5.8115, 'grad_norm': 5.349096298217773, 'learning_rate': 0.00074, 'epoch': 0.01}
  1%|â–         | 74/5000 [01:16<1:21:50,  1.00it/s]  2%|â–         | 75/5000 [01:17<1:22:33,  1.01s/it]                                                   {'loss': 7.2276, 'grad_norm': 4.881514549255371, 'learning_rate': 0.00075, 'epoch': 0.01}
  2%|â–         | 75/5000 [01:17<1:22:33,  1.01s/it]  2%|â–         | 76/5000 [01:18<1:22:24,  1.00s/it]                                                   {'loss': 9.429, 'grad_norm': 5.149118423461914, 'learning_rate': 0.00076, 'epoch': 0.02}
  2%|â–         | 76/5000 [01:18<1:22:24,  1.00s/it]  2%|â–         | 77/5000 [01:19<1:22:08,  1.00s/it]                                                   {'loss': 7.8728, 'grad_norm': 5.433289051055908, 'learning_rate': 0.0007700000000000001, 'epoch': 0.02}
  2%|â–         | 77/5000 [01:19<1:22:08,  1.00s/it]  2%|â–         | 78/5000 [01:20<1:22:17,  1.00s/it]                                                   {'loss': 5.8728, 'grad_norm': 3.802997589111328, 'learning_rate': 0.0007800000000000001, 'epoch': 0.02}
  2%|â–         | 78/5000 [01:20<1:22:17,  1.00s/it]  2%|â–         | 79/5000 [01:21<1:22:12,  1.00s/it]                                                   {'loss': 6.55, 'grad_norm': 3.5818874835968018, 'learning_rate': 0.00079, 'epoch': 0.02}
  2%|â–         | 79/5000 [01:21<1:22:12,  1.00s/it]  2%|â–         | 80/5000 [01:22<1:22:31,  1.01s/it]                                                   {'loss': 6.6002, 'grad_norm': 4.212174892425537, 'learning_rate': 0.0008, 'epoch': 0.02}
  2%|â–         | 80/5000 [01:22<1:22:31,  1.01s/it]  2%|â–         | 81/5000 [01:23<1:23:02,  1.01s/it]                                                   {'loss': 8.7439, 'grad_norm': 6.732834815979004, 'learning_rate': 0.0008100000000000001, 'epoch': 0.02}
  2%|â–         | 81/5000 [01:23<1:23:02,  1.01s/it]  2%|â–         | 82/5000 [01:24<1:22:40,  1.01s/it]                                                   {'loss': 5.398, 'grad_norm': 3.170728921890259, 'learning_rate': 0.00082, 'epoch': 0.02}
  2%|â–         | 82/5000 [01:24<1:22:40,  1.01s/it]  2%|â–         | 83/5000 [01:25<1:22:02,  1.00s/it]                                                   {'loss': 4.7898, 'grad_norm': 3.9448890686035156, 'learning_rate': 0.00083, 'epoch': 0.02}
  2%|â–         | 83/5000 [01:25<1:22:02,  1.00s/it]  2%|â–         | 84/5000 [01:26<1:22:35,  1.01s/it]                                                   {'loss': 6.192, 'grad_norm': 3.403620481491089, 'learning_rate': 0.00084, 'epoch': 0.02}
  2%|â–         | 84/5000 [01:26<1:22:35,  1.01s/it]  2%|â–         | 85/5000 [01:27<1:22:23,  1.01s/it]                                                   {'loss': 6.1793, 'grad_norm': 2.687523365020752, 'learning_rate': 0.00085, 'epoch': 0.02}
  2%|â–         | 85/5000 [01:27<1:22:23,  1.01s/it]  2%|â–         | 86/5000 [01:28<1:21:59,  1.00s/it]                                                   {'loss': 4.2676, 'grad_norm': 1.8815958499908447, 'learning_rate': 0.00086, 'epoch': 0.02}
  2%|â–         | 86/5000 [01:28<1:21:59,  1.00s/it]  2%|â–         | 87/5000 [01:29<1:22:17,  1.00s/it]                                                   {'loss': 7.8393, 'grad_norm': 3.8473517894744873, 'learning_rate': 0.00087, 'epoch': 0.02}
  2%|â–         | 87/5000 [01:29<1:22:17,  1.00s/it]  2%|â–         | 88/5000 [01:30<1:22:08,  1.00s/it]                                                   {'loss': 7.3374, 'grad_norm': 3.0021071434020996, 'learning_rate': 0.00088, 'epoch': 0.02}
  2%|â–         | 88/5000 [01:30<1:22:08,  1.00s/it]  2%|â–         | 89/5000 [01:31<1:22:12,  1.00s/it]                                                   {'loss': 6.1366, 'grad_norm': 4.0509819984436035, 'learning_rate': 0.0008900000000000001, 'epoch': 0.02}
  2%|â–         | 89/5000 [01:31<1:22:12,  1.00s/it]  2%|â–         | 90/5000 [01:32<1:21:38,  1.00it/s]                                                   {'loss': 7.8547, 'grad_norm': 4.46264123916626, 'learning_rate': 0.0009000000000000001, 'epoch': 0.02}
  2%|â–         | 90/5000 [01:32<1:21:38,  1.00it/s]  2%|â–         | 91/5000 [01:33<1:21:49,  1.00s/it]                                                   {'loss': 4.0545, 'grad_norm': 1.6988099813461304, 'learning_rate': 0.00091, 'epoch': 0.02}
  2%|â–         | 91/5000 [01:33<1:21:49,  1.00s/it]  2%|â–         | 92/5000 [01:34<1:21:41,  1.00it/s]                                                   {'loss': 3.6742, 'grad_norm': 1.7613177299499512, 'learning_rate': 0.00092, 'epoch': 0.02}
  2%|â–         | 92/5000 [01:34<1:21:41,  1.00it/s]  2%|â–         | 93/5000 [01:35<1:21:54,  1.00s/it]                                                   {'loss': 2.9356, 'grad_norm': 2.1347897052764893, 'learning_rate': 0.00093, 'epoch': 0.02}
  2%|â–         | 93/5000 [01:35<1:21:54,  1.00s/it]  2%|â–         | 94/5000 [01:36<1:21:41,  1.00it/s]                                                   {'loss': 5.2354, 'grad_norm': 2.20871639251709, 'learning_rate': 0.00094, 'epoch': 0.02}
  2%|â–         | 94/5000 [01:36<1:21:41,  1.00it/s]  2%|â–         | 95/5000 [01:37<1:22:21,  1.01s/it]                                                   {'loss': 3.1652, 'grad_norm': 2.042405843734741, 'learning_rate': 0.00095, 'epoch': 0.02}
  2%|â–         | 95/5000 [01:37<1:22:21,  1.01s/it]  2%|â–         | 96/5000 [01:38<1:21:21,  1.00it/s]                                                   {'loss': 3.9095, 'grad_norm': 1.776888370513916, 'learning_rate': 0.00096, 'epoch': 0.02}
  2%|â–         | 96/5000 [01:38<1:21:21,  1.00it/s]  2%|â–         | 97/5000 [01:39<1:21:01,  1.01it/s]                                                   {'loss': 5.3613, 'grad_norm': 3.6758947372436523, 'learning_rate': 0.0009699999999999999, 'epoch': 0.02}
  2%|â–         | 97/5000 [01:39<1:21:01,  1.01it/s]  2%|â–         | 98/5000 [01:40<1:21:48,  1.00s/it]                                                   {'loss': 5.5793, 'grad_norm': 3.792192220687866, 'learning_rate': 0.00098, 'epoch': 0.02}
  2%|â–         | 98/5000 [01:40<1:21:48,  1.00s/it]  2%|â–         | 99/5000 [01:41<1:22:14,  1.01s/it]                                                   {'loss': 6.4341, 'grad_norm': 3.2512335777282715, 'learning_rate': 0.00099, 'epoch': 0.02}
  2%|â–         | 99/5000 [01:41<1:22:14,  1.01s/it]  2%|â–         | 100/5000 [01:42<1:21:36,  1.00it/s]                                                    {'loss': 2.9164, 'grad_norm': 2.1434926986694336, 'learning_rate': 0.001, 'epoch': 0.02}
  2%|â–         | 100/5000 [01:42<1:21:36,  1.00it/s][2025-10-19 17:15:03,804] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/5000 [01:44<1:50:38,  1.36s/it]                                                    {'loss': 5.5436, 'grad_norm': 2.73100209236145, 'learning_rate': 0.000999795918367347, 'epoch': 0.02}
  2%|â–         | 101/5000 [01:44<1:50:38,  1.36s/it]  2%|â–         | 102/5000 [01:45<1:43:28,  1.27s/it]                                                    {'loss': 5.5431, 'grad_norm': 3.983954906463623, 'learning_rate': 0.0009995918367346939, 'epoch': 0.02}
  2%|â–         | 102/5000 [01:46<1:43:28,  1.27s/it]  2%|â–         | 103/5000 [01:46<1:36:24,  1.18s/it]                                                    {'loss': 6.3331, 'grad_norm': 3.6160242557525635, 'learning_rate': 0.0009993877551020408, 'epoch': 0.02}
  2%|â–         | 103/5000 [01:46<1:36:24,  1.18s/it]  2%|â–         | 104/5000 [01:47<1:32:25,  1.13s/it]                                                    {'loss': 5.4736, 'grad_norm': 3.2733867168426514, 'learning_rate': 0.0009991836734693877, 'epoch': 0.02}
  2%|â–         | 104/5000 [01:48<1:32:25,  1.13s/it]  2%|â–         | 105/5000 [01:48<1:28:58,  1.09s/it]                                                    {'loss': 5.729, 'grad_norm': 4.170352935791016, 'learning_rate': 0.0009989795918367347, 'epoch': 0.02}
  2%|â–         | 105/5000 [01:48<1:28:58,  1.09s/it]  2%|â–         | 106/5000 [01:50<1:27:25,  1.07s/it]                                                    {'loss': 5.8184, 'grad_norm': 3.550410032272339, 'learning_rate': 0.0009987755102040816, 'epoch': 0.02}
  2%|â–         | 106/5000 [01:50<1:27:25,  1.07s/it]  2%|â–         | 107/5000 [01:51<1:26:54,  1.07s/it]                                                    {'loss': 5.2092, 'grad_norm': 2.3588945865631104, 'learning_rate': 0.0009985714285714285, 'epoch': 0.02}
  2%|â–         | 107/5000 [01:51<1:26:54,  1.07s/it]  2%|â–         | 108/5000 [01:52<1:25:07,  1.04s/it]                                                    {'loss': 5.0394, 'grad_norm': 4.752532005310059, 'learning_rate': 0.0009983673469387755, 'epoch': 0.02}
  2%|â–         | 108/5000 [01:52<1:25:07,  1.04s/it]  2%|â–         | 109/5000 [01:53<1:27:42,  1.08s/it]                                                    {'loss': 5.4366, 'grad_norm': 4.250602722167969, 'learning_rate': 0.0009981632653061224, 'epoch': 0.02}
  2%|â–         | 109/5000 [01:53<1:27:42,  1.08s/it]  2%|â–         | 110/5000 [01:54<1:26:49,  1.07s/it]                                                    {'loss': 4.2927, 'grad_norm': 3.500828266143799, 'learning_rate': 0.0009979591836734693, 'epoch': 0.02}
  2%|â–         | 110/5000 [01:54<1:26:49,  1.07s/it]  2%|â–         | 111/5000 [01:55<1:25:20,  1.05s/it]                                                    {'loss': 3.7004, 'grad_norm': 2.354083299636841, 'learning_rate': 0.0009977551020408162, 'epoch': 0.02}
  2%|â–         | 111/5000 [01:55<1:25:20,  1.05s/it]  2%|â–         | 112/5000 [01:56<1:24:22,  1.04s/it]                                                    {'loss': 3.9018, 'grad_norm': 3.0912671089172363, 'learning_rate': 0.0009975510204081632, 'epoch': 0.02}
  2%|â–         | 112/5000 [01:56<1:24:22,  1.04s/it]  2%|â–         | 113/5000 [01:57<1:23:27,  1.02s/it]                                                    {'loss': 2.7159, 'grad_norm': 0.9718952178955078, 'learning_rate': 0.00099734693877551, 'epoch': 0.02}
  2%|â–         | 113/5000 [01:57<1:23:27,  1.02s/it]  2%|â–         | 114/5000 [01:58<1:22:55,  1.02s/it]                                                    {'loss': 3.3199, 'grad_norm': 3.403517961502075, 'learning_rate': 0.000997142857142857, 'epoch': 0.02}
  2%|â–         | 114/5000 [01:58<1:22:55,  1.02s/it]  2%|â–         | 115/5000 [01:59<1:22:07,  1.01s/it]                                                    {'loss': 4.2245, 'grad_norm': 3.4084701538085938, 'learning_rate': 0.000996938775510204, 'epoch': 0.02}
  2%|â–         | 115/5000 [01:59<1:22:07,  1.01s/it]  2%|â–         | 116/5000 [02:00<1:21:53,  1.01s/it]                                                    {'loss': 4.9035, 'grad_norm': 3.546618700027466, 'learning_rate': 0.000996734693877551, 'epoch': 0.02}
  2%|â–         | 116/5000 [02:00<1:21:53,  1.01s/it]  2%|â–         | 117/5000 [02:01<1:21:52,  1.01s/it]                                                    {'loss': 3.4538, 'grad_norm': 3.4507060050964355, 'learning_rate': 0.000996530612244898, 'epoch': 0.02}
  2%|â–         | 117/5000 [02:01<1:21:52,  1.01s/it]  2%|â–         | 118/5000 [02:02<1:22:52,  1.02s/it]                                                    {'loss': 3.4048, 'grad_norm': 2.5699355602264404, 'learning_rate': 0.000996326530612245, 'epoch': 0.02}
  2%|â–         | 118/5000 [02:02<1:22:52,  1.02s/it]  2%|â–         | 119/5000 [02:03<1:22:12,  1.01s/it]                                                    {'loss': 5.0874, 'grad_norm': 3.7487142086029053, 'learning_rate': 0.000996122448979592, 'epoch': 0.02}
  2%|â–         | 119/5000 [02:03<1:22:12,  1.01s/it]  2%|â–         | 120/5000 [02:04<1:21:25,  1.00s/it]                                                    {'loss': 3.7699, 'grad_norm': 4.572410583496094, 'learning_rate': 0.0009959183673469388, 'epoch': 0.02}
  2%|â–         | 120/5000 [02:04<1:21:25,  1.00s/it]  2%|â–         | 121/5000 [02:05<1:21:55,  1.01s/it]                                                    {'loss': 3.2563, 'grad_norm': 2.8586363792419434, 'learning_rate': 0.0009957142857142858, 'epoch': 0.02}
  2%|â–         | 121/5000 [02:05<1:21:55,  1.01s/it]  2%|â–         | 122/5000 [02:06<1:21:45,  1.01s/it]                                                    {'loss': 3.3178, 'grad_norm': 3.5830349922180176, 'learning_rate': 0.0009955102040816327, 'epoch': 0.02}
  2%|â–         | 122/5000 [02:06<1:21:45,  1.01s/it]  2%|â–         | 123/5000 [02:07<1:21:46,  1.01s/it]                                                    {'loss': 2.3441, 'grad_norm': 3.096625328063965, 'learning_rate': 0.0009953061224489796, 'epoch': 0.02}
  2%|â–         | 123/5000 [02:07<1:21:46,  1.01s/it]  2%|â–         | 124/5000 [02:08<1:22:26,  1.01s/it]                                                    {'loss': 3.7163, 'grad_norm': 4.2410569190979, 'learning_rate': 0.0009951020408163265, 'epoch': 0.02}
  2%|â–         | 124/5000 [02:08<1:22:26,  1.01s/it]  2%|â–Ž         | 125/5000 [02:09<1:22:25,  1.01s/it]                                                    {'loss': 4.0439, 'grad_norm': 8.712881088256836, 'learning_rate': 0.0009948979591836735, 'epoch': 0.03}
  2%|â–Ž         | 125/5000 [02:09<1:22:25,  1.01s/it]  3%|â–Ž         | 126/5000 [02:10<1:22:06,  1.01s/it]                                                    {'loss': 2.4872, 'grad_norm': 1.921040654182434, 'learning_rate': 0.0009946938775510204, 'epoch': 0.03}
  3%|â–Ž         | 126/5000 [02:10<1:22:06,  1.01s/it]  3%|â–Ž         | 127/5000 [02:11<1:21:59,  1.01s/it]                                                    {'loss': 3.7064, 'grad_norm': 3.1583590507507324, 'learning_rate': 0.0009944897959183673, 'epoch': 0.03}
  3%|â–Ž         | 127/5000 [02:11<1:21:59,  1.01s/it]  3%|â–Ž         | 128/5000 [02:12<1:21:36,  1.01s/it]                                                    {'loss': 4.4277, 'grad_norm': 4.270075798034668, 'learning_rate': 0.0009942857142857143, 'epoch': 0.03}
  3%|â–Ž         | 128/5000 [02:12<1:21:36,  1.01s/it]  3%|â–Ž         | 129/5000 [02:13<1:21:24,  1.00s/it]                                                    {'loss': 3.4483, 'grad_norm': 3.9853739738464355, 'learning_rate': 0.0009940816326530612, 'epoch': 0.03}
  3%|â–Ž         | 129/5000 [02:13<1:21:24,  1.00s/it]  3%|â–Ž         | 130/5000 [02:14<1:21:37,  1.01s/it]                                                    {'loss': 2.8823, 'grad_norm': 2.797023296356201, 'learning_rate': 0.0009938775510204081, 'epoch': 0.03}
  3%|â–Ž         | 130/5000 [02:14<1:21:37,  1.01s/it]  3%|â–Ž         | 131/5000 [02:15<1:21:39,  1.01s/it]                                                    {'loss': 3.6695, 'grad_norm': 4.074885368347168, 'learning_rate': 0.0009936734693877553, 'epoch': 0.03}
  3%|â–Ž         | 131/5000 [02:15<1:21:39,  1.01s/it]  3%|â–Ž         | 132/5000 [02:16<1:21:01,  1.00it/s]                                                    {'loss': 2.6281, 'grad_norm': 3.283129930496216, 'learning_rate': 0.0009934693877551022, 'epoch': 0.03}
  3%|â–Ž         | 132/5000 [02:16<1:21:01,  1.00it/s]  3%|â–Ž         | 133/5000 [02:17<1:21:29,  1.00s/it]                                                    {'loss': 2.7434, 'grad_norm': 3.1562798023223877, 'learning_rate': 0.0009932653061224491, 'epoch': 0.03}
  3%|â–Ž         | 133/5000 [02:17<1:21:29,  1.00s/it]  3%|â–Ž         | 134/5000 [02:18<1:21:01,  1.00it/s]                                                    {'loss': 3.0443, 'grad_norm': 4.520902156829834, 'learning_rate': 0.000993061224489796, 'epoch': 0.03}
  3%|â–Ž         | 134/5000 [02:18<1:21:01,  1.00it/s]  3%|â–Ž         | 135/5000 [02:19<1:20:46,  1.00it/s]                                                    {'loss': 2.2467, 'grad_norm': 1.729648470878601, 'learning_rate': 0.000992857142857143, 'epoch': 0.03}
  3%|â–Ž         | 135/5000 [02:19<1:20:46,  1.00it/s]  3%|â–Ž         | 136/5000 [02:20<1:20:52,  1.00it/s]                                                    {'loss': 4.475, 'grad_norm': 2.139338731765747, 'learning_rate': 0.00099265306122449, 'epoch': 0.03}
  3%|â–Ž         | 136/5000 [02:20<1:20:52,  1.00it/s]  3%|â–Ž         | 137/5000 [02:21<1:21:08,  1.00s/it]                                                    {'loss': 2.9141, 'grad_norm': 2.008449077606201, 'learning_rate': 0.0009924489795918368, 'epoch': 0.03}
  3%|â–Ž         | 137/5000 [02:21<1:21:08,  1.00s/it]  3%|â–Ž         | 138/5000 [02:22<1:20:41,  1.00it/s]                                                    {'loss': 2.3647, 'grad_norm': 2.8582239151000977, 'learning_rate': 0.0009922448979591838, 'epoch': 0.03}
  3%|â–Ž         | 138/5000 [02:22<1:20:41,  1.00it/s]  3%|â–Ž         | 139/5000 [02:23<1:21:02,  1.00s/it]                                                    {'loss': 2.3267, 'grad_norm': 1.3663705587387085, 'learning_rate': 0.0009920408163265307, 'epoch': 0.03}
  3%|â–Ž         | 139/5000 [02:23<1:21:02,  1.00s/it]  3%|â–Ž         | 140/5000 [02:24<1:21:14,  1.00s/it]                                                    {'loss': 2.7722, 'grad_norm': 3.8645665645599365, 'learning_rate': 0.0009918367346938776, 'epoch': 0.03}
  3%|â–Ž         | 140/5000 [02:24<1:21:14,  1.00s/it]  3%|â–Ž         | 141/5000 [02:25<1:21:15,  1.00s/it]                                                    {'loss': 2.8732, 'grad_norm': 2.2333028316497803, 'learning_rate': 0.0009916326530612246, 'epoch': 0.03}
  3%|â–Ž         | 141/5000 [02:25<1:21:15,  1.00s/it]  3%|â–Ž         | 142/5000 [02:26<1:21:14,  1.00s/it]                                                    {'loss': 2.601, 'grad_norm': 1.8090858459472656, 'learning_rate': 0.0009914285714285715, 'epoch': 0.03}
  3%|â–Ž         | 142/5000 [02:26<1:21:14,  1.00s/it]  3%|â–Ž         | 143/5000 [02:27<1:21:58,  1.01s/it]                                                    {'loss': 4.2909, 'grad_norm': 4.343596458435059, 'learning_rate': 0.0009912244897959184, 'epoch': 0.03}
  3%|â–Ž         | 143/5000 [02:27<1:21:58,  1.01s/it]  3%|â–Ž         | 144/5000 [02:28<1:21:41,  1.01s/it]                                                    {'loss': 1.9812, 'grad_norm': 1.4663270711898804, 'learning_rate': 0.0009910204081632653, 'epoch': 0.03}
  3%|â–Ž         | 144/5000 [02:28<1:21:41,  1.01s/it]  3%|â–Ž         | 145/5000 [02:29<1:21:53,  1.01s/it]                                                    {'loss': 2.7445, 'grad_norm': 1.8904352188110352, 'learning_rate': 0.0009908163265306123, 'epoch': 0.03}
  3%|â–Ž         | 145/5000 [02:29<1:21:53,  1.01s/it]  3%|â–Ž         | 146/5000 [02:30<1:21:48,  1.01s/it]                                                    {'loss': 2.7088, 'grad_norm': 2.235092878341675, 'learning_rate': 0.0009906122448979592, 'epoch': 0.03}
  3%|â–Ž         | 146/5000 [02:30<1:21:48,  1.01s/it]  3%|â–Ž         | 147/5000 [02:31<1:21:34,  1.01s/it]                                                    {'loss': 2.9494, 'grad_norm': 2.857512950897217, 'learning_rate': 0.0009904081632653061, 'epoch': 0.03}
  3%|â–Ž         | 147/5000 [02:31<1:21:34,  1.01s/it]  3%|â–Ž         | 148/5000 [02:32<1:21:48,  1.01s/it]                                                    {'loss': 1.9567, 'grad_norm': 0.8630611896514893, 'learning_rate': 0.000990204081632653, 'epoch': 0.03}
  3%|â–Ž         | 148/5000 [02:32<1:21:48,  1.01s/it]  3%|â–Ž         | 149/5000 [02:33<1:21:32,  1.01s/it]                                                    {'loss': 2.4793, 'grad_norm': 1.790486216545105, 'learning_rate': 0.00099, 'epoch': 0.03}
  3%|â–Ž         | 149/5000 [02:33<1:21:32,  1.01s/it]  3%|â–Ž         | 150/5000 [02:34<1:21:37,  1.01s/it]                                                    {'loss': 2.0518, 'grad_norm': 2.9459753036499023, 'learning_rate': 0.000989795918367347, 'epoch': 0.03}
  3%|â–Ž         | 150/5000 [02:34<1:21:37,  1.01s/it][2025-10-19 17:15:55,529] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/5000 [02:36<1:52:29,  1.39s/it]                                                    {'loss': 2.1317, 'grad_norm': 1.48753023147583, 'learning_rate': 0.0009895918367346939, 'epoch': 0.03}
  3%|â–Ž         | 151/5000 [02:36<1:52:29,  1.39s/it]  3%|â–Ž         | 152/5000 [02:37<1:42:55,  1.27s/it]                                                    {'loss': 2.8195, 'grad_norm': 2.161928415298462, 'learning_rate': 0.0009893877551020408, 'epoch': 0.03}
  3%|â–Ž         | 152/5000 [02:37<1:42:55,  1.27s/it]  3%|â–Ž         | 153/5000 [02:38<1:36:23,  1.19s/it]                                                    {'loss': 2.9961, 'grad_norm': 2.422468662261963, 'learning_rate': 0.0009891836734693877, 'epoch': 0.03}
  3%|â–Ž         | 153/5000 [02:38<1:36:23,  1.19s/it]  3%|â–Ž         | 154/5000 [02:39<1:31:18,  1.13s/it]                                                    {'loss': 1.9206, 'grad_norm': 1.0046429634094238, 'learning_rate': 0.0009889795918367346, 'epoch': 0.03}
  3%|â–Ž         | 154/5000 [02:39<1:31:18,  1.13s/it]  3%|â–Ž         | 155/5000 [02:40<1:27:42,  1.09s/it]                                                    {'loss': 1.8927, 'grad_norm': 1.4379936456680298, 'learning_rate': 0.0009887755102040816, 'epoch': 0.03}
  3%|â–Ž         | 155/5000 [02:40<1:27:42,  1.09s/it]  3%|â–Ž         | 156/5000 [02:41<1:25:40,  1.06s/it]                                                    {'loss': 1.594, 'grad_norm': 0.4536339044570923, 'learning_rate': 0.0009885714285714285, 'epoch': 0.03}
  3%|â–Ž         | 156/5000 [02:41<1:25:40,  1.06s/it]  3%|â–Ž         | 157/5000 [02:42<1:24:02,  1.04s/it]                                                    {'loss': 4.4602, 'grad_norm': 2.1983630657196045, 'learning_rate': 0.0009883673469387754, 'epoch': 0.03}
  3%|â–Ž         | 157/5000 [02:42<1:24:02,  1.04s/it]  3%|â–Ž         | 158/5000 [02:43<1:23:14,  1.03s/it]                                                    {'loss': 2.5489, 'grad_norm': 1.791273593902588, 'learning_rate': 0.0009881632653061224, 'epoch': 0.03}
  3%|â–Ž         | 158/5000 [02:43<1:23:14,  1.03s/it]  3%|â–Ž         | 159/5000 [02:44<1:22:53,  1.03s/it]                                                    {'loss': 2.0873, 'grad_norm': 2.6478960514068604, 'learning_rate': 0.0009879591836734693, 'epoch': 0.03}
  3%|â–Ž         | 159/5000 [02:44<1:22:53,  1.03s/it]  3%|â–Ž         | 160/5000 [02:45<1:22:33,  1.02s/it]                                                    {'loss': 1.8993, 'grad_norm': 1.3297600746154785, 'learning_rate': 0.0009877551020408162, 'epoch': 0.03}
  3%|â–Ž         | 160/5000 [02:45<1:22:33,  1.02s/it]  3%|â–Ž         | 161/5000 [02:46<1:22:10,  1.02s/it]                                                    {'loss': 2.7928, 'grad_norm': 2.7291619777679443, 'learning_rate': 0.0009875510204081631, 'epoch': 0.03}
  3%|â–Ž         | 161/5000 [02:46<1:22:10,  1.02s/it]  3%|â–Ž         | 162/5000 [02:47<1:21:50,  1.01s/it]                                                    {'loss': 1.6542, 'grad_norm': 1.5554369688034058, 'learning_rate': 0.00098734693877551, 'epoch': 0.03}
  3%|â–Ž         | 162/5000 [02:47<1:21:50,  1.01s/it]  3%|â–Ž         | 163/5000 [02:48<1:21:24,  1.01s/it]                                                    {'loss': 1.659, 'grad_norm': 1.9893120527267456, 'learning_rate': 0.0009871428571428572, 'epoch': 0.03}
  3%|â–Ž         | 163/5000 [02:48<1:21:24,  1.01s/it]  3%|â–Ž         | 164/5000 [02:49<1:21:24,  1.01s/it]                                                    {'loss': 3.5604, 'grad_norm': 2.350343704223633, 'learning_rate': 0.0009869387755102042, 'epoch': 0.03}
  3%|â–Ž         | 164/5000 [02:49<1:21:24,  1.01s/it]  3%|â–Ž         | 165/5000 [02:50<1:21:02,  1.01s/it]                                                    {'loss': 3.2537, 'grad_norm': 3.002399444580078, 'learning_rate': 0.000986734693877551, 'epoch': 0.03}
  3%|â–Ž         | 165/5000 [02:50<1:21:02,  1.01s/it]  3%|â–Ž         | 166/5000 [02:51<1:21:47,  1.02s/it]                                                    {'loss': 1.7045, 'grad_norm': 0.9960405230522156, 'learning_rate': 0.000986530612244898, 'epoch': 0.03}
  3%|â–Ž         | 166/5000 [02:51<1:21:47,  1.02s/it]  3%|â–Ž         | 167/5000 [02:52<1:20:58,  1.01s/it]                                                    {'loss': 2.1443, 'grad_norm': 1.5181429386138916, 'learning_rate': 0.000986326530612245, 'epoch': 0.03}
  3%|â–Ž         | 167/5000 [02:52<1:20:58,  1.01s/it]  3%|â–Ž         | 168/5000 [02:53<1:20:52,  1.00s/it]                                                    {'loss': 1.5748, 'grad_norm': 1.7225474119186401, 'learning_rate': 0.0009861224489795919, 'epoch': 0.03}
  3%|â–Ž         | 168/5000 [02:53<1:20:52,  1.00s/it]  3%|â–Ž         | 169/5000 [02:54<1:20:54,  1.00s/it]                                                    {'loss': 2.9313, 'grad_norm': 3.340264081954956, 'learning_rate': 0.0009859183673469388, 'epoch': 0.03}
  3%|â–Ž         | 169/5000 [02:54<1:20:54,  1.00s/it]  3%|â–Ž         | 170/5000 [02:55<1:21:15,  1.01s/it]                                                    {'loss': 2.4032, 'grad_norm': 1.901607632637024, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}
  3%|â–Ž         | 170/5000 [02:55<1:21:15,  1.01s/it]  3%|â–Ž         | 171/5000 [02:56<1:21:13,  1.01s/it]                                                    {'loss': 1.7775, 'grad_norm': 2.2709763050079346, 'learning_rate': 0.0009855102040816327, 'epoch': 0.03}
  3%|â–Ž         | 171/5000 [02:56<1:21:13,  1.01s/it]  3%|â–Ž         | 172/5000 [02:58<1:26:08,  1.07s/it]                                                    {'loss': 2.0081, 'grad_norm': 2.462432622909546, 'learning_rate': 0.0009853061224489796, 'epoch': 0.03}
  3%|â–Ž         | 172/5000 [02:58<1:26:08,  1.07s/it]  3%|â–Ž         | 173/5000 [02:59<1:26:42,  1.08s/it]                                                    {'loss': 2.1262, 'grad_norm': 2.4545531272888184, 'learning_rate': 0.0009851020408163265, 'epoch': 0.03}
  3%|â–Ž         | 173/5000 [02:59<1:26:42,  1.08s/it]  3%|â–Ž         | 174/5000 [03:00<1:24:53,  1.06s/it]                                                    {'loss': 2.5613, 'grad_norm': 2.1213860511779785, 'learning_rate': 0.0009848979591836734, 'epoch': 0.03}
  3%|â–Ž         | 174/5000 [03:00<1:24:53,  1.06s/it]  4%|â–Ž         | 175/5000 [03:01<1:23:48,  1.04s/it]                                                    {'loss': 2.3016, 'grad_norm': 3.1120524406433105, 'learning_rate': 0.0009846938775510204, 'epoch': 0.04}
  4%|â–Ž         | 175/5000 [03:01<1:23:48,  1.04s/it]  4%|â–Ž         | 176/5000 [03:02<1:23:00,  1.03s/it]                                                    {'loss': 1.7821, 'grad_norm': 1.465461015701294, 'learning_rate': 0.0009844897959183673, 'epoch': 0.04}
  4%|â–Ž         | 176/5000 [03:02<1:23:00,  1.03s/it]  4%|â–Ž         | 177/5000 [03:03<1:22:36,  1.03s/it]                                                    {'loss': 2.473, 'grad_norm': 1.9902199506759644, 'learning_rate': 0.0009842857142857142, 'epoch': 0.04}
  4%|â–Ž         | 177/5000 [03:03<1:22:36,  1.03s/it]  4%|â–Ž         | 178/5000 [03:04<1:22:22,  1.03s/it]                                                    {'loss': 2.9783, 'grad_norm': 4.089672088623047, 'learning_rate': 0.0009840816326530614, 'epoch': 0.04}
  4%|â–Ž         | 178/5000 [03:04<1:22:22,  1.03s/it]  4%|â–Ž         | 179/5000 [03:05<1:21:48,  1.02s/it]                                                    {'loss': 3.0328, 'grad_norm': 6.886484622955322, 'learning_rate': 0.0009838775510204083, 'epoch': 0.04}
  4%|â–Ž         | 179/5000 [03:05<1:21:48,  1.02s/it]  4%|â–Ž         | 180/5000 [03:06<1:21:22,  1.01s/it]                                                    {'loss': 2.0068, 'grad_norm': 1.0639485120773315, 'learning_rate': 0.0009836734693877552, 'epoch': 0.04}
  4%|â–Ž         | 180/5000 [03:06<1:21:22,  1.01s/it]  4%|â–Ž         | 181/5000 [03:07<1:21:12,  1.01s/it]                                                    {'loss': 3.309, 'grad_norm': 4.809541702270508, 'learning_rate': 0.0009834693877551022, 'epoch': 0.04}
  4%|â–Ž         | 181/5000 [03:07<1:21:12,  1.01s/it]  4%|â–Ž         | 182/5000 [03:08<1:21:33,  1.02s/it]                                                    {'loss': 1.8124, 'grad_norm': 1.1746810674667358, 'learning_rate': 0.000983265306122449, 'epoch': 0.04}
  4%|â–Ž         | 182/5000 [03:08<1:21:33,  1.02s/it]  4%|â–Ž         | 183/5000 [03:09<1:20:47,  1.01s/it]                                                    {'loss': 2.2959, 'grad_norm': 2.1378002166748047, 'learning_rate': 0.000983061224489796, 'epoch': 0.04}
  4%|â–Ž         | 183/5000 [03:09<1:20:47,  1.01s/it]  4%|â–Ž         | 184/5000 [03:10<1:20:49,  1.01s/it]                                                    {'loss': 1.893, 'grad_norm': 1.298807144165039, 'learning_rate': 0.000982857142857143, 'epoch': 0.04}
  4%|â–Ž         | 184/5000 [03:10<1:20:49,  1.01s/it]  4%|â–Ž         | 185/5000 [03:11<1:20:36,  1.00s/it]                                                    {'loss': 1.8038, 'grad_norm': 1.3965712785720825, 'learning_rate': 0.0009826530612244899, 'epoch': 0.04}
  4%|â–Ž         | 185/5000 [03:11<1:20:36,  1.00s/it]  4%|â–Ž         | 186/5000 [03:12<1:20:42,  1.01s/it]                                                    {'loss': 2.2486, 'grad_norm': 1.7862170934677124, 'learning_rate': 0.0009824489795918368, 'epoch': 0.04}
  4%|â–Ž         | 186/5000 [03:12<1:20:42,  1.01s/it]  4%|â–Ž         | 187/5000 [03:13<1:20:23,  1.00s/it]                                                    {'loss': 2.0442, 'grad_norm': 1.2179509401321411, 'learning_rate': 0.0009822448979591837, 'epoch': 0.04}
  4%|â–Ž         | 187/5000 [03:13<1:20:23,  1.00s/it]  4%|â–         | 188/5000 [03:14<1:21:12,  1.01s/it]                                                    {'loss': 2.011, 'grad_norm': 2.5100815296173096, 'learning_rate': 0.0009820408163265307, 'epoch': 0.04}
  4%|â–         | 188/5000 [03:14<1:21:12,  1.01s/it]  4%|â–         | 189/5000 [03:15<1:21:11,  1.01s/it]                                                    {'loss': 2.4981, 'grad_norm': 1.787794828414917, 'learning_rate': 0.0009818367346938776, 'epoch': 0.04}
  4%|â–         | 189/5000 [03:15<1:21:11,  1.01s/it]  4%|â–         | 190/5000 [03:16<1:20:30,  1.00s/it]                                                    {'loss': 2.7112, 'grad_norm': 2.3965322971343994, 'learning_rate': 0.0009816326530612245, 'epoch': 0.04}
  4%|â–         | 190/5000 [03:16<1:20:30,  1.00s/it]  4%|â–         | 191/5000 [03:17<1:21:08,  1.01s/it]                                                    {'loss': 2.1361, 'grad_norm': 2.3578784465789795, 'learning_rate': 0.0009814285714285715, 'epoch': 0.04}
  4%|â–         | 191/5000 [03:17<1:21:08,  1.01s/it]  4%|â–         | 192/5000 [03:18<1:20:39,  1.01s/it]                                                    {'loss': 1.5846, 'grad_norm': 0.6839715838432312, 'learning_rate': 0.0009812244897959184, 'epoch': 0.04}
  4%|â–         | 192/5000 [03:18<1:20:39,  1.01s/it]  4%|â–         | 193/5000 [03:19<1:20:51,  1.01s/it]                                                    {'loss': 2.0434, 'grad_norm': 1.8153129816055298, 'learning_rate': 0.0009810204081632653, 'epoch': 0.04}
  4%|â–         | 193/5000 [03:19<1:20:51,  1.01s/it]  4%|â–         | 194/5000 [03:20<1:19:58,  1.00it/s]                                                    {'loss': 1.811, 'grad_norm': 1.767248272895813, 'learning_rate': 0.0009808163265306123, 'epoch': 0.04}
  4%|â–         | 194/5000 [03:20<1:19:58,  1.00it/s]  4%|â–         | 195/5000 [03:21<1:19:58,  1.00it/s]                                                    {'loss': 2.6466, 'grad_norm': 2.3340909481048584, 'learning_rate': 0.0009806122448979592, 'epoch': 0.04}
  4%|â–         | 195/5000 [03:21<1:19:58,  1.00it/s]  4%|â–         | 196/5000 [03:22<1:19:26,  1.01it/s]                                                    {'loss': 1.9664, 'grad_norm': 2.346569061279297, 'learning_rate': 0.0009804081632653061, 'epoch': 0.04}
  4%|â–         | 196/5000 [03:22<1:19:26,  1.01it/s]  4%|â–         | 197/5000 [03:23<1:19:45,  1.00it/s]                                                    {'loss': 1.853, 'grad_norm': 1.2169595956802368, 'learning_rate': 0.000980204081632653, 'epoch': 0.04}
  4%|â–         | 197/5000 [03:23<1:19:45,  1.00it/s]  4%|â–         | 198/5000 [03:24<1:19:57,  1.00it/s]                                                    {'loss': 2.8509, 'grad_norm': 4.3001885414123535, 'learning_rate': 0.00098, 'epoch': 0.04}
  4%|â–         | 198/5000 [03:24<1:19:57,  1.00it/s]  4%|â–         | 199/5000 [03:25<1:20:34,  1.01s/it]                                                    {'loss': 2.1949, 'grad_norm': 1.4827001094818115, 'learning_rate': 0.000979795918367347, 'epoch': 0.04}
  4%|â–         | 199/5000 [03:25<1:20:34,  1.01s/it]  4%|â–         | 200/5000 [03:26<1:21:07,  1.01s/it]                                                    {'loss': 2.0495, 'grad_norm': 1.6048684120178223, 'learning_rate': 0.0009795918367346938, 'epoch': 0.04}
  4%|â–         | 200/5000 [03:26<1:21:07,  1.01s/it][2025-10-19 17:16:47,386] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 201/5000 [03:28<1:49:49,  1.37s/it]                                                    {'loss': 1.9051, 'grad_norm': 3.0424602031707764, 'learning_rate': 0.0009793877551020408, 'epoch': 0.04}
  4%|â–         | 201/5000 [03:28<1:49:49,  1.37s/it]  4%|â–         | 202/5000 [03:29<1:40:50,  1.26s/it]                                                    {'loss': 3.01, 'grad_norm': 2.2440738677978516, 'learning_rate': 0.0009791836734693877, 'epoch': 0.04}
  4%|â–         | 202/5000 [03:29<1:40:50,  1.26s/it]  4%|â–         | 203/5000 [03:30<1:34:26,  1.18s/it]                                                    {'loss': 2.7364, 'grad_norm': 1.8223204612731934, 'learning_rate': 0.0009789795918367346, 'epoch': 0.04}
  4%|â–         | 203/5000 [03:30<1:34:26,  1.18s/it]  4%|â–         | 204/5000 [03:31<1:30:27,  1.13s/it]                                                    {'loss': 1.6046, 'grad_norm': 0.8906438946723938, 'learning_rate': 0.0009787755102040815, 'epoch': 0.04}
  4%|â–         | 204/5000 [03:31<1:30:27,  1.13s/it]  4%|â–         | 205/5000 [03:32<1:27:25,  1.09s/it]                                                    {'loss': 1.756, 'grad_norm': 1.1925756931304932, 'learning_rate': 0.0009785714285714285, 'epoch': 0.04}
  4%|â–         | 205/5000 [03:32<1:27:25,  1.09s/it]  4%|â–         | 206/5000 [03:33<1:25:02,  1.06s/it]                                                    {'loss': 2.0727, 'grad_norm': 1.0678297281265259, 'learning_rate': 0.0009783673469387754, 'epoch': 0.04}
  4%|â–         | 206/5000 [03:33<1:25:02,  1.06s/it]  4%|â–         | 207/5000 [03:34<1:23:09,  1.04s/it]                                                    {'loss': 1.6894, 'grad_norm': 1.4212087392807007, 'learning_rate': 0.0009781632653061223, 'epoch': 0.04}
  4%|â–         | 207/5000 [03:34<1:23:09,  1.04s/it]  4%|â–         | 208/5000 [03:35<1:22:08,  1.03s/it]                                                    {'loss': 1.7956, 'grad_norm': 1.8477283716201782, 'learning_rate': 0.0009779591836734693, 'epoch': 0.04}
  4%|â–         | 208/5000 [03:35<1:22:08,  1.03s/it]  4%|â–         | 209/5000 [03:36<1:21:34,  1.02s/it]                                                    {'loss': 1.976, 'grad_norm': 1.5771328210830688, 'learning_rate': 0.0009777551020408164, 'epoch': 0.04}
  4%|â–         | 209/5000 [03:36<1:21:34,  1.02s/it]  4%|â–         | 210/5000 [03:37<1:21:28,  1.02s/it]                                                    {'loss': 3.1775, 'grad_norm': 2.2591803073883057, 'learning_rate': 0.0009775510204081633, 'epoch': 0.04}
  4%|â–         | 210/5000 [03:37<1:21:28,  1.02s/it]  4%|â–         | 211/5000 [03:38<1:20:56,  1.01s/it]                                                    {'loss': 1.6897, 'grad_norm': 2.308941125869751, 'learning_rate': 0.0009773469387755103, 'epoch': 0.04}
  4%|â–         | 211/5000 [03:38<1:20:56,  1.01s/it]  4%|â–         | 212/5000 [03:39<1:20:25,  1.01s/it]                                                    {'loss': 3.3337, 'grad_norm': 2.1971986293792725, 'learning_rate': 0.0009771428571428572, 'epoch': 0.04}
  4%|â–         | 212/5000 [03:39<1:20:25,  1.01s/it]  4%|â–         | 213/5000 [03:40<1:19:54,  1.00s/it]                                                    {'loss': 2.2242, 'grad_norm': 1.4967767000198364, 'learning_rate': 0.0009769387755102041, 'epoch': 0.04}
  4%|â–         | 213/5000 [03:40<1:19:54,  1.00s/it]  4%|â–         | 214/5000 [03:41<1:22:59,  1.04s/it]                                                    {'loss': 2.1918, 'grad_norm': 5.453485488891602, 'learning_rate': 0.000976734693877551, 'epoch': 0.04}
  4%|â–         | 214/5000 [03:41<1:22:59,  1.04s/it]  4%|â–         | 215/5000 [03:42<1:22:37,  1.04s/it]                                                    {'loss': 2.5903, 'grad_norm': 4.491245269775391, 'learning_rate': 0.000976530612244898, 'epoch': 0.04}
  4%|â–         | 215/5000 [03:42<1:22:37,  1.04s/it]  4%|â–         | 216/5000 [03:43<1:21:41,  1.02s/it]                                                    {'loss': 2.1474, 'grad_norm': 1.4624978303909302, 'learning_rate': 0.0009763265306122449, 'epoch': 0.04}
  4%|â–         | 216/5000 [03:43<1:21:41,  1.02s/it]  4%|â–         | 217/5000 [03:44<1:21:02,  1.02s/it]                                                    {'loss': 1.8873, 'grad_norm': 1.5336805582046509, 'learning_rate': 0.000976122448979592, 'epoch': 0.04}
  4%|â–         | 217/5000 [03:44<1:21:02,  1.02s/it]  4%|â–         | 218/5000 [03:45<1:21:19,  1.02s/it]                                                    {'loss': 1.9809, 'grad_norm': 1.5116291046142578, 'learning_rate': 0.0009759183673469389, 'epoch': 0.04}
  4%|â–         | 218/5000 [03:45<1:21:19,  1.02s/it]  4%|â–         | 219/5000 [03:46<1:20:53,  1.02s/it]                                                    {'loss': 2.6906, 'grad_norm': 2.9690959453582764, 'learning_rate': 0.0009757142857142858, 'epoch': 0.04}
  4%|â–         | 219/5000 [03:46<1:20:53,  1.02s/it]  4%|â–         | 220/5000 [03:47<1:20:13,  1.01s/it]                                                    {'loss': 2.9813, 'grad_norm': 2.188429832458496, 'learning_rate': 0.0009755102040816327, 'epoch': 0.04}
  4%|â–         | 220/5000 [03:47<1:20:13,  1.01s/it]  4%|â–         | 221/5000 [03:48<1:20:01,  1.00s/it]                                                    {'loss': 2.11, 'grad_norm': 1.919622540473938, 'learning_rate': 0.0009753061224489797, 'epoch': 0.04}
  4%|â–         | 221/5000 [03:48<1:20:01,  1.00s/it]  4%|â–         | 222/5000 [03:49<1:19:33,  1.00it/s]                                                    {'loss': 3.4896, 'grad_norm': 2.811399221420288, 'learning_rate': 0.0009751020408163266, 'epoch': 0.04}
  4%|â–         | 222/5000 [03:49<1:19:33,  1.00it/s]  4%|â–         | 223/5000 [03:50<1:19:22,  1.00it/s]                                                    {'loss': 1.7376, 'grad_norm': 1.549812912940979, 'learning_rate': 0.0009748979591836735, 'epoch': 0.04}
  4%|â–         | 223/5000 [03:50<1:19:22,  1.00it/s]  4%|â–         | 224/5000 [03:51<1:19:26,  1.00it/s]                                                    {'loss': 2.7865, 'grad_norm': 2.6866846084594727, 'learning_rate': 0.0009746938775510205, 'epoch': 0.04}
  4%|â–         | 224/5000 [03:51<1:19:26,  1.00it/s]  4%|â–         | 225/5000 [03:52<1:19:33,  1.00it/s]                                                    {'loss': 2.245, 'grad_norm': 1.2514299154281616, 'learning_rate': 0.0009744897959183674, 'epoch': 0.04}
  4%|â–         | 225/5000 [03:52<1:19:33,  1.00it/s]  5%|â–         | 226/5000 [03:53<1:19:50,  1.00s/it]                                                    {'loss': 2.0169, 'grad_norm': 2.0539934635162354, 'learning_rate': 0.0009742857142857143, 'epoch': 0.05}
  5%|â–         | 226/5000 [03:53<1:19:50,  1.00s/it]  5%|â–         | 227/5000 [03:54<1:20:11,  1.01s/it]                                                    {'loss': 1.5934, 'grad_norm': 2.4918718338012695, 'learning_rate': 0.0009740816326530612, 'epoch': 0.05}
  5%|â–         | 227/5000 [03:54<1:20:11,  1.01s/it]  5%|â–         | 228/5000 [03:55<1:19:53,  1.00s/it]                                                    {'loss': 2.0933, 'grad_norm': 4.340083599090576, 'learning_rate': 0.0009738775510204082, 'epoch': 0.05}
  5%|â–         | 228/5000 [03:55<1:19:53,  1.00s/it]  5%|â–         | 229/5000 [03:56<1:19:50,  1.00s/it]                                                    {'loss': 3.539, 'grad_norm': 8.804848670959473, 'learning_rate': 0.0009736734693877551, 'epoch': 0.05}
  5%|â–         | 229/5000 [03:56<1:19:50,  1.00s/it]  5%|â–         | 230/5000 [03:57<1:20:06,  1.01s/it]                                                    {'loss': 1.5374, 'grad_norm': 0.5831865072250366, 'learning_rate': 0.000973469387755102, 'epoch': 0.05}
  5%|â–         | 230/5000 [03:57<1:20:06,  1.01s/it]  5%|â–         | 231/5000 [03:58<1:19:21,  1.00it/s]                                                    {'loss': 1.7269, 'grad_norm': 2.2430477142333984, 'learning_rate': 0.000973265306122449, 'epoch': 0.05}
  5%|â–         | 231/5000 [03:58<1:19:21,  1.00it/s]  5%|â–         | 232/5000 [03:59<1:19:02,  1.01it/s]                                                    {'loss': 2.0173, 'grad_norm': 1.8213082551956177, 'learning_rate': 0.000973061224489796, 'epoch': 0.05}
  5%|â–         | 232/5000 [03:59<1:19:02,  1.01it/s]  5%|â–         | 233/5000 [04:00<1:18:43,  1.01it/s]                                                    {'loss': 2.317, 'grad_norm': 2.915327787399292, 'learning_rate': 0.0009728571428571429, 'epoch': 0.05}
  5%|â–         | 233/5000 [04:00<1:18:43,  1.01it/s]  5%|â–         | 234/5000 [04:01<1:18:31,  1.01it/s]                                                    {'loss': 2.933, 'grad_norm': 2.367656707763672, 'learning_rate': 0.0009726530612244899, 'epoch': 0.05}
  5%|â–         | 234/5000 [04:01<1:18:31,  1.01it/s]  5%|â–         | 235/5000 [04:02<1:18:30,  1.01it/s]                                                    {'loss': 2.7376, 'grad_norm': 2.8881330490112305, 'learning_rate': 0.0009724489795918368, 'epoch': 0.05}
  5%|â–         | 235/5000 [04:02<1:18:30,  1.01it/s]  5%|â–         | 236/5000 [04:03<1:18:49,  1.01it/s]                                                    {'loss': 1.74, 'grad_norm': 1.1553692817687988, 'learning_rate': 0.0009722448979591837, 'epoch': 0.05}
  5%|â–         | 236/5000 [04:03<1:18:49,  1.01it/s]  5%|â–         | 237/5000 [04:04<1:19:30,  1.00s/it]                                                    {'loss': 1.4994, 'grad_norm': 0.6068775653839111, 'learning_rate': 0.0009720408163265306, 'epoch': 0.05}
  5%|â–         | 237/5000 [04:04<1:19:30,  1.00s/it]  5%|â–         | 238/5000 [04:05<1:19:12,  1.00it/s]                                                    {'loss': 2.2049, 'grad_norm': 1.1529239416122437, 'learning_rate': 0.0009718367346938776, 'epoch': 0.05}
  5%|â–         | 238/5000 [04:05<1:19:12,  1.00it/s]  5%|â–         | 239/5000 [04:06<1:19:14,  1.00it/s]                                                    {'loss': 1.6289, 'grad_norm': 1.2150120735168457, 'learning_rate': 0.0009716326530612245, 'epoch': 0.05}
  5%|â–         | 239/5000 [04:06<1:19:14,  1.00it/s]  5%|â–         | 240/5000 [04:07<1:19:03,  1.00it/s]                                                    {'loss': 2.3483, 'grad_norm': 2.086108446121216, 'learning_rate': 0.0009714285714285714, 'epoch': 0.05}
  5%|â–         | 240/5000 [04:07<1:19:03,  1.00it/s]  5%|â–         | 241/5000 [04:08<1:18:22,  1.01it/s]                                                    {'loss': 1.9247, 'grad_norm': 1.270063877105713, 'learning_rate': 0.0009712244897959184, 'epoch': 0.05}
  5%|â–         | 241/5000 [04:08<1:18:22,  1.01it/s]  5%|â–         | 242/5000 [04:09<1:18:26,  1.01it/s]                                                    {'loss': 1.9813, 'grad_norm': 1.1864086389541626, 'learning_rate': 0.0009710204081632653, 'epoch': 0.05}
  5%|â–         | 242/5000 [04:09<1:18:26,  1.01it/s]  5%|â–         | 243/5000 [04:10<1:19:39,  1.00s/it]                                                    {'loss': 1.8316, 'grad_norm': 1.1977193355560303, 'learning_rate': 0.0009708163265306122, 'epoch': 0.05}
  5%|â–         | 243/5000 [04:10<1:19:39,  1.00s/it]  5%|â–         | 244/5000 [04:11<1:19:38,  1.00s/it]                                                    {'loss': 2.5655, 'grad_norm': 1.5612239837646484, 'learning_rate': 0.0009706122448979592, 'epoch': 0.05}
  5%|â–         | 244/5000 [04:11<1:19:38,  1.00s/it]  5%|â–         | 245/5000 [04:12<1:19:06,  1.00it/s]                                                    {'loss': 2.8139, 'grad_norm': 2.03865385055542, 'learning_rate': 0.0009704081632653061, 'epoch': 0.05}
  5%|â–         | 245/5000 [04:12<1:19:06,  1.00it/s]  5%|â–         | 246/5000 [04:13<1:18:44,  1.01it/s]                                                    {'loss': 1.895, 'grad_norm': 2.3659651279449463, 'learning_rate': 0.000970204081632653, 'epoch': 0.05}
  5%|â–         | 246/5000 [04:13<1:18:44,  1.01it/s]  5%|â–         | 247/5000 [04:14<1:19:01,  1.00it/s]                                                    {'loss': 1.4867, 'grad_norm': 0.5331040620803833, 'learning_rate': 0.0009699999999999999, 'epoch': 0.05}
  5%|â–         | 247/5000 [04:14<1:19:01,  1.00it/s]  5%|â–         | 248/5000 [04:15<1:19:28,  1.00s/it]                                                    {'loss': 1.4963, 'grad_norm': 0.6280423402786255, 'learning_rate': 0.000969795918367347, 'epoch': 0.05}
  5%|â–         | 248/5000 [04:15<1:19:28,  1.00s/it]  5%|â–         | 249/5000 [04:16<1:19:02,  1.00it/s]                                                    {'loss': 2.0075, 'grad_norm': 1.0774445533752441, 'learning_rate': 0.0009695918367346939, 'epoch': 0.05}
  5%|â–         | 249/5000 [04:16<1:19:02,  1.00it/s]  5%|â–Œ         | 250/5000 [04:17<1:19:10,  1.00s/it]                                                    {'loss': 1.8121, 'grad_norm': 0.7932885885238647, 'learning_rate': 0.0009693877551020408, 'epoch': 0.05}
  5%|â–Œ         | 250/5000 [04:17<1:19:10,  1.00s/it][2025-10-19 17:17:38,708] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 251/5000 [04:19<1:46:40,  1.35s/it]                                                    {'loss': 1.7078, 'grad_norm': 2.9765515327453613, 'learning_rate': 0.0009691836734693878, 'epoch': 0.05}
  5%|â–Œ         | 251/5000 [04:19<1:46:40,  1.35s/it]  5%|â–Œ         | 252/5000 [04:20<1:38:10,  1.24s/it]                                                    {'loss': 2.1078, 'grad_norm': 1.726241946220398, 'learning_rate': 0.0009689795918367347, 'epoch': 0.05}
  5%|â–Œ         | 252/5000 [04:20<1:38:10,  1.24s/it]  5%|â–Œ         | 253/5000 [04:21<1:32:12,  1.17s/it]                                                    {'loss': 1.5577, 'grad_norm': 0.8546958565711975, 'learning_rate': 0.0009687755102040816, 'epoch': 0.05}
  5%|â–Œ         | 253/5000 [04:21<1:32:12,  1.17s/it]  5%|â–Œ         | 254/5000 [04:22<1:28:48,  1.12s/it]                                                    {'loss': 2.0777, 'grad_norm': 1.5974899530410767, 'learning_rate': 0.0009685714285714286, 'epoch': 0.05}
  5%|â–Œ         | 254/5000 [04:22<1:28:48,  1.12s/it]  5%|â–Œ         | 255/5000 [04:23<1:25:32,  1.08s/it]                                                    {'loss': 1.4615, 'grad_norm': 0.5086758136749268, 'learning_rate': 0.0009683673469387755, 'epoch': 0.05}
  5%|â–Œ         | 255/5000 [04:23<1:25:32,  1.08s/it]  5%|â–Œ         | 256/5000 [04:24<1:23:51,  1.06s/it]                                                    {'loss': 2.6663, 'grad_norm': 3.441443920135498, 'learning_rate': 0.0009681632653061224, 'epoch': 0.05}
  5%|â–Œ         | 256/5000 [04:24<1:23:51,  1.06s/it]  5%|â–Œ         | 257/5000 [04:25<1:22:07,  1.04s/it]                                                    {'loss': 1.904, 'grad_norm': 1.92314612865448, 'learning_rate': 0.0009679591836734693, 'epoch': 0.05}
  5%|â–Œ         | 257/5000 [04:25<1:22:07,  1.04s/it]  5%|â–Œ         | 258/5000 [04:26<1:21:37,  1.03s/it]                                                    {'loss': 1.6485, 'grad_norm': 1.2094640731811523, 'learning_rate': 0.0009677551020408163, 'epoch': 0.05}
  5%|â–Œ         | 258/5000 [04:26<1:21:37,  1.03s/it]  5%|â–Œ         | 259/5000 [04:27<1:21:09,  1.03s/it]                                                    {'loss': 3.0318, 'grad_norm': 2.1151342391967773, 'learning_rate': 0.0009675510204081633, 'epoch': 0.05}
  5%|â–Œ         | 259/5000 [04:27<1:21:09,  1.03s/it]  5%|â–Œ         | 260/5000 [04:28<1:20:33,  1.02s/it]                                                    {'loss': 1.6164, 'grad_norm': 1.1810165643692017, 'learning_rate': 0.0009673469387755102, 'epoch': 0.05}
  5%|â–Œ         | 260/5000 [04:28<1:20:33,  1.02s/it]  5%|â–Œ         | 261/5000 [04:29<1:20:21,  1.02s/it]                                                    {'loss': 1.5473, 'grad_norm': 0.8352904915809631, 'learning_rate': 0.0009671428571428572, 'epoch': 0.05}
  5%|â–Œ         | 261/5000 [04:29<1:20:21,  1.02s/it]  5%|â–Œ         | 262/5000 [04:30<1:21:26,  1.03s/it]                                                    {'loss': 1.6461, 'grad_norm': 1.3384121656417847, 'learning_rate': 0.0009669387755102041, 'epoch': 0.05}
  5%|â–Œ         | 262/5000 [04:30<1:21:26,  1.03s/it]  5%|â–Œ         | 263/5000 [04:31<1:22:15,  1.04s/it]                                                    {'loss': 1.8217, 'grad_norm': 2.837470531463623, 'learning_rate': 0.0009667346938775511, 'epoch': 0.05}
  5%|â–Œ         | 263/5000 [04:31<1:22:15,  1.04s/it]  5%|â–Œ         | 264/5000 [04:32<1:21:14,  1.03s/it]                                                    {'loss': 3.2062, 'grad_norm': 7.926948547363281, 'learning_rate': 0.0009665306122448981, 'epoch': 0.05}
  5%|â–Œ         | 264/5000 [04:32<1:21:14,  1.03s/it]  5%|â–Œ         | 265/5000 [04:33<1:20:09,  1.02s/it]                                                    {'loss': 1.5326, 'grad_norm': 0.4564710855484009, 'learning_rate': 0.000966326530612245, 'epoch': 0.05}
  5%|â–Œ         | 265/5000 [04:33<1:20:09,  1.02s/it]  5%|â–Œ         | 266/5000 [04:34<1:18:58,  1.00s/it]                                                    {'loss': 2.0791, 'grad_norm': 0.1374279409646988, 'learning_rate': 0.0009661224489795919, 'epoch': 0.05}
  5%|â–Œ         | 266/5000 [04:34<1:18:58,  1.00s/it]  5%|â–Œ         | 267/5000 [04:35<1:19:09,  1.00s/it]                                                    {'loss': 3.9406, 'grad_norm': 4.750938415527344, 'learning_rate': 0.0009659183673469389, 'epoch': 0.05}
  5%|â–Œ         | 267/5000 [04:35<1:19:09,  1.00s/it]  5%|â–Œ         | 268/5000 [04:36<1:18:46,  1.00it/s]                                                    {'loss': 2.7144, 'grad_norm': 1.543444275856018, 'learning_rate': 0.0009657142857142858, 'epoch': 0.05}
  5%|â–Œ         | 268/5000 [04:36<1:18:46,  1.00it/s]  5%|â–Œ         | 269/5000 [04:37<1:18:57,  1.00s/it]                                                    {'loss': 2.0863, 'grad_norm': 1.7606184482574463, 'learning_rate': 0.0009655102040816327, 'epoch': 0.05}
  5%|â–Œ         | 269/5000 [04:37<1:18:57,  1.00s/it]  5%|â–Œ         | 270/5000 [04:38<1:19:12,  1.00s/it]                                                    {'loss': 2.147, 'grad_norm': 1.8787407875061035, 'learning_rate': 0.0009653061224489796, 'epoch': 0.05}
  5%|â–Œ         | 270/5000 [04:38<1:19:12,  1.00s/it]  5%|â–Œ         | 271/5000 [04:39<1:18:46,  1.00it/s]                                                    {'loss': 2.5691, 'grad_norm': 2.281041383743286, 'learning_rate': 0.0009651020408163266, 'epoch': 0.05}
  5%|â–Œ         | 271/5000 [04:39<1:18:46,  1.00it/s]  5%|â–Œ         | 272/5000 [04:40<1:18:50,  1.00s/it]                                                    {'loss': 1.4407, 'grad_norm': 0.5258229970932007, 'learning_rate': 0.0009648979591836735, 'epoch': 0.05}
  5%|â–Œ         | 272/5000 [04:40<1:18:50,  1.00s/it]  5%|â–Œ         | 273/5000 [04:41<1:18:53,  1.00s/it]                                                    {'loss': 2.0463, 'grad_norm': 1.3122609853744507, 'learning_rate': 0.0009646938775510204, 'epoch': 0.05}
  5%|â–Œ         | 273/5000 [04:41<1:18:53,  1.00s/it]  5%|â–Œ         | 274/5000 [04:42<1:18:45,  1.00it/s]                                                    {'loss': 2.5083, 'grad_norm': 3.4261727333068848, 'learning_rate': 0.0009644897959183674, 'epoch': 0.05}
  5%|â–Œ         | 274/5000 [04:42<1:18:45,  1.00it/s]  6%|â–Œ         | 275/5000 [04:43<1:18:48,  1.00s/it]                                                    {'loss': 1.9493, 'grad_norm': 1.967113971710205, 'learning_rate': 0.0009642857142857143, 'epoch': 0.06}
  6%|â–Œ         | 275/5000 [04:43<1:18:48,  1.00s/it]  6%|â–Œ         | 276/5000 [04:44<1:19:19,  1.01s/it]                                                    {'loss': 1.6085, 'grad_norm': 1.0404714345932007, 'learning_rate': 0.0009640816326530612, 'epoch': 0.06}
  6%|â–Œ         | 276/5000 [04:44<1:19:19,  1.01s/it]  6%|â–Œ         | 277/5000 [04:45<1:18:25,  1.00it/s]                                                    {'loss': 1.5469, 'grad_norm': 0.6293625235557556, 'learning_rate': 0.0009638775510204081, 'epoch': 0.06}
  6%|â–Œ         | 277/5000 [04:45<1:18:25,  1.00it/s]  6%|â–Œ         | 278/5000 [04:46<1:19:12,  1.01s/it]                                                    {'loss': 1.6838, 'grad_norm': 2.4620895385742188, 'learning_rate': 0.0009636734693877551, 'epoch': 0.06}
  6%|â–Œ         | 278/5000 [04:46<1:19:12,  1.01s/it]  6%|â–Œ         | 279/5000 [04:47<1:19:12,  1.01s/it]                                                    {'loss': 1.8285, 'grad_norm': 3.7354886531829834, 'learning_rate': 0.0009634693877551021, 'epoch': 0.06}
  6%|â–Œ         | 279/5000 [04:47<1:19:12,  1.01s/it]  6%|â–Œ         | 280/5000 [04:48<1:19:11,  1.01s/it]                                                    {'loss': 1.6308, 'grad_norm': 1.3164927959442139, 'learning_rate': 0.000963265306122449, 'epoch': 0.06}
  6%|â–Œ         | 280/5000 [04:48<1:19:11,  1.01s/it]  6%|â–Œ         | 281/5000 [04:49<1:18:35,  1.00it/s]                                                    {'loss': 1.7156, 'grad_norm': 0.2644059658050537, 'learning_rate': 0.000963061224489796, 'epoch': 0.06}
  6%|â–Œ         | 281/5000 [04:49<1:18:35,  1.00it/s]  6%|â–Œ         | 282/5000 [04:50<1:18:19,  1.00it/s]                                                    {'loss': 2.1441, 'grad_norm': 1.5420279502868652, 'learning_rate': 0.0009628571428571429, 'epoch': 0.06}
  6%|â–Œ         | 282/5000 [04:50<1:18:19,  1.00it/s]  6%|â–Œ         | 283/5000 [04:51<1:18:11,  1.01it/s]                                                    {'loss': 2.4711, 'grad_norm': 1.6483677625656128, 'learning_rate': 0.0009626530612244898, 'epoch': 0.06}
  6%|â–Œ         | 283/5000 [04:51<1:18:11,  1.01it/s]  6%|â–Œ         | 284/5000 [04:52<1:18:22,  1.00it/s]                                                    {'loss': 3.8529, 'grad_norm': 4.913821220397949, 'learning_rate': 0.0009624489795918368, 'epoch': 0.06}
  6%|â–Œ         | 284/5000 [04:52<1:18:22,  1.00it/s]  6%|â–Œ         | 285/5000 [04:53<1:18:27,  1.00it/s]                                                    {'loss': 2.9896, 'grad_norm': 2.803359270095825, 'learning_rate': 0.0009622448979591837, 'epoch': 0.06}
  6%|â–Œ         | 285/5000 [04:53<1:18:27,  1.00it/s]  6%|â–Œ         | 286/5000 [04:54<1:18:44,  1.00s/it]                                                    {'loss': 1.8962, 'grad_norm': 4.692381381988525, 'learning_rate': 0.0009620408163265306, 'epoch': 0.06}
  6%|â–Œ         | 286/5000 [04:54<1:18:44,  1.00s/it]  6%|â–Œ         | 287/5000 [04:55<1:18:49,  1.00s/it]                                                    {'loss': 1.7615, 'grad_norm': 1.1308470964431763, 'learning_rate': 0.0009618367346938776, 'epoch': 0.06}
  6%|â–Œ         | 287/5000 [04:55<1:18:49,  1.00s/it]  6%|â–Œ         | 288/5000 [04:57<1:20:17,  1.02s/it]                                                    {'loss': 1.6228, 'grad_norm': 1.5572426319122314, 'learning_rate': 0.0009616326530612245, 'epoch': 0.06}
  6%|â–Œ         | 288/5000 [04:57<1:20:17,  1.02s/it]  6%|â–Œ         | 289/5000 [04:58<1:20:04,  1.02s/it]                                                    {'loss': 2.7133, 'grad_norm': 1.5929595232009888, 'learning_rate': 0.0009614285714285714, 'epoch': 0.06}
  6%|â–Œ         | 289/5000 [04:58<1:20:04,  1.02s/it]  6%|â–Œ         | 290/5000 [04:59<1:19:51,  1.02s/it]                                                    {'loss': 1.8883, 'grad_norm': 1.6244760751724243, 'learning_rate': 0.0009612244897959183, 'epoch': 0.06}
  6%|â–Œ         | 290/5000 [04:59<1:19:51,  1.02s/it]  6%|â–Œ         | 291/5000 [05:00<1:18:59,  1.01s/it]                                                    {'loss': 1.7593, 'grad_norm': 0.565373420715332, 'learning_rate': 0.0009610204081632653, 'epoch': 0.06}
  6%|â–Œ         | 291/5000 [05:00<1:18:59,  1.01s/it]  6%|â–Œ         | 292/5000 [05:01<1:18:17,  1.00it/s]                                                    {'loss': 1.7214, 'grad_norm': 1.5234943628311157, 'learning_rate': 0.0009608163265306122, 'epoch': 0.06}
  6%|â–Œ         | 292/5000 [05:01<1:18:17,  1.00it/s]  6%|â–Œ         | 293/5000 [05:01<1:17:57,  1.01it/s]                                                    {'loss': 1.9627, 'grad_norm': 1.7971584796905518, 'learning_rate': 0.0009606122448979591, 'epoch': 0.06}
  6%|â–Œ         | 293/5000 [05:02<1:17:57,  1.01it/s]  6%|â–Œ         | 294/5000 [05:03<1:18:31,  1.00s/it]                                                    {'loss': 2.2793, 'grad_norm': 2.947685956954956, 'learning_rate': 0.0009604081632653062, 'epoch': 0.06}
  6%|â–Œ         | 294/5000 [05:03<1:18:31,  1.00s/it]  6%|â–Œ         | 295/5000 [05:04<1:18:26,  1.00s/it]                                                    {'loss': 1.4335, 'grad_norm': 1.2640272378921509, 'learning_rate': 0.0009602040816326531, 'epoch': 0.06}
  6%|â–Œ         | 295/5000 [05:04<1:18:26,  1.00s/it]  6%|â–Œ         | 296/5000 [05:05<1:18:23,  1.00it/s]                                                    {'loss': 2.35, 'grad_norm': 1.7319536209106445, 'learning_rate': 0.00096, 'epoch': 0.06}
  6%|â–Œ         | 296/5000 [05:05<1:18:23,  1.00it/s]  6%|â–Œ         | 297/5000 [05:06<1:18:59,  1.01s/it]                                                    {'loss': 2.271, 'grad_norm': 1.514769434928894, 'learning_rate': 0.000959795918367347, 'epoch': 0.06}
  6%|â–Œ         | 297/5000 [05:06<1:18:59,  1.01s/it]  6%|â–Œ         | 298/5000 [05:07<1:19:01,  1.01s/it]                                                    {'loss': 2.1301, 'grad_norm': 1.8819676637649536, 'learning_rate': 0.0009595918367346939, 'epoch': 0.06}
  6%|â–Œ         | 298/5000 [05:07<1:19:01,  1.01s/it]  6%|â–Œ         | 299/5000 [05:08<1:18:32,  1.00s/it]                                                    {'loss': 1.815, 'grad_norm': 1.6218897104263306, 'learning_rate': 0.0009593877551020408, 'epoch': 0.06}
  6%|â–Œ         | 299/5000 [05:08<1:18:32,  1.00s/it]  6%|â–Œ         | 300/5000 [05:09<1:18:24,  1.00s/it]                                                    {'loss': 1.4085, 'grad_norm': 1.2764558792114258, 'learning_rate': 0.0009591836734693877, 'epoch': 0.06}
  6%|â–Œ         | 300/5000 [05:09<1:18:24,  1.00s/it][2025-10-19 17:18:30,090] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 301/5000 [05:11<1:48:32,  1.39s/it]                                                    {'loss': 2.1193, 'grad_norm': 2.500519275665283, 'learning_rate': 0.0009589795918367347, 'epoch': 0.06}
  6%|â–Œ         | 301/5000 [05:11<1:48:32,  1.39s/it]  6%|â–Œ         | 302/5000 [05:12<1:39:26,  1.27s/it]                                                    {'loss': 1.6162, 'grad_norm': 1.1945154666900635, 'learning_rate': 0.0009587755102040816, 'epoch': 0.06}
  6%|â–Œ         | 302/5000 [05:12<1:39:26,  1.27s/it]  6%|â–Œ         | 303/5000 [05:13<1:33:09,  1.19s/it]                                                    {'loss': 1.8339, 'grad_norm': 3.267871379852295, 'learning_rate': 0.0009585714285714285, 'epoch': 0.06}
  6%|â–Œ         | 303/5000 [05:13<1:33:09,  1.19s/it]  6%|â–Œ         | 304/5000 [05:14<1:29:19,  1.14s/it]                                                    {'loss': 2.0878, 'grad_norm': 2.4326934814453125, 'learning_rate': 0.0009583673469387755, 'epoch': 0.06}
  6%|â–Œ         | 304/5000 [05:14<1:29:19,  1.14s/it]  6%|â–Œ         | 305/5000 [05:15<1:26:13,  1.10s/it]                                                    {'loss': 2.338, 'grad_norm': 2.436630964279175, 'learning_rate': 0.0009581632653061225, 'epoch': 0.06}
  6%|â–Œ         | 305/5000 [05:15<1:26:13,  1.10s/it]  6%|â–Œ         | 306/5000 [05:16<1:24:15,  1.08s/it]                                                    {'loss': 2.1696, 'grad_norm': 1.9322479963302612, 'learning_rate': 0.0009579591836734694, 'epoch': 0.06}
  6%|â–Œ         | 306/5000 [05:16<1:24:15,  1.08s/it]  6%|â–Œ         | 307/5000 [05:17<1:21:20,  1.04s/it]                                                    {'loss': 4.6781, 'grad_norm': 3.55771803855896, 'learning_rate': 0.0009577551020408164, 'epoch': 0.06}
  6%|â–Œ         | 307/5000 [05:17<1:21:20,  1.04s/it]  6%|â–Œ         | 308/5000 [05:18<1:27:48,  1.12s/it]                                                    {'loss': 2.3183, 'grad_norm': 1.360203742980957, 'learning_rate': 0.0009575510204081633, 'epoch': 0.06}
  6%|â–Œ         | 308/5000 [05:18<1:27:48,  1.12s/it]  6%|â–Œ         | 309/5000 [05:19<1:24:55,  1.09s/it]                                                    {'loss': 2.1138, 'grad_norm': 2.027139663696289, 'learning_rate': 0.0009573469387755102, 'epoch': 0.06}
  6%|â–Œ         | 309/5000 [05:19<1:24:55,  1.09s/it]  6%|â–Œ         | 310/5000 [05:20<1:24:47,  1.08s/it]                                                    {'loss': 1.8639, 'grad_norm': 2.607513904571533, 'learning_rate': 0.0009571428571428573, 'epoch': 0.06}
  6%|â–Œ         | 310/5000 [05:20<1:24:47,  1.08s/it]  6%|â–Œ         | 311/5000 [05:21<1:22:45,  1.06s/it]                                                    {'loss': 3.9366, 'grad_norm': 13.019646644592285, 'learning_rate': 0.0009569387755102042, 'epoch': 0.06}
  6%|â–Œ         | 311/5000 [05:21<1:22:45,  1.06s/it]  6%|â–Œ         | 312/5000 [05:22<1:21:54,  1.05s/it]                                                    {'loss': 3.3154, 'grad_norm': 3.4884326457977295, 'learning_rate': 0.0009567346938775511, 'epoch': 0.06}
  6%|â–Œ         | 312/5000 [05:22<1:21:54,  1.05s/it]  6%|â–‹         | 313/5000 [05:23<1:20:47,  1.03s/it]                                                    {'loss': 2.0062, 'grad_norm': 1.1303932666778564, 'learning_rate': 0.000956530612244898, 'epoch': 0.06}
  6%|â–‹         | 313/5000 [05:23<1:20:47,  1.03s/it]  6%|â–‹         | 314/5000 [05:24<1:20:09,  1.03s/it]                                                    {'loss': 1.8741, 'grad_norm': 1.2843159437179565, 'learning_rate': 0.000956326530612245, 'epoch': 0.06}
  6%|â–‹         | 314/5000 [05:24<1:20:09,  1.03s/it]  6%|â–‹         | 315/5000 [05:25<1:19:21,  1.02s/it]                                                    {'loss': 2.1172, 'grad_norm': 1.0931278467178345, 'learning_rate': 0.0009561224489795919, 'epoch': 0.06}
  6%|â–‹         | 315/5000 [05:25<1:19:21,  1.02s/it]  6%|â–‹         | 316/5000 [05:26<1:19:18,  1.02s/it]                                                    {'loss': 2.4665, 'grad_norm': 2.3961853981018066, 'learning_rate': 0.0009559183673469388, 'epoch': 0.06}
  6%|â–‹         | 316/5000 [05:26<1:19:18,  1.02s/it]  6%|â–‹         | 317/5000 [05:27<1:18:23,  1.00s/it]                                                    {'loss': 2.925, 'grad_norm': 4.176204204559326, 'learning_rate': 0.0009557142857142858, 'epoch': 0.06}
  6%|â–‹         | 317/5000 [05:27<1:18:23,  1.00s/it]  6%|â–‹         | 318/5000 [05:28<1:18:20,  1.00s/it]                                                    {'loss': 1.4899, 'grad_norm': 0.7384371161460876, 'learning_rate': 0.0009555102040816327, 'epoch': 0.06}
  6%|â–‹         | 318/5000 [05:28<1:18:20,  1.00s/it]  6%|â–‹         | 319/5000 [05:29<1:19:09,  1.01s/it]                                                    {'loss': 1.5514, 'grad_norm': 1.728188157081604, 'learning_rate': 0.0009553061224489796, 'epoch': 0.06}
  6%|â–‹         | 319/5000 [05:29<1:19:09,  1.01s/it]  6%|â–‹         | 320/5000 [05:30<1:19:08,  1.01s/it]                                                    {'loss': 1.8173, 'grad_norm': 1.0293186902999878, 'learning_rate': 0.0009551020408163265, 'epoch': 0.06}
  6%|â–‹         | 320/5000 [05:30<1:19:08,  1.01s/it]  6%|â–‹         | 321/5000 [05:31<1:19:25,  1.02s/it]                                                    {'loss': 2.5079, 'grad_norm': 1.5699939727783203, 'learning_rate': 0.0009548979591836735, 'epoch': 0.06}
  6%|â–‹         | 321/5000 [05:31<1:19:25,  1.02s/it]  6%|â–‹         | 322/5000 [05:32<1:19:11,  1.02s/it]                                                    {'loss': 2.4836, 'grad_norm': 1.7104369401931763, 'learning_rate': 0.0009546938775510204, 'epoch': 0.06}
  6%|â–‹         | 322/5000 [05:32<1:19:11,  1.02s/it]  6%|â–‹         | 323/5000 [05:33<1:19:46,  1.02s/it]                                                    {'loss': 1.5338, 'grad_norm': 1.1131093502044678, 'learning_rate': 0.0009544897959183673, 'epoch': 0.06}
  6%|â–‹         | 323/5000 [05:33<1:19:46,  1.02s/it]  6%|â–‹         | 324/5000 [05:34<1:19:35,  1.02s/it]                                                    {'loss': 1.6164, 'grad_norm': 0.5851843953132629, 'learning_rate': 0.0009542857142857143, 'epoch': 0.06}
  6%|â–‹         | 324/5000 [05:34<1:19:35,  1.02s/it]  6%|â–‹         | 325/5000 [05:35<1:19:42,  1.02s/it]                                                    {'loss': 1.7264, 'grad_norm': 0.92362380027771, 'learning_rate': 0.0009540816326530613, 'epoch': 0.07}
  6%|â–‹         | 325/5000 [05:35<1:19:42,  1.02s/it]  7%|â–‹         | 326/5000 [05:36<1:19:20,  1.02s/it]                                                    {'loss': 1.625, 'grad_norm': 1.105610728263855, 'learning_rate': 0.0009538775510204082, 'epoch': 0.07}
  7%|â–‹         | 326/5000 [05:36<1:19:20,  1.02s/it]  7%|â–‹         | 327/5000 [05:37<1:19:04,  1.02s/it]                                                    {'loss': 1.7008, 'grad_norm': 1.883296012878418, 'learning_rate': 0.0009536734693877552, 'epoch': 0.07}
  7%|â–‹         | 327/5000 [05:37<1:19:04,  1.02s/it]  7%|â–‹         | 328/5000 [05:38<1:18:43,  1.01s/it]                                                    {'loss': 1.5509, 'grad_norm': 1.1677972078323364, 'learning_rate': 0.0009534693877551021, 'epoch': 0.07}
  7%|â–‹         | 328/5000 [05:38<1:18:43,  1.01s/it]  7%|â–‹         | 329/5000 [05:39<1:18:34,  1.01s/it]                                                    {'loss': 1.5682, 'grad_norm': 0.90358966588974, 'learning_rate': 0.000953265306122449, 'epoch': 0.07}
  7%|â–‹         | 329/5000 [05:39<1:18:34,  1.01s/it]  7%|â–‹         | 330/5000 [05:40<1:18:38,  1.01s/it]                                                    {'loss': 1.6176, 'grad_norm': 2.0490357875823975, 'learning_rate': 0.000953061224489796, 'epoch': 0.07}
  7%|â–‹         | 330/5000 [05:40<1:18:38,  1.01s/it]  7%|â–‹         | 331/5000 [05:41<1:18:43,  1.01s/it]                                                    {'loss': 4.526, 'grad_norm': 4.455809116363525, 'learning_rate': 0.0009528571428571429, 'epoch': 0.07}
  7%|â–‹         | 331/5000 [05:41<1:18:43,  1.01s/it]  7%|â–‹         | 332/5000 [05:42<1:18:46,  1.01s/it]                                                    {'loss': 2.5105, 'grad_norm': 2.6039721965789795, 'learning_rate': 0.0009526530612244898, 'epoch': 0.07}
  7%|â–‹         | 332/5000 [05:42<1:18:46,  1.01s/it]  7%|â–‹         | 333/5000 [05:43<1:18:30,  1.01s/it]                                                    {'loss': 1.5254, 'grad_norm': 0.1848418116569519, 'learning_rate': 0.0009524489795918367, 'epoch': 0.07}
  7%|â–‹         | 333/5000 [05:43<1:18:30,  1.01s/it]  7%|â–‹         | 334/5000 [05:44<1:18:29,  1.01s/it]                                                    {'loss': 2.1541, 'grad_norm': 1.9078078269958496, 'learning_rate': 0.0009522448979591837, 'epoch': 0.07}
  7%|â–‹         | 334/5000 [05:45<1:18:29,  1.01s/it]  7%|â–‹         | 335/5000 [05:46<1:19:38,  1.02s/it]                                                    {'loss': 2.3098, 'grad_norm': 2.5811867713928223, 'learning_rate': 0.0009520408163265306, 'epoch': 0.07}
  7%|â–‹         | 335/5000 [05:46<1:19:38,  1.02s/it]  7%|â–‹         | 336/5000 [05:47<1:19:58,  1.03s/it]                                                    {'loss': 2.6091, 'grad_norm': 4.479647159576416, 'learning_rate': 0.0009518367346938775, 'epoch': 0.07}
  7%|â–‹         | 336/5000 [05:47<1:19:58,  1.03s/it]  7%|â–‹         | 337/5000 [05:48<1:19:28,  1.02s/it]                                                    {'loss': 2.1484, 'grad_norm': 3.4212911128997803, 'learning_rate': 0.0009516326530612245, 'epoch': 0.07}
  7%|â–‹         | 337/5000 [05:48<1:19:28,  1.02s/it]  7%|â–‹         | 338/5000 [05:49<1:21:17,  1.05s/it]                                                    {'loss': 1.8294, 'grad_norm': 1.412315011024475, 'learning_rate': 0.0009514285714285714, 'epoch': 0.07}
  7%|â–‹         | 338/5000 [05:49<1:21:17,  1.05s/it]  7%|â–‹         | 339/5000 [05:50<1:20:50,  1.04s/it]                                                    {'loss': 1.4751, 'grad_norm': 0.43728432059288025, 'learning_rate': 0.0009512244897959183, 'epoch': 0.07}
  7%|â–‹         | 339/5000 [05:50<1:20:50,  1.04s/it]  7%|â–‹         | 340/5000 [05:51<1:20:47,  1.04s/it]                                                    {'loss': 1.9778, 'grad_norm': 2.086574077606201, 'learning_rate': 0.0009510204081632652, 'epoch': 0.07}
  7%|â–‹         | 340/5000 [05:51<1:20:47,  1.04s/it]  7%|â–‹         | 341/5000 [05:52<1:20:13,  1.03s/it]                                                    {'loss': 2.451, 'grad_norm': 1.9424796104431152, 'learning_rate': 0.0009508163265306123, 'epoch': 0.07}
  7%|â–‹         | 341/5000 [05:52<1:20:13,  1.03s/it]  7%|â–‹         | 342/5000 [05:53<1:19:15,  1.02s/it]                                                    {'loss': 2.2991, 'grad_norm': 1.3957537412643433, 'learning_rate': 0.0009506122448979592, 'epoch': 0.07}
  7%|â–‹         | 342/5000 [05:53<1:19:15,  1.02s/it]  7%|â–‹         | 343/5000 [05:54<1:18:17,  1.01s/it]                                                    {'loss': 1.5875, 'grad_norm': 0.6802746653556824, 'learning_rate': 0.0009504081632653061, 'epoch': 0.07}
  7%|â–‹         | 343/5000 [05:54<1:18:17,  1.01s/it]  7%|â–‹         | 344/5000 [05:55<1:18:06,  1.01s/it]                                                    {'loss': 1.5, 'grad_norm': 0.21403104066848755, 'learning_rate': 0.0009502040816326531, 'epoch': 0.07}
  7%|â–‹         | 344/5000 [05:55<1:18:06,  1.01s/it]  7%|â–‹         | 345/5000 [05:56<1:18:24,  1.01s/it]                                                    {'loss': 2.4613, 'grad_norm': 1.6753137111663818, 'learning_rate': 0.00095, 'epoch': 0.07}
  7%|â–‹         | 345/5000 [05:56<1:18:24,  1.01s/it]  7%|â–‹         | 346/5000 [05:57<1:20:06,  1.03s/it]                                                    {'loss': 1.9102, 'grad_norm': 0.9728277921676636, 'learning_rate': 0.0009497959183673469, 'epoch': 0.07}
  7%|â–‹         | 346/5000 [05:57<1:20:06,  1.03s/it]  7%|â–‹         | 347/5000 [05:58<1:19:25,  1.02s/it]                                                    {'loss': 2.141, 'grad_norm': 4.028161525726318, 'learning_rate': 0.0009495918367346939, 'epoch': 0.07}
  7%|â–‹         | 347/5000 [05:58<1:19:25,  1.02s/it]  7%|â–‹         | 348/5000 [05:59<1:19:42,  1.03s/it]                                                    {'loss': 2.2063, 'grad_norm': 1.9510120153427124, 'learning_rate': 0.0009493877551020408, 'epoch': 0.07}
  7%|â–‹         | 348/5000 [05:59<1:19:42,  1.03s/it]  7%|â–‹         | 349/5000 [06:00<1:19:26,  1.02s/it]                                                    {'loss': 2.1053, 'grad_norm': 1.6888302564620972, 'learning_rate': 0.0009491836734693877, 'epoch': 0.07}
  7%|â–‹         | 349/5000 [06:00<1:19:26,  1.02s/it]  7%|â–‹         | 350/5000 [06:01<1:18:06,  1.01s/it]                                                    {'loss': 2.891, 'grad_norm': 2.4277801513671875, 'learning_rate': 0.0009489795918367348, 'epoch': 0.07}
  7%|â–‹         | 350/5000 [06:01<1:18:06,  1.01s/it][2025-10-19 17:19:22,453] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 351/5000 [06:03<1:45:57,  1.37s/it]                                                    {'loss': 1.4843, 'grad_norm': 1.13492751121521, 'learning_rate': 0.0009487755102040817, 'epoch': 0.07}
  7%|â–‹         | 351/5000 [06:03<1:45:57,  1.37s/it]  7%|â–‹         | 352/5000 [06:04<1:37:59,  1.26s/it]                                                    {'loss': 1.4663, 'grad_norm': 0.5374586582183838, 'learning_rate': 0.0009485714285714286, 'epoch': 0.07}
  7%|â–‹         | 352/5000 [06:04<1:37:59,  1.26s/it]  7%|â–‹         | 353/5000 [06:05<1:31:18,  1.18s/it]                                                    {'loss': 1.6304, 'grad_norm': 0.7511805295944214, 'learning_rate': 0.0009483673469387755, 'epoch': 0.07}
  7%|â–‹         | 353/5000 [06:05<1:31:18,  1.18s/it]  7%|â–‹         | 354/5000 [06:06<1:27:24,  1.13s/it]                                                    {'loss': 1.6692, 'grad_norm': 1.0623385906219482, 'learning_rate': 0.0009481632653061225, 'epoch': 0.07}
  7%|â–‹         | 354/5000 [06:06<1:27:24,  1.13s/it]  7%|â–‹         | 355/5000 [06:07<1:23:55,  1.08s/it]                                                    {'loss': 1.5974, 'grad_norm': 0.9460269808769226, 'learning_rate': 0.0009479591836734694, 'epoch': 0.07}
  7%|â–‹         | 355/5000 [06:07<1:23:55,  1.08s/it]  7%|â–‹         | 356/5000 [06:08<1:21:52,  1.06s/it]                                                    {'loss': 1.6962, 'grad_norm': 0.9824671745300293, 'learning_rate': 0.0009477551020408164, 'epoch': 0.07}
  7%|â–‹         | 356/5000 [06:08<1:21:52,  1.06s/it]  7%|â–‹         | 357/5000 [06:09<1:20:54,  1.05s/it]                                                    {'loss': 2.1184, 'grad_norm': 1.8691977262496948, 'learning_rate': 0.0009475510204081634, 'epoch': 0.07}
  7%|â–‹         | 357/5000 [06:09<1:20:54,  1.05s/it]  7%|â–‹         | 358/5000 [06:10<1:19:50,  1.03s/it]                                                    {'loss': 1.533, 'grad_norm': 0.9041748046875, 'learning_rate': 0.0009473469387755103, 'epoch': 0.07}
  7%|â–‹         | 358/5000 [06:10<1:19:50,  1.03s/it]  7%|â–‹         | 359/5000 [06:11<1:19:01,  1.02s/it]                                                    {'loss': 1.6172, 'grad_norm': 1.3858188390731812, 'learning_rate': 0.0009471428571428572, 'epoch': 0.07}
  7%|â–‹         | 359/5000 [06:11<1:19:01,  1.02s/it]  7%|â–‹         | 360/5000 [06:12<1:19:28,  1.03s/it]                                                    {'loss': 1.7143, 'grad_norm': 0.08141213655471802, 'learning_rate': 0.0009469387755102042, 'epoch': 0.07}
  7%|â–‹         | 360/5000 [06:12<1:19:28,  1.03s/it]  7%|â–‹         | 361/5000 [06:13<1:19:30,  1.03s/it]                                                    {'loss': 1.5677, 'grad_norm': 0.8704814314842224, 'learning_rate': 0.0009467346938775511, 'epoch': 0.07}
  7%|â–‹         | 361/5000 [06:13<1:19:30,  1.03s/it]  7%|â–‹         | 362/5000 [06:14<1:18:39,  1.02s/it]                                                    {'loss': 1.5681, 'grad_norm': 1.1229459047317505, 'learning_rate': 0.000946530612244898, 'epoch': 0.07}
  7%|â–‹         | 362/5000 [06:14<1:18:39,  1.02s/it]  7%|â–‹         | 363/5000 [06:15<1:17:52,  1.01s/it]                                                    {'loss': 2.0286, 'grad_norm': 2.4446377754211426, 'learning_rate': 0.0009463265306122449, 'epoch': 0.07}
  7%|â–‹         | 363/5000 [06:15<1:17:52,  1.01s/it]  7%|â–‹         | 364/5000 [06:16<1:18:54,  1.02s/it]                                                    {'loss': 1.7281, 'grad_norm': 0.9959003925323486, 'learning_rate': 0.0009461224489795919, 'epoch': 0.07}
  7%|â–‹         | 364/5000 [06:16<1:18:54,  1.02s/it]  7%|â–‹         | 365/5000 [06:17<1:18:52,  1.02s/it]                                                    {'loss': 2.1045, 'grad_norm': 1.2622134685516357, 'learning_rate': 0.0009459183673469388, 'epoch': 0.07}
  7%|â–‹         | 365/5000 [06:17<1:18:52,  1.02s/it]  7%|â–‹         | 366/5000 [06:18<1:20:09,  1.04s/it]                                                    {'loss': 2.3584, 'grad_norm': 2.1959774494171143, 'learning_rate': 0.0009457142857142857, 'epoch': 0.07}
  7%|â–‹         | 366/5000 [06:18<1:20:09,  1.04s/it]  7%|â–‹         | 367/5000 [06:19<1:18:59,  1.02s/it]                                                    {'loss': 1.6798, 'grad_norm': 0.9940783381462097, 'learning_rate': 0.0009455102040816327, 'epoch': 0.07}
  7%|â–‹         | 367/5000 [06:19<1:18:59,  1.02s/it]  7%|â–‹         | 368/5000 [06:20<1:18:54,  1.02s/it]                                                    {'loss': 1.5584, 'grad_norm': 0.2380017787218094, 'learning_rate': 0.0009453061224489796, 'epoch': 0.07}
  7%|â–‹         | 368/5000 [06:20<1:18:54,  1.02s/it]  7%|â–‹         | 369/5000 [06:21<1:18:42,  1.02s/it]                                                    {'loss': 1.7137, 'grad_norm': 1.0698429346084595, 'learning_rate': 0.0009451020408163265, 'epoch': 0.07}
  7%|â–‹         | 369/5000 [06:21<1:18:42,  1.02s/it]  7%|â–‹         | 370/5000 [06:22<1:18:00,  1.01s/it]                                                    {'loss': 1.6353, 'grad_norm': 1.0301560163497925, 'learning_rate': 0.0009448979591836734, 'epoch': 0.07}
  7%|â–‹         | 370/5000 [06:22<1:18:00,  1.01s/it]  7%|â–‹         | 371/5000 [06:23<1:18:35,  1.02s/it]                                                    {'loss': 1.7879, 'grad_norm': 2.0278825759887695, 'learning_rate': 0.0009446938775510204, 'epoch': 0.07}
  7%|â–‹         | 371/5000 [06:23<1:18:35,  1.02s/it]  7%|â–‹         | 372/5000 [06:24<1:19:07,  1.03s/it]                                                    {'loss': 2.3144, 'grad_norm': 3.5513741970062256, 'learning_rate': 0.0009444897959183674, 'epoch': 0.07}
  7%|â–‹         | 372/5000 [06:24<1:19:07,  1.03s/it]  7%|â–‹         | 373/5000 [06:25<1:18:21,  1.02s/it]                                                    {'loss': 1.8786, 'grad_norm': 2.912067174911499, 'learning_rate': 0.0009442857142857143, 'epoch': 0.07}
  7%|â–‹         | 373/5000 [06:25<1:18:21,  1.02s/it]  7%|â–‹         | 374/5000 [06:26<1:18:02,  1.01s/it]                                                    {'loss': 1.6032, 'grad_norm': 0.8884400725364685, 'learning_rate': 0.0009440816326530613, 'epoch': 0.07}
  7%|â–‹         | 374/5000 [06:26<1:18:02,  1.01s/it]  8%|â–Š         | 375/5000 [06:27<1:18:22,  1.02s/it]                                                    {'loss': 2.0782, 'grad_norm': 3.699737071990967, 'learning_rate': 0.0009438775510204082, 'epoch': 0.07}
  8%|â–Š         | 375/5000 [06:27<1:18:22,  1.02s/it]  8%|â–Š         | 376/5000 [06:28<1:18:09,  1.01s/it]                                                    {'loss': 2.1584, 'grad_norm': 1.513879418373108, 'learning_rate': 0.0009436734693877551, 'epoch': 0.08}
  8%|â–Š         | 376/5000 [06:28<1:18:09,  1.01s/it]  8%|â–Š         | 377/5000 [06:29<1:18:05,  1.01s/it]                                                    {'loss': 1.9889, 'grad_norm': 0.8663556575775146, 'learning_rate': 0.0009434693877551021, 'epoch': 0.08}
  8%|â–Š         | 377/5000 [06:29<1:18:05,  1.01s/it]  8%|â–Š         | 378/5000 [06:30<1:18:39,  1.02s/it]                                                    {'loss': 1.5495, 'grad_norm': 0.5732849836349487, 'learning_rate': 0.000943265306122449, 'epoch': 0.08}
  8%|â–Š         | 378/5000 [06:30<1:18:39,  1.02s/it]  8%|â–Š         | 379/5000 [06:31<1:17:57,  1.01s/it]                                                    {'loss': 2.4319, 'grad_norm': 1.920133352279663, 'learning_rate': 0.0009430612244897959, 'epoch': 0.08}
  8%|â–Š         | 379/5000 [06:31<1:17:57,  1.01s/it]  8%|â–Š         | 380/5000 [06:32<1:17:21,  1.00s/it]                                                    {'loss': 1.6898, 'grad_norm': 0.6978053450584412, 'learning_rate': 0.0009428571428571429, 'epoch': 0.08}
  8%|â–Š         | 380/5000 [06:32<1:17:21,  1.00s/it]  8%|â–Š         | 381/5000 [06:33<1:17:25,  1.01s/it]                                                    {'loss': 1.9997, 'grad_norm': 1.1848522424697876, 'learning_rate': 0.0009426530612244898, 'epoch': 0.08}
  8%|â–Š         | 381/5000 [06:33<1:17:25,  1.01s/it]  8%|â–Š         | 382/5000 [06:34<1:17:12,  1.00s/it]                                                    {'loss': 2.4105, 'grad_norm': 1.2874423265457153, 'learning_rate': 0.0009424489795918367, 'epoch': 0.08}
  8%|â–Š         | 382/5000 [06:34<1:17:12,  1.00s/it]  8%|â–Š         | 383/5000 [06:35<1:16:41,  1.00it/s]                                                    {'loss': 1.5411, 'grad_norm': 0.9663140177726746, 'learning_rate': 0.0009422448979591836, 'epoch': 0.08}
  8%|â–Š         | 383/5000 [06:35<1:16:41,  1.00it/s]  8%|â–Š         | 384/5000 [06:36<1:16:49,  1.00it/s]                                                    {'loss': 1.5534, 'grad_norm': 0.44696083664894104, 'learning_rate': 0.0009420408163265306, 'epoch': 0.08}
  8%|â–Š         | 384/5000 [06:36<1:16:49,  1.00it/s]  8%|â–Š         | 385/5000 [06:37<1:16:47,  1.00it/s]                                                    {'loss': 1.6503, 'grad_norm': 1.030275821685791, 'learning_rate': 0.0009418367346938775, 'epoch': 0.08}
  8%|â–Š         | 385/5000 [06:37<1:16:47,  1.00it/s]  8%|â–Š         | 386/5000 [06:38<1:17:16,  1.00s/it]                                                    {'loss': 1.5176, 'grad_norm': 0.653575599193573, 'learning_rate': 0.0009416326530612244, 'epoch': 0.08}
  8%|â–Š         | 386/5000 [06:38<1:17:16,  1.00s/it]  8%|â–Š         | 387/5000 [06:39<1:17:08,  1.00s/it]                                                    {'loss': 3.0086, 'grad_norm': 1.826830506324768, 'learning_rate': 0.0009414285714285715, 'epoch': 0.08}
  8%|â–Š         | 387/5000 [06:39<1:17:08,  1.00s/it]  8%|â–Š         | 388/5000 [06:40<1:17:15,  1.01s/it]                                                    {'loss': 2.0629, 'grad_norm': 1.3949605226516724, 'learning_rate': 0.0009412244897959184, 'epoch': 0.08}
  8%|â–Š         | 388/5000 [06:40<1:17:15,  1.01s/it]  8%|â–Š         | 389/5000 [06:42<1:17:48,  1.01s/it]                                                    {'loss': 2.0074, 'grad_norm': 1.8467023372650146, 'learning_rate': 0.0009410204081632653, 'epoch': 0.08}
  8%|â–Š         | 389/5000 [06:42<1:17:48,  1.01s/it]  8%|â–Š         | 390/5000 [06:43<1:17:22,  1.01s/it]                                                    {'loss': 2.4298, 'grad_norm': 2.418546438217163, 'learning_rate': 0.0009408163265306123, 'epoch': 0.08}
  8%|â–Š         | 390/5000 [06:43<1:17:22,  1.01s/it]  8%|â–Š         | 391/5000 [06:43<1:16:58,  1.00s/it]                                                    {'loss': 1.7121, 'grad_norm': 0.13551847636699677, 'learning_rate': 0.0009406122448979592, 'epoch': 0.08}
  8%|â–Š         | 391/5000 [06:43<1:16:58,  1.00s/it]  8%|â–Š         | 392/5000 [06:44<1:16:52,  1.00s/it]                                                    {'loss': 1.5823, 'grad_norm': 0.9555397629737854, 'learning_rate': 0.0009404081632653061, 'epoch': 0.08}
  8%|â–Š         | 392/5000 [06:44<1:16:52,  1.00s/it]  8%|â–Š         | 393/5000 [06:45<1:16:41,  1.00it/s]                                                    {'loss': 1.4378, 'grad_norm': 0.3466716706752777, 'learning_rate': 0.000940204081632653, 'epoch': 0.08}
  8%|â–Š         | 393/5000 [06:45<1:16:41,  1.00it/s]  8%|â–Š         | 394/5000 [06:46<1:16:14,  1.01it/s]                                                    {'loss': 2.1264, 'grad_norm': 1.25346040725708, 'learning_rate': 0.00094, 'epoch': 0.08}
  8%|â–Š         | 394/5000 [06:46<1:16:14,  1.01it/s]  8%|â–Š         | 395/5000 [06:47<1:16:18,  1.01it/s]                                                    {'loss': 1.7257, 'grad_norm': 0.18193629384040833, 'learning_rate': 0.000939795918367347, 'epoch': 0.08}
  8%|â–Š         | 395/5000 [06:47<1:16:18,  1.01it/s]  8%|â–Š         | 396/5000 [06:48<1:16:16,  1.01it/s]                                                    {'loss': 2.4487, 'grad_norm': 1.2964303493499756, 'learning_rate': 0.0009395918367346939, 'epoch': 0.08}
  8%|â–Š         | 396/5000 [06:48<1:16:16,  1.01it/s]  8%|â–Š         | 397/5000 [06:49<1:16:27,  1.00it/s]                                                    {'loss': 2.0526, 'grad_norm': 1.7379355430603027, 'learning_rate': 0.0009393877551020409, 'epoch': 0.08}
  8%|â–Š         | 397/5000 [06:49<1:16:27,  1.00it/s]  8%|â–Š         | 398/5000 [06:50<1:16:00,  1.01it/s]                                                    {'loss': 1.4396, 'grad_norm': 0.15665872395038605, 'learning_rate': 0.0009391836734693878, 'epoch': 0.08}
  8%|â–Š         | 398/5000 [06:50<1:16:00,  1.01it/s]  8%|â–Š         | 399/5000 [06:51<1:16:03,  1.01it/s]                                                    {'loss': 1.6646, 'grad_norm': 0.9736917018890381, 'learning_rate': 0.0009389795918367347, 'epoch': 0.08}
  8%|â–Š         | 399/5000 [06:51<1:16:03,  1.01it/s]  8%|â–Š         | 400/5000 [06:52<1:16:39,  1.00it/s]                                                    {'loss': 1.566, 'grad_norm': 1.3760484457015991, 'learning_rate': 0.0009387755102040817, 'epoch': 0.08}
  8%|â–Š         | 400/5000 [06:52<1:16:39,  1.00it/s][2025-10-19 17:20:14,004] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 401/5000 [06:55<1:49:10,  1.42s/it]                                                    {'loss': 2.2722, 'grad_norm': 1.4264503717422485, 'learning_rate': 0.0009385714285714286, 'epoch': 0.08}
  8%|â–Š         | 401/5000 [06:55<1:49:10,  1.42s/it]  8%|â–Š         | 402/5000 [06:56<1:39:07,  1.29s/it]                                                    {'loss': 1.8044, 'grad_norm': 1.3769530057907104, 'learning_rate': 0.0009383673469387755, 'epoch': 0.08}
  8%|â–Š         | 402/5000 [06:56<1:39:07,  1.29s/it]  8%|â–Š         | 403/5000 [06:57<1:33:46,  1.22s/it]                                                    {'loss': 1.4663, 'grad_norm': 0.8050287961959839, 'learning_rate': 0.0009381632653061226, 'epoch': 0.08}
  8%|â–Š         | 403/5000 [06:57<1:33:46,  1.22s/it]  8%|â–Š         | 404/5000 [06:58<1:28:57,  1.16s/it]                                                    {'loss': 1.5337, 'grad_norm': 1.169142723083496, 'learning_rate': 0.0009379591836734695, 'epoch': 0.08}
  8%|â–Š         | 404/5000 [06:58<1:28:57,  1.16s/it]  8%|â–Š         | 405/5000 [06:59<1:25:07,  1.11s/it]                                                    {'loss': 2.3659, 'grad_norm': 1.1553049087524414, 'learning_rate': 0.0009377551020408164, 'epoch': 0.08}
  8%|â–Š         | 405/5000 [06:59<1:25:07,  1.11s/it]  8%|â–Š         | 406/5000 [07:00<1:22:07,  1.07s/it]                                                    {'loss': 1.4835, 'grad_norm': 0.329606831073761, 'learning_rate': 0.0009375510204081633, 'epoch': 0.08}
  8%|â–Š         | 406/5000 [07:00<1:22:07,  1.07s/it]  8%|â–Š         | 407/5000 [07:01<1:20:45,  1.05s/it]                                                    {'loss': 2.5375, 'grad_norm': 2.356994867324829, 'learning_rate': 0.0009373469387755103, 'epoch': 0.08}
  8%|â–Š         | 407/5000 [07:01<1:20:45,  1.05s/it]  8%|â–Š         | 408/5000 [07:02<1:19:09,  1.03s/it]                                                    {'loss': 1.5578, 'grad_norm': 0.423870712518692, 'learning_rate': 0.0009371428571428572, 'epoch': 0.08}
  8%|â–Š         | 408/5000 [07:02<1:19:09,  1.03s/it]  8%|â–Š         | 409/5000 [07:03<1:19:19,  1.04s/it]                                                    {'loss': 1.6851, 'grad_norm': 1.229405403137207, 'learning_rate': 0.0009369387755102041, 'epoch': 0.08}
  8%|â–Š         | 409/5000 [07:03<1:19:19,  1.04s/it]  8%|â–Š         | 410/5000 [07:04<1:19:11,  1.04s/it]                                                    {'loss': 2.7286, 'grad_norm': 2.2561147212982178, 'learning_rate': 0.0009367346938775511, 'epoch': 0.08}
  8%|â–Š         | 410/5000 [07:04<1:19:11,  1.04s/it]  8%|â–Š         | 411/5000 [07:05<1:18:27,  1.03s/it]                                                    {'loss': 1.4476, 'grad_norm': 0.490371435880661, 'learning_rate': 0.000936530612244898, 'epoch': 0.08}
  8%|â–Š         | 411/5000 [07:05<1:18:27,  1.03s/it]  8%|â–Š         | 412/5000 [07:06<1:18:07,  1.02s/it]                                                    {'loss': 1.588, 'grad_norm': 2.0480780601501465, 'learning_rate': 0.0009363265306122449, 'epoch': 0.08}
  8%|â–Š         | 412/5000 [07:06<1:18:07,  1.02s/it]  8%|â–Š         | 413/5000 [07:07<1:18:27,  1.03s/it]                                                    {'loss': 1.9666, 'grad_norm': 1.6183050870895386, 'learning_rate': 0.0009361224489795918, 'epoch': 0.08}
  8%|â–Š         | 413/5000 [07:07<1:18:27,  1.03s/it]  8%|â–Š         | 414/5000 [07:08<1:19:51,  1.04s/it]                                                    {'loss': 1.7016, 'grad_norm': 1.0205847024917603, 'learning_rate': 0.0009359183673469388, 'epoch': 0.08}
  8%|â–Š         | 414/5000 [07:08<1:19:51,  1.04s/it]  8%|â–Š         | 415/5000 [07:09<1:20:34,  1.05s/it]                                                    {'loss': 1.5577, 'grad_norm': 0.6628273129463196, 'learning_rate': 0.0009357142857142857, 'epoch': 0.08}
  8%|â–Š         | 415/5000 [07:09<1:20:34,  1.05s/it]  8%|â–Š         | 416/5000 [07:10<1:20:04,  1.05s/it]                                                    {'loss': 1.7625, 'grad_norm': 0.7896495461463928, 'learning_rate': 0.0009355102040816326, 'epoch': 0.08}
  8%|â–Š         | 416/5000 [07:10<1:20:04,  1.05s/it]  8%|â–Š         | 417/5000 [07:11<1:19:56,  1.05s/it]                                                    {'loss': 1.5189, 'grad_norm': 0.43856164813041687, 'learning_rate': 0.0009353061224489796, 'epoch': 0.08}
  8%|â–Š         | 417/5000 [07:11<1:19:56,  1.05s/it]  8%|â–Š         | 418/5000 [07:12<1:20:04,  1.05s/it]                                                    {'loss': 1.8217, 'grad_norm': 1.097067952156067, 'learning_rate': 0.0009351020408163266, 'epoch': 0.08}
  8%|â–Š         | 418/5000 [07:12<1:20:04,  1.05s/it]  8%|â–Š         | 419/5000 [07:13<1:19:44,  1.04s/it]                                                    {'loss': 2.106, 'grad_norm': 1.9273511171340942, 'learning_rate': 0.0009348979591836735, 'epoch': 0.08}
  8%|â–Š         | 419/5000 [07:13<1:19:44,  1.04s/it]  8%|â–Š         | 420/5000 [07:14<1:19:37,  1.04s/it]                                                    {'loss': 1.5274, 'grad_norm': 0.8592933416366577, 'learning_rate': 0.0009346938775510205, 'epoch': 0.08}
  8%|â–Š         | 420/5000 [07:14<1:19:37,  1.04s/it]  8%|â–Š         | 421/5000 [07:15<1:20:10,  1.05s/it]                                                    {'loss': 1.6302, 'grad_norm': 1.2656491994857788, 'learning_rate': 0.0009344897959183674, 'epoch': 0.08}
  8%|â–Š         | 421/5000 [07:15<1:20:10,  1.05s/it]  8%|â–Š         | 422/5000 [07:16<1:19:32,  1.04s/it]                                                    {'loss': 1.6155, 'grad_norm': 1.239307165145874, 'learning_rate': 0.0009342857142857143, 'epoch': 0.08}
  8%|â–Š         | 422/5000 [07:17<1:19:32,  1.04s/it]  8%|â–Š         | 423/5000 [07:18<1:19:03,  1.04s/it]                                                    {'loss': 1.5733, 'grad_norm': 1.5502647161483765, 'learning_rate': 0.0009340816326530612, 'epoch': 0.08}
  8%|â–Š         | 423/5000 [07:18<1:19:03,  1.04s/it]  8%|â–Š         | 424/5000 [07:19<1:18:46,  1.03s/it]                                                    {'loss': 1.4363, 'grad_norm': 0.38195663690567017, 'learning_rate': 0.0009338775510204082, 'epoch': 0.08}
  8%|â–Š         | 424/5000 [07:19<1:18:46,  1.03s/it]  8%|â–Š         | 425/5000 [07:20<1:18:05,  1.02s/it]                                                    {'loss': 1.8942, 'grad_norm': 1.3977038860321045, 'learning_rate': 0.0009336734693877551, 'epoch': 0.09}
  8%|â–Š         | 425/5000 [07:20<1:18:05,  1.02s/it]  9%|â–Š         | 426/5000 [07:21<1:17:53,  1.02s/it]                                                    {'loss': 1.931, 'grad_norm': 1.2978184223175049, 'learning_rate': 0.000933469387755102, 'epoch': 0.09}
  9%|â–Š         | 426/5000 [07:21<1:17:53,  1.02s/it]  9%|â–Š         | 427/5000 [07:22<1:17:25,  1.02s/it]                                                    {'loss': 1.6964, 'grad_norm': 1.4016698598861694, 'learning_rate': 0.000933265306122449, 'epoch': 0.09}
  9%|â–Š         | 427/5000 [07:22<1:17:25,  1.02s/it]  9%|â–Š         | 428/5000 [07:23<1:17:29,  1.02s/it]                                                    {'loss': 1.5373, 'grad_norm': 0.6899721622467041, 'learning_rate': 0.0009330612244897959, 'epoch': 0.09}
  9%|â–Š         | 428/5000 [07:23<1:17:29,  1.02s/it]  9%|â–Š         | 429/5000 [07:24<1:17:29,  1.02s/it]                                                    {'loss': 1.7514, 'grad_norm': 1.0491031408309937, 'learning_rate': 0.0009328571428571428, 'epoch': 0.09}
  9%|â–Š         | 429/5000 [07:24<1:17:29,  1.02s/it]  9%|â–Š         | 430/5000 [07:25<1:17:38,  1.02s/it]                                                    {'loss': 2.2573, 'grad_norm': 1.1778361797332764, 'learning_rate': 0.0009326530612244898, 'epoch': 0.09}
  9%|â–Š         | 430/5000 [07:25<1:17:38,  1.02s/it]  9%|â–Š         | 431/5000 [07:26<1:17:06,  1.01s/it]                                                    {'loss': 1.9444, 'grad_norm': 3.107478141784668, 'learning_rate': 0.0009324489795918367, 'epoch': 0.09}
  9%|â–Š         | 431/5000 [07:26<1:17:06,  1.01s/it]  9%|â–Š         | 432/5000 [07:27<1:16:44,  1.01s/it]                                                    {'loss': 1.7474, 'grad_norm': 1.160799503326416, 'learning_rate': 0.0009322448979591836, 'epoch': 0.09}
  9%|â–Š         | 432/5000 [07:27<1:16:44,  1.01s/it]  9%|â–Š         | 433/5000 [07:28<1:16:47,  1.01s/it]                                                    {'loss': 1.6388, 'grad_norm': 0.6926300525665283, 'learning_rate': 0.0009320408163265305, 'epoch': 0.09}
  9%|â–Š         | 433/5000 [07:28<1:16:47,  1.01s/it]  9%|â–Š         | 434/5000 [07:29<1:16:49,  1.01s/it]                                                    {'loss': 1.4559, 'grad_norm': 0.4404091536998749, 'learning_rate': 0.0009318367346938776, 'epoch': 0.09}
  9%|â–Š         | 434/5000 [07:29<1:16:49,  1.01s/it]  9%|â–Š         | 435/5000 [07:30<1:17:04,  1.01s/it]                                                    {'loss': 1.9536, 'grad_norm': 2.9082233905792236, 'learning_rate': 0.0009316326530612245, 'epoch': 0.09}
  9%|â–Š         | 435/5000 [07:30<1:17:04,  1.01s/it]  9%|â–Š         | 436/5000 [07:31<1:18:25,  1.03s/it]                                                    {'loss': 2.1451, 'grad_norm': 1.4697574377059937, 'learning_rate': 0.0009314285714285714, 'epoch': 0.09}
  9%|â–Š         | 436/5000 [07:31<1:18:25,  1.03s/it]  9%|â–Š         | 437/5000 [07:32<1:17:56,  1.02s/it]                                                    {'loss': 2.0463, 'grad_norm': 1.195717453956604, 'learning_rate': 0.0009312244897959184, 'epoch': 0.09}
  9%|â–Š         | 437/5000 [07:32<1:17:56,  1.02s/it]  9%|â–‰         | 438/5000 [07:33<1:18:04,  1.03s/it]                                                    {'loss': 2.3194, 'grad_norm': 4.413830757141113, 'learning_rate': 0.0009310204081632653, 'epoch': 0.09}
  9%|â–‰         | 438/5000 [07:33<1:18:04,  1.03s/it]  9%|â–‰         | 439/5000 [07:34<1:17:52,  1.02s/it]                                                    {'loss': 1.5491, 'grad_norm': 0.31719523668289185, 'learning_rate': 0.0009308163265306122, 'epoch': 0.09}
  9%|â–‰         | 439/5000 [07:34<1:17:52,  1.02s/it]  9%|â–‰         | 440/5000 [07:35<1:17:13,  1.02s/it]                                                    {'loss': 1.879, 'grad_norm': 0.6731221079826355, 'learning_rate': 0.0009306122448979592, 'epoch': 0.09}
  9%|â–‰         | 440/5000 [07:35<1:17:13,  1.02s/it]  9%|â–‰         | 441/5000 [07:36<1:17:15,  1.02s/it]                                                    {'loss': 1.6522, 'grad_norm': 1.7585185766220093, 'learning_rate': 0.0009304081632653062, 'epoch': 0.09}
  9%|â–‰         | 441/5000 [07:36<1:17:15,  1.02s/it]  9%|â–‰         | 442/5000 [07:37<1:17:07,  1.02s/it]                                                    {'loss': 1.5848, 'grad_norm': 0.7731220722198486, 'learning_rate': 0.0009302040816326531, 'epoch': 0.09}
  9%|â–‰         | 442/5000 [07:37<1:17:07,  1.02s/it]  9%|â–‰         | 443/5000 [07:38<1:17:05,  1.02s/it]                                                    {'loss': 1.623, 'grad_norm': 1.4457449913024902, 'learning_rate': 0.00093, 'epoch': 0.09}
  9%|â–‰         | 443/5000 [07:38<1:17:05,  1.02s/it]  9%|â–‰         | 444/5000 [07:39<1:16:26,  1.01s/it]                                                    {'loss': 1.4576, 'grad_norm': 0.6480210423469543, 'learning_rate': 0.000929795918367347, 'epoch': 0.09}
  9%|â–‰         | 444/5000 [07:39<1:16:26,  1.01s/it]  9%|â–‰         | 445/5000 [07:40<1:16:56,  1.01s/it]                                                    {'loss': 2.5524, 'grad_norm': 3.1630725860595703, 'learning_rate': 0.0009295918367346939, 'epoch': 0.09}
  9%|â–‰         | 445/5000 [07:40<1:16:56,  1.01s/it]  9%|â–‰         | 446/5000 [07:41<1:15:56,  1.00s/it]                                                    {'loss': 1.9505, 'grad_norm': 1.0103063583374023, 'learning_rate': 0.0009293877551020408, 'epoch': 0.09}
  9%|â–‰         | 446/5000 [07:41<1:15:56,  1.00s/it]  9%|â–‰         | 447/5000 [07:42<1:16:57,  1.01s/it]                                                    {'loss': 2.808, 'grad_norm': 2.688042402267456, 'learning_rate': 0.0009291836734693878, 'epoch': 0.09}
  9%|â–‰         | 447/5000 [07:42<1:16:57,  1.01s/it]  9%|â–‰         | 448/5000 [07:43<1:18:07,  1.03s/it]                                                    {'loss': 1.7869, 'grad_norm': 1.7620350122451782, 'learning_rate': 0.0009289795918367347, 'epoch': 0.09}
  9%|â–‰         | 448/5000 [07:43<1:18:07,  1.03s/it]  9%|â–‰         | 449/5000 [07:44<1:17:45,  1.03s/it]                                                    {'loss': 1.9014, 'grad_norm': 1.2507730722427368, 'learning_rate': 0.0009287755102040817, 'epoch': 0.09}
  9%|â–‰         | 449/5000 [07:44<1:17:45,  1.03s/it]  9%|â–‰         | 450/5000 [07:45<1:17:06,  1.02s/it]                                                    {'loss': 2.0115, 'grad_norm': 1.387434482574463, 'learning_rate': 0.0009285714285714287, 'epoch': 0.09}
  9%|â–‰         | 450/5000 [07:45<1:17:06,  1.02s/it][2025-10-19 17:21:06,503] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 451/5000 [07:47<1:44:45,  1.38s/it]                                                    {'loss': 1.7797, 'grad_norm': 0.6131173372268677, 'learning_rate': 0.0009283673469387756, 'epoch': 0.09}
  9%|â–‰         | 451/5000 [07:47<1:44:45,  1.38s/it]  9%|â–‰         | 452/5000 [07:48<1:36:14,  1.27s/it]                                                    {'loss': 1.792, 'grad_norm': 1.1532455682754517, 'learning_rate': 0.0009281632653061225, 'epoch': 0.09}
  9%|â–‰         | 452/5000 [07:48<1:36:14,  1.27s/it]  9%|â–‰         | 453/5000 [07:49<1:30:44,  1.20s/it]                                                    {'loss': 1.4896, 'grad_norm': 0.5600833892822266, 'learning_rate': 0.0009279591836734695, 'epoch': 0.09}
  9%|â–‰         | 453/5000 [07:49<1:30:44,  1.20s/it]  9%|â–‰         | 454/5000 [07:50<1:25:53,  1.13s/it]                                                    {'loss': 2.1765, 'grad_norm': 1.5191558599472046, 'learning_rate': 0.0009277551020408164, 'epoch': 0.09}
  9%|â–‰         | 454/5000 [07:50<1:25:53,  1.13s/it]  9%|â–‰         | 455/5000 [07:51<1:23:19,  1.10s/it]                                                    {'loss': 2.6176, 'grad_norm': 1.9300605058670044, 'learning_rate': 0.0009275510204081633, 'epoch': 0.09}
  9%|â–‰         | 455/5000 [07:51<1:23:19,  1.10s/it]  9%|â–‰         | 456/5000 [07:52<1:21:07,  1.07s/it]                                                    {'loss': 1.5749, 'grad_norm': 0.332825630903244, 'learning_rate': 0.0009273469387755102, 'epoch': 0.09}
  9%|â–‰         | 456/5000 [07:52<1:21:07,  1.07s/it]  9%|â–‰         | 457/5000 [07:53<1:19:28,  1.05s/it]                                                    {'loss': 2.0693, 'grad_norm': 1.3839291334152222, 'learning_rate': 0.0009271428571428572, 'epoch': 0.09}
  9%|â–‰         | 457/5000 [07:53<1:19:28,  1.05s/it]  9%|â–‰         | 458/5000 [07:54<1:18:41,  1.04s/it]                                                    {'loss': 1.5922, 'grad_norm': 1.016163945198059, 'learning_rate': 0.0009269387755102041, 'epoch': 0.09}
  9%|â–‰         | 458/5000 [07:54<1:18:41,  1.04s/it]  9%|â–‰         | 459/5000 [07:55<1:17:55,  1.03s/it]                                                    {'loss': 1.5343, 'grad_norm': 0.5186021327972412, 'learning_rate': 0.000926734693877551, 'epoch': 0.09}
  9%|â–‰         | 459/5000 [07:55<1:17:55,  1.03s/it]  9%|â–‰         | 460/5000 [07:56<1:17:15,  1.02s/it]                                                    {'loss': 1.6327, 'grad_norm': 0.9696716070175171, 'learning_rate': 0.000926530612244898, 'epoch': 0.09}
  9%|â–‰         | 460/5000 [07:56<1:17:15,  1.02s/it]  9%|â–‰         | 461/5000 [07:57<1:17:19,  1.02s/it]                                                    {'loss': 1.5481, 'grad_norm': 0.4862956404685974, 'learning_rate': 0.0009263265306122449, 'epoch': 0.09}
  9%|â–‰         | 461/5000 [07:57<1:17:19,  1.02s/it]  9%|â–‰         | 462/5000 [07:58<1:16:36,  1.01s/it]                                                    {'loss': 1.892, 'grad_norm': 1.9578754901885986, 'learning_rate': 0.0009261224489795918, 'epoch': 0.09}
  9%|â–‰         | 462/5000 [07:58<1:16:36,  1.01s/it]  9%|â–‰         | 463/5000 [07:59<1:15:57,  1.00s/it]                                                    {'loss': 1.5878, 'grad_norm': 0.9119318723678589, 'learning_rate': 0.0009259183673469388, 'epoch': 0.09}
  9%|â–‰         | 463/5000 [07:59<1:15:57,  1.00s/it]  9%|â–‰         | 464/5000 [08:00<1:15:46,  1.00s/it]                                                    {'loss': 1.5304, 'grad_norm': 0.7299596667289734, 'learning_rate': 0.0009257142857142857, 'epoch': 0.09}
  9%|â–‰         | 464/5000 [08:00<1:15:46,  1.00s/it]  9%|â–‰         | 465/5000 [08:01<1:15:25,  1.00it/s]                                                    {'loss': 2.1073, 'grad_norm': 1.9202841520309448, 'learning_rate': 0.0009255102040816327, 'epoch': 0.09}
  9%|â–‰         | 465/5000 [08:01<1:15:25,  1.00it/s]  9%|â–‰         | 466/5000 [08:02<1:14:58,  1.01it/s]                                                    {'loss': 1.6719, 'grad_norm': 0.6914573907852173, 'learning_rate': 0.0009253061224489796, 'epoch': 0.09}
  9%|â–‰         | 466/5000 [08:02<1:14:58,  1.01it/s]  9%|â–‰         | 467/5000 [08:03<1:16:38,  1.01s/it]                                                    {'loss': 2.7618, 'grad_norm': 1.8835389614105225, 'learning_rate': 0.0009251020408163266, 'epoch': 0.09}
  9%|â–‰         | 467/5000 [08:03<1:16:38,  1.01s/it]  9%|â–‰         | 468/5000 [08:04<1:16:44,  1.02s/it]                                                    {'loss': 1.5526, 'grad_norm': 1.3561171293258667, 'learning_rate': 0.0009248979591836735, 'epoch': 0.09}
  9%|â–‰         | 468/5000 [08:04<1:16:44,  1.02s/it]  9%|â–‰         | 469/5000 [08:05<1:16:23,  1.01s/it]                                                    {'loss': 2.0816, 'grad_norm': 1.234838604927063, 'learning_rate': 0.0009246938775510204, 'epoch': 0.09}
  9%|â–‰         | 469/5000 [08:05<1:16:23,  1.01s/it]  9%|â–‰         | 470/5000 [08:06<1:17:57,  1.03s/it]                                                    {'loss': 2.3771, 'grad_norm': 2.5151615142822266, 'learning_rate': 0.0009244897959183674, 'epoch': 0.09}
  9%|â–‰         | 470/5000 [08:06<1:17:57,  1.03s/it]  9%|â–‰         | 471/5000 [08:07<1:17:37,  1.03s/it]                                                    {'loss': 1.6037, 'grad_norm': 1.2808676958084106, 'learning_rate': 0.0009242857142857143, 'epoch': 0.09}
  9%|â–‰         | 471/5000 [08:07<1:17:37,  1.03s/it]  9%|â–‰         | 472/5000 [08:08<1:17:12,  1.02s/it]                                                    {'loss': 2.9561, 'grad_norm': 2.5953938961029053, 'learning_rate': 0.0009240816326530612, 'epoch': 0.09}
  9%|â–‰         | 472/5000 [08:08<1:17:12,  1.02s/it]  9%|â–‰         | 473/5000 [08:09<1:16:16,  1.01s/it]                                                    {'loss': 1.6857, 'grad_norm': 1.6717164516448975, 'learning_rate': 0.0009238775510204082, 'epoch': 0.09}
  9%|â–‰         | 473/5000 [08:09<1:16:16,  1.01s/it]  9%|â–‰         | 474/5000 [08:10<1:15:44,  1.00s/it]                                                    {'loss': 1.4441, 'grad_norm': 0.856545090675354, 'learning_rate': 0.0009236734693877551, 'epoch': 0.09}
  9%|â–‰         | 474/5000 [08:10<1:15:44,  1.00s/it] 10%|â–‰         | 475/5000 [08:11<1:16:02,  1.01s/it]                                                    {'loss': 1.4885, 'grad_norm': 0.6821287274360657, 'learning_rate': 0.000923469387755102, 'epoch': 0.1}
 10%|â–‰         | 475/5000 [08:11<1:16:02,  1.01s/it] 10%|â–‰         | 476/5000 [08:12<1:15:42,  1.00s/it]                                                    {'loss': 1.529, 'grad_norm': 0.12431120872497559, 'learning_rate': 0.0009232653061224489, 'epoch': 0.1}
 10%|â–‰         | 476/5000 [08:12<1:15:42,  1.00s/it] 10%|â–‰         | 477/5000 [08:13<1:15:32,  1.00s/it]                                                    {'loss': 1.9665, 'grad_norm': 0.9556698799133301, 'learning_rate': 0.0009230612244897959, 'epoch': 0.1}
 10%|â–‰         | 477/5000 [08:13<1:15:32,  1.00s/it] 10%|â–‰         | 478/5000 [08:15<1:20:43,  1.07s/it]                                                    {'loss': 1.5393, 'grad_norm': 1.2909135818481445, 'learning_rate': 0.0009228571428571428, 'epoch': 0.1}
 10%|â–‰         | 478/5000 [08:15<1:20:43,  1.07s/it] 10%|â–‰         | 479/5000 [08:16<1:19:16,  1.05s/it]                                                    {'loss': 1.8218, 'grad_norm': 2.2192037105560303, 'learning_rate': 0.0009226530612244897, 'epoch': 0.1}
 10%|â–‰         | 479/5000 [08:16<1:19:16,  1.05s/it] 10%|â–‰         | 480/5000 [08:17<1:18:10,  1.04s/it]                                                    {'loss': 1.5352, 'grad_norm': 0.2672934830188751, 'learning_rate': 0.0009224489795918367, 'epoch': 0.1}
 10%|â–‰         | 480/5000 [08:17<1:18:10,  1.04s/it] 10%|â–‰         | 481/5000 [08:18<1:16:18,  1.01s/it]                                                    {'loss': 2.7218, 'grad_norm': 2.5378661155700684, 'learning_rate': 0.0009222448979591837, 'epoch': 0.1}
 10%|â–‰         | 481/5000 [08:18<1:16:18,  1.01s/it] 10%|â–‰         | 482/5000 [08:19<1:15:55,  1.01s/it]                                                    {'loss': 1.7568, 'grad_norm': 0.36655691266059875, 'learning_rate': 0.0009220408163265306, 'epoch': 0.1}
 10%|â–‰         | 482/5000 [08:19<1:15:55,  1.01s/it] 10%|â–‰         | 483/5000 [08:20<1:15:35,  1.00s/it]                                                    {'loss': 1.7685, 'grad_norm': 1.8027467727661133, 'learning_rate': 0.0009218367346938776, 'epoch': 0.1}
 10%|â–‰         | 483/5000 [08:20<1:15:35,  1.00s/it] 10%|â–‰         | 484/5000 [08:21<1:15:00,  1.00it/s]                                                    {'loss': 1.4612, 'grad_norm': 0.6636118292808533, 'learning_rate': 0.0009216326530612245, 'epoch': 0.1}
 10%|â–‰         | 484/5000 [08:21<1:15:00,  1.00it/s] 10%|â–‰         | 485/5000 [08:22<1:15:45,  1.01s/it]                                                    {'loss': 1.6566, 'grad_norm': 0.8778289556503296, 'learning_rate': 0.0009214285714285714, 'epoch': 0.1}
 10%|â–‰         | 485/5000 [08:22<1:15:45,  1.01s/it] 10%|â–‰         | 486/5000 [08:23<1:15:19,  1.00s/it]                                                    {'loss': 2.0307, 'grad_norm': 1.1714926958084106, 'learning_rate': 0.0009212244897959185, 'epoch': 0.1}
 10%|â–‰         | 486/5000 [08:23<1:15:19,  1.00s/it] 10%|â–‰         | 487/5000 [08:24<1:15:20,  1.00s/it]                                                    {'loss': 1.8569, 'grad_norm': 1.0610203742980957, 'learning_rate': 0.0009210204081632654, 'epoch': 0.1}
 10%|â–‰         | 487/5000 [08:24<1:15:20,  1.00s/it] 10%|â–‰         | 488/5000 [08:25<1:15:08,  1.00it/s]                                                    {'loss': 2.0664, 'grad_norm': 2.6666669845581055, 'learning_rate': 0.0009208163265306123, 'epoch': 0.1}
 10%|â–‰         | 488/5000 [08:25<1:15:08,  1.00it/s] 10%|â–‰         | 489/5000 [08:26<1:15:14,  1.00s/it]                                                    {'loss': 1.8557, 'grad_norm': 1.2683242559432983, 'learning_rate': 0.0009206122448979592, 'epoch': 0.1}
 10%|â–‰         | 489/5000 [08:26<1:15:14,  1.00s/it] 10%|â–‰         | 490/5000 [08:27<1:15:19,  1.00s/it]                                                    {'loss': 1.6213, 'grad_norm': 1.1576664447784424, 'learning_rate': 0.0009204081632653062, 'epoch': 0.1}
 10%|â–‰         | 490/5000 [08:27<1:15:19,  1.00s/it] 10%|â–‰         | 491/5000 [08:28<1:15:18,  1.00s/it]                                                    {'loss': 1.6381, 'grad_norm': 0.8402018547058105, 'learning_rate': 0.0009202040816326531, 'epoch': 0.1}
 10%|â–‰         | 491/5000 [08:28<1:15:18,  1.00s/it] 10%|â–‰         | 492/5000 [08:29<1:15:11,  1.00s/it]                                                    {'loss': 1.8579, 'grad_norm': 2.579230546951294, 'learning_rate': 0.00092, 'epoch': 0.1}
 10%|â–‰         | 492/5000 [08:29<1:15:11,  1.00s/it] 10%|â–‰         | 493/5000 [08:30<1:14:54,  1.00it/s]                                                    {'loss': 1.6037, 'grad_norm': 0.7718861103057861, 'learning_rate': 0.000919795918367347, 'epoch': 0.1}
 10%|â–‰         | 493/5000 [08:30<1:14:54,  1.00it/s] 10%|â–‰         | 494/5000 [08:31<1:15:07,  1.00s/it]                                                    {'loss': 1.9373, 'grad_norm': 1.520429253578186, 'learning_rate': 0.0009195918367346939, 'epoch': 0.1}
 10%|â–‰         | 494/5000 [08:31<1:15:07,  1.00s/it] 10%|â–‰         | 495/5000 [08:32<1:14:47,  1.00it/s]                                                    {'loss': 2.1558, 'grad_norm': 3.0538437366485596, 'learning_rate': 0.0009193877551020408, 'epoch': 0.1}
 10%|â–‰         | 495/5000 [08:32<1:14:47,  1.00it/s] 10%|â–‰         | 496/5000 [08:33<1:15:44,  1.01s/it]                                                    {'loss': 1.7121, 'grad_norm': 0.0726342424750328, 'learning_rate': 0.0009191836734693879, 'epoch': 0.1}
 10%|â–‰         | 496/5000 [08:33<1:15:44,  1.01s/it] 10%|â–‰         | 497/5000 [08:34<1:15:40,  1.01s/it]                                                    {'loss': 1.6356, 'grad_norm': 0.4343470335006714, 'learning_rate': 0.0009189795918367348, 'epoch': 0.1}
 10%|â–‰         | 497/5000 [08:34<1:15:40,  1.01s/it] 10%|â–‰         | 498/5000 [08:35<1:18:21,  1.04s/it]                                                    {'loss': 1.5802, 'grad_norm': 0.4745837450027466, 'learning_rate': 0.0009187755102040817, 'epoch': 0.1}
 10%|â–‰         | 498/5000 [08:35<1:18:21,  1.04s/it] 10%|â–‰         | 499/5000 [08:36<1:18:24,  1.05s/it]                                                    {'loss': 1.5812, 'grad_norm': 0.9223241806030273, 'learning_rate': 0.0009185714285714286, 'epoch': 0.1}
 10%|â–‰         | 499/5000 [08:36<1:18:24,  1.05s/it] 10%|â–ˆ         | 500/5000 [08:37<1:16:51,  1.02s/it]                                                    {'loss': 2.128, 'grad_norm': 0.2307487577199936, 'learning_rate': 0.0009183673469387756, 'epoch': 0.1}
 10%|â–ˆ         | 500/5000 [08:37<1:16:51,  1.02s/it][2025-10-19 17:21:58,327] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 501/5000 [08:39<1:43:32,  1.38s/it]                                                    {'loss': 1.7185, 'grad_norm': 0.8740386366844177, 'learning_rate': 0.0009181632653061225, 'epoch': 0.1}
 10%|â–ˆ         | 501/5000 [08:39<1:43:32,  1.38s/it] 10%|â–ˆ         | 502/5000 [08:40<1:35:24,  1.27s/it]                                                    {'loss': 1.7939, 'grad_norm': 1.7893729209899902, 'learning_rate': 0.0009179591836734694, 'epoch': 0.1}
 10%|â–ˆ         | 502/5000 [08:40<1:35:24,  1.27s/it] 10%|â–ˆ         | 503/5000 [08:41<1:29:29,  1.19s/it]                                                    {'loss': 1.5723, 'grad_norm': 0.372115820646286, 'learning_rate': 0.0009177551020408164, 'epoch': 0.1}
 10%|â–ˆ         | 503/5000 [08:41<1:29:29,  1.19s/it] 10%|â–ˆ         | 504/5000 [08:42<1:26:32,  1.15s/it]                                                    {'loss': 1.9464, 'grad_norm': 1.7220121622085571, 'learning_rate': 0.0009175510204081633, 'epoch': 0.1}
 10%|â–ˆ         | 504/5000 [08:42<1:26:32,  1.15s/it] 10%|â–ˆ         | 505/5000 [08:43<1:24:05,  1.12s/it]                                                    {'loss': 1.6395, 'grad_norm': 0.758970320224762, 'learning_rate': 0.0009173469387755102, 'epoch': 0.1}
 10%|â–ˆ         | 505/5000 [08:43<1:24:05,  1.12s/it] 10%|â–ˆ         | 506/5000 [08:44<1:21:52,  1.09s/it]                                                    {'loss': 1.5577, 'grad_norm': 0.34088045358657837, 'learning_rate': 0.0009171428571428571, 'epoch': 0.1}
 10%|â–ˆ         | 506/5000 [08:44<1:21:52,  1.09s/it] 10%|â–ˆ         | 507/5000 [08:45<1:19:54,  1.07s/it]                                                    {'loss': 2.0006, 'grad_norm': 1.4914865493774414, 'learning_rate': 0.0009169387755102041, 'epoch': 0.1}
 10%|â–ˆ         | 507/5000 [08:45<1:19:54,  1.07s/it] 10%|â–ˆ         | 508/5000 [08:46<1:19:07,  1.06s/it]                                                    {'loss': 1.5177, 'grad_norm': 0.9517716765403748, 'learning_rate': 0.000916734693877551, 'epoch': 0.1}
 10%|â–ˆ         | 508/5000 [08:46<1:19:07,  1.06s/it] 10%|â–ˆ         | 509/5000 [08:47<1:18:00,  1.04s/it]                                                    {'loss': 1.4949, 'grad_norm': 0.8292415142059326, 'learning_rate': 0.0009165306122448979, 'epoch': 0.1}
 10%|â–ˆ         | 509/5000 [08:47<1:18:00,  1.04s/it] 10%|â–ˆ         | 510/5000 [08:48<1:17:33,  1.04s/it]                                                    {'loss': 1.7561, 'grad_norm': 1.0431874990463257, 'learning_rate': 0.0009163265306122449, 'epoch': 0.1}
 10%|â–ˆ         | 510/5000 [08:48<1:17:33,  1.04s/it] 10%|â–ˆ         | 511/5000 [08:49<1:16:33,  1.02s/it]                                                    {'loss': 1.8465, 'grad_norm': 0.8869406580924988, 'learning_rate': 0.0009161224489795919, 'epoch': 0.1}
 10%|â–ˆ         | 511/5000 [08:49<1:16:33,  1.02s/it] 10%|â–ˆ         | 512/5000 [08:50<1:15:54,  1.01s/it]                                                    {'loss': 1.5384, 'grad_norm': 0.20705395936965942, 'learning_rate': 0.0009159183673469388, 'epoch': 0.1}
 10%|â–ˆ         | 512/5000 [08:50<1:15:54,  1.01s/it] 10%|â–ˆ         | 513/5000 [08:51<1:15:42,  1.01s/it]                                                    {'loss': 2.2239, 'grad_norm': 2.6543805599212646, 'learning_rate': 0.0009157142857142858, 'epoch': 0.1}
 10%|â–ˆ         | 513/5000 [08:51<1:15:42,  1.01s/it] 10%|â–ˆ         | 514/5000 [08:52<1:15:19,  1.01s/it]                                                    {'loss': 1.857, 'grad_norm': 0.9034392833709717, 'learning_rate': 0.0009155102040816327, 'epoch': 0.1}
 10%|â–ˆ         | 514/5000 [08:52<1:15:19,  1.01s/it] 10%|â–ˆ         | 515/5000 [08:53<1:15:32,  1.01s/it]                                                    {'loss': 2.1209, 'grad_norm': 1.787129282951355, 'learning_rate': 0.0009153061224489796, 'epoch': 0.1}
 10%|â–ˆ         | 515/5000 [08:53<1:15:32,  1.01s/it] 10%|â–ˆ         | 516/5000 [08:54<1:15:42,  1.01s/it]                                                    {'loss': 1.5759, 'grad_norm': 0.8692038059234619, 'learning_rate': 0.0009151020408163266, 'epoch': 0.1}
 10%|â–ˆ         | 516/5000 [08:54<1:15:42,  1.01s/it] 10%|â–ˆ         | 517/5000 [08:55<1:15:14,  1.01s/it]                                                    {'loss': 2.0558, 'grad_norm': 1.0783641338348389, 'learning_rate': 0.0009148979591836735, 'epoch': 0.1}
 10%|â–ˆ         | 517/5000 [08:55<1:15:14,  1.01s/it] 10%|â–ˆ         | 518/5000 [08:56<1:15:03,  1.00s/it]                                                    {'loss': 1.9468, 'grad_norm': 1.3361214399337769, 'learning_rate': 0.0009146938775510204, 'epoch': 0.1}
 10%|â–ˆ         | 518/5000 [08:56<1:15:03,  1.00s/it] 10%|â–ˆ         | 519/5000 [08:57<1:15:10,  1.01s/it]                                                    {'loss': 1.6, 'grad_norm': 0.5268970727920532, 'learning_rate': 0.0009144897959183673, 'epoch': 0.1}
 10%|â–ˆ         | 519/5000 [08:57<1:15:10,  1.01s/it] 10%|â–ˆ         | 520/5000 [08:58<1:14:36,  1.00it/s]                                                    {'loss': 1.5942, 'grad_norm': 0.44583457708358765, 'learning_rate': 0.0009142857142857143, 'epoch': 0.1}
 10%|â–ˆ         | 520/5000 [08:58<1:14:36,  1.00it/s] 10%|â–ˆ         | 521/5000 [08:59<1:14:06,  1.01it/s]                                                    {'loss': 1.5505, 'grad_norm': 0.907220184803009, 'learning_rate': 0.0009140816326530612, 'epoch': 0.1}
 10%|â–ˆ         | 521/5000 [08:59<1:14:06,  1.01it/s] 10%|â–ˆ         | 522/5000 [09:00<1:14:00,  1.01it/s]                                                    {'loss': 1.7995, 'grad_norm': 0.8291626572608948, 'learning_rate': 0.0009138775510204081, 'epoch': 0.1}
 10%|â–ˆ         | 522/5000 [09:00<1:14:00,  1.01it/s] 10%|â–ˆ         | 523/5000 [09:01<1:14:09,  1.01it/s]                                                    {'loss': 1.8456, 'grad_norm': 0.9758831262588501, 'learning_rate': 0.0009136734693877551, 'epoch': 0.1}
 10%|â–ˆ         | 523/5000 [09:01<1:14:09,  1.01it/s] 10%|â–ˆ         | 524/5000 [09:02<1:14:47,  1.00s/it]                                                    {'loss': 1.5766, 'grad_norm': 0.8046346306800842, 'learning_rate': 0.000913469387755102, 'epoch': 0.1}
 10%|â–ˆ         | 524/5000 [09:02<1:14:47,  1.00s/it] 10%|â–ˆ         | 525/5000 [09:03<1:14:53,  1.00s/it]                                                    {'loss': 1.5446, 'grad_norm': 1.0184190273284912, 'learning_rate': 0.0009132653061224489, 'epoch': 0.1}
 10%|â–ˆ         | 525/5000 [09:03<1:14:53,  1.00s/it] 11%|â–ˆ         | 526/5000 [09:04<1:14:46,  1.00s/it]                                                    {'loss': 1.6159, 'grad_norm': 0.7951899766921997, 'learning_rate': 0.0009130612244897958, 'epoch': 0.11}
 11%|â–ˆ         | 526/5000 [09:04<1:14:46,  1.00s/it] 11%|â–ˆ         | 527/5000 [09:05<1:14:20,  1.00it/s]                                                    {'loss': 1.6715, 'grad_norm': 0.8245750665664673, 'learning_rate': 0.0009128571428571429, 'epoch': 0.11}
 11%|â–ˆ         | 527/5000 [09:05<1:14:20,  1.00it/s] 11%|â–ˆ         | 528/5000 [09:06<1:14:24,  1.00it/s]                                                    {'loss': 1.6422, 'grad_norm': 0.6221805214881897, 'learning_rate': 0.0009126530612244898, 'epoch': 0.11}
 11%|â–ˆ         | 528/5000 [09:06<1:14:24,  1.00it/s] 11%|â–ˆ         | 529/5000 [09:07<1:14:33,  1.00s/it]                                                    {'loss': 1.6211, 'grad_norm': 1.2502024173736572, 'learning_rate': 0.0009124489795918367, 'epoch': 0.11}
 11%|â–ˆ         | 529/5000 [09:07<1:14:33,  1.00s/it] 11%|â–ˆ         | 530/5000 [09:08<1:14:39,  1.00s/it]                                                    {'loss': 1.562, 'grad_norm': 0.7883314490318298, 'learning_rate': 0.0009122448979591837, 'epoch': 0.11}
 11%|â–ˆ         | 530/5000 [09:08<1:14:39,  1.00s/it] 11%|â–ˆ         | 531/5000 [09:09<1:14:54,  1.01s/it]                                                    {'loss': 1.7098, 'grad_norm': 1.3791917562484741, 'learning_rate': 0.0009120408163265306, 'epoch': 0.11}
 11%|â–ˆ         | 531/5000 [09:09<1:14:54,  1.01s/it] 11%|â–ˆ         | 532/5000 [09:10<1:14:51,  1.01s/it]                                                    {'loss': 1.9195, 'grad_norm': 1.7887718677520752, 'learning_rate': 0.0009118367346938776, 'epoch': 0.11}
 11%|â–ˆ         | 532/5000 [09:10<1:14:51,  1.01s/it] 11%|â–ˆ         | 533/5000 [09:11<1:15:11,  1.01s/it]                                                    {'loss': 1.9147, 'grad_norm': 2.077836275100708, 'learning_rate': 0.0009116326530612246, 'epoch': 0.11}
 11%|â–ˆ         | 533/5000 [09:11<1:15:11,  1.01s/it] 11%|â–ˆ         | 534/5000 [09:12<1:16:21,  1.03s/it]                                                    {'loss': 2.8288, 'grad_norm': 3.3550832271575928, 'learning_rate': 0.0009114285714285715, 'epoch': 0.11}
 11%|â–ˆ         | 534/5000 [09:12<1:16:21,  1.03s/it] 11%|â–ˆ         | 535/5000 [09:13<1:16:39,  1.03s/it]                                                    {'loss': 1.7147, 'grad_norm': 0.9056141376495361, 'learning_rate': 0.0009112244897959184, 'epoch': 0.11}
 11%|â–ˆ         | 535/5000 [09:13<1:16:39,  1.03s/it] 11%|â–ˆ         | 536/5000 [09:14<1:16:05,  1.02s/it]                                                    {'loss': 1.5801, 'grad_norm': 0.6307865381240845, 'learning_rate': 0.0009110204081632654, 'epoch': 0.11}
 11%|â–ˆ         | 536/5000 [09:14<1:16:05,  1.02s/it] 11%|â–ˆ         | 537/5000 [09:15<1:15:50,  1.02s/it]                                                    {'loss': 1.7043, 'grad_norm': 1.414008378982544, 'learning_rate': 0.0009108163265306123, 'epoch': 0.11}
 11%|â–ˆ         | 537/5000 [09:15<1:15:50,  1.02s/it] 11%|â–ˆ         | 538/5000 [09:16<1:15:28,  1.01s/it]                                                    {'loss': 2.1192, 'grad_norm': 1.574769139289856, 'learning_rate': 0.0009106122448979592, 'epoch': 0.11}
 11%|â–ˆ         | 538/5000 [09:16<1:15:28,  1.01s/it]