==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test1-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/train.log
W1022 17:50:53.300000 127932558124864 torch/distributed/run.py:779] 
W1022 17:50:53.300000 127932558124864 torch/distributed/run.py:779] *****************************************
W1022 17:50:53.300000 127932558124864 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 17:50:53.300000 127932558124864 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-22 17:51:03,477] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.05it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.78it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251022_175103-25wka482
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test1-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/25wka482
[2025-10-22 17:51:05,057] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.20it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.05it/s]
[2025-10-22 17:51:05,681] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-22 17:51:05,686] INFO [src.utils:19] Loading lora adapter from TailTokenDetachPrefixWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-22 17:51:14,728] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-22 17:51:15,851] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-22 17:51:15,852] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-22 17:51:20,641] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-22 17:51:20,642] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-22 17:51:21,435] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-22 17:51:21,436] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-22 17:51:21,436] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-22 17:51:21,438] INFO [src.utils:19] ==================================================
[2025-10-22 17:51:21,438] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-22 17:51:21,439] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 17:51:21,440] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 17:51:21,440] INFO [src.utils:19] ==================================================
[2025-10-22 17:51:23,205] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 17:51:23,205] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 17:51:23,205] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 17:51:23,205] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 17:51:23,205] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 17:51:23,205] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 17:51:23,205] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 17:51:23,205] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 17:51:23,206] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 17:51:23,207] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 17:51:23,207] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 17:51:23,207] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 17:51:23,207] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 17:51:23,208] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 17:51:23,214] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-22 17:51:23,216] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-22 17:51:23,220] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-10-22 17:51:23,223] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.base.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1022 17:51:26.987193476 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1022 17:51:26.020791754 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:50:12,  4.10s/it]                                                  {'loss': 8.7669, 'grad_norm': 1197.531982421875, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:50:12,  4.10s/it]  0%|          | 2/6000 [00:06<5:22:31,  3.23s/it]                                                  {'loss': 9.0246, 'grad_norm': 1204.8668212890625, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:22:31,  3.23s/it]  0%|          | 3/6000 [00:09<4:57:45,  2.98s/it]                                                  {'loss': 7.1067, 'grad_norm': 938.8687744140625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:57:45,  2.98s/it]  0%|          | 4/6000 [00:12<4:45:23,  2.86s/it]                                                  {'loss': 8.5244, 'grad_norm': 963.2070922851562, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:45:23,  2.86s/it]  0%|          | 5/6000 [00:14<4:36:53,  2.77s/it]                                                  {'loss': 7.4094, 'grad_norm': 902.2342529296875, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:36:53,  2.77s/it]  0%|          | 6/6000 [00:17<4:32:50,  2.73s/it]                                                  {'loss': 7.263, 'grad_norm': 860.2373046875, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:32:50,  2.73s/it]  0%|          | 7/6000 [00:19<4:28:41,  2.69s/it]                                                  {'loss': 6.5642, 'grad_norm': 576.979248046875, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:19<4:28:41,  2.69s/it]  0%|          | 8/6000 [00:22<4:25:26,  2.66s/it]                                                  {'loss': 5.427, 'grad_norm': 494.036376953125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:25:26,  2.66s/it]  0%|          | 9/6000 [00:25<4:26:06,  2.67s/it]                                                  {'loss': 4.321, 'grad_norm': 128.1309814453125, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:26:06,  2.67s/it]  0%|          | 10/6000 [00:27<4:23:19,  2.64s/it]                                                   {'loss': 4.6451, 'grad_norm': 184.8380889892578, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:27<4:23:19,  2.64s/it]  0%|          | 11/6000 [00:30<4:30:45,  2.71s/it]                                                   {'loss': 4.6445, 'grad_norm': 285.9744873046875, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:30<4:30:45,  2.71s/it]  0%|          | 12/6000 [00:33<4:33:10,  2.74s/it]                                                   {'loss': 4.2424, 'grad_norm': 102.06009674072266, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:33<4:33:10,  2.74s/it]  0%|          | 13/6000 [00:36<4:30:17,  2.71s/it]                                                   {'loss': 4.2901, 'grad_norm': 206.14952087402344, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:30:17,  2.71s/it]  0%|          | 14/6000 [00:38<4:30:02,  2.71s/it]                                                   {'loss': 4.108, 'grad_norm': 97.76451873779297, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:38<4:30:02,  2.71s/it]  0%|          | 15/6000 [00:41<4:26:54,  2.68s/it]                                                   {'loss': 3.964, 'grad_norm': 60.558963775634766, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:41<4:26:54,  2.68s/it]  0%|          | 16/6000 [00:44<4:24:14,  2.65s/it]                                                   {'loss': 3.9034, 'grad_norm': 56.09486770629883, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:44<4:24:14,  2.65s/it]  0%|          | 17/6000 [00:46<4:24:49,  2.66s/it]                                                   {'loss': 3.7944, 'grad_norm': 37.73667907714844, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:46<4:24:49,  2.66s/it]  0%|          | 18/6000 [00:49<4:25:49,  2.67s/it]                                                   {'loss': 3.6734, 'grad_norm': 29.393068313598633, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:49<4:25:49,  2.67s/it]  0%|          | 19/6000 [00:52<4:25:14,  2.66s/it]                                                   {'loss': 3.7071, 'grad_norm': 59.789268493652344, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:52<4:25:14,  2.66s/it]  0%|          | 20/6000 [00:54<4:24:21,  2.65s/it]                                                   {'loss': 3.6028, 'grad_norm': 28.051551818847656, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:54<4:24:21,  2.65s/it]  0%|          | 21/6000 [00:57<4:28:07,  2.69s/it]                                                   {'loss': 3.5389, 'grad_norm': 25.44078826904297, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:57<4:28:07,  2.69s/it]  0%|          | 22/6000 [01:00<4:29:33,  2.71s/it]                                                   {'loss': 3.5337, 'grad_norm': 47.81126403808594, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:00<4:29:33,  2.71s/it]  0%|          | 23/6000 [01:02<4:27:30,  2.69s/it]                                                   {'loss': 3.5104, 'grad_norm': 84.92901611328125, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:02<4:27:30,  2.69s/it]  0%|          | 24/6000 [01:05<4:27:56,  2.69s/it]                                                   {'loss': 3.3918, 'grad_norm': 22.15203857421875, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:05<4:27:56,  2.69s/it]  0%|          | 25/6000 [01:08<4:27:25,  2.69s/it]                                                   {'loss': 3.3063, 'grad_norm': 33.90629959106445, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:08<4:27:25,  2.69s/it]  0%|          | 26/6000 [01:10<4:29:06,  2.70s/it]                                                   {'loss': 3.2118, 'grad_norm': 34.296897888183594, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:10<4:29:06,  2.70s/it]  0%|          | 27/6000 [01:13<4:27:32,  2.69s/it]                                                   {'loss': 3.0992, 'grad_norm': 36.15656280517578, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:13<4:27:32,  2.69s/it]  0%|          | 28/6000 [01:17<4:51:53,  2.93s/it]                                                   {'loss': 2.9127, 'grad_norm': 27.74761962890625, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:17<4:51:53,  2.93s/it]  0%|          | 29/6000 [01:19<4:41:02,  2.82s/it]                                                   {'loss': 2.8294, 'grad_norm': 16.04824447631836, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:19<4:41:02,  2.82s/it]  0%|          | 30/6000 [01:22<4:35:55,  2.77s/it]                                                   {'loss': 2.7769, 'grad_norm': 6.134461402893066, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:22<4:35:55,  2.77s/it]  1%|          | 31/6000 [01:24<4:30:34,  2.72s/it]                                                   {'loss': 2.7782, 'grad_norm': 7.53316593170166, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:24<4:30:34,  2.72s/it]  1%|          | 32/6000 [01:27<4:28:02,  2.69s/it]                                                   {'loss': 2.819, 'grad_norm': 44.4343147277832, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:27<4:28:02,  2.69s/it]  1%|          | 33/6000 [01:30<4:27:10,  2.69s/it]                                                   {'loss': 2.7645, 'grad_norm': 13.639453887939453, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:30<4:27:10,  2.69s/it]  1%|          | 34/6000 [01:32<4:26:30,  2.68s/it]                                                   {'loss': 2.8167, 'grad_norm': 8.38218879699707, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:32<4:26:30,  2.68s/it]  1%|          | 35/6000 [01:35<4:27:10,  2.69s/it]                                                   {'loss': 2.8207, 'grad_norm': 22.65057373046875, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:35<4:27:10,  2.69s/it]  1%|          | 36/6000 [01:38<4:24:24,  2.66s/it]                                                   {'loss': 2.8016, 'grad_norm': 11.90919303894043, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:38<4:24:24,  2.66s/it]  1%|          | 37/6000 [01:40<4:23:53,  2.66s/it]                                                   {'loss': 2.8054, 'grad_norm': 16.87653350830078, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:40<4:23:53,  2.66s/it]  1%|          | 38/6000 [01:43<4:22:45,  2.64s/it]                                                   {'loss': 2.7901, 'grad_norm': 8.811712265014648, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:43<4:22:45,  2.64s/it]  1%|          | 39/6000 [01:46<4:22:49,  2.65s/it]                                                   {'loss': 2.851, 'grad_norm': 13.464256286621094, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:46<4:22:49,  2.65s/it]  1%|          | 40/6000 [01:48<4:24:34,  2.66s/it]                                                   {'loss': 2.7769, 'grad_norm': 8.315295219421387, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:48<4:24:34,  2.66s/it]  1%|          | 41/6000 [01:51<4:22:35,  2.64s/it]                                                   {'loss': 2.7774, 'grad_norm': 9.63100528717041, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:51<4:22:35,  2.64s/it]  1%|          | 42/6000 [01:54<4:23:40,  2.66s/it]                                                   {'loss': 2.7758, 'grad_norm': 5.40770149230957, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:54<4:23:40,  2.66s/it]  1%|          | 43/6000 [01:57<4:53:27,  2.96s/it]                                                   {'loss': 2.8191, 'grad_norm': 19.061063766479492, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [01:57<4:53:27,  2.96s/it]  1%|          | 44/6000 [02:00<4:56:59,  2.99s/it]                                                   {'loss': 2.7714, 'grad_norm': 6.727499961853027, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:00<4:56:59,  2.99s/it]  1%|          | 45/6000 [02:03<4:48:24,  2.91s/it]                                                   {'loss': 2.8045, 'grad_norm': 13.989238739013672, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:03<4:48:24,  2.91s/it]  1%|          | 46/6000 [02:06<4:43:53,  2.86s/it]                                                   {'loss': 2.7963, 'grad_norm': 28.127540588378906, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:06<4:43:53,  2.86s/it]  1%|          | 47/6000 [02:08<4:37:18,  2.80s/it]                                                   {'loss': 2.8243, 'grad_norm': 72.6961441040039, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:08<4:37:18,  2.80s/it]  1%|          | 48/6000 [02:11<4:35:54,  2.78s/it]                                                   {'loss': 2.7822, 'grad_norm': 14.287941932678223, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:11<4:35:54,  2.78s/it]  1%|          | 49/6000 [02:14<4:31:36,  2.74s/it]                                                   {'loss': 2.7732, 'grad_norm': 15.071451187133789, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:14<4:31:36,  2.74s/it]  1%|          | 50/6000 [02:17<4:34:03,  2.76s/it]                                                   {'loss': 2.7867, 'grad_norm': 8.839877128601074, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:17<4:34:03,  2.76s/it][2025-10-22 17:53:40,529] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:21<5:28:58,  3.32s/it]                                                   {'loss': 2.8042, 'grad_norm': 10.845252990722656, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:21<5:28:58,  3.32s/it]  1%|          | 52/6000 [02:24<5:08:07,  3.11s/it]                                                   {'loss': 2.7881, 'grad_norm': 12.954708099365234, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:24<5:08:07,  3.11s/it]  1%|          | 53/6000 [02:27<4:57:44,  3.00s/it]                                                   {'loss': 2.9274, 'grad_norm': 38.353660583496094, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:27<4:57:44,  3.00s/it]  1%|          | 54/6000 [02:29<4:48:07,  2.91s/it]                                                   {'loss': 2.7794, 'grad_norm': 5.969261646270752, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:29<4:48:07,  2.91s/it]  1%|          | 55/6000 [02:32<4:39:20,  2.82s/it]                                                   {'loss': 2.7898, 'grad_norm': 6.339816093444824, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:32<4:39:20,  2.82s/it]  1%|          | 56/6000 [02:35<4:36:59,  2.80s/it]                                                   {'loss': 2.8043, 'grad_norm': 8.19140338897705, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:35<4:36:59,  2.80s/it]  1%|          | 57/6000 [02:37<4:32:42,  2.75s/it]                                                   {'loss': 2.7831, 'grad_norm': 4.5415191650390625, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:37<4:32:42,  2.75s/it]  1%|          | 58/6000 [02:40<4:30:40,  2.73s/it]                                                   {'loss': 2.7606, 'grad_norm': 6.441575050354004, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:40<4:30:40,  2.73s/it]  1%|          | 59/6000 [02:43<4:26:11,  2.69s/it]                                                   {'loss': 2.783, 'grad_norm': 3.450857639312744, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:43<4:26:11,  2.69s/it]  1%|          | 60/6000 [02:45<4:23:28,  2.66s/it]                                                   {'loss': 2.7825, 'grad_norm': 5.588309288024902, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:45<4:23:28,  2.66s/it]  1%|          | 61/6000 [02:48<4:20:05,  2.63s/it]                                                   {'loss': 2.7832, 'grad_norm': 5.86192512512207, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:48<4:20:05,  2.63s/it]  1%|          | 62/6000 [02:50<4:20:50,  2.64s/it]                                                   {'loss': 2.7963, 'grad_norm': 4.28816032409668, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:50<4:20:50,  2.64s/it]  1%|          | 63/6000 [02:53<4:19:24,  2.62s/it]                                                   {'loss': 2.7717, 'grad_norm': 6.660029888153076, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:53<4:19:24,  2.62s/it]  1%|          | 64/6000 [02:56<4:18:11,  2.61s/it]                                                   {'loss': 2.7868, 'grad_norm': 5.81131649017334, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [02:56<4:18:11,  2.61s/it]  1%|          | 65/6000 [02:58<4:16:55,  2.60s/it]                                                   {'loss': 2.8211, 'grad_norm': 12.878567695617676, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [02:58<4:16:55,  2.60s/it]  1%|          | 66/6000 [03:01<4:29:50,  2.73s/it]                                                   {'loss': 2.7567, 'grad_norm': 8.82884693145752, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:01<4:29:50,  2.73s/it]  1%|          | 67/6000 [03:04<4:27:47,  2.71s/it]                                                   {'loss': 2.7963, 'grad_norm': 6.8834381103515625, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:04<4:27:47,  2.71s/it]  1%|          | 68/6000 [03:06<4:23:11,  2.66s/it]                                                   {'loss': 2.7772, 'grad_norm': 9.437823295593262, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:06<4:23:11,  2.66s/it]  1%|          | 69/6000 [03:09<4:22:06,  2.65s/it]                                                   {'loss': 2.787, 'grad_norm': 5.283975124359131, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:09<4:22:06,  2.65s/it]  1%|          | 70/6000 [03:12<4:24:21,  2.67s/it]                                                   {'loss': 2.8014, 'grad_norm': 9.508991241455078, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:12<4:24:21,  2.67s/it]  1%|          | 71/6000 [03:14<4:22:19,  2.65s/it]                                                   {'loss': 2.7858, 'grad_norm': 12.400345802307129, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:14<4:22:19,  2.65s/it]  1%|          | 72/6000 [03:18<4:41:20,  2.85s/it]                                                   {'loss': 2.7788, 'grad_norm': 6.661800384521484, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:18<4:41:20,  2.85s/it]  1%|          | 73/6000 [03:20<4:35:34,  2.79s/it]                                                   {'loss': 2.8242, 'grad_norm': 8.857029914855957, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:20<4:35:34,  2.79s/it]  1%|          | 74/6000 [03:23<4:30:38,  2.74s/it]                                                   {'loss': 2.7841, 'grad_norm': 6.176974296569824, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:23<4:30:38,  2.74s/it]  1%|â–         | 75/6000 [03:26<4:26:43,  2.70s/it]                                                   {'loss': 2.785, 'grad_norm': 4.668432712554932, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:26<4:26:43,  2.70s/it]  1%|â–         | 76/6000 [03:28<4:28:00,  2.71s/it]                                                   {'loss': 2.8045, 'grad_norm': 5.068649768829346, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:28<4:28:00,  2.71s/it]  1%|â–         | 77/6000 [03:31<4:27:28,  2.71s/it]                                                   {'loss': 2.8511, 'grad_norm': 4.3960280418396, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:31<4:27:28,  2.71s/it]  1%|â–         | 78/6000 [03:34<4:37:21,  2.81s/it]                                                   {'loss': 2.782, 'grad_norm': 5.755253791809082, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:34<4:37:21,  2.81s/it]  1%|â–         | 79/6000 [03:37<4:32:20,  2.76s/it]                                                   {'loss': 2.7589, 'grad_norm': 6.068089962005615, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:37<4:32:20,  2.76s/it]  1%|â–         | 80/6000 [03:40<4:36:24,  2.80s/it]                                                   {'loss': 2.7685, 'grad_norm': 8.891051292419434, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:40<4:36:24,  2.80s/it]  1%|â–         | 81/6000 [03:42<4:32:41,  2.76s/it]                                                   {'loss': 2.7824, 'grad_norm': 6.598160266876221, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:42<4:32:41,  2.76s/it]  1%|â–         | 82/6000 [03:45<4:30:38,  2.74s/it]                                                   {'loss': 2.7823, 'grad_norm': 6.540577411651611, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:45<4:30:38,  2.74s/it]  1%|â–         | 83/6000 [03:48<4:26:15,  2.70s/it]                                                   {'loss': 2.795, 'grad_norm': 6.823362827301025, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:48<4:26:15,  2.70s/it]  1%|â–         | 84/6000 [03:50<4:27:03,  2.71s/it]                                                   {'loss': 2.7907, 'grad_norm': 8.313796043395996, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:50<4:27:03,  2.71s/it]  1%|â–         | 85/6000 [03:53<4:35:14,  2.79s/it]                                                   {'loss': 2.7853, 'grad_norm': 6.232477188110352, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:53<4:35:14,  2.79s/it]  1%|â–         | 86/6000 [03:56<4:31:10,  2.75s/it]                                                   {'loss': 2.7848, 'grad_norm': 6.790623188018799, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [03:56<4:31:10,  2.75s/it]  1%|â–         | 87/6000 [03:58<4:26:14,  2.70s/it]                                                   {'loss': 2.799, 'grad_norm': 4.819604873657227, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [03:58<4:26:14,  2.70s/it]  1%|â–         | 88/6000 [04:01<4:25:37,  2.70s/it]                                                   {'loss': 2.8341, 'grad_norm': 17.8790283203125, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:01<4:25:37,  2.70s/it]  1%|â–         | 89/6000 [04:04<4:23:00,  2.67s/it]                                                   {'loss': 2.7732, 'grad_norm': 4.3033671379089355, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:04<4:23:00,  2.67s/it]  2%|â–         | 90/6000 [04:06<4:24:10,  2.68s/it]                                                   {'loss': 2.8392, 'grad_norm': 6.012906074523926, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:06<4:24:10,  2.68s/it]  2%|â–         | 91/6000 [04:09<4:23:00,  2.67s/it]                                                   {'loss': 2.7598, 'grad_norm': 5.158658027648926, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:09<4:23:00,  2.67s/it]  2%|â–         | 92/6000 [04:12<4:33:32,  2.78s/it]                                                   {'loss': 2.7763, 'grad_norm': 8.078295707702637, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:12<4:33:32,  2.78s/it]  2%|â–         | 93/6000 [04:15<4:33:13,  2.78s/it]                                                   {'loss': 2.7814, 'grad_norm': 4.934529781341553, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:15<4:33:13,  2.78s/it]  2%|â–         | 94/6000 [04:18<4:29:05,  2.73s/it]                                                   {'loss': 2.8027, 'grad_norm': 5.868139743804932, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:18<4:29:05,  2.73s/it]  2%|â–         | 95/6000 [04:20<4:27:26,  2.72s/it]                                                   {'loss': 2.7644, 'grad_norm': 3.7316625118255615, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:20<4:27:26,  2.72s/it]  2%|â–         | 96/6000 [04:23<4:24:59,  2.69s/it]                                                   {'loss': 2.7694, 'grad_norm': 4.870955944061279, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:23<4:24:59,  2.69s/it]  2%|â–         | 97/6000 [04:26<4:23:28,  2.68s/it]                                                   {'loss': 2.7668, 'grad_norm': 4.639068126678467, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:26<4:23:28,  2.68s/it]  2%|â–         | 98/6000 [04:28<4:21:41,  2.66s/it]                                                   {'loss': 2.7887, 'grad_norm': 4.130671501159668, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:28<4:21:41,  2.66s/it]  2%|â–         | 99/6000 [04:31<4:20:02,  2.64s/it]                                                   {'loss': 2.7625, 'grad_norm': 4.861369609832764, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:31<4:20:02,  2.64s/it]  2%|â–         | 100/6000 [04:33<4:18:49,  2.63s/it]                                                    {'loss': 2.7747, 'grad_norm': 3.825232982635498, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:33<4:18:49,  2.63s/it][2025-10-22 17:55:57,255] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:39<5:42:37,  3.48s/it]                                                    {'loss': 2.819, 'grad_norm': 18.541460037231445, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:39<5:42:37,  3.48s/it]  2%|â–         | 102/6000 [04:41<5:17:44,  3.23s/it]                                                    {'loss': 2.7905, 'grad_norm': 4.930663585662842, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:41<5:17:44,  3.23s/it]  2%|â–         | 103/6000 [04:44<5:01:32,  3.07s/it]                                                    {'loss': 2.7847, 'grad_norm': 3.830509662628174, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:44<5:01:32,  3.07s/it]  2%|â–         | 104/6000 [04:47<4:55:41,  3.01s/it]                                                    {'loss': 2.7705, 'grad_norm': 2.83184814453125, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:47<4:55:41,  3.01s/it]  2%|â–         | 105/6000 [04:50<4:47:14,  2.92s/it]                                                    {'loss': 2.7916, 'grad_norm': 4.464313507080078, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:50<4:47:14,  2.92s/it]  2%|â–         | 106/6000 [04:53<4:43:04,  2.88s/it]                                                    {'loss': 2.7786, 'grad_norm': 2.2510149478912354, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:53<4:43:04,  2.88s/it]  2%|â–         | 107/6000 [04:55<4:35:54,  2.81s/it]                                                    {'loss': 2.7958, 'grad_norm': 2.6104342937469482, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [04:55<4:35:54,  2.81s/it]  2%|â–         | 108/6000 [04:58<4:33:49,  2.79s/it]                                                    {'loss': 2.7749, 'grad_norm': 2.2802865505218506, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [04:58<4:33:49,  2.79s/it]  2%|â–         | 109/6000 [05:01<4:42:18,  2.88s/it]                                                    {'loss': 2.8326, 'grad_norm': 2.6821858882904053, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:01<4:42:18,  2.88s/it]  2%|â–         | 110/6000 [05:04<4:35:08,  2.80s/it]                                                    {'loss': 2.8489, 'grad_norm': 1.4653695821762085, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:04<4:35:08,  2.80s/it]  2%|â–         | 111/6000 [05:06<4:32:45,  2.78s/it]                                                    {'loss': 2.7753, 'grad_norm': 2.079235553741455, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:06<4:32:45,  2.78s/it]  2%|â–         | 112/6000 [05:10<4:46:00,  2.91s/it]                                                    {'loss': 2.7752, 'grad_norm': 1.6788002252578735, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:10<4:46:00,  2.91s/it]  2%|â–         | 113/6000 [05:12<4:39:06,  2.84s/it]                                                    {'loss': 2.771, 'grad_norm': 1.386096715927124, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:12<4:39:06,  2.84s/it]  2%|â–         | 114/6000 [05:15<4:35:22,  2.81s/it]                                                    {'loss': 2.7765, 'grad_norm': 2.140199899673462, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:15<4:35:22,  2.81s/it]  2%|â–         | 115/6000 [05:18<4:30:38,  2.76s/it]                                                    {'loss': 2.8498, 'grad_norm': 2.1340277194976807, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:18<4:30:38,  2.76s/it]  2%|â–         | 116/6000 [05:20<4:27:38,  2.73s/it]                                                    {'loss': 2.7917, 'grad_norm': 1.702850341796875, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:20<4:27:38,  2.73s/it]  2%|â–         | 117/6000 [05:23<4:26:16,  2.72s/it]                                                    {'loss': 2.9023, 'grad_norm': 2.0448250770568848, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:23<4:26:16,  2.72s/it]  2%|â–         | 118/6000 [05:26<4:40:27,  2.86s/it]                                                    {'loss': 2.7734, 'grad_norm': 3.878596305847168, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:26<4:40:27,  2.86s/it]  2%|â–         | 119/6000 [05:29<4:34:42,  2.80s/it]                                                    {'loss': 2.7732, 'grad_norm': 2.403472900390625, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:29<4:34:42,  2.80s/it]  2%|â–         | 120/6000 [05:32<4:31:22,  2.77s/it]                                                    {'loss': 2.7709, 'grad_norm': 7.501160621643066, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:32<4:31:22,  2.77s/it]  2%|â–         | 121/6000 [05:34<4:29:20,  2.75s/it]                                                    {'loss': 2.7719, 'grad_norm': 7.510261058807373, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:34<4:29:20,  2.75s/it]  2%|â–         | 122/6000 [05:37<4:31:13,  2.77s/it]                                                    {'loss': 2.7845, 'grad_norm': 3.211094617843628, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:37<4:31:13,  2.77s/it]  2%|â–         | 123/6000 [05:40<4:27:58,  2.74s/it]                                                    {'loss': 2.7885, 'grad_norm': 1.7285970449447632, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:40<4:27:58,  2.74s/it]  2%|â–         | 124/6000 [05:42<4:23:29,  2.69s/it]                                                    {'loss': 2.8177, 'grad_norm': 1.9933269023895264, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:42<4:23:29,  2.69s/it]  2%|â–         | 125/6000 [05:45<4:28:57,  2.75s/it]                                                    {'loss': 2.9446, 'grad_norm': 2.6404123306274414, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:45<4:28:57,  2.75s/it]  2%|â–         | 126/6000 [05:48<4:29:18,  2.75s/it]                                                    {'loss': 2.7764, 'grad_norm': 4.477931976318359, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:48<4:29:18,  2.75s/it]  2%|â–         | 127/6000 [05:51<4:30:34,  2.76s/it]                                                    {'loss': 2.7775, 'grad_norm': 2.115342617034912, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:51<4:30:34,  2.76s/it]  2%|â–         | 128/6000 [05:53<4:26:32,  2.72s/it]                                                    {'loss': 2.7874, 'grad_norm': 2.335946798324585, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [05:53<4:26:32,  2.72s/it]  2%|â–         | 129/6000 [05:56<4:26:27,  2.72s/it]                                                    {'loss': 2.851, 'grad_norm': 2.610887289047241, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [05:56<4:26:27,  2.72s/it]  2%|â–         | 130/6000 [05:59<4:24:10,  2.70s/it]                                                    {'loss': 2.7779, 'grad_norm': 2.4710729122161865, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [05:59<4:24:10,  2.70s/it]  2%|â–         | 131/6000 [06:01<4:22:36,  2.68s/it]                                                    {'loss': 2.7824, 'grad_norm': 5.597806453704834, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:01<4:22:36,  2.68s/it]  2%|â–         | 132/6000 [06:04<4:19:58,  2.66s/it]                                                    {'loss': 2.8009, 'grad_norm': 7.005764007568359, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:04<4:19:58,  2.66s/it]  2%|â–         | 133/6000 [06:07<4:20:04,  2.66s/it]                                                    {'loss': 2.7728, 'grad_norm': 3.400974750518799, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:07<4:20:04,  2.66s/it]  2%|â–         | 134/6000 [06:09<4:25:45,  2.72s/it]                                                    {'loss': 2.7617, 'grad_norm': 3.2133290767669678, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:09<4:25:45,  2.72s/it]  2%|â–         | 135/6000 [06:12<4:25:04,  2.71s/it]                                                    {'loss': 2.8025, 'grad_norm': 3.8370680809020996, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:12<4:25:04,  2.71s/it]  2%|â–         | 136/6000 [06:15<4:23:59,  2.70s/it]                                                    {'loss': 2.7855, 'grad_norm': 3.3961715698242188, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:15<4:23:59,  2.70s/it]  2%|â–         | 137/6000 [06:18<4:38:49,  2.85s/it]                                                    {'loss': 2.768, 'grad_norm': 3.6793506145477295, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:18<4:38:49,  2.85s/it]  2%|â–         | 138/6000 [06:21<4:32:05,  2.78s/it]                                                    {'loss': 2.7806, 'grad_norm': 2.543443441390991, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:21<4:32:05,  2.78s/it]  2%|â–         | 139/6000 [06:23<4:31:32,  2.78s/it]                                                    {'loss': 2.8256, 'grad_norm': 4.346631050109863, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:23<4:31:32,  2.78s/it]  2%|â–         | 140/6000 [06:26<4:38:24,  2.85s/it]                                                    {'loss': 2.7756, 'grad_norm': 2.599430561065674, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:26<4:38:24,  2.85s/it]  2%|â–         | 141/6000 [06:29<4:34:12,  2.81s/it]                                                    {'loss': 2.768, 'grad_norm': 2.871143341064453, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:29<4:34:12,  2.81s/it]  2%|â–         | 142/6000 [06:32<4:27:28,  2.74s/it]                                                    {'loss': 2.7728, 'grad_norm': 2.272918224334717, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:32<4:27:28,  2.74s/it]  2%|â–         | 143/6000 [06:34<4:25:43,  2.72s/it]                                                    {'loss': 2.7754, 'grad_norm': 2.8832123279571533, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:34<4:25:43,  2.72s/it]  2%|â–         | 144/6000 [06:37<4:24:36,  2.71s/it]                                                    {'loss': 2.7673, 'grad_norm': 2.8060848712921143, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:37<4:24:36,  2.71s/it]  2%|â–         | 145/6000 [06:40<4:22:08,  2.69s/it]                                                    {'loss': 2.7955, 'grad_norm': 3.7893307209014893, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:40<4:22:08,  2.69s/it]  2%|â–         | 146/6000 [06:42<4:22:50,  2.69s/it]                                                    {'loss': 2.8084, 'grad_norm': 5.3134379386901855, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:42<4:22:50,  2.69s/it]  2%|â–         | 147/6000 [06:45<4:20:57,  2.68s/it]                                                    {'loss': 2.743, 'grad_norm': 6.346710681915283, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:45<4:20:57,  2.68s/it]  2%|â–         | 148/6000 [06:48<4:21:19,  2.68s/it]                                                    {'loss': 2.7816, 'grad_norm': 6.784042835235596, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:48<4:21:19,  2.68s/it]  2%|â–         | 149/6000 [06:50<4:19:54,  2.67s/it]                                                    {'loss': 2.7766, 'grad_norm': 6.77567195892334, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [06:50<4:19:54,  2.67s/it]  2%|â–Ž         | 150/6000 [06:53<4:18:20,  2.65s/it]                                                    {'loss': 2.8101, 'grad_norm': 7.793860912322998, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [06:53<4:18:20,  2.65s/it][2025-10-22 17:58:16,932] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [06:58<5:23:31,  3.32s/it]                                                    {'loss': 2.838, 'grad_norm': 14.608290672302246, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [06:58<5:23:31,  3.32s/it]  3%|â–Ž         | 152/6000 [07:01<5:04:30,  3.12s/it]                                                    {'loss': 2.7659, 'grad_norm': 12.501039505004883, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:01<5:04:30,  3.12s/it]  3%|â–Ž         | 153/6000 [07:03<4:50:26,  2.98s/it]                                                    {'loss': 2.8005, 'grad_norm': 6.311398983001709, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:03<4:50:26,  2.98s/it]  3%|â–Ž         | 154/6000 [07:06<4:44:42,  2.92s/it]                                                    {'loss': 2.7657, 'grad_norm': 5.5206828117370605, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:06<4:44:42,  2.92s/it]  3%|â–Ž         | 155/6000 [07:09<4:37:44,  2.85s/it]                                                    {'loss': 2.8015, 'grad_norm': 2.700195550918579, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:09<4:37:44,  2.85s/it]  3%|â–Ž         | 156/6000 [07:11<4:34:37,  2.82s/it]                                                    {'loss': 2.8449, 'grad_norm': 3.099152088165283, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:11<4:34:37,  2.82s/it]  3%|â–Ž         | 157/6000 [07:15<4:43:30,  2.91s/it]                                                    {'loss': 2.7594, 'grad_norm': 2.9690675735473633, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:15<4:43:30,  2.91s/it]  3%|â–Ž         | 158/6000 [07:17<4:34:44,  2.82s/it]                                                    {'loss': 2.7759, 'grad_norm': 2.9582250118255615, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:17<4:34:44,  2.82s/it]  3%|â–Ž         | 159/6000 [07:20<4:30:08,  2.77s/it]                                                    {'loss': 2.7948, 'grad_norm': 2.9455037117004395, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:20<4:30:08,  2.77s/it]  3%|â–Ž         | 160/6000 [07:23<4:43:00,  2.91s/it]                                                    {'loss': 2.8025, 'grad_norm': 2.8677990436553955, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:23<4:43:00,  2.91s/it]  3%|â–Ž         | 161/6000 [07:26<4:39:20,  2.87s/it]                                                    {'loss': 2.7898, 'grad_norm': 6.33489990234375, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:26<4:39:20,  2.87s/it]  3%|â–Ž         | 162/6000 [07:28<4:32:49,  2.80s/it]                                                    {'loss': 2.7912, 'grad_norm': 3.7102298736572266, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:29<4:32:49,  2.80s/it]  3%|â–Ž         | 163/6000 [07:32<4:44:13,  2.92s/it]                                                    {'loss': 2.7701, 'grad_norm': 3.885199785232544, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:32<4:44:13,  2.92s/it]  3%|â–Ž         | 164/6000 [07:34<4:35:06,  2.83s/it]                                                    {'loss': 2.906, 'grad_norm': 3.731915235519409, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:34<4:35:06,  2.83s/it]  3%|â–Ž         | 165/6000 [07:37<4:33:11,  2.81s/it]                                                    {'loss': 2.7746, 'grad_norm': 3.1372954845428467, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:37<4:33:11,  2.81s/it]  3%|â–Ž         | 166/6000 [07:40<4:28:02,  2.76s/it]                                                    {'loss': 2.774, 'grad_norm': 2.558820962905884, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:40<4:28:02,  2.76s/it]  3%|â–Ž         | 167/6000 [07:42<4:27:05,  2.75s/it]                                                    {'loss': 2.7988, 'grad_norm': 4.325538158416748, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:42<4:27:05,  2.75s/it]  3%|â–Ž         | 168/6000 [07:45<4:27:55,  2.76s/it]                                                    {'loss': 2.8118, 'grad_norm': 2.555358409881592, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:45<4:27:55,  2.76s/it]  3%|â–Ž         | 169/6000 [07:48<4:38:50,  2.87s/it]                                                    {'loss': 2.8191, 'grad_norm': 5.1318559646606445, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:48<4:38:50,  2.87s/it]  3%|â–Ž         | 170/6000 [07:51<4:30:45,  2.79s/it]                                                    {'loss': 2.8003, 'grad_norm': 3.9275639057159424, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [07:51<4:30:45,  2.79s/it]  3%|â–Ž         | 171/6000 [07:53<4:24:20,  2.72s/it]                                                    {'loss': 2.7762, 'grad_norm': 2.2446722984313965, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [07:53<4:24:20,  2.72s/it]  3%|â–Ž         | 172/6000 [07:56<4:26:09,  2.74s/it]                                                    {'loss': 2.8, 'grad_norm': 2.4505364894866943, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [07:56<4:26:09,  2.74s/it]  3%|â–Ž         | 173/6000 [07:59<4:24:40,  2.73s/it]                                                    {'loss': 2.7823, 'grad_norm': 1.5498980283737183, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [07:59<4:24:40,  2.73s/it]  3%|â–Ž         | 174/6000 [08:02<4:43:11,  2.92s/it]                                                    {'loss': 2.7755, 'grad_norm': 1.7445741891860962, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:02<4:43:11,  2.92s/it]  3%|â–Ž         | 175/6000 [08:05<4:39:23,  2.88s/it]                                                    {'loss': 2.7947, 'grad_norm': 1.5628433227539062, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:05<4:39:23,  2.88s/it]  3%|â–Ž         | 176/6000 [08:08<4:42:23,  2.91s/it]                                                    {'loss': 2.7755, 'grad_norm': 1.9874430894851685, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:08<4:42:23,  2.91s/it]  3%|â–Ž         | 177/6000 [08:11<4:34:05,  2.82s/it]                                                    {'loss': 2.7966, 'grad_norm': 1.5314363241195679, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:11<4:34:05,  2.82s/it]  3%|â–Ž         | 178/6000 [08:13<4:30:49,  2.79s/it]                                                    {'loss': 2.7878, 'grad_norm': 1.4245305061340332, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:13<4:30:49,  2.79s/it]  3%|â–Ž         | 179/6000 [08:16<4:27:09,  2.75s/it]                                                    {'loss': 2.7889, 'grad_norm': 1.1043672561645508, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:16<4:27:09,  2.75s/it]  3%|â–Ž         | 180/6000 [08:19<4:28:06,  2.76s/it]                                                    {'loss': 2.7881, 'grad_norm': 0.9770687818527222, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:19<4:28:06,  2.76s/it]  3%|â–Ž         | 181/6000 [08:22<4:25:50,  2.74s/it]                                                    {'loss': 2.8033, 'grad_norm': 1.0971219539642334, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:22<4:25:50,  2.74s/it]  3%|â–Ž         | 182/6000 [08:24<4:24:45,  2.73s/it]                                                    {'loss': 2.8303, 'grad_norm': 1.4760735034942627, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:24<4:24:45,  2.73s/it]  3%|â–Ž         | 183/6000 [08:27<4:24:07,  2.72s/it]                                                    {'loss': 2.7731, 'grad_norm': 1.1365975141525269, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:27<4:24:07,  2.72s/it]  3%|â–Ž         | 184/6000 [08:30<4:22:30,  2.71s/it]                                                    {'loss': 2.7862, 'grad_norm': 1.6214983463287354, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:30<4:22:30,  2.71s/it]  3%|â–Ž         | 185/6000 [08:32<4:22:21,  2.71s/it]                                                    {'loss': 2.7878, 'grad_norm': 1.665736198425293, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:32<4:22:21,  2.71s/it]  3%|â–Ž         | 186/6000 [08:35<4:21:54,  2.70s/it]                                                    {'loss': 2.7883, 'grad_norm': 3.042910575866699, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:35<4:21:54,  2.70s/it]  3%|â–Ž         | 187/6000 [08:38<4:20:39,  2.69s/it]                                                    {'loss': 2.7656, 'grad_norm': 1.9346394538879395, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:38<4:20:39,  2.69s/it]  3%|â–Ž         | 188/6000 [08:40<4:18:07,  2.66s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.7258871793746948, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:40<4:18:07,  2.66s/it]  3%|â–Ž         | 189/6000 [08:43<4:17:59,  2.66s/it]                                                    {'loss': 2.7652, 'grad_norm': 2.38702392578125, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:43<4:17:59,  2.66s/it]  3%|â–Ž         | 190/6000 [08:46<4:17:06,  2.66s/it]                                                    {'loss': 2.7991, 'grad_norm': 5.5142436027526855, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:46<4:17:06,  2.66s/it]  3%|â–Ž         | 191/6000 [08:48<4:19:57,  2.69s/it]                                                    {'loss': 2.7962, 'grad_norm': 5.90826940536499, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [08:48<4:19:57,  2.69s/it]  3%|â–Ž         | 192/6000 [08:52<4:37:00,  2.86s/it]                                                    {'loss': 2.9923, 'grad_norm': 59.03152084350586, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [08:52<4:37:00,  2.86s/it]  3%|â–Ž         | 193/6000 [08:55<4:40:10,  2.89s/it]                                                    {'loss': 2.7907, 'grad_norm': 3.3719327449798584, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [08:55<4:40:10,  2.89s/it]  3%|â–Ž         | 194/6000 [08:57<4:32:40,  2.82s/it]                                                    {'loss': 2.7897, 'grad_norm': 2.1487417221069336, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [08:57<4:32:40,  2.82s/it]  3%|â–Ž         | 195/6000 [09:00<4:29:40,  2.79s/it]                                                    {'loss': 2.8063, 'grad_norm': 1.0717554092407227, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:00<4:29:40,  2.79s/it]  3%|â–Ž         | 196/6000 [09:03<4:26:51,  2.76s/it]                                                    {'loss': 2.7974, 'grad_norm': 1.1848989725112915, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:03<4:26:51,  2.76s/it]  3%|â–Ž         | 197/6000 [09:06<4:29:05,  2.78s/it]                                                    {'loss': 2.7936, 'grad_norm': 0.8307886719703674, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:06<4:29:05,  2.78s/it]  3%|â–Ž         | 198/6000 [09:08<4:28:59,  2.78s/it]                                                    {'loss': 2.7847, 'grad_norm': 1.1610825061798096, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:08<4:28:59,  2.78s/it]  3%|â–Ž         | 199/6000 [09:11<4:24:21,  2.73s/it]                                                    {'loss': 2.773, 'grad_norm': 0.8712139129638672, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:11<4:24:21,  2.73s/it]  3%|â–Ž         | 200/6000 [09:14<4:36:13,  2.86s/it]                                                    {'loss': 2.7762, 'grad_norm': 0.9222383499145508, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:14<4:36:13,  2.86s/it][2025-10-22 18:00:37,978] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test1-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:19<5:29:13,  3.41s/it]                                                    {'loss': 2.8047, 'grad_norm': 0.9202626347541809, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:19<5:29:13,  3.41s/it]  3%|â–Ž         | 202/6000 [09:21<5:06:09,  3.17s/it]                                                    {'loss': 2.7819, 'grad_norm': 1.2684342861175537, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:21<5:06:09,  3.17s/it]  3%|â–Ž         | 203/6000 [09:24<4:51:53,  3.02s/it]                                                    {'loss': 2.7909, 'grad_norm': 1.0374263525009155, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:24<4:51:53,  3.02s/it]  3%|â–Ž         | 204/6000 [09:27<4:44:59,  2.95s/it]                                                    {'loss': 2.7767, 'grad_norm': 1.3016055822372437, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:27<4:44:59,  2.95s/it]  3%|â–Ž         | 205/6000 [09:29<4:36:44,  2.87s/it]                                                    {'loss': 2.7747, 'grad_norm': 1.8549896478652954, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:29<4:36:44,  2.87s/it]  3%|â–Ž         | 206/6000 [09:32<4:29:31,  2.79s/it]                                                    {'loss': 2.8205, 'grad_norm': 1.7765674591064453, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:32<4:29:31,  2.79s/it]  3%|â–Ž         | 207/6000 [09:35<4:24:59,  2.74s/it]                                                    {'loss': 2.8692, 'grad_norm': 1.7960864305496216, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:35<4:24:59,  2.74s/it]  3%|â–Ž         | 208/6000 [09:37<4:22:15,  2.72s/it]                                                    {'loss': 2.7871, 'grad_norm': 1.356490135192871, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:37<4:22:15,  2.72s/it]  3%|â–Ž         | 209/6000 [09:40<4:20:40,  2.70s/it]                                                    {'loss': 2.7761, 'grad_norm': 1.3948055505752563, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:40<4:20:40,  2.70s/it]  4%|â–Ž         | 210/6000 [09:43<4:31:52,  2.82s/it]                                                    {'loss': 2.7778, 'grad_norm': 1.3578228950500488, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:43<4:31:52,  2.82s/it]  4%|â–Ž         | 211/6000 [09:46<4:32:09,  2.82s/it]                                                    {'loss': 2.7922, 'grad_norm': 1.141358733177185, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [09:46<4:32:09,  2.82s/it]  4%|â–Ž         | 212/6000 [09:49<4:38:02,  2.88s/it]                                                    {'loss': 2.7758, 'grad_norm': 0.949200451374054, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [09:49<4:38:02,  2.88s/it]  4%|â–Ž         | 213/6000 [09:52<4:34:16,  2.84s/it]                                                    {'loss': 2.7795, 'grad_norm': 0.8266555666923523, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [09:52<4:34:16,  2.84s/it]  4%|â–Ž         | 214/6000 [09:54<4:27:33,  2.77s/it]                                                    {'loss': 2.7812, 'grad_norm': 0.946435809135437, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [09:54<4:27:33,  2.77s/it]  4%|â–Ž         | 215/6000 [09:57<4:23:27,  2.73s/it]                                                    {'loss': 2.8012, 'grad_norm': 0.916143536567688, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [09:57<4:23:27,  2.73s/it]  4%|â–Ž         | 216/6000 [10:00<4:23:01,  2.73s/it]                                                    {'loss': 2.7833, 'grad_norm': 0.8627703785896301, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:00<4:23:01,  2.73s/it]  4%|â–Ž         | 217/6000 [10:03<4:24:38,  2.75s/it]                                                    {'loss': 2.8238, 'grad_norm': 0.5973319411277771, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:03<4:24:38,  2.75s/it]  4%|â–Ž         | 218/6000 [10:05<4:24:21,  2.74s/it]                                                    {'loss': 2.7881, 'grad_norm': 0.4892723858356476, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:05<4:24:21,  2.74s/it]  4%|â–Ž         | 219/6000 [10:08<4:19:26,  2.69s/it]                                                    {'loss': 2.8206, 'grad_norm': 0.6204646229743958, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:08<4:19:26,  2.69s/it]  4%|â–Ž         | 220/6000 [10:10<4:18:46,  2.69s/it]                                                    {'loss': 2.777, 'grad_norm': 0.6664259433746338, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:10<4:18:46,  2.69s/it]  4%|â–Ž         | 221/6000 [10:13<4:19:53,  2.70s/it]                                                    {'loss': 2.8064, 'grad_norm': 0.7622836828231812, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:13<4:19:53,  2.70s/it]  4%|â–Ž         | 222/6000 [10:16<4:18:53,  2.69s/it]                                                    {'loss': 2.7716, 'grad_norm': 0.6232953667640686, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:16<4:18:53,  2.69s/it]  4%|â–Ž         | 223/6000 [10:19<4:17:42,  2.68s/it]                                                    {'loss': 2.776, 'grad_norm': 0.5887770652770996, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:19<4:17:42,  2.68s/it]  4%|â–Ž         | 224/6000 [10:21<4:14:54,  2.65s/it]                                                    {'loss': 2.8173, 'grad_norm': 0.615415632724762, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:21<4:14:54,  2.65s/it]  4%|â–         | 225/6000 [10:24<4:15:46,  2.66s/it]                                                    {'loss': 2.7737, 'grad_norm': 0.6488717198371887, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:24<4:15:46,  2.66s/it]  4%|â–         | 226/6000 [10:27<4:28:54,  2.79s/it]                                                    {'loss': 2.7806, 'grad_norm': 0.7746310234069824, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:27<4:28:54,  2.79s/it]  4%|â–         | 227/6000 [10:30<4:27:43,  2.78s/it]                                                    {'loss': 2.8217, 'grad_norm': 0.49766528606414795, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:30<4:27:43,  2.78s/it]  4%|â–         | 228/6000 [10:32<4:24:43,  2.75s/it]                                                    {'loss': 2.7786, 'grad_norm': 0.8423561453819275, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:32<4:24:43,  2.75s/it]  4%|â–         | 229/6000 [10:35<4:19:00,  2.69s/it]                                                    {'loss': 2.7918, 'grad_norm': 0.664801836013794, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:35<4:19:00,  2.69s/it]  4%|â–         | 230/6000 [10:38<4:18:35,  2.69s/it]                                                    {'loss': 2.805, 'grad_norm': 0.7448467016220093, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:38<4:18:35,  2.69s/it]  4%|â–         | 231/6000 [10:40<4:16:09,  2.66s/it]                                                    {'loss': 2.7817, 'grad_norm': 0.6373269557952881, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:40<4:16:09,  2.66s/it]  4%|â–         | 232/6000 [10:43<4:17:17,  2.68s/it]                                                    {'loss': 2.7738, 'grad_norm': 0.8278130888938904, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [10:43<4:17:17,  2.68s/it]  4%|â–         | 233/6000 [10:46<4:27:05,  2.78s/it]                                                    {'loss': 2.7758, 'grad_norm': 0.6107097864151001, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [10:46<4:27:05,  2.78s/it]  4%|â–         | 234/6000 [10:49<4:32:00,  2.83s/it]                                                    {'loss': 2.7735, 'grad_norm': 0.6633318066596985, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [10:49<4:32:00,  2.83s/it]