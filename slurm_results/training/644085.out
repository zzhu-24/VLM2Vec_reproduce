==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailToken-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 5000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/train.log
W1019 17:29:53.812000 136027599525696 torch/distributed/run.py:779] 
W1019 17:29:53.812000 136027599525696 torch/distributed/run.py:779] *****************************************
W1019 17:29:53.812000 136027599525696 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1019 17:29:53.812000 136027599525696 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-19 17:30:05,505] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.10it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.81it/s]
wandb: setting up run ykot0zzc
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_173005-ykot0zzc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailToken-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/ykot0zzc
[2025-10-19 17:30:07,027] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.21it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.05it/s]
[2025-10-19 17:30:07,662] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-19 17:30:07,667] INFO [src.utils:19] Loading lora adapter from TailTokenWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-19 17:30:16,711] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-19 17:30:17,904] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-19 17:30:17,905] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-19 17:30:22,513] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-19 17:30:22,514] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-19 17:30:23,357] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-19 17:30:23,358] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-19 17:30:23,358] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-19 17:30:23,360] INFO [src.utils:19] ==================================================
[2025-10-19 17:30:23,360] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-19 17:30:23,362] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:30:23,363] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:30:23,363] INFO [src.utils:19] ==================================================
[2025-10-19 17:30:24,634] INFO [src.utils:19] âœ… Custom optimizer (gme.learnable_token only) enabled
[2025-10-19 17:30:25,075] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:30:25,076] INFO [src.trainer:343]   Num examples = 160,000
[2025-10-19 17:30:25,076] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:30:25,076] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-19 17:30:25,076] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-19 17:30:25,076] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:30:25,076] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:30:25,080] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:30:25,081] INFO [src.trainer:343]   Num examples = 160,000
[2025-10-19 17:30:25,082] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:30:25,082] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-19 17:30:25,082] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-19 17:30:25,083] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:30:25,083] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:30:25,084] INFO [src.trainer:351]   Number of trainable parameters = 1,536
[2025-10-19 17:30:25,091] INFO [src.trainer:351]   Number of trainable parameters = 1,536
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1019 17:30:28.036905228 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1019 17:30:28.085651777 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/5000 [00:03<5:23:22,  3.88s/it]                                                  {'loss': 10.8339, 'grad_norm': 5.083098888397217, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 1/5000 [00:03<5:23:22,  3.88s/it]  0%|          | 2/5000 [00:06<4:12:03,  3.03s/it]                                                  {'loss': 7.9677, 'grad_norm': 3.3069896697998047, 'learning_rate': 2e-05, 'epoch': 0.0}
  0%|          | 2/5000 [00:06<4:12:03,  3.03s/it]  0%|          | 3/5000 [00:08<3:53:15,  2.80s/it]                                                  {'loss': 8.4221, 'grad_norm': 3.659317970275879, 'learning_rate': 3e-05, 'epoch': 0.0}
  0%|          | 3/5000 [00:08<3:53:15,  2.80s/it]  0%|          | 4/5000 [00:11<3:43:39,  2.69s/it]                                                  {'loss': 8.7536, 'grad_norm': 3.7915613651275635, 'learning_rate': 4e-05, 'epoch': 0.0}
  0%|          | 4/5000 [00:11<3:43:39,  2.69s/it]  0%|          | 5/5000 [00:13<3:37:57,  2.62s/it]                                                  {'loss': 8.9146, 'grad_norm': 3.721524238586426, 'learning_rate': 5e-05, 'epoch': 0.0}
  0%|          | 5/5000 [00:13<3:37:57,  2.62s/it]  0%|          | 6/5000 [00:16<3:34:56,  2.58s/it]                                                  {'loss': 10.4233, 'grad_norm': 4.45608377456665, 'learning_rate': 6e-05, 'epoch': 0.0}
  0%|          | 6/5000 [00:16<3:34:56,  2.58s/it]  0%|          | 7/5000 [00:18<3:32:49,  2.56s/it]                                                  {'loss': 10.2865, 'grad_norm': 4.553493022918701, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.0}
  0%|          | 7/5000 [00:18<3:32:49,  2.56s/it]  0%|          | 8/5000 [00:21<3:27:41,  2.50s/it]                                                  {'loss': 10.2113, 'grad_norm': 4.104763031005859, 'learning_rate': 8e-05, 'epoch': 0.0}
  0%|          | 8/5000 [00:21<3:27:41,  2.50s/it]  0%|          | 9/5000 [00:23<3:30:02,  2.52s/it]                                                  {'loss': 7.2841, 'grad_norm': 3.0576605796813965, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.0}
  0%|          | 9/5000 [00:23<3:30:02,  2.52s/it]  0%|          | 10/5000 [00:26<3:26:36,  2.48s/it]                                                   {'loss': 9.9327, 'grad_norm': 4.005979537963867, 'learning_rate': 0.0001, 'epoch': 0.0}
  0%|          | 10/5000 [00:26<3:26:36,  2.48s/it]  0%|          | 11/5000 [00:29<3:34:53,  2.58s/it]                                                   {'loss': 11.2463, 'grad_norm': 4.961185455322266, 'learning_rate': 0.00011, 'epoch': 0.0}
  0%|          | 11/5000 [00:29<3:34:53,  2.58s/it]  0%|          | 12/5000 [00:31<3:38:38,  2.63s/it]                                                   {'loss': 10.3486, 'grad_norm': 4.617913246154785, 'learning_rate': 0.00012, 'epoch': 0.0}
  0%|          | 12/5000 [00:31<3:38:38,  2.63s/it]  0%|          | 13/5000 [00:34<3:37:08,  2.61s/it]                                                   {'loss': 10.0106, 'grad_norm': 4.458800792694092, 'learning_rate': 0.00013000000000000002, 'epoch': 0.0}
  0%|          | 13/5000 [00:34<3:37:08,  2.61s/it]  0%|          | 14/5000 [00:37<3:38:41,  2.63s/it]                                                   {'loss': 10.603, 'grad_norm': 4.678225994110107, 'learning_rate': 0.00014000000000000001, 'epoch': 0.0}
  0%|          | 14/5000 [00:37<3:38:41,  2.63s/it]  0%|          | 15/5000 [00:39<3:35:05,  2.59s/it]                                                   {'loss': 9.2695, 'grad_norm': 4.3146867752075195, 'learning_rate': 0.00015, 'epoch': 0.0}
  0%|          | 15/5000 [00:39<3:35:05,  2.59s/it]  0%|          | 16/5000 [00:41<3:32:16,  2.56s/it]                                                   {'loss': 9.8396, 'grad_norm': 3.918166160583496, 'learning_rate': 0.00016, 'epoch': 0.0}
  0%|          | 16/5000 [00:41<3:32:16,  2.56s/it]  0%|          | 17/5000 [00:44<3:30:52,  2.54s/it]                                                   {'loss': 9.1811, 'grad_norm': 4.023673057556152, 'learning_rate': 0.00017, 'epoch': 0.0}
  0%|          | 17/5000 [00:44<3:30:52,  2.54s/it]  0%|          | 18/5000 [00:47<3:31:42,  2.55s/it]                                                   {'loss': 8.2559, 'grad_norm': 3.643026351928711, 'learning_rate': 0.00017999999999999998, 'epoch': 0.0}
  0%|          | 18/5000 [00:47<3:31:42,  2.55s/it]  0%|          | 19/5000 [00:49<3:30:22,  2.53s/it]                                                   {'loss': 9.8159, 'grad_norm': 4.175889492034912, 'learning_rate': 0.00019, 'epoch': 0.0}
  0%|          | 19/5000 [00:49<3:30:22,  2.53s/it]  0%|          | 20/5000 [00:52<3:29:36,  2.53s/it]                                                   {'loss': 9.9198, 'grad_norm': 4.689266204833984, 'learning_rate': 0.0002, 'epoch': 0.0}
  0%|          | 20/5000 [00:52<3:29:36,  2.53s/it]  0%|          | 21/5000 [00:54<3:34:17,  2.58s/it]                                                   {'loss': 7.7941, 'grad_norm': 3.804499864578247, 'learning_rate': 0.00021, 'epoch': 0.0}
  0%|          | 21/5000 [00:54<3:34:17,  2.58s/it]  0%|          | 22/5000 [00:57<3:33:41,  2.58s/it]                                                   {'loss': 9.8562, 'grad_norm': 3.541241407394409, 'learning_rate': 0.00022, 'epoch': 0.0}
  0%|          | 22/5000 [00:57<3:33:41,  2.58s/it]  0%|          | 23/5000 [00:59<3:31:53,  2.55s/it]                                                   {'loss': 9.8564, 'grad_norm': 3.8486766815185547, 'learning_rate': 0.00023, 'epoch': 0.0}
  0%|          | 23/5000 [00:59<3:31:53,  2.55s/it]  0%|          | 24/5000 [01:02<3:31:59,  2.56s/it]                                                   {'loss': 6.4488, 'grad_norm': 2.821211576461792, 'learning_rate': 0.00024, 'epoch': 0.0}
  0%|          | 24/5000 [01:02<3:31:59,  2.56s/it]  0%|          | 25/5000 [01:04<3:30:28,  2.54s/it]                                                   {'loss': 9.0048, 'grad_norm': 3.895115375518799, 'learning_rate': 0.00025, 'epoch': 0.01}
  0%|          | 25/5000 [01:04<3:30:28,  2.54s/it]  1%|          | 26/5000 [01:07<3:32:22,  2.56s/it]                                                   {'loss': 10.6906, 'grad_norm': 4.5008320808410645, 'learning_rate': 0.00026000000000000003, 'epoch': 0.01}
  1%|          | 26/5000 [01:07<3:32:22,  2.56s/it]  1%|          | 27/5000 [01:10<3:32:23,  2.56s/it]                                                   {'loss': 10.5576, 'grad_norm': 4.993772983551025, 'learning_rate': 0.00027, 'epoch': 0.01}
  1%|          | 27/5000 [01:10<3:32:23,  2.56s/it]  1%|          | 28/5000 [01:13<3:53:13,  2.81s/it]                                                   {'loss': 8.4532, 'grad_norm': 3.8221888542175293, 'learning_rate': 0.00028000000000000003, 'epoch': 0.01}
  1%|          | 28/5000 [01:13<3:53:13,  2.81s/it]  1%|          | 29/5000 [01:15<3:44:13,  2.71s/it]                                                   {'loss': 8.8029, 'grad_norm': 3.216134786605835, 'learning_rate': 0.00029, 'epoch': 0.01}
  1%|          | 29/5000 [01:15<3:44:13,  2.71s/it]  1%|          | 30/5000 [01:18<3:40:26,  2.66s/it]                                                   {'loss': 9.3, 'grad_norm': 4.094573020935059, 'learning_rate': 0.0003, 'epoch': 0.01}
  1%|          | 30/5000 [01:18<3:40:26,  2.66s/it]  1%|          | 31/5000 [01:21<3:37:28,  2.63s/it]                                                   {'loss': 7.9672, 'grad_norm': 3.5223047733306885, 'learning_rate': 0.00031, 'epoch': 0.01}
  1%|          | 31/5000 [01:21<3:37:28,  2.63s/it]  1%|          | 32/5000 [01:23<3:34:32,  2.59s/it]                                                   {'loss': 8.7859, 'grad_norm': 3.7045037746429443, 'learning_rate': 0.00032, 'epoch': 0.01}
  1%|          | 32/5000 [01:23<3:34:32,  2.59s/it]  1%|          | 33/5000 [01:26<3:34:23,  2.59s/it]                                                   {'loss': 8.9883, 'grad_norm': 3.590636730194092, 'learning_rate': 0.00033, 'epoch': 0.01}
  1%|          | 33/5000 [01:26<3:34:23,  2.59s/it]  1%|          | 34/5000 [01:28<3:32:29,  2.57s/it]                                                   {'loss': 10.4087, 'grad_norm': 5.4784722328186035, 'learning_rate': 0.00034, 'epoch': 0.01}
  1%|          | 34/5000 [01:28<3:32:29,  2.57s/it]  1%|          | 35/5000 [01:31<3:34:01,  2.59s/it]                                                   {'loss': 8.3144, 'grad_norm': 3.0885860919952393, 'learning_rate': 0.00035, 'epoch': 0.01}
  1%|          | 35/5000 [01:31<3:34:01,  2.59s/it]  1%|          | 36/5000 [01:33<3:30:15,  2.54s/it]                                                   {'loss': 10.0388, 'grad_norm': 3.944532871246338, 'learning_rate': 0.00035999999999999997, 'epoch': 0.01}
  1%|          | 36/5000 [01:33<3:30:15,  2.54s/it]  1%|          | 37/5000 [01:36<3:28:58,  2.53s/it]                                                   {'loss': 7.8643, 'grad_norm': 3.0064940452575684, 'learning_rate': 0.00037, 'epoch': 0.01}
  1%|          | 37/5000 [01:36<3:28:58,  2.53s/it]  1%|          | 38/5000 [01:38<3:26:59,  2.50s/it]                                                   {'loss': 8.2457, 'grad_norm': 3.437138557434082, 'learning_rate': 0.00038, 'epoch': 0.01}
  1%|          | 38/5000 [01:38<3:26:59,  2.50s/it]  1%|          | 39/5000 [01:41<3:26:54,  2.50s/it]                                                   {'loss': 6.9767, 'grad_norm': 2.2846126556396484, 'learning_rate': 0.00039000000000000005, 'epoch': 0.01}
  1%|          | 39/5000 [01:41<3:26:54,  2.50s/it]  1%|          | 40/5000 [01:43<3:28:25,  2.52s/it]                                                   {'loss': 9.2944, 'grad_norm': 3.520918846130371, 'learning_rate': 0.0004, 'epoch': 0.01}
  1%|          | 40/5000 [01:43<3:28:25,  2.52s/it]  1%|          | 41/5000 [01:46<3:27:49,  2.51s/it]                                                   {'loss': 9.1956, 'grad_norm': 3.4364328384399414, 'learning_rate': 0.00041, 'epoch': 0.01}
  1%|          | 41/5000 [01:46<3:27:49,  2.51s/it]  1%|          | 42/5000 [01:48<3:28:34,  2.52s/it]                                                   {'loss': 9.2587, 'grad_norm': 3.7148241996765137, 'learning_rate': 0.00042, 'epoch': 0.01}
  1%|          | 42/5000 [01:48<3:28:34,  2.52s/it]  1%|          | 43/5000 [01:52<3:54:00,  2.83s/it]                                                   {'loss': 7.8386, 'grad_norm': 3.0898349285125732, 'learning_rate': 0.00043, 'epoch': 0.01}
  1%|          | 43/5000 [01:52<3:54:00,  2.83s/it]  1%|          | 44/5000 [01:55<3:58:19,  2.89s/it]                                                   {'loss': 8.3567, 'grad_norm': 3.485466957092285, 'learning_rate': 0.00044, 'epoch': 0.01}
  1%|          | 44/5000 [01:55<3:58:19,  2.89s/it]  1%|          | 45/5000 [01:57<3:51:29,  2.80s/it]                                                   {'loss': 8.5906, 'grad_norm': 3.574552059173584, 'learning_rate': 0.00045000000000000004, 'epoch': 0.01}
  1%|          | 45/5000 [01:57<3:51:29,  2.80s/it]  1%|          | 46/5000 [02:00<3:47:07,  2.75s/it]                                                   {'loss': 7.7441, 'grad_norm': 3.3225927352905273, 'learning_rate': 0.00046, 'epoch': 0.01}
  1%|          | 46/5000 [02:00<3:47:07,  2.75s/it]  1%|          | 47/5000 [02:03<3:41:25,  2.68s/it]                                                   {'loss': 7.9834, 'grad_norm': 3.2625186443328857, 'learning_rate': 0.00047, 'epoch': 0.01}
  1%|          | 47/5000 [02:03<3:41:25,  2.68s/it]  1%|          | 48/5000 [02:05<3:40:54,  2.68s/it]                                                   {'loss': 7.8743, 'grad_norm': 3.0526812076568604, 'learning_rate': 0.00048, 'epoch': 0.01}
  1%|          | 48/5000 [02:05<3:40:54,  2.68s/it]  1%|          | 49/5000 [02:08<3:35:08,  2.61s/it]                                                   {'loss': 7.5942, 'grad_norm': 3.384382724761963, 'learning_rate': 0.00049, 'epoch': 0.01}
  1%|          | 49/5000 [02:08<3:35:08,  2.61s/it]  1%|          | 50/5000 [02:10<3:38:31,  2.65s/it]                                                   {'loss': 7.6739, 'grad_norm': 2.740912437438965, 'learning_rate': 0.0005, 'epoch': 0.01}
  1%|          | 50/5000 [02:10<3:38:31,  2.65s/it][2025-10-19 17:32:36,229] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/5000 [02:14<4:05:26,  2.98s/it]                                                   {'loss': 7.6269, 'grad_norm': 2.909257650375366, 'learning_rate': 0.00051, 'epoch': 0.01}
  1%|          | 51/5000 [02:14<4:05:26,  2.98s/it]  1%|          | 52/5000 [02:17<3:52:31,  2.82s/it]                                                   {'loss': 7.2377, 'grad_norm': 2.7817938327789307, 'learning_rate': 0.0005200000000000001, 'epoch': 0.01}
  1%|          | 52/5000 [02:17<3:52:31,  2.82s/it]  1%|          | 53/5000 [02:19<3:47:30,  2.76s/it]                                                   {'loss': 10.3938, 'grad_norm': 3.8764283657073975, 'learning_rate': 0.0005300000000000001, 'epoch': 0.01}
  1%|          | 53/5000 [02:19<3:47:30,  2.76s/it]  1%|          | 54/5000 [02:22<3:39:16,  2.66s/it]                                                   {'loss': 8.5688, 'grad_norm': 3.3551952838897705, 'learning_rate': 0.00054, 'epoch': 0.01}
  1%|          | 54/5000 [02:22<3:39:16,  2.66s/it]  1%|          | 55/5000 [02:24<3:34:49,  2.61s/it]                                                   {'loss': 8.7779, 'grad_norm': 4.075362205505371, 'learning_rate': 0.00055, 'epoch': 0.01}
  1%|          | 55/5000 [02:24<3:34:49,  2.61s/it]  1%|          | 56/5000 [02:27<3:33:54,  2.60s/it]                                                   {'loss': 7.8927, 'grad_norm': 3.143848180770874, 'learning_rate': 0.0005600000000000001, 'epoch': 0.01}
  1%|          | 56/5000 [02:27<3:33:54,  2.60s/it]  1%|          | 57/5000 [02:29<3:31:21,  2.57s/it]                                                   {'loss': 8.0286, 'grad_norm': 3.827143430709839, 'learning_rate': 0.00057, 'epoch': 0.01}
  1%|          | 57/5000 [02:29<3:31:21,  2.57s/it]  1%|          | 58/5000 [02:32<3:29:37,  2.54s/it]                                                   {'loss': 8.176, 'grad_norm': 3.1924595832824707, 'learning_rate': 0.00058, 'epoch': 0.01}
  1%|          | 58/5000 [02:32<3:29:37,  2.54s/it]  1%|          | 59/5000 [02:34<3:25:49,  2.50s/it]                                                   {'loss': 8.4929, 'grad_norm': 3.372147560119629, 'learning_rate': 0.00059, 'epoch': 0.01}
  1%|          | 59/5000 [02:34<3:25:49,  2.50s/it]  1%|          | 60/5000 [02:37<3:24:09,  2.48s/it]                                                   {'loss': 7.2565, 'grad_norm': 2.4487218856811523, 'learning_rate': 0.0006, 'epoch': 0.01}
  1%|          | 60/5000 [02:37<3:24:09,  2.48s/it]  1%|          | 61/5000 [02:39<3:22:17,  2.46s/it]                                                   {'loss': 6.7957, 'grad_norm': 2.41943621635437, 'learning_rate': 0.00061, 'epoch': 0.01}
  1%|          | 61/5000 [02:39<3:22:17,  2.46s/it]  1%|          | 62/5000 [02:42<3:24:26,  2.48s/it]                                                   {'loss': 8.6769, 'grad_norm': 3.560673236846924, 'learning_rate': 0.00062, 'epoch': 0.01}
  1%|          | 62/5000 [02:42<3:24:26,  2.48s/it]  1%|â–         | 63/5000 [02:44<3:24:42,  2.49s/it]                                                   {'loss': 7.1013, 'grad_norm': 2.2915616035461426, 'learning_rate': 0.00063, 'epoch': 0.01}
  1%|â–         | 63/5000 [02:44<3:24:42,  2.49s/it]  1%|â–         | 64/5000 [02:46<3:22:51,  2.47s/it]                                                   {'loss': 6.9952, 'grad_norm': 2.43569016456604, 'learning_rate': 0.00064, 'epoch': 0.01}
  1%|â–         | 64/5000 [02:46<3:22:51,  2.47s/it]  1%|â–         | 65/5000 [02:49<3:20:58,  2.44s/it]                                                   {'loss': 6.8483, 'grad_norm': 2.1755950450897217, 'learning_rate': 0.0006500000000000001, 'epoch': 0.01}
  1%|â–         | 65/5000 [02:49<3:20:58,  2.44s/it]  1%|â–         | 66/5000 [02:52<3:33:04,  2.59s/it]                                                   {'loss': 6.469, 'grad_norm': 2.4926064014434814, 'learning_rate': 0.00066, 'epoch': 0.01}
  1%|â–         | 66/5000 [02:52<3:33:04,  2.59s/it]  1%|â–         | 67/5000 [02:54<3:30:24,  2.56s/it]                                                   {'loss': 6.5592, 'grad_norm': 2.384286642074585, 'learning_rate': 0.00067, 'epoch': 0.01}
  1%|â–         | 67/5000 [02:54<3:30:24,  2.56s/it]  1%|â–         | 68/5000 [02:57<3:26:14,  2.51s/it]                                                   {'loss': 6.5793, 'grad_norm': 2.2316501140594482, 'learning_rate': 0.00068, 'epoch': 0.01}
  1%|â–         | 68/5000 [02:57<3:26:14,  2.51s/it]  1%|â–         | 69/5000 [02:59<3:25:20,  2.50s/it]                                                   {'loss': 8.2836, 'grad_norm': 3.5923948287963867, 'learning_rate': 0.00069, 'epoch': 0.01}
  1%|â–         | 69/5000 [02:59<3:25:20,  2.50s/it]  1%|â–         | 70/5000 [03:02<3:28:48,  2.54s/it]                                                   {'loss': 6.6288, 'grad_norm': 2.3372416496276855, 'learning_rate': 0.0007, 'epoch': 0.01}
  1%|â–         | 70/5000 [03:02<3:28:48,  2.54s/it]  1%|â–         | 71/5000 [03:04<3:24:58,  2.50s/it]                                                   {'loss': 8.1501, 'grad_norm': 2.7485134601593018, 'learning_rate': 0.00071, 'epoch': 0.01}
  1%|â–         | 71/5000 [03:04<3:24:58,  2.50s/it]  1%|â–         | 72/5000 [03:07<3:35:05,  2.62s/it]                                                   {'loss': 7.7116, 'grad_norm': 3.0485455989837646, 'learning_rate': 0.0007199999999999999, 'epoch': 0.01}
  1%|â–         | 72/5000 [03:07<3:35:05,  2.62s/it]  1%|â–         | 73/5000 [03:10<3:33:20,  2.60s/it]                                                   {'loss': 6.9547, 'grad_norm': 2.6045563220977783, 'learning_rate': 0.00073, 'epoch': 0.01}
  1%|â–         | 73/5000 [03:10<3:33:20,  2.60s/it]  1%|â–         | 74/5000 [03:12<3:28:20,  2.54s/it]                                                   {'loss': 7.0652, 'grad_norm': 2.6777563095092773, 'learning_rate': 0.00074, 'epoch': 0.01}
  1%|â–         | 74/5000 [03:12<3:28:20,  2.54s/it]  2%|â–         | 75/5000 [03:14<3:24:27,  2.49s/it]                                                   {'loss': 5.9643, 'grad_norm': 1.7928144931793213, 'learning_rate': 0.00075, 'epoch': 0.01}
  2%|â–         | 75/5000 [03:14<3:24:27,  2.49s/it]  2%|â–         | 76/5000 [03:17<3:27:26,  2.53s/it]                                                   {'loss': 7.1732, 'grad_norm': 3.0555624961853027, 'learning_rate': 0.00076, 'epoch': 0.02}
  2%|â–         | 76/5000 [03:17<3:27:26,  2.53s/it]  2%|â–         | 77/5000 [03:20<3:27:57,  2.53s/it]                                                   {'loss': 8.2925, 'grad_norm': 3.5195229053497314, 'learning_rate': 0.0007700000000000001, 'epoch': 0.02}
  2%|â–         | 77/5000 [03:20<3:27:57,  2.53s/it]  2%|â–         | 78/5000 [03:22<3:36:11,  2.64s/it]                                                   {'loss': 6.8813, 'grad_norm': 2.605862617492676, 'learning_rate': 0.0007800000000000001, 'epoch': 0.02}
  2%|â–         | 78/5000 [03:22<3:36:11,  2.64s/it]  2%|â–         | 79/5000 [03:25<3:34:27,  2.61s/it]                                                   {'loss': 7.0108, 'grad_norm': 2.8017218112945557, 'learning_rate': 0.00079, 'epoch': 0.02}
  2%|â–         | 79/5000 [03:25<3:34:27,  2.61s/it]  2%|â–         | 80/5000 [03:27<3:31:45,  2.58s/it]                                                   {'loss': 5.9603, 'grad_norm': 1.9917219877243042, 'learning_rate': 0.0008, 'epoch': 0.02}
  2%|â–         | 80/5000 [03:27<3:31:45,  2.58s/it]  2%|â–         | 81/5000 [03:30<3:29:49,  2.56s/it]                                                   {'loss': 6.9624, 'grad_norm': 2.770055055618286, 'learning_rate': 0.0008100000000000001, 'epoch': 0.02}
  2%|â–         | 81/5000 [03:30<3:29:49,  2.56s/it]  2%|â–         | 82/5000 [03:33<3:29:34,  2.56s/it]                                                   {'loss': 6.8953, 'grad_norm': 2.5392308235168457, 'learning_rate': 0.00082, 'epoch': 0.02}
  2%|â–         | 82/5000 [03:33<3:29:34,  2.56s/it]  2%|â–         | 83/5000 [03:35<3:28:15,  2.54s/it]                                                   {'loss': 8.0839, 'grad_norm': 2.7961275577545166, 'learning_rate': 0.00083, 'epoch': 0.02}
  2%|â–         | 83/5000 [03:35<3:28:15,  2.54s/it]  2%|â–         | 84/5000 [03:38<3:30:22,  2.57s/it]                                                   {'loss': 6.0792, 'grad_norm': 1.8193696737289429, 'learning_rate': 0.00084, 'epoch': 0.02}
  2%|â–         | 84/5000 [03:38<3:30:22,  2.57s/it]  2%|â–         | 85/5000 [03:40<3:36:53,  2.65s/it]                                                   {'loss': 6.8407, 'grad_norm': 2.512162446975708, 'learning_rate': 0.00085, 'epoch': 0.02}
  2%|â–         | 85/5000 [03:40<3:36:53,  2.65s/it]  2%|â–         | 86/5000 [03:43<3:33:46,  2.61s/it]                                                   {'loss': 5.4631, 'grad_norm': 1.7133616209030151, 'learning_rate': 0.00086, 'epoch': 0.02}
  2%|â–         | 86/5000 [03:43<3:33:46,  2.61s/it]  2%|â–         | 87/5000 [03:45<3:29:45,  2.56s/it]                                                   {'loss': 7.2215, 'grad_norm': 2.759911060333252, 'learning_rate': 0.00087, 'epoch': 0.02}
  2%|â–         | 87/5000 [03:45<3:29:45,  2.56s/it]  2%|â–         | 88/5000 [03:48<3:28:24,  2.55s/it]                                                   {'loss': 7.7175, 'grad_norm': 2.3739593029022217, 'learning_rate': 0.00088, 'epoch': 0.02}
  2%|â–         | 88/5000 [03:48<3:28:24,  2.55s/it]  2%|â–         | 89/5000 [03:50<3:25:02,  2.50s/it]                                                   {'loss': 5.7615, 'grad_norm': 1.5788546800613403, 'learning_rate': 0.0008900000000000001, 'epoch': 0.02}
  2%|â–         | 89/5000 [03:50<3:25:02,  2.50s/it]  2%|â–         | 90/5000 [03:53<3:26:56,  2.53s/it]                                                   {'loss': 5.7103, 'grad_norm': 1.7298119068145752, 'learning_rate': 0.0009000000000000001, 'epoch': 0.02}
  2%|â–         | 90/5000 [03:53<3:26:56,  2.53s/it]  2%|â–         | 91/5000 [03:56<3:28:16,  2.55s/it]                                                   {'loss': 5.7616, 'grad_norm': 1.6879284381866455, 'learning_rate': 0.00091, 'epoch': 0.02}
  2%|â–         | 91/5000 [03:56<3:28:16,  2.55s/it]  2%|â–         | 92/5000 [03:58<3:36:19,  2.64s/it]                                                   {'loss': 6.2779, 'grad_norm': 1.8846023082733154, 'learning_rate': 0.00092, 'epoch': 0.02}
  2%|â–         | 92/5000 [03:58<3:36:19,  2.64s/it]  2%|â–         | 93/5000 [04:01<3:37:09,  2.66s/it]                                                   {'loss': 6.3957, 'grad_norm': 1.943343162536621, 'learning_rate': 0.00093, 'epoch': 0.02}
  2%|â–         | 93/5000 [04:01<3:37:09,  2.66s/it]  2%|â–         | 94/5000 [04:04<3:34:06,  2.62s/it]                                                   {'loss': 6.6721, 'grad_norm': 2.2039334774017334, 'learning_rate': 0.00094, 'epoch': 0.02}
  2%|â–         | 94/5000 [04:04<3:34:06,  2.62s/it]  2%|â–         | 95/5000 [04:06<3:31:36,  2.59s/it]                                                   {'loss': 5.9562, 'grad_norm': 2.303063154220581, 'learning_rate': 0.00095, 'epoch': 0.02}
  2%|â–         | 95/5000 [04:06<3:31:36,  2.59s/it]  2%|â–         | 96/5000 [04:09<3:29:36,  2.56s/it]                                                   {'loss': 5.7206, 'grad_norm': 1.890728235244751, 'learning_rate': 0.00096, 'epoch': 0.02}
  2%|â–         | 96/5000 [04:09<3:29:36,  2.56s/it]  2%|â–         | 97/5000 [04:11<3:28:54,  2.56s/it]                                                   {'loss': 6.4842, 'grad_norm': 1.9426096677780151, 'learning_rate': 0.0009699999999999999, 'epoch': 0.02}
  2%|â–         | 97/5000 [04:11<3:28:54,  2.56s/it]  2%|â–         | 98/5000 [04:14<3:26:50,  2.53s/it]                                                   {'loss': 5.4009, 'grad_norm': 1.5790603160858154, 'learning_rate': 0.00098, 'epoch': 0.02}
  2%|â–         | 98/5000 [04:14<3:26:50,  2.53s/it]  2%|â–         | 99/5000 [04:16<3:24:32,  2.50s/it]                                                   {'loss': 5.2974, 'grad_norm': 1.5883183479309082, 'learning_rate': 0.00099, 'epoch': 0.02}
  2%|â–         | 99/5000 [04:16<3:24:32,  2.50s/it]  2%|â–         | 100/5000 [04:19<3:22:51,  2.48s/it]                                                    {'loss': 5.8283, 'grad_norm': 1.7879329919815063, 'learning_rate': 0.001, 'epoch': 0.02}
  2%|â–         | 100/5000 [04:19<3:22:51,  2.48s/it][2025-10-19 17:34:44,342] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/5000 [04:23<4:15:19,  3.13s/it]                                                    {'loss': 5.6603, 'grad_norm': 1.6522637605667114, 'learning_rate': 0.000999795918367347, 'epoch': 0.02}
  2%|â–         | 101/5000 [04:23<4:15:19,  3.13s/it]  2%|â–         | 102/5000 [04:26<3:59:27,  2.93s/it]                                                    {'loss': 5.3707, 'grad_norm': 1.4362695217132568, 'learning_rate': 0.0009995918367346939, 'epoch': 0.02}
  2%|â–         | 102/5000 [04:26<3:59:27,  2.93s/it]  2%|â–         | 103/5000 [04:28<3:51:33,  2.84s/it]                                                    {'loss': 6.2596, 'grad_norm': 1.9373674392700195, 'learning_rate': 0.0009993877551020408, 'epoch': 0.02}
  2%|â–         | 103/5000 [04:28<3:51:33,  2.84s/it]  2%|â–         | 104/5000 [04:31<3:51:00,  2.83s/it]                                                    {'loss': 5.1309, 'grad_norm': 1.8903969526290894, 'learning_rate': 0.0009991836734693877, 'epoch': 0.02}
  2%|â–         | 104/5000 [04:31<3:51:00,  2.83s/it]  2%|â–         | 105/5000 [04:34<3:45:45,  2.77s/it]                                                    {'loss': 5.5478, 'grad_norm': 1.550462007522583, 'learning_rate': 0.0009989795918367347, 'epoch': 0.02}
  2%|â–         | 105/5000 [04:34<3:45:45,  2.77s/it]  2%|â–         | 106/5000 [04:36<3:43:46,  2.74s/it]                                                    {'loss': 5.2383, 'grad_norm': 1.4474594593048096, 'learning_rate': 0.0009987755102040816, 'epoch': 0.02}
  2%|â–         | 106/5000 [04:36<3:43:46,  2.74s/it]  2%|â–         | 107/5000 [04:39<3:36:25,  2.65s/it]                                                    {'loss': 5.8101, 'grad_norm': 1.7814462184906006, 'learning_rate': 0.0009985714285714285, 'epoch': 0.02}
  2%|â–         | 107/5000 [04:39<3:36:25,  2.65s/it]  2%|â–         | 108/5000 [04:41<3:35:19,  2.64s/it]                                                    {'loss': 5.9635, 'grad_norm': 2.0479660034179688, 'learning_rate': 0.0009983673469387755, 'epoch': 0.02}
  2%|â–         | 108/5000 [04:41<3:35:19,  2.64s/it]  2%|â–         | 109/5000 [04:44<3:42:47,  2.73s/it]                                                    {'loss': 5.6445, 'grad_norm': 1.801108717918396, 'learning_rate': 0.0009981632653061224, 'epoch': 0.02}
  2%|â–         | 109/5000 [04:44<3:42:47,  2.73s/it]  2%|â–         | 110/5000 [04:47<3:38:23,  2.68s/it]                                                    {'loss': 5.4177, 'grad_norm': 1.8352434635162354, 'learning_rate': 0.0009979591836734693, 'epoch': 0.02}
  2%|â–         | 110/5000 [04:47<3:38:23,  2.68s/it]  2%|â–         | 111/5000 [04:50<3:39:29,  2.69s/it]                                                    {'loss': 4.8744, 'grad_norm': 1.2794767618179321, 'learning_rate': 0.0009977551020408162, 'epoch': 0.02}
  2%|â–         | 111/5000 [04:50<3:39:29,  2.69s/it]  2%|â–         | 112/5000 [04:53<3:48:53,  2.81s/it]                                                    {'loss': 5.8642, 'grad_norm': 1.8214038610458374, 'learning_rate': 0.0009975510204081632, 'epoch': 0.02}
  2%|â–         | 112/5000 [04:53<3:48:53,  2.81s/it]  2%|â–         | 113/5000 [04:55<3:42:45,  2.73s/it]                                                    {'loss': 5.1929, 'grad_norm': 1.590128779411316, 'learning_rate': 0.00099734693877551, 'epoch': 0.02}
  2%|â–         | 113/5000 [04:55<3:42:45,  2.73s/it]  2%|â–         | 114/5000 [04:58<3:39:20,  2.69s/it]                                                    {'loss': 5.3894, 'grad_norm': 1.7550290822982788, 'learning_rate': 0.000997142857142857, 'epoch': 0.02}
  2%|â–         | 114/5000 [04:58<3:39:20,  2.69s/it]  2%|â–         | 115/5000 [05:01<3:36:58,  2.67s/it]                                                    {'loss': 5.0323, 'grad_norm': 1.2388724088668823, 'learning_rate': 0.000996938775510204, 'epoch': 0.02}
  2%|â–         | 115/5000 [05:01<3:36:58,  2.67s/it]  2%|â–         | 116/5000 [05:03<3:34:15,  2.63s/it]                                                    {'loss': 4.7163, 'grad_norm': 1.0412007570266724, 'learning_rate': 0.000996734693877551, 'epoch': 0.02}
  2%|â–         | 116/5000 [05:03<3:34:15,  2.63s/it]  2%|â–         | 117/5000 [05:06<3:31:45,  2.60s/it]                                                    {'loss': 5.6372, 'grad_norm': 2.0703508853912354, 'learning_rate': 0.000996530612244898, 'epoch': 0.02}
  2%|â–         | 117/5000 [05:06<3:31:45,  2.60s/it]  2%|â–         | 118/5000 [05:09<3:43:40,  2.75s/it]                                                    {'loss': 5.1465, 'grad_norm': 1.6512998342514038, 'learning_rate': 0.000996326530612245, 'epoch': 0.02}
  2%|â–         | 118/5000 [05:09<3:43:40,  2.75s/it]  2%|â–         | 119/5000 [05:11<3:37:19,  2.67s/it]                                                    {'loss': 4.5228, 'grad_norm': 1.2120239734649658, 'learning_rate': 0.000996122448979592, 'epoch': 0.02}
  2%|â–         | 119/5000 [05:11<3:37:19,  2.67s/it]  2%|â–         | 120/5000 [05:14<3:34:42,  2.64s/it]                                                    {'loss': 5.578, 'grad_norm': 1.4346861839294434, 'learning_rate': 0.0009959183673469388, 'epoch': 0.02}
  2%|â–         | 120/5000 [05:14<3:34:42,  2.64s/it]  2%|â–         | 121/5000 [05:16<3:33:49,  2.63s/it]                                                    {'loss': 4.7811, 'grad_norm': 1.285236120223999, 'learning_rate': 0.0009957142857142858, 'epoch': 0.02}
  2%|â–         | 121/5000 [05:16<3:33:49,  2.63s/it]  2%|â–         | 122/5000 [05:19<3:35:32,  2.65s/it]                                                    {'loss': 5.123, 'grad_norm': 1.5656009912490845, 'learning_rate': 0.0009955102040816327, 'epoch': 0.02}
  2%|â–         | 122/5000 [05:19<3:35:32,  2.65s/it]  2%|â–         | 123/5000 [05:22<3:30:59,  2.60s/it]                                                    {'loss': 4.8302, 'grad_norm': 1.4217398166656494, 'learning_rate': 0.0009953061224489796, 'epoch': 0.02}
  2%|â–         | 123/5000 [05:22<3:30:59,  2.60s/it]  2%|â–         | 124/5000 [05:24<3:27:25,  2.55s/it]                                                    {'loss': 4.4077, 'grad_norm': 0.9922136068344116, 'learning_rate': 0.0009951020408163265, 'epoch': 0.02}
  2%|â–         | 124/5000 [05:24<3:27:25,  2.55s/it]  2%|â–Ž         | 125/5000 [05:27<3:32:49,  2.62s/it]                                                    {'loss': 4.0308, 'grad_norm': 0.6363404989242554, 'learning_rate': 0.0009948979591836735, 'epoch': 0.03}
  2%|â–Ž         | 125/5000 [05:27<3:32:49,  2.62s/it]  3%|â–Ž         | 126/5000 [05:29<3:32:05,  2.61s/it]                                                    {'loss': 4.6796, 'grad_norm': 1.401320457458496, 'learning_rate': 0.0009946938775510204, 'epoch': 0.03}
  3%|â–Ž         | 126/5000 [05:29<3:32:05,  2.61s/it]  3%|â–Ž         | 127/5000 [05:32<3:40:47,  2.72s/it]                                                    {'loss': 4.5203, 'grad_norm': 1.334438681602478, 'learning_rate': 0.0009944897959183673, 'epoch': 0.03}
  3%|â–Ž         | 127/5000 [05:32<3:40:47,  2.72s/it]  3%|â–Ž         | 128/5000 [05:35<3:42:57,  2.75s/it]                                                    {'loss': 4.5444, 'grad_norm': 1.0886640548706055, 'learning_rate': 0.0009942857142857143, 'epoch': 0.03}
  3%|â–Ž         | 128/5000 [05:35<3:42:57,  2.75s/it]  3%|â–Ž         | 129/5000 [05:38<3:37:17,  2.68s/it]                                                    {'loss': 5.4189, 'grad_norm': 1.9836419820785522, 'learning_rate': 0.0009940816326530612, 'epoch': 0.03}
  3%|â–Ž         | 129/5000 [05:38<3:37:17,  2.68s/it]  3%|â–Ž         | 130/5000 [05:40<3:34:42,  2.65s/it]                                                    {'loss': 4.1866, 'grad_norm': 1.1550843715667725, 'learning_rate': 0.0009938775510204081, 'epoch': 0.03}
  3%|â–Ž         | 130/5000 [05:40<3:34:42,  2.65s/it]  3%|â–Ž         | 131/5000 [05:43<3:32:25,  2.62s/it]                                                    {'loss': 4.8363, 'grad_norm': 1.2934174537658691, 'learning_rate': 0.0009936734693877553, 'epoch': 0.03}
  3%|â–Ž         | 131/5000 [05:43<3:32:25,  2.62s/it]  3%|â–Ž         | 132/5000 [05:45<3:28:20,  2.57s/it]                                                    {'loss': 4.4641, 'grad_norm': 1.1092804670333862, 'learning_rate': 0.0009934693877551022, 'epoch': 0.03}
  3%|â–Ž         | 132/5000 [05:45<3:28:20,  2.57s/it]  3%|â–Ž         | 133/5000 [05:48<3:28:49,  2.57s/it]                                                    {'loss': 4.4725, 'grad_norm': 1.429953932762146, 'learning_rate': 0.0009932653061224491, 'epoch': 0.03}
  3%|â–Ž         | 133/5000 [05:48<3:28:49,  2.57s/it]  3%|â–Ž         | 134/5000 [05:50<3:31:43,  2.61s/it]                                                    {'loss': 4.645, 'grad_norm': 1.5205005407333374, 'learning_rate': 0.000993061224489796, 'epoch': 0.03}
  3%|â–Ž         | 134/5000 [05:51<3:31:43,  2.61s/it]  3%|â–Ž         | 135/5000 [05:53<3:30:02,  2.59s/it]                                                    {'loss': 4.7509, 'grad_norm': 1.4124919176101685, 'learning_rate': 0.000992857142857143, 'epoch': 0.03}
  3%|â–Ž         | 135/5000 [05:53<3:30:02,  2.59s/it]  3%|â–Ž         | 136/5000 [05:56<3:27:03,  2.55s/it]                                                    {'loss': 4.4372, 'grad_norm': 1.3033442497253418, 'learning_rate': 0.00099265306122449, 'epoch': 0.03}
  3%|â–Ž         | 136/5000 [05:56<3:27:03,  2.55s/it]  3%|â–Ž         | 137/5000 [05:58<3:37:09,  2.68s/it]                                                    {'loss': 4.0693, 'grad_norm': 1.1354639530181885, 'learning_rate': 0.0009924489795918368, 'epoch': 0.03}
  3%|â–Ž         | 137/5000 [05:59<3:37:09,  2.68s/it]  3%|â–Ž         | 138/5000 [06:01<3:33:01,  2.63s/it]                                                    {'loss': 4.1446, 'grad_norm': 1.2611517906188965, 'learning_rate': 0.0009922448979591838, 'epoch': 0.03}
  3%|â–Ž         | 138/5000 [06:01<3:33:01,  2.63s/it]  3%|â–Ž         | 139/5000 [06:04<3:33:55,  2.64s/it]                                                    {'loss': 4.0384, 'grad_norm': 1.2218818664550781, 'learning_rate': 0.0009920408163265307, 'epoch': 0.03}
  3%|â–Ž         | 139/5000 [06:04<3:33:55,  2.64s/it]  3%|â–Ž         | 140/5000 [06:07<3:41:05,  2.73s/it]                                                    {'loss': 4.4086, 'grad_norm': 1.519199252128601, 'learning_rate': 0.0009918367346938776, 'epoch': 0.03}
  3%|â–Ž         | 140/5000 [06:07<3:41:05,  2.73s/it]  3%|â–Ž         | 141/5000 [06:09<3:37:43,  2.69s/it]                                                    {'loss': 3.9811, 'grad_norm': 1.1441209316253662, 'learning_rate': 0.0009916326530612246, 'epoch': 0.03}
  3%|â–Ž         | 141/5000 [06:09<3:37:43,  2.69s/it]  3%|â–Ž         | 142/5000 [06:12<3:32:30,  2.62s/it]                                                    {'loss': 4.4737, 'grad_norm': 1.2490373849868774, 'learning_rate': 0.0009914285714285715, 'epoch': 0.03}
  3%|â–Ž         | 142/5000 [06:12<3:32:30,  2.62s/it]  3%|â–Ž         | 143/5000 [06:14<3:29:49,  2.59s/it]                                                    {'loss': 4.1118, 'grad_norm': 1.1856064796447754, 'learning_rate': 0.0009912244897959184, 'epoch': 0.03}
  3%|â–Ž         | 143/5000 [06:14<3:29:49,  2.59s/it]  3%|â–Ž         | 144/5000 [06:17<3:29:37,  2.59s/it]                                                    {'loss': 4.4593, 'grad_norm': 1.4720194339752197, 'learning_rate': 0.0009910204081632653, 'epoch': 0.03}
  3%|â–Ž         | 144/5000 [06:17<3:29:37,  2.59s/it]  3%|â–Ž         | 145/5000 [06:19<3:25:58,  2.55s/it]                                                    {'loss': 3.8137, 'grad_norm': 0.8712949156761169, 'learning_rate': 0.0009908163265306123, 'epoch': 0.03}
  3%|â–Ž         | 145/5000 [06:19<3:25:58,  2.55s/it]  3%|â–Ž         | 146/5000 [06:22<3:26:56,  2.56s/it]                                                    {'loss': 4.0798, 'grad_norm': 1.1329302787780762, 'learning_rate': 0.0009906122448979592, 'epoch': 0.03}
  3%|â–Ž         | 146/5000 [06:22<3:26:56,  2.56s/it]  3%|â–Ž         | 147/5000 [06:24<3:25:15,  2.54s/it]                                                    {'loss': 3.7273, 'grad_norm': 0.9846523404121399, 'learning_rate': 0.0009904081632653061, 'epoch': 0.03}
  3%|â–Ž         | 147/5000 [06:24<3:25:15,  2.54s/it]  3%|â–Ž         | 148/5000 [06:27<3:26:05,  2.55s/it]                                                    {'loss': 4.0268, 'grad_norm': 1.1191127300262451, 'learning_rate': 0.000990204081632653, 'epoch': 0.03}
  3%|â–Ž         | 148/5000 [06:27<3:26:05,  2.55s/it]  3%|â–Ž         | 149/5000 [06:29<3:23:46,  2.52s/it]                                                    {'loss': 4.083, 'grad_norm': 1.421965479850769, 'learning_rate': 0.00099, 'epoch': 0.03}
  3%|â–Ž         | 149/5000 [06:29<3:23:46,  2.52s/it]  3%|â–Ž         | 150/5000 [06:32<3:21:50,  2.50s/it]                                                    {'loss': 4.4467, 'grad_norm': 1.5089577436447144, 'learning_rate': 0.000989795918367347, 'epoch': 0.03}
  3%|â–Ž         | 150/5000 [06:32<3:21:50,  2.50s/it][2025-10-19 17:36:57,552] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/5000 [06:35<3:51:22,  2.86s/it]                                                    {'loss': 4.0787, 'grad_norm': 1.411752462387085, 'learning_rate': 0.0009895918367346939, 'epoch': 0.03}
  3%|â–Ž         | 151/5000 [06:35<3:51:22,  2.86s/it]  3%|â–Ž         | 152/5000 [06:38<3:45:49,  2.79s/it]                                                    {'loss': 3.7513, 'grad_norm': 0.9082400798797607, 'learning_rate': 0.0009893877551020408, 'epoch': 0.03}
  3%|â–Ž         | 152/5000 [06:38<3:45:49,  2.79s/it]  3%|â–Ž         | 153/5000 [06:40<3:35:48,  2.67s/it]                                                    {'loss': 3.7305, 'grad_norm': 1.1236317157745361, 'learning_rate': 0.0009891836734693877, 'epoch': 0.03}
  3%|â–Ž         | 153/5000 [06:41<3:35:48,  2.67s/it]  3%|â–Ž         | 154/5000 [06:43<3:33:44,  2.65s/it]                                                    {'loss': 4.1615, 'grad_norm': 1.0052416324615479, 'learning_rate': 0.0009889795918367346, 'epoch': 0.03}
  3%|â–Ž         | 154/5000 [06:43<3:33:44,  2.65s/it]  3%|â–Ž         | 155/5000 [06:46<3:32:11,  2.63s/it]                                                    {'loss': 3.6977, 'grad_norm': 1.1736191511154175, 'learning_rate': 0.0009887755102040816, 'epoch': 0.03}
  3%|â–Ž         | 155/5000 [06:46<3:32:11,  2.63s/it]  3%|â–Ž         | 156/5000 [06:48<3:32:02,  2.63s/it]                                                    {'loss': 3.6969, 'grad_norm': 0.8038434386253357, 'learning_rate': 0.0009885714285714285, 'epoch': 0.03}
  3%|â–Ž         | 156/5000 [06:48<3:32:02,  2.63s/it]  3%|â–Ž         | 157/5000 [06:51<3:41:34,  2.75s/it]                                                    {'loss': 4.1243, 'grad_norm': 1.2374683618545532, 'learning_rate': 0.0009883673469387754, 'epoch': 0.03}
  3%|â–Ž         | 157/5000 [06:51<3:41:34,  2.75s/it]  3%|â–Ž         | 158/5000 [06:54<3:35:33,  2.67s/it]                                                    {'loss': 3.4381, 'grad_norm': 0.9307206273078918, 'learning_rate': 0.0009881632653061224, 'epoch': 0.03}
  3%|â–Ž         | 158/5000 [06:54<3:35:33,  2.67s/it]  3%|â–Ž         | 159/5000 [06:56<3:31:59,  2.63s/it]                                                    {'loss': 3.3515, 'grad_norm': 0.897088885307312, 'learning_rate': 0.0009879591836734693, 'epoch': 0.03}
  3%|â–Ž         | 159/5000 [06:56<3:31:59,  2.63s/it]  3%|â–Ž         | 160/5000 [06:59<3:42:49,  2.76s/it]                                                    {'loss': 4.0577, 'grad_norm': 1.1961158514022827, 'learning_rate': 0.0009877551020408162, 'epoch': 0.03}
  3%|â–Ž         | 160/5000 [06:59<3:42:49,  2.76s/it]  3%|â–Ž         | 161/5000 [07:02<3:40:50,  2.74s/it]                                                    {'loss': 3.3166, 'grad_norm': 0.9424606561660767, 'learning_rate': 0.0009875510204081631, 'epoch': 0.03}
  3%|â–Ž         | 161/5000 [07:02<3:40:50,  2.74s/it]  3%|â–Ž         | 162/5000 [07:05<3:34:41,  2.66s/it]                                                    {'loss': 3.4327, 'grad_norm': 0.7555464506149292, 'learning_rate': 0.00098734693877551, 'epoch': 0.03}
  3%|â–Ž         | 162/5000 [07:05<3:34:41,  2.66s/it]  3%|â–Ž         | 163/5000 [07:08<3:41:39,  2.75s/it]                                                    {'loss': 3.6706, 'grad_norm': 1.0010651350021362, 'learning_rate': 0.0009871428571428572, 'epoch': 0.03}
  3%|â–Ž         | 163/5000 [07:08<3:41:39,  2.75s/it]  3%|â–Ž         | 164/5000 [07:10<3:36:09,  2.68s/it]                                                    {'loss': 3.4132, 'grad_norm': 0.6274054050445557, 'learning_rate': 0.0009869387755102042, 'epoch': 0.03}
  3%|â–Ž         | 164/5000 [07:10<3:36:09,  2.68s/it]  3%|â–Ž         | 165/5000 [07:13<3:36:36,  2.69s/it]                                                    {'loss': 3.556, 'grad_norm': 0.8861424922943115, 'learning_rate': 0.000986734693877551, 'epoch': 0.03}
  3%|â–Ž         | 165/5000 [07:13<3:36:36,  2.69s/it]  3%|â–Ž         | 166/5000 [07:15<3:31:55,  2.63s/it]                                                    {'loss': 3.4689, 'grad_norm': 0.883844256401062, 'learning_rate': 0.000986530612244898, 'epoch': 0.03}
  3%|â–Ž         | 166/5000 [07:15<3:31:55,  2.63s/it]  3%|â–Ž         | 167/5000 [07:18<3:31:07,  2.62s/it]                                                    {'loss': 3.5503, 'grad_norm': 1.2543069124221802, 'learning_rate': 0.000986326530612245, 'epoch': 0.03}
  3%|â–Ž         | 167/5000 [07:18<3:31:07,  2.62s/it]  3%|â–Ž         | 168/5000 [07:20<3:29:18,  2.60s/it]                                                    {'loss': 3.8778, 'grad_norm': 1.1123676300048828, 'learning_rate': 0.0009861224489795919, 'epoch': 0.03}
  3%|â–Ž         | 168/5000 [07:20<3:29:18,  2.60s/it]  3%|â–Ž         | 169/5000 [07:23<3:38:17,  2.71s/it]                                                    {'loss': 3.3652, 'grad_norm': 0.8692740797996521, 'learning_rate': 0.0009859183673469388, 'epoch': 0.03}
  3%|â–Ž         | 169/5000 [07:23<3:38:17,  2.71s/it]  3%|â–Ž         | 170/5000 [07:26<3:31:59,  2.63s/it]                                                    {'loss': 3.5222, 'grad_norm': 0.9317466616630554, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}
  3%|â–Ž         | 170/5000 [07:26<3:31:59,  2.63s/it]  3%|â–Ž         | 171/5000 [07:28<3:26:34,  2.57s/it]                                                    {'loss': 3.3784, 'grad_norm': 0.840583324432373, 'learning_rate': 0.0009855102040816327, 'epoch': 0.03}
  3%|â–Ž         | 171/5000 [07:28<3:26:34,  2.57s/it]  3%|â–Ž         | 172/5000 [07:31<3:28:09,  2.59s/it]                                                    {'loss': 4.118, 'grad_norm': 0.9840334057807922, 'learning_rate': 0.0009853061224489796, 'epoch': 0.03}
  3%|â–Ž         | 172/5000 [07:31<3:28:09,  2.59s/it]  3%|â–Ž         | 173/5000 [07:33<3:28:06,  2.59s/it]                                                    {'loss': 3.4622, 'grad_norm': 0.8697267770767212, 'learning_rate': 0.0009851020408163265, 'epoch': 0.03}
  3%|â–Ž         | 173/5000 [07:33<3:28:06,  2.59s/it]  3%|â–Ž         | 174/5000 [07:37<3:43:40,  2.78s/it]                                                    {'loss': 3.607, 'grad_norm': 1.0505167245864868, 'learning_rate': 0.0009848979591836734, 'epoch': 0.03}
  3%|â–Ž         | 174/5000 [07:37<3:43:40,  2.78s/it]  4%|â–Ž         | 175/5000 [07:39<3:40:20,  2.74s/it]                                                    {'loss': 3.9069, 'grad_norm': 1.164170503616333, 'learning_rate': 0.0009846938775510204, 'epoch': 0.04}
  4%|â–Ž         | 175/5000 [07:39<3:40:20,  2.74s/it]  4%|â–Ž         | 176/5000 [07:42<3:43:21,  2.78s/it]                                                    {'loss': 4.1675, 'grad_norm': 1.290706992149353, 'learning_rate': 0.0009844897959183673, 'epoch': 0.04}
  4%|â–Ž         | 176/5000 [07:42<3:43:21,  2.78s/it]  4%|â–Ž         | 177/5000 [07:45<3:39:21,  2.73s/it]                                                    {'loss': 3.288, 'grad_norm': 0.7060943841934204, 'learning_rate': 0.0009842857142857142, 'epoch': 0.04}
  4%|â–Ž         | 177/5000 [07:45<3:39:21,  2.73s/it]  4%|â–Ž         | 178/5000 [07:47<3:36:53,  2.70s/it]                                                    {'loss': 3.604, 'grad_norm': 0.9751729965209961, 'learning_rate': 0.0009840816326530614, 'epoch': 0.04}
  4%|â–Ž         | 178/5000 [07:47<3:36:53,  2.70s/it]  4%|â–Ž         | 179/5000 [07:50<3:32:54,  2.65s/it]                                                    {'loss': 3.577, 'grad_norm': 1.032148003578186, 'learning_rate': 0.0009838775510204083, 'epoch': 0.04}
  4%|â–Ž         | 179/5000 [07:50<3:32:54,  2.65s/it]  4%|â–Ž         | 180/5000 [07:53<3:33:44,  2.66s/it]                                                    {'loss': 3.4644, 'grad_norm': 0.9728327989578247, 'learning_rate': 0.0009836734693877552, 'epoch': 0.04}
  4%|â–Ž         | 180/5000 [07:53<3:33:44,  2.66s/it]  4%|â–Ž         | 181/5000 [07:55<3:33:07,  2.65s/it]                                                    {'loss': 3.2624, 'grad_norm': 0.6778220534324646, 'learning_rate': 0.0009834693877551022, 'epoch': 0.04}
  4%|â–Ž         | 181/5000 [07:55<3:33:07,  2.65s/it]  4%|â–Ž         | 182/5000 [07:58<3:31:17,  2.63s/it]                                                    {'loss': 3.5714, 'grad_norm': 0.8393412828445435, 'learning_rate': 0.000983265306122449, 'epoch': 0.04}
  4%|â–Ž         | 182/5000 [07:58<3:31:17,  2.63s/it]  4%|â–Ž         | 183/5000 [08:00<3:28:24,  2.60s/it]                                                    {'loss': 3.0097, 'grad_norm': 0.4844951927661896, 'learning_rate': 0.000983061224489796, 'epoch': 0.04}
  4%|â–Ž         | 183/5000 [08:00<3:28:24,  2.60s/it]  4%|â–Ž         | 184/5000 [08:03<3:27:19,  2.58s/it]                                                    {'loss': 3.2256, 'grad_norm': 0.6559401750564575, 'learning_rate': 0.000982857142857143, 'epoch': 0.04}
  4%|â–Ž         | 184/5000 [08:03<3:27:19,  2.58s/it]  4%|â–Ž         | 185/5000 [08:06<3:26:27,  2.57s/it]                                                    {'loss': 3.2458, 'grad_norm': 0.5149304866790771, 'learning_rate': 0.0009826530612244899, 'epoch': 0.04}
  4%|â–Ž         | 185/5000 [08:06<3:26:27,  2.57s/it]  4%|â–Ž         | 186/5000 [08:08<3:27:27,  2.59s/it]                                                    {'loss': 3.2612, 'grad_norm': 0.5917626023292542, 'learning_rate': 0.0009824489795918368, 'epoch': 0.04}
  4%|â–Ž         | 186/5000 [08:08<3:27:27,  2.59s/it]  4%|â–Ž         | 187/5000 [08:11<3:25:49,  2.57s/it]                                                    {'loss': 3.6549, 'grad_norm': 1.0906404256820679, 'learning_rate': 0.0009822448979591837, 'epoch': 0.04}
  4%|â–Ž         | 187/5000 [08:11<3:25:49,  2.57s/it]  4%|â–         | 188/5000 [08:13<3:25:31,  2.56s/it]                                                    {'loss': 3.367, 'grad_norm': 0.7302111983299255, 'learning_rate': 0.0009820408163265307, 'epoch': 0.04}
  4%|â–         | 188/5000 [08:13<3:25:31,  2.56s/it]  4%|â–         | 189/5000 [08:16<3:24:18,  2.55s/it]                                                    {'loss': 3.282, 'grad_norm': 0.6859704256057739, 'learning_rate': 0.0009818367346938776, 'epoch': 0.04}
  4%|â–         | 189/5000 [08:16<3:24:18,  2.55s/it]  4%|â–         | 190/5000 [08:18<3:20:57,  2.51s/it]                                                    {'loss': 3.3668, 'grad_norm': 0.9586151242256165, 'learning_rate': 0.0009816326530612245, 'epoch': 0.04}
  4%|â–         | 190/5000 [08:18<3:20:57,  2.51s/it]  4%|â–         | 191/5000 [08:21<3:23:02,  2.53s/it]                                                    {'loss': 3.2753, 'grad_norm': 0.68986576795578, 'learning_rate': 0.0009814285714285715, 'epoch': 0.04}
  4%|â–         | 191/5000 [08:21<3:23:02,  2.53s/it]  4%|â–         | 192/5000 [08:24<3:36:33,  2.70s/it]                                                    {'loss': 3.3285, 'grad_norm': 0.6466238498687744, 'learning_rate': 0.0009812244897959184, 'epoch': 0.04}
  4%|â–         | 192/5000 [08:24<3:36:33,  2.70s/it]  4%|â–         | 193/5000 [08:27<3:41:22,  2.76s/it]                                                    {'loss': 3.1697, 'grad_norm': 0.671592652797699, 'learning_rate': 0.0009810204081632653, 'epoch': 0.04}
  4%|â–         | 193/5000 [08:27<3:41:22,  2.76s/it]  4%|â–         | 194/5000 [08:29<3:33:22,  2.66s/it]                                                    {'loss': 2.9334, 'grad_norm': 0.34889498353004456, 'learning_rate': 0.0009808163265306123, 'epoch': 0.04}
  4%|â–         | 194/5000 [08:29<3:33:22,  2.66s/it]  4%|â–         | 195/5000 [08:32<3:32:07,  2.65s/it]                                                    {'loss': 3.1832, 'grad_norm': 0.8153069019317627, 'learning_rate': 0.0009806122448979592, 'epoch': 0.04}
  4%|â–         | 195/5000 [08:32<3:32:07,  2.65s/it]  4%|â–         | 196/5000 [08:34<3:29:43,  2.62s/it]                                                    {'loss': 3.43, 'grad_norm': 0.6012806296348572, 'learning_rate': 0.0009804081632653061, 'epoch': 0.04}
  4%|â–         | 196/5000 [08:34<3:29:43,  2.62s/it]  4%|â–         | 197/5000 [08:37<3:32:44,  2.66s/it]                                                    {'loss': 3.1449, 'grad_norm': 0.6080950498580933, 'learning_rate': 0.000980204081632653, 'epoch': 0.04}
  4%|â–         | 197/5000 [08:37<3:32:44,  2.66s/it]  4%|â–         | 198/5000 [08:40<3:33:13,  2.66s/it]                                                    {'loss': 3.1223, 'grad_norm': 0.37262997031211853, 'learning_rate': 0.00098, 'epoch': 0.04}
  4%|â–         | 198/5000 [08:40<3:33:13,  2.66s/it]  4%|â–         | 199/5000 [08:42<3:29:03,  2.61s/it]                                                    {'loss': 3.3181, 'grad_norm': 0.5121440887451172, 'learning_rate': 0.000979795918367347, 'epoch': 0.04}
  4%|â–         | 199/5000 [08:42<3:29:03,  2.61s/it]  4%|â–         | 200/5000 [08:45<3:38:48,  2.74s/it]                                                    {'loss': 3.3566, 'grad_norm': 0.7940093278884888, 'learning_rate': 0.0009795918367346938, 'epoch': 0.04}
  4%|â–         | 200/5000 [08:45<3:38:48,  2.74s/it][2025-10-19 17:39:11,041] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 201/5000 [08:49<4:02:36,  3.03s/it]                                                    {'loss': 3.2872, 'grad_norm': 0.7109451293945312, 'learning_rate': 0.0009793877551020408, 'epoch': 0.04}
  4%|â–         | 201/5000 [08:49<4:02:36,  3.03s/it]  4%|â–         | 202/5000 [08:52<3:50:23,  2.88s/it]                                                    {'loss': 3.5208, 'grad_norm': 0.6017087697982788, 'learning_rate': 0.0009791836734693877, 'epoch': 0.04}
  4%|â–         | 202/5000 [08:52<3:50:23,  2.88s/it]  4%|â–         | 203/5000 [08:54<3:43:04,  2.79s/it]                                                    {'loss': 3.23, 'grad_norm': 0.7150355577468872, 'learning_rate': 0.0009789795918367346, 'epoch': 0.04}
  4%|â–         | 203/5000 [08:54<3:43:04,  2.79s/it]  4%|â–         | 204/5000 [08:57<3:40:33,  2.76s/it]                                                    {'loss': 3.0059, 'grad_norm': 0.41717472672462463, 'learning_rate': 0.0009787755102040815, 'epoch': 0.04}
  4%|â–         | 204/5000 [08:57<3:40:33,  2.76s/it]  4%|â–         | 205/5000 [08:59<3:32:52,  2.66s/it]                                                    {'loss': 3.0332, 'grad_norm': 0.4943525791168213, 'learning_rate': 0.0009785714285714285, 'epoch': 0.04}
  4%|â–         | 205/5000 [08:59<3:32:52,  2.66s/it]  4%|â–         | 206/5000 [09:02<3:26:52,  2.59s/it]                                                    {'loss': 3.3998, 'grad_norm': 1.0072194337844849, 'learning_rate': 0.0009783673469387754, 'epoch': 0.04}
  4%|â–         | 206/5000 [09:02<3:26:52,  2.59s/it]  4%|â–         | 207/5000 [09:04<3:24:17,  2.56s/it]                                                    {'loss': 3.0462, 'grad_norm': 0.36041972041130066, 'learning_rate': 0.0009781632653061223, 'epoch': 0.04}
  4%|â–         | 207/5000 [09:04<3:24:17,  2.56s/it]  4%|â–         | 208/5000 [09:07<3:22:49,  2.54s/it]                                                    {'loss': 3.6205, 'grad_norm': 0.9583965539932251, 'learning_rate': 0.0009779591836734693, 'epoch': 0.04}
  4%|â–         | 208/5000 [09:07<3:22:49,  2.54s/it]  4%|â–         | 209/5000 [09:09<3:24:18,  2.56s/it]                                                    {'loss': 3.0154, 'grad_norm': 0.4324621558189392, 'learning_rate': 0.0009777551020408164, 'epoch': 0.04}
  4%|â–         | 209/5000 [09:09<3:24:18,  2.56s/it]  4%|â–         | 210/5000 [09:12<3:34:14,  2.68s/it]                                                    {'loss': 3.2364, 'grad_norm': 0.616901159286499, 'learning_rate': 0.0009775510204081633, 'epoch': 0.04}
  4%|â–         | 210/5000 [09:12<3:34:14,  2.68s/it]  4%|â–         | 211/5000 [09:15<3:35:38,  2.70s/it]                                                    {'loss': 3.2638, 'grad_norm': 0.4893365502357483, 'learning_rate': 0.0009773469387755103, 'epoch': 0.04}
  4%|â–         | 211/5000 [09:15<3:35:38,  2.70s/it]  4%|â–         | 212/5000 [09:18<3:41:24,  2.77s/it]                                                    {'loss': 3.2513, 'grad_norm': 0.5424397587776184, 'learning_rate': 0.0009771428571428572, 'epoch': 0.04}
  4%|â–         | 212/5000 [09:18<3:41:24,  2.77s/it]  4%|â–         | 213/5000 [09:21<3:38:36,  2.74s/it]                                                    {'loss': 3.1898, 'grad_norm': 0.35737642645835876, 'learning_rate': 0.0009769387755102041, 'epoch': 0.04}
  4%|â–         | 213/5000 [09:21<3:38:36,  2.74s/it]  4%|â–         | 214/5000 [09:23<3:32:25,  2.66s/it]                                                    {'loss': 3.07, 'grad_norm': 0.5970240831375122, 'learning_rate': 0.000976734693877551, 'epoch': 0.04}
  4%|â–         | 214/5000 [09:23<3:32:25,  2.66s/it]  4%|â–         | 215/5000 [09:26<3:28:58,  2.62s/it]                                                    {'loss': 3.1945, 'grad_norm': 0.47193044424057007, 'learning_rate': 0.000976530612244898, 'epoch': 0.04}
  4%|â–         | 215/5000 [09:26<3:28:58,  2.62s/it]  4%|â–         | 216/5000 [09:28<3:29:08,  2.62s/it]                                                    {'loss': 3.1943, 'grad_norm': 0.6011174321174622, 'learning_rate': 0.0009763265306122449, 'epoch': 0.04}
  4%|â–         | 216/5000 [09:28<3:29:08,  2.62s/it]  4%|â–         | 217/5000 [09:31<3:29:50,  2.63s/it]                                                    {'loss': 3.311, 'grad_norm': 0.6534751653671265, 'learning_rate': 0.000976122448979592, 'epoch': 0.04}
  4%|â–         | 217/5000 [09:31<3:29:50,  2.63s/it]  4%|â–         | 218/5000 [09:33<3:30:04,  2.64s/it]                                                    {'loss': 3.3272, 'grad_norm': 0.5583832263946533, 'learning_rate': 0.0009759183673469389, 'epoch': 0.04}
  4%|â–         | 218/5000 [09:33<3:30:04,  2.64s/it]  4%|â–         | 219/5000 [09:36<3:25:39,  2.58s/it]                                                    {'loss': 3.2554, 'grad_norm': 0.5171962380409241, 'learning_rate': 0.0009757142857142858, 'epoch': 0.04}
  4%|â–         | 219/5000 [09:36<3:25:39,  2.58s/it]  4%|â–         | 220/5000 [09:38<3:24:38,  2.57s/it]                                                    {'loss': 3.3102, 'grad_norm': 0.5585327744483948, 'learning_rate': 0.0009755102040816327, 'epoch': 0.04}
  4%|â–         | 220/5000 [09:38<3:24:38,  2.57s/it]  4%|â–         | 221/5000 [09:41<3:24:36,  2.57s/it]                                                    {'loss': 3.0358, 'grad_norm': 0.3578976094722748, 'learning_rate': 0.0009753061224489797, 'epoch': 0.04}
  4%|â–         | 221/5000 [09:41<3:24:36,  2.57s/it]  4%|â–         | 222/5000 [09:44<3:23:45,  2.56s/it]                                                    {'loss': 3.019, 'grad_norm': 0.5710535049438477, 'learning_rate': 0.0009751020408163266, 'epoch': 0.04}
  4%|â–         | 222/5000 [09:44<3:23:45,  2.56s/it]  4%|â–         | 223/5000 [09:46<3:23:09,  2.55s/it]                                                    {'loss': 3.0268, 'grad_norm': 0.6169949769973755, 'learning_rate': 0.0009748979591836735, 'epoch': 0.04}
  4%|â–         | 223/5000 [09:46<3:23:09,  2.55s/it]  4%|â–         | 224/5000 [09:49<3:20:31,  2.52s/it]                                                    {'loss': 3.1598, 'grad_norm': 0.5956951975822449, 'learning_rate': 0.0009746938775510205, 'epoch': 0.04}
  4%|â–         | 224/5000 [09:49<3:20:31,  2.52s/it]  4%|â–         | 225/5000 [09:51<3:23:06,  2.55s/it]                                                    {'loss': 3.2028, 'grad_norm': 0.6154933571815491, 'learning_rate': 0.0009744897959183674, 'epoch': 0.04}
  4%|â–         | 225/5000 [09:51<3:23:06,  2.55s/it]  5%|â–         | 226/5000 [09:54<3:33:24,  2.68s/it]                                                    {'loss': 3.4451, 'grad_norm': 0.9725199341773987, 'learning_rate': 0.0009742857142857143, 'epoch': 0.05}
  5%|â–         | 226/5000 [09:54<3:33:24,  2.68s/it]  5%|â–         | 227/5000 [09:57<3:28:29,  2.62s/it]                                                    {'loss': 3.1554, 'grad_norm': 0.587456226348877, 'learning_rate': 0.0009740816326530612, 'epoch': 0.05}
  5%|â–         | 227/5000 [09:57<3:28:29,  2.62s/it]  5%|â–         | 228/5000 [09:59<3:27:38,  2.61s/it]                                                    {'loss': 3.3476, 'grad_norm': 0.7573415040969849, 'learning_rate': 0.0009738775510204082, 'epoch': 0.05}
  5%|â–         | 228/5000 [09:59<3:27:38,  2.61s/it]  5%|â–         | 229/5000 [10:02<3:24:07,  2.57s/it]                                                    {'loss': 3.126, 'grad_norm': 0.4898006021976471, 'learning_rate': 0.0009736734693877551, 'epoch': 0.05}
  5%|â–         | 229/5000 [10:02<3:24:07,  2.57s/it]  5%|â–         | 230/5000 [10:04<3:26:00,  2.59s/it]                                                    {'loss': 3.0622, 'grad_norm': 0.3653813302516937, 'learning_rate': 0.000973469387755102, 'epoch': 0.05}
  5%|â–         | 230/5000 [10:04<3:26:00,  2.59s/it]  5%|â–         | 231/5000 [10:07<3:23:12,  2.56s/it]                                                    {'loss': 3.0077, 'grad_norm': 0.35551148653030396, 'learning_rate': 0.000973265306122449, 'epoch': 0.05}
  5%|â–         | 231/5000 [10:07<3:23:12,  2.56s/it]  5%|â–         | 232/5000 [10:09<3:23:24,  2.56s/it]                                                    {'loss': 3.2607, 'grad_norm': 0.6114916205406189, 'learning_rate': 0.000973061224489796, 'epoch': 0.05}
  5%|â–         | 232/5000 [10:09<3:23:24,  2.56s/it]  5%|â–         | 233/5000 [10:12<3:32:19,  2.67s/it]                                                    {'loss': 2.99, 'grad_norm': 0.5252779126167297, 'learning_rate': 0.0009728571428571429, 'epoch': 0.05}
  5%|â–         | 233/5000 [10:12<3:32:19,  2.67s/it]  5%|â–         | 234/5000 [10:15<3:36:48,  2.73s/it]                                                    {'loss': 2.9842, 'grad_norm': 0.2983006238937378, 'learning_rate': 0.0009726530612244899, 'epoch': 0.05}
  5%|â–         | 234/5000 [10:15<3:36:48,  2.73s/it]  5%|â–         | 235/5000 [10:18<3:29:57,  2.64s/it]                                                    {'loss': 3.1337, 'grad_norm': 0.4623333811759949, 'learning_rate': 0.0009724489795918368, 'epoch': 0.05}
  5%|â–         | 235/5000 [10:18<3:29:57,  2.64s/it]  5%|â–         | 236/5000 [10:20<3:28:51,  2.63s/it]                                                    {'loss': 2.9606, 'grad_norm': 0.28628799319267273, 'learning_rate': 0.0009722448979591837, 'epoch': 0.05}
  5%|â–         | 236/5000 [10:20<3:28:51,  2.63s/it]  5%|â–         | 237/5000 [10:23<3:28:02,  2.62s/it]                                                    {'loss': 3.0001, 'grad_norm': 0.39800018072128296, 'learning_rate': 0.0009720408163265306, 'epoch': 0.05}
  5%|â–         | 237/5000 [10:23<3:28:02,  2.62s/it]  5%|â–         | 238/5000 [10:26<3:37:35,  2.74s/it]                                                    {'loss': 3.1991, 'grad_norm': 0.4224746525287628, 'learning_rate': 0.0009718367346938776, 'epoch': 0.05}
  5%|â–         | 238/5000 [10:26<3:37:35,  2.74s/it]  5%|â–         | 239/5000 [10:28<3:33:58,  2.70s/it]                                                    {'loss': 3.0008, 'grad_norm': 0.36809882521629333, 'learning_rate': 0.0009716326530612245, 'epoch': 0.05}
  5%|â–         | 239/5000 [10:28<3:33:58,  2.70s/it]  5%|â–         | 240/5000 [10:31<3:30:02,  2.65s/it]                                                    {'loss': 3.0507, 'grad_norm': 0.3819423317909241, 'learning_rate': 0.0009714285714285714, 'epoch': 0.05}
  5%|â–         | 240/5000 [10:31<3:30:02,  2.65s/it]  5%|â–         | 241/5000 [10:34<3:28:43,  2.63s/it]                                                    {'loss': 2.9955, 'grad_norm': 0.40993019938468933, 'learning_rate': 0.0009712244897959184, 'epoch': 0.05}
  5%|â–         | 241/5000 [10:34<3:28:43,  2.63s/it]  5%|â–         | 242/5000 [10:36<3:24:33,  2.58s/it]                                                    {'loss': 3.2047, 'grad_norm': 0.5037246346473694, 'learning_rate': 0.0009710204081632653, 'epoch': 0.05}
  5%|â–         | 242/5000 [10:36<3:24:33,  2.58s/it]  5%|â–         | 243/5000 [10:39<3:33:15,  2.69s/it]                                                    {'loss': 3.0246, 'grad_norm': 0.38420015573501587, 'learning_rate': 0.0009708163265306122, 'epoch': 0.05}
  5%|â–         | 243/5000 [10:39<3:33:15,  2.69s/it]  5%|â–         | 244/5000 [10:41<3:28:27,  2.63s/it]                                                    {'loss': 3.0811, 'grad_norm': 0.3755912482738495, 'learning_rate': 0.0009706122448979592, 'epoch': 0.05}
  5%|â–         | 244/5000 [10:41<3:28:27,  2.63s/it]  5%|â–         | 245/5000 [10:44<3:27:58,  2.62s/it]                                                    {'loss': 3.2014, 'grad_norm': 0.6708925366401672, 'learning_rate': 0.0009704081632653061, 'epoch': 0.05}
  5%|â–         | 245/5000 [10:44<3:27:58,  2.62s/it]  5%|â–         | 246/5000 [10:47<3:28:56,  2.64s/it]                                                    {'loss': 2.9569, 'grad_norm': 0.4098925292491913, 'learning_rate': 0.000970204081632653, 'epoch': 0.05}
  5%|â–         | 246/5000 [10:47<3:28:56,  2.64s/it]  5%|â–         | 247/5000 [10:49<3:26:04,  2.60s/it]                                                    {'loss': 3.1079, 'grad_norm': 0.6373005509376526, 'learning_rate': 0.0009699999999999999, 'epoch': 0.05}
  5%|â–         | 247/5000 [10:49<3:26:04,  2.60s/it]  5%|â–         | 248/5000 [10:52<3:28:24,  2.63s/it]                                                    {'loss': 3.1969, 'grad_norm': 0.5097540020942688, 'learning_rate': 0.000969795918367347, 'epoch': 0.05}
  5%|â–         | 248/5000 [10:52<3:28:24,  2.63s/it]  5%|â–         | 249/5000 [10:54<3:24:34,  2.58s/it]                                                    {'loss': 3.0174, 'grad_norm': 0.5296029448509216, 'learning_rate': 0.0009695918367346939, 'epoch': 0.05}
  5%|â–         | 249/5000 [10:54<3:24:34,  2.58s/it]  5%|â–Œ         | 250/5000 [10:57<3:22:59,  2.56s/it]                                                    {'loss': 2.9615, 'grad_norm': 0.31886085867881775, 'learning_rate': 0.0009693877551020408, 'epoch': 0.05}
  5%|â–Œ         | 250/5000 [10:57<3:22:59,  2.56s/it][2025-10-19 17:41:22,734] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 251/5000 [11:00<3:46:09,  2.86s/it]                                                    {'loss': 3.5678, 'grad_norm': 0.6268176436424255, 'learning_rate': 0.0009691836734693878, 'epoch': 0.05}
  5%|â–Œ         | 251/5000 [11:00<3:46:09,  2.86s/it]  5%|â–Œ         | 252/5000 [11:03<3:43:01,  2.82s/it]                                                    {'loss': 3.0842, 'grad_norm': 0.4535662829875946, 'learning_rate': 0.0009689795918367347, 'epoch': 0.05}
  5%|â–Œ         | 252/5000 [11:03<3:43:01,  2.82s/it]  5%|â–Œ         | 253/5000 [11:06<3:36:43,  2.74s/it]                                                    {'loss': 2.9393, 'grad_norm': 0.25813722610473633, 'learning_rate': 0.0009687755102040816, 'epoch': 0.05}
  5%|â–Œ         | 253/5000 [11:06<3:36:43,  2.74s/it]  5%|â–Œ         | 254/5000 [11:08<3:33:58,  2.71s/it]                                                    {'loss': 3.1569, 'grad_norm': 0.6374441385269165, 'learning_rate': 0.0009685714285714286, 'epoch': 0.05}
  5%|â–Œ         | 254/5000 [11:08<3:33:58,  2.71s/it]  5%|â–Œ         | 255/5000 [11:11<3:27:05,  2.62s/it]                                                    {'loss': 3.0836, 'grad_norm': 0.7043533325195312, 'learning_rate': 0.0009683673469387755, 'epoch': 0.05}
  5%|â–Œ         | 255/5000 [11:11<3:27:05,  2.62s/it]  5%|â–Œ         | 256/5000 [11:14<3:34:46,  2.72s/it]                                                    {'loss': 3.265, 'grad_norm': 0.5061066150665283, 'learning_rate': 0.0009681632653061224, 'epoch': 0.05}
  5%|â–Œ         | 256/5000 [11:14<3:34:46,  2.72s/it]  5%|â–Œ         | 257/5000 [11:16<3:29:19,  2.65s/it]                                                    {'loss': 3.0412, 'grad_norm': 0.4706795811653137, 'learning_rate': 0.0009679591836734693, 'epoch': 0.05}
  5%|â–Œ         | 257/5000 [11:16<3:29:19,  2.65s/it]  5%|â–Œ         | 258/5000 [11:19<3:25:42,  2.60s/it]                                                    {'loss': 2.9956, 'grad_norm': 0.3282093405723572, 'learning_rate': 0.0009677551020408163, 'epoch': 0.05}
  5%|â–Œ         | 258/5000 [11:19<3:25:42,  2.60s/it]  5%|â–Œ         | 259/5000 [11:21<3:22:30,  2.56s/it]                                                    {'loss': 2.8959, 'grad_norm': 0.35773032903671265, 'learning_rate': 0.0009675510204081633, 'epoch': 0.05}
  5%|â–Œ         | 259/5000 [11:21<3:22:30,  2.56s/it]  5%|â–Œ         | 260/5000 [11:24<3:21:43,  2.55s/it]                                                    {'loss': 2.9788, 'grad_norm': 0.3983703553676605, 'learning_rate': 0.0009673469387755102, 'epoch': 0.05}
  5%|â–Œ         | 260/5000 [11:24<3:21:43,  2.55s/it]  5%|â–Œ         | 261/5000 [11:26<3:26:02,  2.61s/it]                                                    {'loss': 3.2801, 'grad_norm': 0.7666468024253845, 'learning_rate': 0.0009671428571428572, 'epoch': 0.05}
  5%|â–Œ         | 261/5000 [11:26<3:26:02,  2.61s/it]  5%|â–Œ         | 262/5000 [11:29<3:24:58,  2.60s/it]                                                    {'loss': 2.8821, 'grad_norm': 0.317913681268692, 'learning_rate': 0.0009669387755102041, 'epoch': 0.05}
  5%|â–Œ         | 262/5000 [11:29<3:24:58,  2.60s/it]  5%|â–Œ         | 263/5000 [11:32<3:36:01,  2.74s/it]                                                    {'loss': 3.3524, 'grad_norm': 0.7408596277236938, 'learning_rate': 0.0009667346938775511, 'epoch': 0.05}
  5%|â–Œ         | 263/5000 [11:32<3:36:01,  2.74s/it]  5%|â–Œ         | 264/5000 [11:35<3:28:30,  2.64s/it]                                                    {'loss': 3.2897, 'grad_norm': 0.6015687584877014, 'learning_rate': 0.0009665306122448981, 'epoch': 0.05}
  5%|â–Œ         | 264/5000 [11:35<3:28:30,  2.64s/it]  5%|â–Œ         | 265/5000 [11:37<3:29:55,  2.66s/it]                                                    {'loss': 3.0338, 'grad_norm': 0.49593186378479004, 'learning_rate': 0.000966326530612245, 'epoch': 0.05}
  5%|â–Œ         | 265/5000 [11:37<3:29:55,  2.66s/it]  5%|â–Œ         | 266/5000 [11:40<3:29:36,  2.66s/it]                                                    {'loss': 3.2581, 'grad_norm': 0.4888994097709656, 'learning_rate': 0.0009661224489795919, 'epoch': 0.05}
  5%|â–Œ         | 266/5000 [11:40<3:29:36,  2.66s/it]  5%|â–Œ         | 267/5000 [11:42<3:23:49,  2.58s/it]                                                    {'loss': 3.041, 'grad_norm': 0.6156624555587769, 'learning_rate': 0.0009659183673469389, 'epoch': 0.05}
  5%|â–Œ         | 267/5000 [11:42<3:23:49,  2.58s/it]  5%|â–Œ         | 268/5000 [11:45<3:21:50,  2.56s/it]                                                    {'loss': 3.2658, 'grad_norm': 0.42765846848487854, 'learning_rate': 0.0009657142857142858, 'epoch': 0.05}
  5%|â–Œ         | 268/5000 [11:45<3:21:50,  2.56s/it]  5%|â–Œ         | 269/5000 [11:48<3:25:36,  2.61s/it]                                                    {'loss': 2.8907, 'grad_norm': 0.3151068091392517, 'learning_rate': 0.0009655102040816327, 'epoch': 0.05}
  5%|â–Œ         | 269/5000 [11:48<3:25:36,  2.61s/it]  5%|â–Œ         | 270/5000 [11:50<3:23:20,  2.58s/it]                                                    {'loss': 2.9822, 'grad_norm': 0.29931291937828064, 'learning_rate': 0.0009653061224489796, 'epoch': 0.05}
  5%|â–Œ         | 270/5000 [11:50<3:23:20,  2.58s/it]  5%|â–Œ         | 271/5000 [11:52<3:19:37,  2.53s/it]                                                    {'loss': 3.0789, 'grad_norm': 0.5668434500694275, 'learning_rate': 0.0009651020408163266, 'epoch': 0.05}
  5%|â–Œ         | 271/5000 [11:52<3:19:37,  2.53s/it]  5%|â–Œ         | 272/5000 [11:55<3:19:24,  2.53s/it]                                                    {'loss': 3.0948, 'grad_norm': 0.5297086834907532, 'learning_rate': 0.0009648979591836735, 'epoch': 0.05}
  5%|â–Œ         | 272/5000 [11:55<3:19:24,  2.53s/it]  5%|â–Œ         | 273/5000 [11:58<3:21:44,  2.56s/it]                                                    {'loss': 2.9423, 'grad_norm': 0.3309541940689087, 'learning_rate': 0.0009646938775510204, 'epoch': 0.05}
  5%|â–Œ         | 273/5000 [11:58<3:21:44,  2.56s/it]  5%|â–Œ         | 274/5000 [12:01<3:30:10,  2.67s/it]                                                    {'loss': 3.3366, 'grad_norm': 0.6287992596626282, 'learning_rate': 0.0009644897959183674, 'epoch': 0.05}
  5%|â–Œ         | 274/5000 [12:01<3:30:10,  2.67s/it]  6%|â–Œ         | 275/5000 [12:03<3:27:53,  2.64s/it]                                                    {'loss': 3.0, 'grad_norm': 0.39702194929122925, 'learning_rate': 0.0009642857142857143, 'epoch': 0.06}
  6%|â–Œ         | 275/5000 [12:03<3:27:53,  2.64s/it]  6%|â–Œ         | 276/5000 [12:06<3:26:52,  2.63s/it]                                                    {'loss': 3.0525, 'grad_norm': 0.3492225706577301, 'learning_rate': 0.0009640816326530612, 'epoch': 0.06}
  6%|â–Œ         | 276/5000 [12:06<3:26:52,  2.63s/it]  6%|â–Œ         | 277/5000 [12:08<3:23:09,  2.58s/it]                                                    {'loss': 2.9154, 'grad_norm': 0.42517268657684326, 'learning_rate': 0.0009638775510204081, 'epoch': 0.06}
  6%|â–Œ         | 277/5000 [12:08<3:23:09,  2.58s/it]  6%|â–Œ         | 278/5000 [12:11<3:20:55,  2.55s/it]                                                    {'loss': 2.9958, 'grad_norm': 0.5981501340866089, 'learning_rate': 0.0009636734693877551, 'epoch': 0.06}
  6%|â–Œ         | 278/5000 [12:11<3:20:55,  2.55s/it]  6%|â–Œ         | 279/5000 [12:13<3:23:00,  2.58s/it]                                                    {'loss': 2.9458, 'grad_norm': 0.29349708557128906, 'learning_rate': 0.0009634693877551021, 'epoch': 0.06}
  6%|â–Œ         | 279/5000 [12:13<3:23:00,  2.58s/it]  6%|â–Œ         | 280/5000 [12:16<3:21:25,  2.56s/it]                                                    {'loss': 2.8791, 'grad_norm': 0.2571489214897156, 'learning_rate': 0.000963265306122449, 'epoch': 0.06}
  6%|â–Œ         | 280/5000 [12:16<3:21:25,  2.56s/it]  6%|â–Œ         | 281/5000 [12:18<3:21:57,  2.57s/it]                                                    {'loss': 3.0925, 'grad_norm': 0.526085615158081, 'learning_rate': 0.000963061224489796, 'epoch': 0.06}
  6%|â–Œ         | 281/5000 [12:18<3:21:57,  2.57s/it]  6%|â–Œ         | 282/5000 [12:21<3:30:41,  2.68s/it]                                                    {'loss': 3.2164, 'grad_norm': 0.7113844156265259, 'learning_rate': 0.0009628571428571429, 'epoch': 0.06}
  6%|â–Œ         | 282/5000 [12:21<3:30:41,  2.68s/it]  6%|â–Œ         | 283/5000 [12:24<3:35:02,  2.74s/it]                                                    {'loss': 3.2562, 'grad_norm': 0.7260287404060364, 'learning_rate': 0.0009626530612244898, 'epoch': 0.06}
  6%|â–Œ         | 283/5000 [12:24<3:35:02,  2.74s/it]  6%|â–Œ         | 284/5000 [12:27<3:45:46,  2.87s/it]                                                    {'loss': 3.083, 'grad_norm': 0.49697041511535645, 'learning_rate': 0.0009624489795918368, 'epoch': 0.06}
  6%|â–Œ         | 284/5000 [12:27<3:45:46,  2.87s/it]  6%|â–Œ         | 285/5000 [12:30<3:41:13,  2.82s/it]                                                    {'loss': 3.3234, 'grad_norm': 0.897943913936615, 'learning_rate': 0.0009622448979591837, 'epoch': 0.06}
  6%|â–Œ         | 285/5000 [12:30<3:41:13,  2.82s/it]  6%|â–Œ         | 286/5000 [12:33<3:32:58,  2.71s/it]                                                    {'loss': 3.0623, 'grad_norm': 0.5614140629768372, 'learning_rate': 0.0009620408163265306, 'epoch': 0.06}
  6%|â–Œ         | 286/5000 [12:33<3:32:58,  2.71s/it]  6%|â–Œ         | 287/5000 [12:35<3:30:14,  2.68s/it]                                                    {'loss': 2.8819, 'grad_norm': 0.3555564880371094, 'learning_rate': 0.0009618367346938776, 'epoch': 0.06}
  6%|â–Œ         | 287/5000 [12:35<3:30:14,  2.68s/it]  6%|â–Œ         | 288/5000 [12:38<3:26:56,  2.64s/it]                                                    {'loss': 3.1834, 'grad_norm': 0.781178891658783, 'learning_rate': 0.0009616326530612245, 'epoch': 0.06}
  6%|â–Œ         | 288/5000 [12:38<3:26:56,  2.64s/it]  6%|â–Œ         | 289/5000 [12:40<3:28:59,  2.66s/it]                                                    {'loss': 3.0325, 'grad_norm': 0.40920305252075195, 'learning_rate': 0.0009614285714285714, 'epoch': 0.06}
  6%|â–Œ         | 289/5000 [12:40<3:28:59,  2.66s/it]  6%|â–Œ         | 290/5000 [12:43<3:26:03,  2.63s/it]                                                    {'loss': 3.241, 'grad_norm': 0.908613383769989, 'learning_rate': 0.0009612244897959183, 'epoch': 0.06}
  6%|â–Œ         | 290/5000 [12:43<3:26:03,  2.63s/it]  6%|â–Œ         | 291/5000 [12:45<3:21:22,  2.57s/it]                                                    {'loss': 3.1503, 'grad_norm': 0.38449421525001526, 'learning_rate': 0.0009610204081632653, 'epoch': 0.06}
  6%|â–Œ         | 291/5000 [12:45<3:21:22,  2.57s/it]  6%|â–Œ         | 292/5000 [12:48<3:19:13,  2.54s/it]                                                    {'loss': 2.9484, 'grad_norm': 0.3318977952003479, 'learning_rate': 0.0009608163265306122, 'epoch': 0.06}
  6%|â–Œ         | 292/5000 [12:48<3:19:13,  2.54s/it]  6%|â–Œ         | 293/5000 [12:50<3:16:55,  2.51s/it]                                                    {'loss': 3.1259, 'grad_norm': 0.6197716593742371, 'learning_rate': 0.0009606122448979591, 'epoch': 0.06}
  6%|â–Œ         | 293/5000 [12:50<3:16:55,  2.51s/it]  6%|â–Œ         | 294/5000 [12:53<3:17:19,  2.52s/it]                                                    {'loss': 2.9206, 'grad_norm': 0.3065708577632904, 'learning_rate': 0.0009604081632653062, 'epoch': 0.06}
  6%|â–Œ         | 294/5000 [12:53<3:17:19,  2.52s/it]  6%|â–Œ         | 295/5000 [12:55<3:17:07,  2.51s/it]                                                    {'loss': 3.0393, 'grad_norm': 0.34376493096351624, 'learning_rate': 0.0009602040816326531, 'epoch': 0.06}
  6%|â–Œ         | 295/5000 [12:55<3:17:07,  2.51s/it]  6%|â–Œ         | 296/5000 [12:58<3:20:43,  2.56s/it]                                                    {'loss': 3.1228, 'grad_norm': 0.48861128091812134, 'learning_rate': 0.00096, 'epoch': 0.06}
  6%|â–Œ         | 296/5000 [12:58<3:20:43,  2.56s/it]  6%|â–Œ         | 297/5000 [13:01<3:21:20,  2.57s/it]                                                    {'loss': 3.1762, 'grad_norm': 0.5152445435523987, 'learning_rate': 0.000959795918367347, 'epoch': 0.06}
  6%|â–Œ         | 297/5000 [13:01<3:21:20,  2.57s/it]  6%|â–Œ         | 298/5000 [13:03<3:26:57,  2.64s/it]                                                    {'loss': 3.1315, 'grad_norm': 0.5080228447914124, 'learning_rate': 0.0009595918367346939, 'epoch': 0.06}
  6%|â–Œ         | 298/5000 [13:03<3:26:57,  2.64s/it]  6%|â–Œ         | 299/5000 [13:07<3:44:22,  2.86s/it]                                                    {'loss': 3.2948, 'grad_norm': 0.4094470739364624, 'learning_rate': 0.0009593877551020408, 'epoch': 0.06}
  6%|â–Œ         | 299/5000 [13:07<3:44:22,  2.86s/it]  6%|â–Œ         | 300/5000 [13:09<3:37:37,  2.78s/it]                                                    {'loss': 2.9135, 'grad_norm': 0.27067264914512634, 'learning_rate': 0.0009591836734693877, 'epoch': 0.06}
  6%|â–Œ         | 300/5000 [13:09<3:37:37,  2.78s/it][2025-10-19 17:43:35,154] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 301/5000 [13:13<4:01:34,  3.08s/it]                                                    {'loss': 3.4646, 'grad_norm': 0.8001262545585632, 'learning_rate': 0.0009589795918367347, 'epoch': 0.06}
  6%|â–Œ         | 301/5000 [13:13<4:01:34,  3.08s/it]  6%|â–Œ         | 302/5000 [13:16<3:49:07,  2.93s/it]                                                    {'loss': 3.0123, 'grad_norm': 0.38791683316230774, 'learning_rate': 0.0009587755102040816, 'epoch': 0.06}
  6%|â–Œ         | 302/5000 [13:16<3:49:07,  2.93s/it]  6%|â–Œ         | 303/5000 [13:18<3:40:47,  2.82s/it]                                                    {'loss': 2.9857, 'grad_norm': 0.4423736333847046, 'learning_rate': 0.0009585714285714285, 'epoch': 0.06}
  6%|â–Œ         | 303/5000 [13:18<3:40:47,  2.82s/it]  6%|â–Œ         | 304/5000 [13:21<3:35:38,  2.76s/it]                                                    {'loss': 2.9838, 'grad_norm': 0.4956742227077484, 'learning_rate': 0.0009583673469387755, 'epoch': 0.06}
  6%|â–Œ         | 304/5000 [13:21<3:35:38,  2.76s/it]  6%|â–Œ         | 305/5000 [13:23<3:27:45,  2.66s/it]                                                    {'loss': 2.9271, 'grad_norm': 0.23886270821094513, 'learning_rate': 0.0009581632653061225, 'epoch': 0.06}
  6%|â–Œ         | 305/5000 [13:23<3:27:45,  2.66s/it]  6%|â–Œ         | 306/5000 [13:26<3:23:46,  2.60s/it]                                                    {'loss': 3.1, 'grad_norm': 0.5072311162948608, 'learning_rate': 0.0009579591836734694, 'epoch': 0.06}
  6%|â–Œ         | 306/5000 [13:26<3:23:46,  2.60s/it]  6%|â–Œ         | 307/5000 [13:28<3:21:44,  2.58s/it]                                                    {'loss': 3.0659, 'grad_norm': 0.3987462818622589, 'learning_rate': 0.0009577551020408164, 'epoch': 0.06}
  6%|â–Œ         | 307/5000 [13:28<3:21:44,  2.58s/it]  6%|â–Œ         | 308/5000 [13:31<3:20:58,  2.57s/it]                                                    {'loss': 2.9278, 'grad_norm': 0.3727334141731262, 'learning_rate': 0.0009575510204081633, 'epoch': 0.06}
  6%|â–Œ         | 308/5000 [13:31<3:20:58,  2.57s/it]  6%|â–Œ         | 309/5000 [13:34<3:23:34,  2.60s/it]                                                    {'loss': 2.9972, 'grad_norm': 0.44878652691841125, 'learning_rate': 0.0009573469387755102, 'epoch': 0.06}
  6%|â–Œ         | 309/5000 [13:34<3:23:34,  2.60s/it]  6%|â–Œ         | 310/5000 [13:36<3:21:06,  2.57s/it]                                                    {'loss': 2.916, 'grad_norm': 0.26255738735198975, 'learning_rate': 0.0009571428571428573, 'epoch': 0.06}
  6%|â–Œ         | 310/5000 [13:36<3:21:06,  2.57s/it]  6%|â–Œ         | 311/5000 [13:39<3:21:39,  2.58s/it]                                                    {'loss': 2.9832, 'grad_norm': 0.5618961453437805, 'learning_rate': 0.0009569387755102042, 'epoch': 0.06}
  6%|â–Œ         | 311/5000 [13:39<3:21:39,  2.58s/it]  6%|â–Œ         | 312/5000 [13:41<3:20:22,  2.56s/it]                                                    {'loss': 2.9243, 'grad_norm': 0.2586900591850281, 'learning_rate': 0.0009567346938775511, 'epoch': 0.06}
  6%|â–Œ         | 312/5000 [13:41<3:20:22,  2.56s/it]  6%|â–‹         | 313/5000 [13:44<3:20:12,  2.56s/it]                                                    {'loss': 3.0769, 'grad_norm': 0.3498181700706482, 'learning_rate': 0.000956530612244898, 'epoch': 0.06}
  6%|â–‹         | 313/5000 [13:44<3:20:12,  2.56s/it]  6%|â–‹         | 314/5000 [13:46<3:20:04,  2.56s/it]                                                    {'loss': 2.9705, 'grad_norm': 0.7254247665405273, 'learning_rate': 0.000956326530612245, 'epoch': 0.06}
  6%|â–‹         | 314/5000 [13:46<3:20:04,  2.56s/it]  6%|â–‹         | 315/5000 [13:49<3:17:08,  2.52s/it]                                                    {'loss': 3.1305, 'grad_norm': 0.49426233768463135, 'learning_rate': 0.0009561224489795919, 'epoch': 0.06}
  6%|â–‹         | 315/5000 [13:49<3:17:08,  2.52s/it]  6%|â–‹         | 316/5000 [13:51<3:17:13,  2.53s/it]                                                    {'loss': 3.0058, 'grad_norm': 0.39152052998542786, 'learning_rate': 0.0009559183673469388, 'epoch': 0.06}
  6%|â–‹         | 316/5000 [13:51<3:17:13,  2.53s/it]  6%|â–‹         | 317/5000 [13:54<3:21:22,  2.58s/it]                                                    {'loss': 2.9133, 'grad_norm': 0.2454213798046112, 'learning_rate': 0.0009557142857142858, 'epoch': 0.06}
  6%|â–‹         | 317/5000 [13:54<3:21:22,  2.58s/it]  6%|â–‹         | 318/5000 [13:57<3:26:49,  2.65s/it]                                                    {'loss': 2.9556, 'grad_norm': 0.3426465392112732, 'learning_rate': 0.0009555102040816327, 'epoch': 0.06}
  6%|â–‹         | 318/5000 [13:57<3:26:49,  2.65s/it]  6%|â–‹         | 319/5000 [13:59<3:24:06,  2.62s/it]                                                    {'loss': 3.0353, 'grad_norm': 0.4541272819042206, 'learning_rate': 0.0009553061224489796, 'epoch': 0.06}
  6%|â–‹         | 319/5000 [13:59<3:24:06,  2.62s/it]  6%|â–‹         | 320/5000 [14:02<3:23:52,  2.61s/it]                                                    {'loss': 3.0595, 'grad_norm': 0.46642202138900757, 'learning_rate': 0.0009551020408163265, 'epoch': 0.06}
  6%|â–‹         | 320/5000 [14:02<3:23:52,  2.61s/it]  6%|â–‹         | 321/5000 [14:05<3:30:53,  2.70s/it]                                                    {'loss': 3.0884, 'grad_norm': 0.5171788334846497, 'learning_rate': 0.0009548979591836735, 'epoch': 0.06}
  6%|â–‹         | 321/5000 [14:05<3:30:53,  2.70s/it]  6%|â–‹         | 322/5000 [14:07<3:26:51,  2.65s/it]                                                    {'loss': 2.8691, 'grad_norm': 0.2566542625427246, 'learning_rate': 0.0009546938775510204, 'epoch': 0.06}
  6%|â–‹         | 322/5000 [14:07<3:26:51,  2.65s/it]  6%|â–‹         | 323/5000 [14:10<3:37:08,  2.79s/it]                                                    {'loss': 3.1776, 'grad_norm': 0.478086918592453, 'learning_rate': 0.0009544897959183673, 'epoch': 0.06}
  6%|â–‹         | 323/5000 [14:10<3:37:08,  2.79s/it]  6%|â–‹         | 324/5000 [14:13<3:28:28,  2.68s/it]                                                    {'loss': 3.3317, 'grad_norm': 0.5636104345321655, 'learning_rate': 0.0009542857142857143, 'epoch': 0.06}
  6%|â–‹         | 324/5000 [14:13<3:28:28,  2.68s/it]