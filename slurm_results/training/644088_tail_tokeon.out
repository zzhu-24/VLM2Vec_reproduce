==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailToken-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 128 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 5000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/train.log
W1019 17:46:28.670000 135642233521984 torch/distributed/run.py:779] 
W1019 17:46:28.670000 135642233521984 torch/distributed/run.py:779] *****************************************
W1019 17:46:28.670000 135642233521984 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1019 17:46:28.670000 135642233521984 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-19 17:46:39,246] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.65it/s]
wandb: setting up run ac3o0ciw
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_174639-ac3o0ciw
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailToken-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/ac3o0ciw
[2025-10-19 17:46:40,798] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.98it/s]
[2025-10-19 17:46:41,423] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-19 17:46:41,427] INFO [src.utils:19] Loading lora adapter from TailTokenWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-19 17:46:50,325] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-19 17:46:51,534] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-19 17:46:51,534] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-19 17:46:55,854] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-19 17:46:55,855] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-19 17:46:56,781] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-19 17:46:56,782] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-19 17:46:56,782] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=256
		estimated num step per epoch=833.15234375
		interleave_batch_size=0.0
[2025-10-19 17:46:56,783] INFO [src.utils:19] ==================================================
[2025-10-19 17:46:56,784] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-19 17:46:56,784] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:46:56,785] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:46:56,786] INFO [src.utils:19] ==================================================
[2025-10-19 17:46:58,071] INFO [src.utils:19] âœ… Custom optimizer (gme.learnable_token only) enabled
[2025-10-19 17:46:58,530] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:46:58,530] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:46:58,531] INFO [src.trainer:343]   Num examples = 1,280,000
[2025-10-19 17:46:58,531] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:46:58,531] INFO [src.trainer:345]   Instantaneous batch size per device = 128
[2025-10-19 17:46:58,531] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 256
[2025-10-19 17:46:58,531] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:46:58,531] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:46:58,531] INFO [src.trainer:343]   Num examples = 1,280,000
[2025-10-19 17:46:58,531] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:46:58,532] INFO [src.trainer:345]   Instantaneous batch size per device = 128
[2025-10-19 17:46:58,532] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 256
[2025-10-19 17:46:58,532] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:46:58,533] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:46:58,539] INFO [src.trainer:351]   Number of trainable parameters = 1,536
[2025-10-19 17:46:58,541] INFO [src.trainer:351]   Number of trainable parameters = 1,536
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1019 17:47:17.863429568 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1019 17:47:18.147116741 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/5000 [00:26<36:42:10, 26.43s/it]                                                   {'loss': 32.9808, 'grad_norm': 17.725679397583008, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 1/5000 [00:26<36:42:10, 26.43s/it]  0%|          | 2/5000 [00:50<34:45:53, 25.04s/it]                                                   {'loss': 33.2803, 'grad_norm': 17.949134826660156, 'learning_rate': 2e-05, 'epoch': 0.0}
  0%|          | 2/5000 [00:50<34:45:53, 25.04s/it]  0%|          | 3/5000 [01:12<32:59:19, 23.77s/it]                                                   {'loss': 32.3885, 'grad_norm': 17.788362503051758, 'learning_rate': 3e-05, 'epoch': 0.0}
  0%|          | 3/5000 [01:12<32:59:19, 23.77s/it]  0%|          | 4/5000 [01:40<34:56:05, 25.17s/it]                                                   {'loss': 33.0347, 'grad_norm': 18.068668365478516, 'learning_rate': 4e-05, 'epoch': 0.0}
  0%|          | 4/5000 [01:40<34:56:05, 25.17s/it]  0%|          | 5/5000 [02:00<32:39:11, 23.53s/it]                                                   {'loss': 32.6793, 'grad_norm': 17.752363204956055, 'learning_rate': 5e-05, 'epoch': 0.0}
  0%|          | 5/5000 [02:00<32:39:11, 23.53s/it]  0%|          | 6/5000 [02:29<34:58:55, 25.22s/it]                                                   {'loss': 32.9456, 'grad_norm': 18.116798400878906, 'learning_rate': 6e-05, 'epoch': 0.0}
  0%|          | 6/5000 [02:29<34:58:55, 25.22s/it]  0%|          | 7/5000 [02:50<33:21:35, 24.05s/it]                                                   {'loss': 32.559, 'grad_norm': 17.94120216369629, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.0}
  0%|          | 7/5000 [02:50<33:21:35, 24.05s/it]  0%|          | 8/5000 [03:11<31:45:05, 22.90s/it]                                                   {'loss': 32.6965, 'grad_norm': 17.98188018798828, 'learning_rate': 8e-05, 'epoch': 0.0}
  0%|          | 8/5000 [03:11<31:45:05, 22.90s/it]  0%|          | 9/5000 [03:34<32:04:15, 23.13s/it]                                                   {'loss': 32.2062, 'grad_norm': 17.577482223510742, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.0}
  0%|          | 9/5000 [03:34<32:04:15, 23.13s/it]  0%|          | 10/5000 [03:59<32:39:34, 23.56s/it]                                                    {'loss': 32.6166, 'grad_norm': 17.903974533081055, 'learning_rate': 0.0001, 'epoch': 0.0}
  0%|          | 10/5000 [03:59<32:39:34, 23.56s/it]  0%|          | 11/5000 [04:23<32:42:24, 23.60s/it]                                                    {'loss': 32.9033, 'grad_norm': 17.832359313964844, 'learning_rate': 0.00011, 'epoch': 0.0}
  0%|          | 11/5000 [04:23<32:42:24, 23.60s/it]  0%|          | 12/5000 [04:47<32:50:00, 23.70s/it]                                                    {'loss': 32.3033, 'grad_norm': 17.755556106567383, 'learning_rate': 0.00012, 'epoch': 0.0}
  0%|          | 12/5000 [04:47<32:50:00, 23.70s/it]  0%|          | 13/5000 [05:15<34:41:41, 25.05s/it]                                                    {'loss': 31.9593, 'grad_norm': 17.60359764099121, 'learning_rate': 0.00013000000000000002, 'epoch': 0.0}
  0%|          | 13/5000 [05:15<34:41:41, 25.05s/it]  0%|          | 14/5000 [05:41<35:07:48, 25.36s/it]                                                    {'loss': 33.0212, 'grad_norm': 17.808597564697266, 'learning_rate': 0.00014000000000000001, 'epoch': 0.0}
  0%|          | 14/5000 [05:41<35:07:48, 25.36s/it]  0%|          | 15/5000 [06:06<34:55:16, 25.22s/it]                                                    {'loss': 32.0494, 'grad_norm': 17.54547691345215, 'learning_rate': 0.00015, 'epoch': 0.0}
  0%|          | 15/5000 [06:06<34:55:16, 25.22s/it]  0%|          | 16/5000 [06:28<33:46:51, 24.40s/it]                                                    {'loss': 31.5583, 'grad_norm': 17.671518325805664, 'learning_rate': 0.00016, 'epoch': 0.0}
  0%|          | 16/5000 [06:28<33:46:51, 24.40s/it]  0%|          | 17/5000 [06:50<32:42:35, 23.63s/it]                                                    {'loss': 32.8079, 'grad_norm': 17.81682777404785, 'learning_rate': 0.00017, 'epoch': 0.0}
  0%|          | 17/5000 [06:50<32:42:35, 23.63s/it]  0%|          | 18/5000 [07:17<34:00:06, 24.57s/it]                                                    {'loss': 31.9743, 'grad_norm': 17.535585403442383, 'learning_rate': 0.00017999999999999998, 'epoch': 0.0}
  0%|          | 18/5000 [07:17<34:00:06, 24.57s/it]  0%|          | 19/5000 [07:38<32:35:43, 23.56s/it]                                                    {'loss': 31.0819, 'grad_norm': 17.4420223236084, 'learning_rate': 0.00019, 'epoch': 0.0}
  0%|          | 19/5000 [07:38<32:35:43, 23.56s/it]  0%|          | 20/5000 [08:04<33:36:09, 24.29s/it]                                                    {'loss': 31.8401, 'grad_norm': 17.522192001342773, 'learning_rate': 0.0002, 'epoch': 0.0}
  0%|          | 20/5000 [08:04<33:36:09, 24.29s/it]  0%|          | 21/5000 [08:28<33:29:15, 24.21s/it]                                                    {'loss': 31.5156, 'grad_norm': 17.45735740661621, 'learning_rate': 0.00021, 'epoch': 0.0}
  0%|          | 21/5000 [08:28<33:29:15, 24.21s/it]  0%|          | 22/5000 [08:55<34:26:12, 24.90s/it]                                                    {'loss': 31.9144, 'grad_norm': 17.2354679107666, 'learning_rate': 0.00022, 'epoch': 0.0}
  0%|          | 22/5000 [08:55<34:26:12, 24.90s/it]  0%|          | 23/5000 [09:16<33:00:19, 23.87s/it]                                                    {'loss': 32.524, 'grad_norm': 17.509315490722656, 'learning_rate': 0.00023, 'epoch': 0.0}
  0%|          | 23/5000 [09:16<33:00:19, 23.87s/it]  0%|          | 24/5000 [09:42<33:45:44, 24.43s/it]                                                    {'loss': 31.277, 'grad_norm': 17.407773971557617, 'learning_rate': 0.00024, 'epoch': 0.0}
  0%|          | 24/5000 [09:42<33:45:44, 24.43s/it]  0%|          | 25/5000 [10:07<34:01:01, 24.62s/it]                                                    {'loss': 31.1361, 'grad_norm': 17.353559494018555, 'learning_rate': 0.00025, 'epoch': 0.01}
  0%|          | 25/5000 [10:07<34:01:01, 24.62s/it]  1%|          | 26/5000 [10:28<32:47:28, 23.73s/it]                                                    {'loss': 31.0582, 'grad_norm': 17.376781463623047, 'learning_rate': 0.00026000000000000003, 'epoch': 0.01}
  1%|          | 26/5000 [10:28<32:47:28, 23.73s/it]  1%|          | 27/5000 [10:54<33:34:32, 24.31s/it]                                                    {'loss': 30.9947, 'grad_norm': 17.33339500427246, 'learning_rate': 0.00027, 'epoch': 0.01}
  1%|          | 27/5000 [10:54<33:34:32, 24.31s/it]  1%|          | 28/5000 [11:15<32:21:37, 23.43s/it]                                                    {'loss': 31.5889, 'grad_norm': 17.184303283691406, 'learning_rate': 0.00028000000000000003, 'epoch': 0.01}
  1%|          | 28/5000 [11:15<32:21:37, 23.43s/it]  1%|          | 29/5000 [11:40<32:54:49, 23.84s/it]                                                    {'loss': 31.0641, 'grad_norm': 17.2819881439209, 'learning_rate': 0.00029, 'epoch': 0.01}
  1%|          | 29/5000 [11:40<32:54:49, 23.84s/it]  1%|          | 30/5000 [12:06<33:51:39, 24.53s/it]                                                    {'loss': 31.4769, 'grad_norm': 17.18438148498535, 'learning_rate': 0.0003, 'epoch': 0.01}
  1%|          | 30/5000 [12:06<33:51:39, 24.53s/it]  1%|          | 31/5000 [12:30<33:25:55, 24.22s/it]                                                    {'loss': 30.3283, 'grad_norm': 17.189739227294922, 'learning_rate': 0.00031, 'epoch': 0.01}
  1%|          | 31/5000 [12:30<33:25:55, 24.22s/it]  1%|          | 32/5000 [12:54<33:26:24, 24.23s/it]                                                    {'loss': 31.1971, 'grad_norm': 17.172210693359375, 'learning_rate': 0.00032, 'epoch': 0.01}
  1%|          | 32/5000 [12:54<33:26:24, 24.23s/it]  1%|          | 33/5000 [13:19<33:32:51, 24.31s/it]                                                    {'loss': 30.2804, 'grad_norm': 17.038442611694336, 'learning_rate': 0.00033, 'epoch': 0.01}
  1%|          | 33/5000 [13:19<33:32:51, 24.31s/it]  1%|          | 34/5000 [13:41<32:54:58, 23.86s/it]                                                    {'loss': 30.5756, 'grad_norm': 17.301607131958008, 'learning_rate': 0.00034, 'epoch': 0.01}
  1%|          | 34/5000 [13:41<32:54:58, 23.86s/it]  1%|          | 35/5000 [14:06<33:03:30, 23.97s/it]                                                    {'loss': 29.7968, 'grad_norm': 16.991497039794922, 'learning_rate': 0.00035, 'epoch': 0.01}
  1%|          | 35/5000 [14:06<33:03:30, 23.97s/it]  1%|          | 36/5000 [14:32<34:06:14, 24.73s/it]                                                    {'loss': 29.4136, 'grad_norm': 16.907636642456055, 'learning_rate': 0.00035999999999999997, 'epoch': 0.01}
  1%|          | 36/5000 [14:32<34:06:14, 24.73s/it]  1%|          | 37/5000 [14:54<33:04:38, 23.99s/it]                                                    {'loss': 30.2799, 'grad_norm': 16.825119018554688, 'learning_rate': 0.00037, 'epoch': 0.01}
  1%|          | 37/5000 [14:54<33:04:38, 23.99s/it]  1%|          | 38/5000 [15:22<34:43:44, 25.20s/it]                                                    {'loss': 29.9085, 'grad_norm': 16.982261657714844, 'learning_rate': 0.00038, 'epoch': 0.01}
  1%|          | 38/5000 [15:22<34:43:44, 25.20s/it]  1%|          | 39/5000 [15:44<33:09:40, 24.06s/it]                                                    {'loss': 29.5794, 'grad_norm': 16.953750610351562, 'learning_rate': 0.00039000000000000005, 'epoch': 0.01}
  1%|          | 39/5000 [15:44<33:09:40, 24.06s/it]  1%|          | 40/5000 [16:07<32:57:00, 23.92s/it]                                                    {'loss': 28.9061, 'grad_norm': 16.962080001831055, 'learning_rate': 0.0004, 'epoch': 0.01}
  1%|          | 40/5000 [16:07<32:57:00, 23.92s/it]  1%|          | 41/5000 [16:33<33:30:02, 24.32s/it]                                                    {'loss': 28.9508, 'grad_norm': 16.775291442871094, 'learning_rate': 0.00041, 'epoch': 0.01}
  1%|          | 41/5000 [16:33<33:30:02, 24.32s/it]  1%|          | 42/5000 [16:54<32:06:04, 23.31s/it]                                                    {'loss': 29.4894, 'grad_norm': 16.959531784057617, 'learning_rate': 0.00042, 'epoch': 0.01}
  1%|          | 42/5000 [16:54<32:06:04, 23.31s/it]  1%|          | 43/5000 [17:20<33:13:19, 24.13s/it]                                                    {'loss': 28.8705, 'grad_norm': 16.83367919921875, 'learning_rate': 0.00043, 'epoch': 0.01}
  1%|          | 43/5000 [17:20<33:13:19, 24.13s/it]  1%|          | 44/5000 [17:43<32:48:30, 23.83s/it]                                                    {'loss': 27.9367, 'grad_norm': 16.96211814880371, 'learning_rate': 0.00044, 'epoch': 0.01}
  1%|          | 44/5000 [17:43<32:48:30, 23.83s/it]  1%|          | 45/5000 [18:05<32:10:30, 23.38s/it]                                                    {'loss': 27.4997, 'grad_norm': 17.024564743041992, 'learning_rate': 0.00045000000000000004, 'epoch': 0.01}
  1%|          | 45/5000 [18:05<32:10:30, 23.38s/it]  1%|          | 46/5000 [18:29<32:28:57, 23.60s/it]                                                    {'loss': 26.9484, 'grad_norm': 16.882436752319336, 'learning_rate': 0.00046, 'epoch': 0.01}
  1%|          | 46/5000 [18:29<32:28:57, 23.60s/it]  1%|          | 47/5000 [18:50<31:15:23, 22.72s/it]                                                    {'loss': 27.8175, 'grad_norm': 17.011226654052734, 'learning_rate': 0.00047, 'epoch': 0.01}
  1%|          | 47/5000 [18:50<31:15:23, 22.72s/it]  1%|          | 48/5000 [19:13<31:32:16, 22.93s/it]                                                    {'loss': 28.0131, 'grad_norm': 17.34690284729004, 'learning_rate': 0.00048, 'epoch': 0.01}
  1%|          | 48/5000 [19:13<31:32:16, 22.93s/it]  1%|          | 49/5000 [19:38<32:17:42, 23.48s/it]                                                    {'loss': 26.9036, 'grad_norm': 17.637338638305664, 'learning_rate': 0.00049, 'epoch': 0.01}
  1%|          | 49/5000 [19:38<32:17:42, 23.48s/it]  1%|          | 50/5000 [20:01<32:09:50, 23.39s/it]                                                    {'loss': 27.1179, 'grad_norm': 17.389686584472656, 'learning_rate': 0.0005, 'epoch': 0.01}
  1%|          | 50/5000 [20:01<32:09:50, 23.39s/it][2025-10-19 18:07:00,578] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/5000 [20:29<33:58:46, 24.72s/it]                                                    {'loss': 26.1477, 'grad_norm': 17.38417625427246, 'learning_rate': 0.00051, 'epoch': 0.01}
  1%|          | 51/5000 [20:29<33:58:46, 24.72s/it]  1%|          | 52/5000 [20:55<34:35:35, 25.17s/it]                                                    {'loss': 26.321, 'grad_norm': 17.50910758972168, 'learning_rate': 0.0005200000000000001, 'epoch': 0.01}
  1%|          | 52/5000 [20:55<34:35:35, 25.17s/it]  1%|          | 53/5000 [21:20<34:14:24, 24.92s/it]                                                    {'loss': 25.3136, 'grad_norm': 17.60872459411621, 'learning_rate': 0.0005300000000000001, 'epoch': 0.01}
  1%|          | 53/5000 [21:20<34:14:24, 24.92s/it]  1%|          | 54/5000 [21:40<32:25:48, 23.60s/it]                                                    {'loss': 26.3663, 'grad_norm': 17.605663299560547, 'learning_rate': 0.00054, 'epoch': 0.01}
  1%|          | 54/5000 [21:40<32:25:48, 23.60s/it]  1%|          | 55/5000 [22:02<31:30:43, 22.94s/it]                                                    {'loss': 26.037, 'grad_norm': 17.764625549316406, 'learning_rate': 0.00055, 'epoch': 0.01}
  1%|          | 55/5000 [22:02<31:30:43, 22.94s/it]  1%|          | 56/5000 [22:26<32:02:48, 23.34s/it]                                                    {'loss': 25.4858, 'grad_norm': 17.897850036621094, 'learning_rate': 0.0005600000000000001, 'epoch': 0.01}
  1%|          | 56/5000 [22:26<32:02:48, 23.34s/it]  1%|          | 57/5000 [22:49<31:56:56, 23.27s/it]                                                    {'loss': 25.2605, 'grad_norm': 17.857229232788086, 'learning_rate': 0.00057, 'epoch': 0.01}
  1%|          | 57/5000 [22:49<31:56:56, 23.27s/it]  1%|          | 58/5000 [23:10<30:55:55, 22.53s/it]                                                    {'loss': 25.8521, 'grad_norm': 18.253698348999023, 'learning_rate': 0.00058, 'epoch': 0.01}
  1%|          | 58/5000 [23:10<30:55:55, 22.53s/it]  1%|          | 59/5000 [23:32<30:39:04, 22.33s/it]                                                    {'loss': 24.6804, 'grad_norm': 18.259946823120117, 'learning_rate': 0.00059, 'epoch': 0.01}
  1%|          | 59/5000 [23:32<30:39:04, 22.33s/it]  1%|          | 60/5000 [23:52<29:53:23, 21.78s/it]                                                    {'loss': 23.7875, 'grad_norm': 18.478046417236328, 'learning_rate': 0.0006, 'epoch': 0.01}
  1%|          | 60/5000 [23:52<29:53:23, 21.78s/it]  1%|          | 61/5000 [24:18<31:21:25, 22.86s/it]                                                    {'loss': 23.8821, 'grad_norm': 18.47706413269043, 'learning_rate': 0.00061, 'epoch': 0.01}
  1%|          | 61/5000 [24:18<31:21:25, 22.86s/it]  1%|          | 62/5000 [24:40<31:15:15, 22.79s/it]                                                    {'loss': 23.6884, 'grad_norm': 18.908573150634766, 'learning_rate': 0.00062, 'epoch': 0.01}
  1%|          | 62/5000 [24:40<31:15:15, 22.79s/it]  1%|â–         | 63/5000 [25:02<30:45:24, 22.43s/it]                                                    {'loss': 23.0702, 'grad_norm': 18.89166259765625, 'learning_rate': 0.00063, 'epoch': 0.01}
  1%|â–         | 63/5000 [25:02<30:45:24, 22.43s/it]  1%|â–         | 64/5000 [25:26<31:19:31, 22.85s/it]                                                    {'loss': 22.6645, 'grad_norm': 18.860628128051758, 'learning_rate': 0.00064, 'epoch': 0.01}
  1%|â–         | 64/5000 [25:26<31:19:31, 22.85s/it]  1%|â–         | 65/5000 [25:49<31:41:43, 23.12s/it]                                                    {'loss': 22.1872, 'grad_norm': 19.066295623779297, 'learning_rate': 0.0006500000000000001, 'epoch': 0.01}
  1%|â–         | 65/5000 [25:49<31:41:43, 23.12s/it]  1%|â–         | 66/5000 [26:11<31:04:19, 22.67s/it]                                                    {'loss': 22.3849, 'grad_norm': 19.145553588867188, 'learning_rate': 0.00066, 'epoch': 0.01}
  1%|â–         | 66/5000 [26:11<31:04:19, 22.67s/it]  1%|â–         | 67/5000 [26:32<30:24:59, 22.20s/it]                                                    {'loss': 21.4268, 'grad_norm': 18.786497116088867, 'learning_rate': 0.00067, 'epoch': 0.01}
  1%|â–         | 67/5000 [26:32<30:24:59, 22.20s/it]  1%|â–         | 68/5000 [26:53<30:00:51, 21.91s/it]                                                    {'loss': 21.4661, 'grad_norm': 19.202417373657227, 'learning_rate': 0.00068, 'epoch': 0.01}
  1%|â–         | 68/5000 [26:53<30:00:51, 21.91s/it]  1%|â–         | 69/5000 [27:21<32:33:22, 23.77s/it]                                                    {'loss': 20.7704, 'grad_norm': 18.9866943359375, 'learning_rate': 0.00069, 'epoch': 0.01}
  1%|â–         | 69/5000 [27:21<32:33:22, 23.77s/it]  1%|â–         | 70/5000 [27:44<32:00:51, 23.38s/it]                                                    {'loss': 20.2657, 'grad_norm': 18.92247772216797, 'learning_rate': 0.0007, 'epoch': 0.01}
  1%|â–         | 70/5000 [27:44<32:00:51, 23.38s/it]  1%|â–         | 71/5000 [28:11<33:35:38, 24.54s/it]                                                    {'loss': 20.5872, 'grad_norm': 18.897804260253906, 'learning_rate': 0.00071, 'epoch': 0.01}
  1%|â–         | 71/5000 [28:11<33:35:38, 24.54s/it]  1%|â–         | 72/5000 [28:41<35:52:07, 26.20s/it]                                                    {'loss': 18.7388, 'grad_norm': 18.383304595947266, 'learning_rate': 0.0007199999999999999, 'epoch': 0.01}
  1%|â–         | 72/5000 [28:41<35:52:07, 26.20s/it]  1%|â–         | 73/5000 [29:11<37:31:06, 27.41s/it]                                                    {'loss': 19.5857, 'grad_norm': 18.609922409057617, 'learning_rate': 0.00073, 'epoch': 0.01}
  1%|â–         | 73/5000 [29:11<37:31:06, 27.41s/it]  1%|â–         | 74/5000 [29:36<36:27:16, 26.64s/it]                                                    {'loss': 18.4954, 'grad_norm': 18.147693634033203, 'learning_rate': 0.00074, 'epoch': 0.01}
  1%|â–         | 74/5000 [29:36<36:27:16, 26.64s/it]  2%|â–         | 75/5000 [30:04<36:47:25, 26.89s/it]                                                    {'loss': 18.3833, 'grad_norm': 18.1557559967041, 'learning_rate': 0.00075, 'epoch': 0.01}
  2%|â–         | 75/5000 [30:04<36:47:25, 26.89s/it]  2%|â–         | 76/5000 [30:29<35:59:27, 26.31s/it]                                                    {'loss': 17.971, 'grad_norm': 17.356645584106445, 'learning_rate': 0.00076, 'epoch': 0.02}
  2%|â–         | 76/5000 [30:29<35:59:27, 26.31s/it]  2%|â–         | 77/5000 [30:50<33:54:08, 24.79s/it]                                                    {'loss': 17.0656, 'grad_norm': 16.89594268798828, 'learning_rate': 0.0007700000000000001, 'epoch': 0.02}
  2%|â–         | 77/5000 [30:50<33:54:08, 24.79s/it]  2%|â–         | 78/5000 [31:11<32:12:26, 23.56s/it]                                                    {'loss': 17.3005, 'grad_norm': 16.80997657775879, 'learning_rate': 0.0007800000000000001, 'epoch': 0.02}
  2%|â–         | 78/5000 [31:11<32:12:26, 23.56s/it]  2%|â–         | 79/5000 [31:38<33:47:17, 24.72s/it]                                                    {'loss': 16.6587, 'grad_norm': 15.993206024169922, 'learning_rate': 0.00079, 'epoch': 0.02}
  2%|â–         | 79/5000 [31:38<33:47:17, 24.72s/it]  2%|â–         | 80/5000 [32:05<34:37:53, 25.34s/it]                                                    {'loss': 16.5173, 'grad_norm': 15.148974418640137, 'learning_rate': 0.0008, 'epoch': 0.02}
  2%|â–         | 80/5000 [32:05<34:37:53, 25.34s/it]  2%|â–         | 81/5000 [32:29<34:04:25, 24.94s/it]                                                    {'loss': 15.9884, 'grad_norm': 14.888419151306152, 'learning_rate': 0.0008100000000000001, 'epoch': 0.02}
  2%|â–         | 81/5000 [32:29<34:04:25, 24.94s/it]  2%|â–         | 82/5000 [32:53<33:42:27, 24.67s/it]                                                    {'loss': 15.6516, 'grad_norm': 14.059409141540527, 'learning_rate': 0.00082, 'epoch': 0.02}
  2%|â–         | 82/5000 [32:53<33:42:27, 24.67s/it]  2%|â–         | 83/5000 [33:17<33:16:14, 24.36s/it]                                                    {'loss': 15.1531, 'grad_norm': 13.218663215637207, 'learning_rate': 0.00083, 'epoch': 0.02}
  2%|â–         | 83/5000 [33:17<33:16:14, 24.36s/it]  2%|â–         | 84/5000 [33:38<32:14:50, 23.61s/it]                                                    {'loss': 15.4767, 'grad_norm': 12.639010429382324, 'learning_rate': 0.00084, 'epoch': 0.02}
  2%|â–         | 84/5000 [33:38<32:14:50, 23.61s/it]  2%|â–         | 85/5000 [34:00<31:15:32, 22.90s/it]                                                    {'loss': 14.178, 'grad_norm': 11.537003517150879, 'learning_rate': 0.00085, 'epoch': 0.02}
  2%|â–         | 85/5000 [34:00<31:15:32, 22.90s/it]  2%|â–         | 86/5000 [34:26<32:35:42, 23.88s/it]                                                    {'loss': 13.4984, 'grad_norm': 10.99255084991455, 'learning_rate': 0.00086, 'epoch': 0.02}
  2%|â–         | 86/5000 [34:26<32:35:42, 23.88s/it]  2%|â–         | 87/5000 [34:50<32:38:16, 23.92s/it]                                                    {'loss': 13.554, 'grad_norm': 10.141480445861816, 'learning_rate': 0.00087, 'epoch': 0.02}
  2%|â–         | 87/5000 [34:50<32:38:16, 23.92s/it]  2%|â–         | 88/5000 [35:16<33:27:29, 24.52s/it]                                                    {'loss': 13.6224, 'grad_norm': 9.679733276367188, 'learning_rate': 0.00088, 'epoch': 0.02}
  2%|â–         | 88/5000 [35:16<33:27:29, 24.52s/it]  2%|â–         | 89/5000 [35:40<33:16:33, 24.39s/it]                                                    {'loss': 12.9784, 'grad_norm': 8.741966247558594, 'learning_rate': 0.0008900000000000001, 'epoch': 0.02}
  2%|â–         | 89/5000 [35:40<33:16:33, 24.39s/it]  2%|â–         | 90/5000 [36:03<32:55:19, 24.14s/it]                                                    {'loss': 12.3964, 'grad_norm': 7.934696674346924, 'learning_rate': 0.0009000000000000001, 'epoch': 0.02}
  2%|â–         | 90/5000 [36:03<32:55:19, 24.14s/it]  2%|â–         | 91/5000 [36:27<32:49:30, 24.07s/it]                                                    {'loss': 12.4457, 'grad_norm': 7.475056171417236, 'learning_rate': 0.00091, 'epoch': 0.02}
  2%|â–         | 91/5000 [36:27<32:49:30, 24.07s/it]  2%|â–         | 92/5000 [36:52<33:10:06, 24.33s/it]                                                    {'loss': 12.1859, 'grad_norm': 6.684350490570068, 'learning_rate': 0.00092, 'epoch': 0.02}
  2%|â–         | 92/5000 [36:52<33:10:06, 24.33s/it]  2%|â–         | 93/5000 [37:13<31:46:10, 23.31s/it]                                                    {'loss': 11.843, 'grad_norm': 6.402169227600098, 'learning_rate': 0.00093, 'epoch': 0.02}
  2%|â–         | 93/5000 [37:13<31:46:10, 23.31s/it]  2%|â–         | 94/5000 [37:40<33:08:11, 24.32s/it]                                                    {'loss': 11.5035, 'grad_norm': 6.0971550941467285, 'learning_rate': 0.00094, 'epoch': 0.02}
  2%|â–         | 94/5000 [37:40<33:08:11, 24.32s/it]  2%|â–         | 95/5000 [38:02<32:18:40, 23.71s/it]                                                    {'loss': 11.8025, 'grad_norm': 5.795527458190918, 'learning_rate': 0.00095, 'epoch': 0.02}
  2%|â–         | 95/5000 [38:02<32:18:40, 23.71s/it]  2%|â–         | 96/5000 [38:27<32:38:22, 23.96s/it]                                                    {'loss': 12.07, 'grad_norm': 5.6474528312683105, 'learning_rate': 0.00096, 'epoch': 0.02}
  2%|â–         | 96/5000 [38:27<32:38:22, 23.96s/it]  2%|â–         | 97/5000 [38:48<31:43:30, 23.29s/it]                                                    {'loss': 10.7667, 'grad_norm': 4.77960205078125, 'learning_rate': 0.0009699999999999999, 'epoch': 0.02}
  2%|â–         | 97/5000 [38:48<31:43:30, 23.29s/it]  2%|â–         | 98/5000 [39:12<31:43:34, 23.30s/it]                                                    {'loss': 11.1164, 'grad_norm': 4.750923156738281, 'learning_rate': 0.00098, 'epoch': 0.02}
  2%|â–         | 98/5000 [39:12<31:43:34, 23.30s/it]  2%|â–         | 99/5000 [39:33<31:01:37, 22.79s/it]                                                    {'loss': 11.218, 'grad_norm': 4.82695198059082, 'learning_rate': 0.00099, 'epoch': 0.02}
  2%|â–         | 99/5000 [39:33<31:01:37, 22.79s/it]  2%|â–         | 100/5000 [39:57<31:15:27, 22.96s/it]                                                     {'loss': 10.7117, 'grad_norm': 4.074253559112549, 'learning_rate': 0.001, 'epoch': 0.02}
  2%|â–         | 100/5000 [39:57<31:15:27, 22.96s/it][2025-10-19 18:26:55,924] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/5000 [40:20<31:18:01, 23.00s/it]                                                     {'loss': 11.0592, 'grad_norm': 4.328531265258789, 'learning_rate': 0.000999795918367347, 'epoch': 0.02}
  2%|â–         | 101/5000 [40:20<31:18:01, 23.00s/it]  2%|â–         | 102/5000 [40:46<32:45:20, 24.08s/it]                                                     {'loss': 10.325, 'grad_norm': 3.689269542694092, 'learning_rate': 0.0009995918367346939, 'epoch': 0.02}
  2%|â–         | 102/5000 [40:46<32:45:20, 24.08s/it]  2%|â–         | 103/5000 [41:12<33:30:44, 24.64s/it]                                                     {'loss': 10.0923, 'grad_norm': 3.6634693145751953, 'learning_rate': 0.0009993877551020408, 'epoch': 0.02}
  2%|â–         | 103/5000 [41:12<33:30:44, 24.64s/it]  2%|â–         | 104/5000 [41:42<35:32:24, 26.13s/it]                                                     {'loss': 11.0681, 'grad_norm': 4.014886856079102, 'learning_rate': 0.0009991836734693877, 'epoch': 0.02}
  2%|â–         | 104/5000 [41:42<35:32:24, 26.13s/it]  2%|â–         | 105/5000 [42:10<36:28:24, 26.82s/it]                                                     {'loss': 10.3208, 'grad_norm': 3.532618999481201, 'learning_rate': 0.0009989795918367347, 'epoch': 0.02}
  2%|â–         | 105/5000 [42:10<36:28:24, 26.82s/it]  2%|â–         | 106/5000 [42:35<35:30:20, 26.12s/it]                                                     {'loss': 10.2896, 'grad_norm': 3.4781787395477295, 'learning_rate': 0.0009987755102040816, 'epoch': 0.02}
  2%|â–         | 106/5000 [42:35<35:30:20, 26.12s/it]  2%|â–         | 107/5000 [42:57<33:41:40, 24.79s/it]                                                     {'loss': 10.0205, 'grad_norm': 3.12541127204895, 'learning_rate': 0.0009985714285714285, 'epoch': 0.02}
  2%|â–         | 107/5000 [42:57<33:41:40, 24.79s/it]  2%|â–         | 108/5000 [43:21<33:23:39, 24.57s/it]                                                     {'loss': 9.5845, 'grad_norm': 2.9197559356689453, 'learning_rate': 0.0009983673469387755, 'epoch': 0.02}
  2%|â–         | 108/5000 [43:21<33:23:39, 24.57s/it]  2%|â–         | 109/5000 [43:44<32:51:36, 24.19s/it]                                                     {'loss': 9.9053, 'grad_norm': 2.988015651702881, 'learning_rate': 0.0009981632653061224, 'epoch': 0.02}
  2%|â–         | 109/5000 [43:44<32:51:36, 24.19s/it]  2%|â–         | 110/5000 [44:09<33:22:03, 24.57s/it]                                                     {'loss': 9.5677, 'grad_norm': 2.700500726699829, 'learning_rate': 0.0009979591836734693, 'epoch': 0.02}
  2%|â–         | 110/5000 [44:09<33:22:03, 24.57s/it]  2%|â–         | 111/5000 [44:32<32:30:26, 23.94s/it]                                                     {'loss': 9.7316, 'grad_norm': 2.806835174560547, 'learning_rate': 0.0009977551020408162, 'epoch': 0.02}
  2%|â–         | 111/5000 [44:32<32:30:26, 23.94s/it]  2%|â–         | 112/5000 [44:53<31:23:58, 23.13s/it]                                                     {'loss': 9.2801, 'grad_norm': 2.491013765335083, 'learning_rate': 0.0009975510204081632, 'epoch': 0.02}
  2%|â–         | 112/5000 [44:53<31:23:58, 23.13s/it]  2%|â–         | 113/5000 [45:21<33:14:46, 24.49s/it]                                                     {'loss': 9.0342, 'grad_norm': 2.277956247329712, 'learning_rate': 0.00099734693877551, 'epoch': 0.02}
  2%|â–         | 113/5000 [45:21<33:14:46, 24.49s/it]  2%|â–         | 114/5000 [45:48<34:28:15, 25.40s/it]                                                     {'loss': 9.3332, 'grad_norm': 2.4723801612854004, 'learning_rate': 0.000997142857142857, 'epoch': 0.02}
  2%|â–         | 114/5000 [45:48<34:28:15, 25.40s/it]  2%|â–         | 115/5000 [46:18<36:16:06, 26.73s/it]                                                     {'loss': 9.4619, 'grad_norm': 2.4290051460266113, 'learning_rate': 0.000996938775510204, 'epoch': 0.02}
  2%|â–         | 115/5000 [46:18<36:16:06, 26.73s/it]  2%|â–         | 116/5000 [46:40<34:18:04, 25.28s/it]                                                     {'loss': 9.0535, 'grad_norm': 2.3600690364837646, 'learning_rate': 0.000996734693877551, 'epoch': 0.02}
  2%|â–         | 116/5000 [46:40<34:18:04, 25.28s/it]  2%|â–         | 117/5000 [47:04<33:38:05, 24.80s/it]                                                     {'loss': 9.0937, 'grad_norm': 2.3121426105499268, 'learning_rate': 0.000996530612244898, 'epoch': 0.02}
  2%|â–         | 117/5000 [47:04<33:38:05, 24.80s/it]  2%|â–         | 118/5000 [47:28<33:39:30, 24.82s/it]                                                     {'loss': 8.862, 'grad_norm': 2.0910370349884033, 'learning_rate': 0.000996326530612245, 'epoch': 0.02}
  2%|â–         | 118/5000 [47:28<33:39:30, 24.82s/it]  2%|â–         | 119/5000 [47:53<33:31:43, 24.73s/it]                                                     {'loss': 9.1228, 'grad_norm': 2.2610504627227783, 'learning_rate': 0.000996122448979592, 'epoch': 0.02}
  2%|â–         | 119/5000 [47:53<33:31:43, 24.73s/it]  2%|â–         | 120/5000 [48:15<32:29:09, 23.97s/it]                                                     {'loss': 8.9043, 'grad_norm': 1.9881103038787842, 'learning_rate': 0.0009959183673469388, 'epoch': 0.02}
  2%|â–         | 120/5000 [48:15<32:29:09, 23.97s/it]  2%|â–         | 121/5000 [48:39<32:31:03, 23.99s/it]                                                     {'loss': 8.6246, 'grad_norm': 1.8965660333633423, 'learning_rate': 0.0009957142857142858, 'epoch': 0.02}
  2%|â–         | 121/5000 [48:39<32:31:03, 23.99s/it]  2%|â–         | 122/5000 [49:01<31:29:31, 23.24s/it]                                                     {'loss': 8.4645, 'grad_norm': 1.7348999977111816, 'learning_rate': 0.0009955102040816327, 'epoch': 0.02}
  2%|â–         | 122/5000 [49:01<31:29:31, 23.24s/it]  2%|â–         | 123/5000 [49:22<30:38:16, 22.62s/it]                                                     {'loss': 8.8953, 'grad_norm': 1.933853030204773, 'learning_rate': 0.0009953061224489796, 'epoch': 0.02}
  2%|â–         | 123/5000 [49:22<30:38:16, 22.62s/it]  2%|â–         | 124/5000 [49:46<31:19:43, 23.13s/it]                                                     {'loss': 8.3155, 'grad_norm': 1.7496916055679321, 'learning_rate': 0.0009951020408163265, 'epoch': 0.02}
  2%|â–         | 124/5000 [49:46<31:19:43, 23.13s/it]  2%|â–Ž         | 125/5000 [50:08<30:45:32, 22.71s/it]                                                     {'loss': 8.2814, 'grad_norm': 1.664917230606079, 'learning_rate': 0.0009948979591836735, 'epoch': 0.03}
  2%|â–Ž         | 125/5000 [50:08<30:45:32, 22.71s/it]  3%|â–Ž         | 126/5000 [50:29<29:54:49, 22.09s/it]                                                     {'loss': 7.9887, 'grad_norm': 1.5256032943725586, 'learning_rate': 0.0009946938775510204, 'epoch': 0.03}
  3%|â–Ž         | 126/5000 [50:29<29:54:49, 22.09s/it]  3%|â–Ž         | 127/5000 [50:51<29:51:04, 22.05s/it]                                                     {'loss': 8.1766, 'grad_norm': 1.643880009651184, 'learning_rate': 0.0009944897959183673, 'epoch': 0.03}
  3%|â–Ž         | 127/5000 [50:51<29:51:04, 22.05s/it]  3%|â–Ž         | 128/5000 [51:12<29:35:49, 21.87s/it]                                                     {'loss': 8.2883, 'grad_norm': 1.6051008701324463, 'learning_rate': 0.0009942857142857143, 'epoch': 0.03}
  3%|â–Ž         | 128/5000 [51:12<29:35:49, 21.87s/it]  3%|â–Ž         | 129/5000 [51:32<28:49:40, 21.31s/it]                                                     {'loss': 8.0734, 'grad_norm': 1.528218388557434, 'learning_rate': 0.0009940816326530612, 'epoch': 0.03}
  3%|â–Ž         | 129/5000 [51:32<28:49:40, 21.31s/it]  3%|â–Ž         | 130/5000 [51:56<29:55:40, 22.12s/it]                                                     {'loss': 8.2123, 'grad_norm': 1.4724609851837158, 'learning_rate': 0.0009938775510204081, 'epoch': 0.03}
  3%|â–Ž         | 130/5000 [51:56<29:55:40, 22.12s/it]  3%|â–Ž         | 131/5000 [52:20<30:45:28, 22.74s/it]                                                     {'loss': 8.0384, 'grad_norm': 1.425812840461731, 'learning_rate': 0.0009936734693877553, 'epoch': 0.03}
  3%|â–Ž         | 131/5000 [52:20<30:45:28, 22.74s/it]  3%|â–Ž         | 132/5000 [52:48<32:53:34, 24.33s/it]                                                     {'loss': 8.347, 'grad_norm': 1.6000014543533325, 'learning_rate': 0.0009934693877551022, 'epoch': 0.03}
  3%|â–Ž         | 132/5000 [52:48<32:53:34, 24.33s/it]  3%|â–Ž         | 133/5000 [53:12<32:40:13, 24.17s/it]                                                     {'loss': 8.0648, 'grad_norm': 1.475974202156067, 'learning_rate': 0.0009932653061224491, 'epoch': 0.03}
  3%|â–Ž         | 133/5000 [53:12<32:40:13, 24.17s/it]  3%|â–Ž         | 134/5000 [53:33<31:18:00, 23.16s/it]                                                     {'loss': 7.8884, 'grad_norm': 1.3487688302993774, 'learning_rate': 0.000993061224489796, 'epoch': 0.03}
  3%|â–Ž         | 134/5000 [53:33<31:18:00, 23.16s/it]  3%|â–Ž         | 135/5000 [53:57<31:45:01, 23.49s/it]                                                     {'loss': 7.8858, 'grad_norm': 1.396760106086731, 'learning_rate': 0.000992857142857143, 'epoch': 0.03}
  3%|â–Ž         | 135/5000 [53:57<31:45:01, 23.49s/it]  3%|â–Ž         | 136/5000 [54:20<31:19:25, 23.18s/it]                                                     {'loss': 7.7055, 'grad_norm': 1.2667961120605469, 'learning_rate': 0.00099265306122449, 'epoch': 0.03}
  3%|â–Ž         | 136/5000 [54:20<31:19:25, 23.18s/it]  3%|â–Ž         | 137/5000 [54:47<33:06:02, 24.50s/it]                                                     {'loss': 7.6376, 'grad_norm': 1.2164973020553589, 'learning_rate': 0.0009924489795918368, 'epoch': 0.03}
  3%|â–Ž         | 137/5000 [54:47<33:06:02, 24.50s/it]  3%|â–Ž         | 138/5000 [55:13<33:44:56, 24.99s/it]                                                     {'loss': 7.7879, 'grad_norm': 1.2459057569503784, 'learning_rate': 0.0009922448979591838, 'epoch': 0.03}
  3%|â–Ž         | 138/5000 [55:13<33:44:56, 24.99s/it]  3%|â–Ž         | 139/5000 [55:35<32:19:33, 23.94s/it]                                                     {'loss': 7.6393, 'grad_norm': 1.3114598989486694, 'learning_rate': 0.0009920408163265307, 'epoch': 0.03}
  3%|â–Ž         | 139/5000 [55:35<32:19:33, 23.94s/it]  3%|â–Ž         | 140/5000 [55:56<31:23:58, 23.26s/it]                                                     {'loss': 7.6432, 'grad_norm': 1.2970404624938965, 'learning_rate': 0.0009918367346938776, 'epoch': 0.03}
  3%|â–Ž         | 140/5000 [55:56<31:23:58, 23.26s/it]  3%|â–Ž         | 141/5000 [56:20<31:39:35, 23.46s/it]                                                     {'loss': 7.3588, 'grad_norm': 1.1141581535339355, 'learning_rate': 0.0009916326530612246, 'epoch': 0.03}
  3%|â–Ž         | 141/5000 [56:20<31:39:35, 23.46s/it]  3%|â–Ž         | 142/5000 [56:47<32:50:44, 24.34s/it]                                                     {'loss': 7.6962, 'grad_norm': 1.2496376037597656, 'learning_rate': 0.0009914285714285715, 'epoch': 0.03}
  3%|â–Ž         | 142/5000 [56:47<32:50:44, 24.34s/it]  3%|â–Ž         | 143/5000 [57:09<31:49:10, 23.58s/it]                                                     {'loss': 7.3903, 'grad_norm': 1.0976709127426147, 'learning_rate': 0.0009912244897959184, 'epoch': 0.03}
  3%|â–Ž         | 143/5000 [57:09<31:49:10, 23.58s/it]  3%|â–Ž         | 144/5000 [57:32<31:46:54, 23.56s/it]                                                     {'loss': 7.4342, 'grad_norm': 1.2231396436691284, 'learning_rate': 0.0009910204081632653, 'epoch': 0.03}
  3%|â–Ž         | 144/5000 [57:32<31:46:54, 23.56s/it]  3%|â–Ž         | 145/5000 [57:53<30:50:18, 22.87s/it]                                                     {'loss': 7.3667, 'grad_norm': 1.1151453256607056, 'learning_rate': 0.0009908163265306123, 'epoch': 0.03}
  3%|â–Ž         | 145/5000 [57:53<30:50:18, 22.87s/it]  3%|â–Ž         | 146/5000 [58:18<31:22:01, 23.26s/it]                                                     {'loss': 7.6147, 'grad_norm': 1.2462184429168701, 'learning_rate': 0.0009906122448979592, 'epoch': 0.03}
  3%|â–Ž         | 146/5000 [58:18<31:22:01, 23.26s/it]  3%|â–Ž         | 147/5000 [58:40<30:57:22, 22.96s/it]                                                     {'loss': 7.1896, 'grad_norm': 0.9530077576637268, 'learning_rate': 0.0009904081632653061, 'epoch': 0.03}
  3%|â–Ž         | 147/5000 [58:40<30:57:22, 22.96s/it]  3%|â–Ž         | 148/5000 [59:10<33:52:59, 25.14s/it]                                                     {'loss': 7.4416, 'grad_norm': 1.1600955724716187, 'learning_rate': 0.000990204081632653, 'epoch': 0.03}
  3%|â–Ž         | 148/5000 [59:10<33:52:59, 25.14s/it]  3%|â–Ž         | 149/5000 [59:32<32:47:47, 24.34s/it]                                                     {'loss': 7.2785, 'grad_norm': 1.0412275791168213, 'learning_rate': 0.00099, 'epoch': 0.03}
  3%|â–Ž         | 149/5000 [59:32<32:47:47, 24.34s/it]  3%|â–Ž         | 150/5000 [59:55<31:57:03, 23.72s/it]                                                     {'loss': 7.3409, 'grad_norm': 1.0671998262405396, 'learning_rate': 0.000989795918367347, 'epoch': 0.03}
  3%|â–Ž         | 150/5000 [59:55<31:57:03, 23.72s/it][2025-10-19 18:46:53,990] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/5000 [1:00:20<32:39:35, 24.25s/it]                                                       {'loss': 7.1966, 'grad_norm': 0.9573364853858948, 'learning_rate': 0.0009895918367346939, 'epoch': 0.03}
  3%|â–Ž         | 151/5000 [1:00:20<32:39:35, 24.25s/it]  3%|â–Ž         | 152/5000 [1:00:42<31:43:41, 23.56s/it]                                                       {'loss': 7.2334, 'grad_norm': 0.9939814805984497, 'learning_rate': 0.0009893877551020408, 'epoch': 0.03}
  3%|â–Ž         | 152/5000 [1:00:42<31:43:41, 23.56s/it]  3%|â–Ž         | 153/5000 [1:01:09<32:52:06, 24.41s/it]                                                       {'loss': 7.2914, 'grad_norm': 1.0652376413345337, 'learning_rate': 0.0009891836734693877, 'epoch': 0.03}
  3%|â–Ž         | 153/5000 [1:01:09<32:52:06, 24.41s/it]  3%|â–Ž         | 154/5000 [1:01:34<33:23:34, 24.81s/it]                                                       {'loss': 7.2467, 'grad_norm': 0.9730420112609863, 'learning_rate': 0.0009889795918367346, 'epoch': 0.03}
  3%|â–Ž         | 154/5000 [1:01:34<33:23:34, 24.81s/it]  3%|â–Ž         | 155/5000 [1:01:59<33:10:34, 24.65s/it]                                                       {'loss': 7.1767, 'grad_norm': 0.9637170433998108, 'learning_rate': 0.0009887755102040816, 'epoch': 0.03}
  3%|â–Ž         | 155/5000 [1:01:59<33:10:34, 24.65s/it]  3%|â–Ž         | 156/5000 [1:02:21<32:03:41, 23.83s/it]                                                       {'loss': 6.9752, 'grad_norm': 0.8986541032791138, 'learning_rate': 0.0009885714285714285, 'epoch': 0.03}
  3%|â–Ž         | 156/5000 [1:02:21<32:03:41, 23.83s/it]  3%|â–Ž         | 157/5000 [1:02:45<32:20:11, 24.04s/it]                                                       {'loss': 6.9064, 'grad_norm': 0.8201278448104858, 'learning_rate': 0.0009883673469387754, 'epoch': 0.03}
  3%|â–Ž         | 157/5000 [1:02:45<32:20:11, 24.04s/it]  3%|â–Ž         | 158/5000 [1:03:09<32:26:41, 24.12s/it]                                                       {'loss': 6.8451, 'grad_norm': 0.825650691986084, 'learning_rate': 0.0009881632653061224, 'epoch': 0.03}
  3%|â–Ž         | 158/5000 [1:03:09<32:26:41, 24.12s/it]  3%|â–Ž         | 159/5000 [1:03:35<33:13:34, 24.71s/it]                                                       {'loss': 7.1741, 'grad_norm': 0.9361866116523743, 'learning_rate': 0.0009879591836734693, 'epoch': 0.03}
  3%|â–Ž         | 159/5000 [1:03:35<33:13:34, 24.71s/it]  3%|â–Ž         | 160/5000 [1:03:57<31:58:58, 23.79s/it]                                                       {'loss': 7.0146, 'grad_norm': 0.9339292049407959, 'learning_rate': 0.0009877551020408162, 'epoch': 0.03}
  3%|â–Ž         | 160/5000 [1:03:57<31:58:58, 23.79s/it]  3%|â–Ž         | 161/5000 [1:04:20<31:29:35, 23.43s/it]                                                       {'loss': 6.8743, 'grad_norm': 0.8164488673210144, 'learning_rate': 0.0009875510204081631, 'epoch': 0.03}
  3%|â–Ž         | 161/5000 [1:04:20<31:29:35, 23.43s/it]  3%|â–Ž         | 162/5000 [1:04:41<30:39:15, 22.81s/it]                                                       {'loss': 6.9874, 'grad_norm': 0.9861832857131958, 'learning_rate': 0.00098734693877551, 'epoch': 0.03}
  3%|â–Ž         | 162/5000 [1:04:41<30:39:15, 22.81s/it]  3%|â–Ž         | 163/5000 [1:05:04<30:43:05, 22.86s/it]                                                       {'loss': 6.792, 'grad_norm': 0.8027768135070801, 'learning_rate': 0.0009871428571428572, 'epoch': 0.03}
  3%|â–Ž         | 163/5000 [1:05:04<30:43:05, 22.86s/it]  3%|â–Ž         | 164/5000 [1:05:25<30:04:03, 22.38s/it]                                                       {'loss': 6.8174, 'grad_norm': 0.8316373229026794, 'learning_rate': 0.0009869387755102042, 'epoch': 0.03}
  3%|â–Ž         | 164/5000 [1:05:25<30:04:03, 22.38s/it]  3%|â–Ž         | 165/5000 [1:05:47<29:36:37, 22.05s/it]                                                       {'loss': 6.7892, 'grad_norm': 0.8678160309791565, 'learning_rate': 0.000986734693877551, 'epoch': 0.03}
  3%|â–Ž         | 165/5000 [1:05:47<29:36:37, 22.05s/it]  3%|â–Ž         | 166/5000 [1:06:11<30:31:35, 22.73s/it]                                                       {'loss': 6.8131, 'grad_norm': 0.8188384175300598, 'learning_rate': 0.000986530612244898, 'epoch': 0.03}
  3%|â–Ž         | 166/5000 [1:06:11<30:31:35, 22.73s/it]  3%|â–Ž         | 167/5000 [1:06:35<30:57:10, 23.06s/it]                                                       {'loss': 6.7397, 'grad_norm': 0.7714900374412537, 'learning_rate': 0.000986326530612245, 'epoch': 0.03}
  3%|â–Ž         | 167/5000 [1:06:35<30:57:10, 23.06s/it]  3%|â–Ž         | 168/5000 [1:07:00<32:01:11, 23.86s/it]                                                       {'loss': 6.7725, 'grad_norm': 0.8068130016326904, 'learning_rate': 0.0009861224489795919, 'epoch': 0.03}
  3%|â–Ž         | 168/5000 [1:07:00<32:01:11, 23.86s/it]  3%|â–Ž         | 169/5000 [1:07:26<32:48:53, 24.45s/it]                                                       {'loss': 6.6957, 'grad_norm': 0.7749123573303223, 'learning_rate': 0.0009859183673469388, 'epoch': 0.03}
  3%|â–Ž         | 169/5000 [1:07:26<32:48:53, 24.45s/it]  3%|â–Ž         | 170/5000 [1:07:51<32:54:40, 24.53s/it]                                                       {'loss': 6.6096, 'grad_norm': 0.7249861359596252, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}
  3%|â–Ž         | 170/5000 [1:07:51<32:54:40, 24.53s/it]  3%|â–Ž         | 171/5000 [1:08:15<32:49:49, 24.48s/it]                                                       {'loss': 6.88, 'grad_norm': 0.7881072759628296, 'learning_rate': 0.0009855102040816327, 'epoch': 0.03}
  3%|â–Ž         | 171/5000 [1:08:15<32:49:49, 24.48s/it]  3%|â–Ž         | 172/5000 [1:08:44<34:32:51, 25.76s/it]                                                       {'loss': 6.7172, 'grad_norm': 0.6760733723640442, 'learning_rate': 0.0009853061224489796, 'epoch': 0.03}
  3%|â–Ž         | 172/5000 [1:08:44<34:32:51, 25.76s/it]  3%|â–Ž         | 173/5000 [1:09:10<34:30:34, 25.74s/it]                                                       {'loss': 6.5811, 'grad_norm': 0.6922517418861389, 'learning_rate': 0.0009851020408163265, 'epoch': 0.03}
  3%|â–Ž         | 173/5000 [1:09:10<34:30:34, 25.74s/it]  3%|â–Ž         | 174/5000 [1:09:36<34:37:20, 25.83s/it]                                                       {'loss': 6.5259, 'grad_norm': 0.6363639831542969, 'learning_rate': 0.0009848979591836734, 'epoch': 0.03}
  3%|â–Ž         | 174/5000 [1:09:36<34:37:20, 25.83s/it]  4%|â–Ž         | 175/5000 [1:09:57<32:36:45, 24.33s/it]                                                       {'loss': 6.6024, 'grad_norm': 0.6872149109840393, 'learning_rate': 0.0009846938775510204, 'epoch': 0.04}
  4%|â–Ž         | 175/5000 [1:09:57<32:36:45, 24.33s/it]  4%|â–Ž         | 176/5000 [1:10:20<32:24:08, 24.18s/it]                                                       {'loss': 6.5888, 'grad_norm': 0.6615959405899048, 'learning_rate': 0.0009844897959183673, 'epoch': 0.04}
  4%|â–Ž         | 176/5000 [1:10:20<32:24:08, 24.18s/it]  4%|â–Ž         | 177/5000 [1:10:45<32:35:29, 24.33s/it]                                                       {'loss': 6.5566, 'grad_norm': 0.6356592178344727, 'learning_rate': 0.0009842857142857142, 'epoch': 0.04}
  4%|â–Ž         | 177/5000 [1:10:45<32:35:29, 24.33s/it]  4%|â–Ž         | 178/5000 [1:11:07<31:42:14, 23.67s/it]                                                       {'loss': 6.5638, 'grad_norm': 0.620576798915863, 'learning_rate': 0.0009840816326530614, 'epoch': 0.04}
  4%|â–Ž         | 178/5000 [1:11:07<31:42:14, 23.67s/it]  4%|â–Ž         | 179/5000 [1:11:29<31:06:23, 23.23s/it]                                                       {'loss': 6.5251, 'grad_norm': 0.6226354241371155, 'learning_rate': 0.0009838775510204083, 'epoch': 0.04}
  4%|â–Ž         | 179/5000 [1:11:29<31:06:23, 23.23s/it]  4%|â–Ž         | 180/5000 [1:11:54<31:30:36, 23.53s/it]                                                       {'loss': 6.7461, 'grad_norm': 0.7189934253692627, 'learning_rate': 0.0009836734693877552, 'epoch': 0.04}
  4%|â–Ž         | 180/5000 [1:11:54<31:30:36, 23.53s/it]  4%|â–Ž         | 181/5000 [1:12:15<30:39:36, 22.90s/it]                                                       {'loss': 6.5352, 'grad_norm': 0.6377566456794739, 'learning_rate': 0.0009834693877551022, 'epoch': 0.04}
  4%|â–Ž         | 181/5000 [1:12:15<30:39:36, 22.90s/it]  4%|â–Ž         | 182/5000 [1:12:37<30:04:12, 22.47s/it]                                                       {'loss': 6.4253, 'grad_norm': 0.543153703212738, 'learning_rate': 0.000983265306122449, 'epoch': 0.04}
  4%|â–Ž         | 182/5000 [1:12:37<30:04:12, 22.47s/it]  4%|â–Ž         | 183/5000 [1:13:00<30:23:34, 22.71s/it]                                                       {'loss': 6.6205, 'grad_norm': 0.6303394436836243, 'learning_rate': 0.000983061224489796, 'epoch': 0.04}
  4%|â–Ž         | 183/5000 [1:13:00<30:23:34, 22.71s/it]  4%|â–Ž         | 184/5000 [1:13:24<31:01:42, 23.19s/it]                                                       {'loss': 6.4374, 'grad_norm': 0.5542505383491516, 'learning_rate': 0.000982857142857143, 'epoch': 0.04}
  4%|â–Ž         | 184/5000 [1:13:24<31:01:42, 23.19s/it]  4%|â–Ž         | 185/5000 [1:13:51<32:17:00, 24.14s/it]                                                       {'loss': 6.5712, 'grad_norm': 0.6955971121788025, 'learning_rate': 0.0009826530612244899, 'epoch': 0.04}
  4%|â–Ž         | 185/5000 [1:13:51<32:17:00, 24.14s/it]  4%|â–Ž         | 186/5000 [1:14:14<32:08:30, 24.04s/it]                                                       {'loss': 6.3831, 'grad_norm': 0.5703328847885132, 'learning_rate': 0.0009824489795918368, 'epoch': 0.04}
  4%|â–Ž         | 186/5000 [1:14:14<32:08:30, 24.04s/it]  4%|â–Ž         | 187/5000 [1:14:39<32:14:33, 24.12s/it]                                                       {'loss': 6.3816, 'grad_norm': 0.541463315486908, 'learning_rate': 0.0009822448979591837, 'epoch': 0.04}
  4%|â–Ž         | 187/5000 [1:14:39<32:14:33, 24.12s/it]  4%|â–         | 188/5000 [1:15:00<31:17:38, 23.41s/it]                                                       {'loss': 6.3846, 'grad_norm': 0.5582947731018066, 'learning_rate': 0.0009820408163265307, 'epoch': 0.04}
  4%|â–         | 188/5000 [1:15:00<31:17:38, 23.41s/it]  4%|â–         | 189/5000 [1:15:22<30:23:42, 22.74s/it]                                                       {'loss': 6.3502, 'grad_norm': 0.5680674910545349, 'learning_rate': 0.0009818367346938776, 'epoch': 0.04}
  4%|â–         | 189/5000 [1:15:22<30:23:42, 22.74s/it]  4%|â–         | 190/5000 [1:15:45<30:32:14, 22.86s/it]                                                       {'loss': 6.3731, 'grad_norm': 0.6045693755149841, 'learning_rate': 0.0009816326530612245, 'epoch': 0.04}
  4%|â–         | 190/5000 [1:15:45<30:32:14, 22.86s/it]  4%|â–         | 191/5000 [1:16:12<32:06:19, 24.03s/it]                                                       {'loss': 6.3135, 'grad_norm': 0.5657447576522827, 'learning_rate': 0.0009814285714285715, 'epoch': 0.04}
  4%|â–         | 191/5000 [1:16:12<32:06:19, 24.03s/it]  4%|â–         | 192/5000 [1:16:35<31:55:42, 23.91s/it]                                                       {'loss': 6.3351, 'grad_norm': 0.572125256061554, 'learning_rate': 0.0009812244897959184, 'epoch': 0.04}
  4%|â–         | 192/5000 [1:16:35<31:55:42, 23.91s/it]  4%|â–         | 193/5000 [1:16:58<31:19:07, 23.45s/it]                                                       {'loss': 6.3158, 'grad_norm': 0.5424365401268005, 'learning_rate': 0.0009810204081632653, 'epoch': 0.04}
  4%|â–         | 193/5000 [1:16:58<31:19:07, 23.45s/it]  4%|â–         | 194/5000 [1:17:19<30:33:26, 22.89s/it]                                                       {'loss': 6.3602, 'grad_norm': 0.5835161209106445, 'learning_rate': 0.0009808163265306123, 'epoch': 0.04}
  4%|â–         | 194/5000 [1:17:19<30:33:26, 22.89s/it]  4%|â–         | 195/5000 [1:17:43<31:08:41, 23.33s/it]                                                       {'loss': 6.4447, 'grad_norm': 0.6220322847366333, 'learning_rate': 0.0009806122448979592, 'epoch': 0.04}
  4%|â–         | 195/5000 [1:17:43<31:08:41, 23.33s/it]  4%|â–         | 196/5000 [1:18:05<30:28:45, 22.84s/it]                                                       {'loss': 6.2942, 'grad_norm': 0.5222175717353821, 'learning_rate': 0.0009804081632653061, 'epoch': 0.04}
  4%|â–         | 196/5000 [1:18:05<30:28:45, 22.84s/it]  4%|â–         | 197/5000 [1:18:27<30:13:17, 22.65s/it]                                                       {'loss': 6.3184, 'grad_norm': 0.5406212210655212, 'learning_rate': 0.000980204081632653, 'epoch': 0.04}
  4%|â–         | 197/5000 [1:18:27<30:13:17, 22.65s/it]  4%|â–         | 198/5000 [1:18:52<30:51:29, 23.13s/it]                                                       {'loss': 6.219, 'grad_norm': 0.5315670371055603, 'learning_rate': 0.00098, 'epoch': 0.04}
  4%|â–         | 198/5000 [1:18:52<30:51:29, 23.13s/it]  4%|â–         | 199/5000 [1:19:15<31:02:35, 23.28s/it]                                                       {'loss': 6.2559, 'grad_norm': 0.52785325050354, 'learning_rate': 0.000979795918367347, 'epoch': 0.04}
  4%|â–         | 199/5000 [1:19:15<31:02:35, 23.28s/it]  4%|â–         | 200/5000 [1:19:37<30:27:43, 22.85s/it]                                                       {'loss': 6.1645, 'grad_norm': 0.48830074071884155, 'learning_rate': 0.0009795918367346938, 'epoch': 0.04}
  4%|â–         | 200/5000 [1:19:37<30:27:43, 22.85s/it][2025-10-19 19:06:36,310] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 201/5000 [1:20:08<33:35:09, 25.19s/it]                                                       {'loss': 6.3873, 'grad_norm': 0.5700525641441345, 'learning_rate': 0.0009793877551020408, 'epoch': 0.04}
  4%|â–         | 201/5000 [1:20:08<33:35:09, 25.19s/it]  4%|â–         | 202/5000 [1:20:29<32:03:14, 24.05s/it]                                                       {'loss': 6.0725, 'grad_norm': 0.4336394667625427, 'learning_rate': 0.0009791836734693877, 'epoch': 0.04}
  4%|â–         | 202/5000 [1:20:29<32:03:14, 24.05s/it]  4%|â–         | 203/5000 [1:20:54<32:29:41, 24.39s/it]                                                       {'loss': 6.2519, 'grad_norm': 0.5005239248275757, 'learning_rate': 0.0009789795918367346, 'epoch': 0.04}
  4%|â–         | 203/5000 [1:20:54<32:29:41, 24.39s/it]  4%|â–         | 204/5000 [1:21:17<31:48:39, 23.88s/it]                                                       {'loss': 6.1556, 'grad_norm': 0.4779445230960846, 'learning_rate': 0.0009787755102040815, 'epoch': 0.04}
  4%|â–         | 204/5000 [1:21:17<31:48:39, 23.88s/it]  4%|â–         | 205/5000 [1:21:43<32:33:33, 24.44s/it]                                                       {'loss': 6.136, 'grad_norm': 0.5015978217124939, 'learning_rate': 0.0009785714285714285, 'epoch': 0.04}
  4%|â–         | 205/5000 [1:21:43<32:33:33, 24.44s/it]  4%|â–         | 206/5000 [1:22:05<31:38:55, 23.77s/it]                                                       {'loss': 6.1904, 'grad_norm': 0.4813084602355957, 'learning_rate': 0.0009783673469387754, 'epoch': 0.04}
  4%|â–         | 206/5000 [1:22:05<31:38:55, 23.77s/it]  4%|â–         | 207/5000 [1:22:31<32:32:22, 24.44s/it]                                                       {'loss': 6.1821, 'grad_norm': 0.48905640840530396, 'learning_rate': 0.0009781632653061223, 'epoch': 0.04}
  4%|â–         | 207/5000 [1:22:31<32:32:22, 24.44s/it]  4%|â–         | 208/5000 [1:23:00<34:17:39, 25.76s/it]                                                       {'loss': 6.2463, 'grad_norm': 0.5145388841629028, 'learning_rate': 0.0009779591836734693, 'epoch': 0.04}
  4%|â–         | 208/5000 [1:23:00<34:17:39, 25.76s/it]  4%|â–         | 209/5000 [1:23:23<33:20:09, 25.05s/it]                                                       {'loss': 6.0194, 'grad_norm': 0.4274628758430481, 'learning_rate': 0.0009777551020408164, 'epoch': 0.04}
  4%|â–         | 209/5000 [1:23:23<33:20:09, 25.05s/it]  4%|â–         | 210/5000 [1:23:49<33:42:15, 25.33s/it]                                                       {'loss': 6.0906, 'grad_norm': 0.4875619411468506, 'learning_rate': 0.0009775510204081633, 'epoch': 0.04}
  4%|â–         | 210/5000 [1:23:49<33:42:15, 25.33s/it]  4%|â–         | 211/5000 [1:24:16<34:15:29, 25.75s/it]                                                       {'loss': 6.1869, 'grad_norm': 0.5170350670814514, 'learning_rate': 0.0009773469387755103, 'epoch': 0.04}
  4%|â–         | 211/5000 [1:24:16<34:15:29, 25.75s/it]  4%|â–         | 212/5000 [1:24:40<33:37:52, 25.29s/it]                                                       {'loss': 6.0517, 'grad_norm': 0.4893924593925476, 'learning_rate': 0.0009771428571428572, 'epoch': 0.04}
  4%|â–         | 212/5000 [1:24:40<33:37:52, 25.29s/it]  4%|â–         | 213/5000 [1:25:01<31:53:09, 23.98s/it]                                                       {'loss': 6.1167, 'grad_norm': 0.524819016456604, 'learning_rate': 0.0009769387755102041, 'epoch': 0.04}
  4%|â–         | 213/5000 [1:25:01<31:53:09, 23.98s/it]  4%|â–         | 214/5000 [1:25:22<30:33:22, 22.98s/it]                                                       {'loss': 6.164, 'grad_norm': 0.5515829920768738, 'learning_rate': 0.000976734693877551, 'epoch': 0.04}
  4%|â–         | 214/5000 [1:25:22<30:33:22, 22.98s/it]  4%|â–         | 215/5000 [1:25:45<30:44:42, 23.13s/it]                                                       {'loss': 6.0123, 'grad_norm': 0.5029453039169312, 'learning_rate': 0.000976530612244898, 'epoch': 0.04}
  4%|â–         | 215/5000 [1:25:45<30:44:42, 23.13s/it]  4%|â–         | 216/5000 [1:26:07<30:04:23, 22.63s/it]                                                       {'loss': 6.0205, 'grad_norm': 0.4536835551261902, 'learning_rate': 0.0009763265306122449, 'epoch': 0.04}
  4%|â–         | 216/5000 [1:26:07<30:04:23, 22.63s/it]  4%|â–         | 217/5000 [1:26:31<30:46:12, 23.16s/it]                                                       {'loss': 6.0543, 'grad_norm': 0.574399471282959, 'learning_rate': 0.000976122448979592, 'epoch': 0.04}
  4%|â–         | 217/5000 [1:26:31<30:46:12, 23.16s/it]  4%|â–         | 218/5000 [1:26:55<31:06:52, 23.42s/it]                                                       {'loss': 6.0623, 'grad_norm': 0.5399495959281921, 'learning_rate': 0.0009759183673469389, 'epoch': 0.04}
  4%|â–         | 218/5000 [1:26:55<31:06:52, 23.42s/it]  4%|â–         | 219/5000 [1:27:20<31:33:22, 23.76s/it]                                                       {'loss': 6.07, 'grad_norm': 0.5083782076835632, 'learning_rate': 0.0009757142857142858, 'epoch': 0.04}
  4%|â–         | 219/5000 [1:27:20<31:33:22, 23.76s/it]  4%|â–         | 220/5000 [1:27:43<31:29:48, 23.72s/it]                                                       {'loss': 6.0414, 'grad_norm': 0.5421655178070068, 'learning_rate': 0.0009755102040816327, 'epoch': 0.04}
  4%|â–         | 220/5000 [1:27:43<31:29:48, 23.72s/it]  4%|â–         | 221/5000 [1:28:15<34:31:35, 26.01s/it]                                                       {'loss': 5.9567, 'grad_norm': 0.44601714611053467, 'learning_rate': 0.0009753061224489797, 'epoch': 0.04}
  4%|â–         | 221/5000 [1:28:15<34:31:35, 26.01s/it]  4%|â–         | 222/5000 [1:28:39<33:47:09, 25.46s/it]                                                       {'loss': 6.0575, 'grad_norm': 0.5244585871696472, 'learning_rate': 0.0009751020408163266, 'epoch': 0.04}
  4%|â–         | 222/5000 [1:28:39<33:47:09, 25.46s/it]  4%|â–         | 223/5000 [1:29:01<32:23:54, 24.42s/it]                                                       {'loss': 6.1547, 'grad_norm': 0.5974780917167664, 'learning_rate': 0.0009748979591836735, 'epoch': 0.04}
  4%|â–         | 223/5000 [1:29:01<32:23:54, 24.42s/it]  4%|â–         | 224/5000 [1:29:24<32:01:06, 24.13s/it]                                                       {'loss': 5.9995, 'grad_norm': 0.5232061147689819, 'learning_rate': 0.0009746938775510205, 'epoch': 0.04}
  4%|â–         | 224/5000 [1:29:24<32:01:06, 24.13s/it]  4%|â–         | 225/5000 [1:29:49<32:23:42, 24.42s/it]                                                       {'loss': 5.9743, 'grad_norm': 0.49937665462493896, 'learning_rate': 0.0009744897959183674, 'epoch': 0.04}
  4%|â–         | 225/5000 [1:29:49<32:23:42, 24.42s/it]  5%|â–         | 226/5000 [1:30:10<30:51:49, 23.27s/it]                                                       {'loss': 6.0123, 'grad_norm': 0.49486643075942993, 'learning_rate': 0.0009742857142857143, 'epoch': 0.05}
  5%|â–         | 226/5000 [1:30:10<30:51:49, 23.27s/it]  5%|â–         | 227/5000 [1:30:33<30:54:30, 23.31s/it]                                                       {'loss': 5.979, 'grad_norm': 0.49106746912002563, 'learning_rate': 0.0009740816326530612, 'epoch': 0.05}
  5%|â–         | 227/5000 [1:30:33<30:54:30, 23.31s/it]  5%|â–         | 228/5000 [1:30:57<30:59:26, 23.38s/it]                                                       {'loss': 6.05, 'grad_norm': 0.5575754046440125, 'learning_rate': 0.0009738775510204082, 'epoch': 0.05}
  5%|â–         | 228/5000 [1:30:57<30:59:26, 23.38s/it]  5%|â–         | 229/5000 [1:31:22<31:32:35, 23.80s/it]                                                       {'loss': 5.8979, 'grad_norm': 0.469849169254303, 'learning_rate': 0.0009736734693877551, 'epoch': 0.05}
  5%|â–         | 229/5000 [1:31:22<31:32:35, 23.80s/it]  5%|â–         | 230/5000 [1:31:45<31:33:13, 23.81s/it]                                                       {'loss': 5.91, 'grad_norm': 0.4951131343841553, 'learning_rate': 0.000973469387755102, 'epoch': 0.05}
  5%|â–         | 230/5000 [1:31:45<31:33:13, 23.81s/it]  5%|â–         | 231/5000 [1:32:09<31:21:18, 23.67s/it]                                                       {'loss': 5.9441, 'grad_norm': 0.4904165267944336, 'learning_rate': 0.000973265306122449, 'epoch': 0.05}
  5%|â–         | 231/5000 [1:32:09<31:21:18, 23.67s/it]  5%|â–         | 232/5000 [1:32:34<32:01:55, 24.19s/it]                                                       {'loss': 5.9784, 'grad_norm': 0.512887716293335, 'learning_rate': 0.000973061224489796, 'epoch': 0.05}
  5%|â–         | 232/5000 [1:32:34<32:01:55, 24.19s/it]  5%|â–         | 233/5000 [1:33:05<34:42:29, 26.21s/it]                                                       {'loss': 5.9361, 'grad_norm': 0.47288084030151367, 'learning_rate': 0.0009728571428571429, 'epoch': 0.05}
  5%|â–         | 233/5000 [1:33:05<34:42:29, 26.21s/it]  5%|â–         | 234/5000 [1:33:31<34:38:50, 26.17s/it]                                                       {'loss': 5.9533, 'grad_norm': 0.4816335141658783, 'learning_rate': 0.0009726530612244899, 'epoch': 0.05}
  5%|â–         | 234/5000 [1:33:31<34:38:50, 26.17s/it]  5%|â–         | 235/5000 [1:33:55<33:45:00, 25.50s/it]                                                       {'loss': 5.9755, 'grad_norm': 0.5007156133651733, 'learning_rate': 0.0009724489795918368, 'epoch': 0.05}
  5%|â–         | 235/5000 [1:33:55<33:45:00, 25.50s/it]  5%|â–         | 236/5000 [1:34:19<33:03:12, 24.98s/it]                                                       {'loss': 5.9448, 'grad_norm': 0.5056930780410767, 'learning_rate': 0.0009722448979591837, 'epoch': 0.05}
  5%|â–         | 236/5000 [1:34:19<33:03:12, 24.98s/it]  5%|â–         | 237/5000 [1:34:47<34:08:45, 25.81s/it]                                                       {'loss': 5.9689, 'grad_norm': 0.5035123229026794, 'learning_rate': 0.0009720408163265306, 'epoch': 0.05}
  5%|â–         | 237/5000 [1:34:47<34:08:45, 25.81s/it]  5%|â–         | 238/5000 [1:35:15<34:57:36, 26.43s/it]                                                       {'loss': 5.8805, 'grad_norm': 0.5084850192070007, 'learning_rate': 0.0009718367346938776, 'epoch': 0.05}
  5%|â–         | 238/5000 [1:35:15<34:57:36, 26.43s/it]  5%|â–         | 239/5000 [1:35:38<33:55:10, 25.65s/it]                                                       {'loss': 5.8552, 'grad_norm': 0.47099408507347107, 'learning_rate': 0.0009716326530612245, 'epoch': 0.05}
  5%|â–         | 239/5000 [1:35:38<33:55:10, 25.65s/it]  5%|â–         | 240/5000 [1:36:01<32:33:16, 24.62s/it]                                                       {'loss': 5.9086, 'grad_norm': 0.4906404912471771, 'learning_rate': 0.0009714285714285714, 'epoch': 0.05}
  5%|â–         | 240/5000 [1:36:01<32:33:16, 24.62s/it]  5%|â–         | 241/5000 [1:36:22<31:24:17, 23.76s/it]                                                       {'loss': 5.8418, 'grad_norm': 0.487239271402359, 'learning_rate': 0.0009712244897959184, 'epoch': 0.05}
  5%|â–         | 241/5000 [1:36:22<31:24:17, 23.76s/it]  5%|â–         | 242/5000 [1:36:44<30:32:55, 23.11s/it]                                                       {'loss': 5.7782, 'grad_norm': 0.43141838908195496, 'learning_rate': 0.0009710204081632653, 'epoch': 0.05}
  5%|â–         | 242/5000 [1:36:44<30:32:55, 23.11s/it]  5%|â–         | 243/5000 [1:37:08<30:59:19, 23.45s/it]                                                       {'loss': 5.8577, 'grad_norm': 0.4831143915653229, 'learning_rate': 0.0009708163265306122, 'epoch': 0.05}
  5%|â–         | 243/5000 [1:37:08<30:59:19, 23.45s/it]  5%|â–         | 244/5000 [1:37:32<31:18:58, 23.70s/it]                                                       {'loss': 5.8642, 'grad_norm': 0.4637787342071533, 'learning_rate': 0.0009706122448979592, 'epoch': 0.05}
  5%|â–         | 244/5000 [1:37:32<31:18:58, 23.70s/it]  5%|â–         | 245/5000 [1:37:56<31:08:58, 23.58s/it]                                                       {'loss': 5.8326, 'grad_norm': 0.48525112867355347, 'learning_rate': 0.0009704081632653061, 'epoch': 0.05}
  5%|â–         | 245/5000 [1:37:56<31:08:58, 23.58s/it]  5%|â–         | 246/5000 [1:38:21<31:52:55, 24.14s/it]                                                       {'loss': 5.7106, 'grad_norm': 0.5092429518699646, 'learning_rate': 0.000970204081632653, 'epoch': 0.05}
  5%|â–         | 246/5000 [1:38:21<31:52:55, 24.14s/it]  5%|â–         | 247/5000 [1:38:45<31:35:30, 23.93s/it]                                                       {'loss': 5.8591, 'grad_norm': 0.48195159435272217, 'learning_rate': 0.0009699999999999999, 'epoch': 0.05}
  5%|â–         | 247/5000 [1:38:45<31:35:30, 23.93s/it]  5%|â–         | 248/5000 [1:39:07<31:02:03, 23.51s/it]                                                       {'loss': 5.9144, 'grad_norm': 0.5125524997711182, 'learning_rate': 0.000969795918367347, 'epoch': 0.05}
  5%|â–         | 248/5000 [1:39:07<31:02:03, 23.51s/it]  5%|â–         | 249/5000 [1:39:29<30:09:29, 22.85s/it]                                                       {'loss': 5.7803, 'grad_norm': 0.4784034788608551, 'learning_rate': 0.0009695918367346939, 'epoch': 0.05}
  5%|â–         | 249/5000 [1:39:29<30:09:29, 22.85s/it]  5%|â–Œ         | 250/5000 [1:39:53<30:52:05, 23.39s/it]                                                       {'loss': 5.8573, 'grad_norm': 0.5689514875411987, 'learning_rate': 0.0009693877551020408, 'epoch': 0.05}
  5%|â–Œ         | 250/5000 [1:39:53<30:52:05, 23.39s/it][2025-10-19 19:26:52,404] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 251/5000 [1:40:18<31:26:52, 23.84s/it]                                                       {'loss': 5.7408, 'grad_norm': 0.48296669125556946, 'learning_rate': 0.0009691836734693878, 'epoch': 0.05}
  5%|â–Œ         | 251/5000 [1:40:18<31:26:52, 23.84s/it]  5%|â–Œ         | 252/5000 [1:40:42<31:40:49, 24.02s/it]                                                       {'loss': 5.767, 'grad_norm': 0.5168393850326538, 'learning_rate': 0.0009689795918367347, 'epoch': 0.05}
  5%|â–Œ         | 252/5000 [1:40:42<31:40:49, 24.02s/it]  5%|â–Œ         | 253/5000 [1:41:07<31:51:46, 24.16s/it]                                                       {'loss': 5.7356, 'grad_norm': 0.5566030144691467, 'learning_rate': 0.0009687755102040816, 'epoch': 0.05}
  5%|â–Œ         | 253/5000 [1:41:07<31:51:46, 24.16s/it]  5%|â–Œ         | 254/5000 [1:41:33<32:43:39, 24.83s/it]                                                       {'loss': 5.7011, 'grad_norm': 0.4924922585487366, 'learning_rate': 0.0009685714285714286, 'epoch': 0.05}
  5%|â–Œ         | 254/5000 [1:41:33<32:43:39, 24.83s/it]  5%|â–Œ         | 255/5000 [1:41:57<32:19:10, 24.52s/it]                                                       {'loss': 5.6943, 'grad_norm': 0.5172798037528992, 'learning_rate': 0.0009683673469387755, 'epoch': 0.05}
  5%|â–Œ         | 255/5000 [1:41:57<32:19:10, 24.52s/it]  5%|â–Œ         | 256/5000 [1:42:20<31:28:55, 23.89s/it]                                                       {'loss': 5.7311, 'grad_norm': 0.5377264618873596, 'learning_rate': 0.0009681632653061224, 'epoch': 0.05}
  5%|â–Œ         | 256/5000 [1:42:20<31:28:55, 23.89s/it]  5%|â–Œ         | 257/5000 [1:42:43<31:18:09, 23.76s/it]                                                       {'loss': 5.7149, 'grad_norm': 0.5298267602920532, 'learning_rate': 0.0009679591836734693, 'epoch': 0.05}
  5%|â–Œ         | 257/5000 [1:42:43<31:18:09, 23.76s/it]  5%|â–Œ         | 258/5000 [1:43:10<32:24:52, 24.61s/it]                                                       {'loss': 5.6746, 'grad_norm': 0.540899395942688, 'learning_rate': 0.0009677551020408163, 'epoch': 0.05}
  5%|â–Œ         | 258/5000 [1:43:10<32:24:52, 24.61s/it]  5%|â–Œ         | 259/5000 [1:43:41<35:01:46, 26.60s/it]                                                       {'loss': 5.6375, 'grad_norm': 0.5138341188430786, 'learning_rate': 0.0009675510204081633, 'epoch': 0.05}
  5%|â–Œ         | 259/5000 [1:43:41<35:01:46, 26.60s/it]  5%|â–Œ         | 260/5000 [1:44:05<33:57:11, 25.79s/it]                                                       {'loss': 5.7334, 'grad_norm': 0.5819451808929443, 'learning_rate': 0.0009673469387755102, 'epoch': 0.05}
  5%|â–Œ         | 260/5000 [1:44:05<33:57:11, 25.79s/it]  5%|â–Œ         | 261/5000 [1:44:29<33:29:18, 25.44s/it]                                                       {'loss': 5.7591, 'grad_norm': 0.599900484085083, 'learning_rate': 0.0009671428571428572, 'epoch': 0.05}
  5%|â–Œ         | 261/5000 [1:44:29<33:29:18, 25.44s/it]  5%|â–Œ         | 262/5000 [1:44:53<32:55:49, 25.02s/it]                                                       {'loss': 5.6186, 'grad_norm': 0.5463294982910156, 'learning_rate': 0.0009669387755102041, 'epoch': 0.05}
  5%|â–Œ         | 262/5000 [1:44:53<32:55:49, 25.02s/it]  5%|â–Œ         | 263/5000 [1:45:17<32:32:40, 24.73s/it]                                                       {'loss': 5.6166, 'grad_norm': 0.551837146282196, 'learning_rate': 0.0009667346938775511, 'epoch': 0.05}
  5%|â–Œ         | 263/5000 [1:45:17<32:32:40, 24.73s/it]  5%|â–Œ         | 264/5000 [1:45:41<32:14:56, 24.51s/it]                                                       {'loss': 5.6843, 'grad_norm': 0.595463216304779, 'learning_rate': 0.0009665306122448981, 'epoch': 0.05}
  5%|â–Œ         | 264/5000 [1:45:41<32:14:56, 24.51s/it]  5%|â–Œ         | 265/5000 [1:46:02<30:50:26, 23.45s/it]                                                       {'loss': 5.6231, 'grad_norm': 0.5619881749153137, 'learning_rate': 0.000966326530612245, 'epoch': 0.05}
  5%|â–Œ         | 265/5000 [1:46:02<30:50:26, 23.45s/it]  5%|â–Œ         | 266/5000 [1:46:27<31:23:37, 23.87s/it]                                                       {'loss': 5.5596, 'grad_norm': 0.5629468560218811, 'learning_rate': 0.0009661224489795919, 'epoch': 0.05}
  5%|â–Œ         | 266/5000 [1:46:27<31:23:37, 23.87s/it]  5%|â–Œ         | 267/5000 [1:46:51<31:30:14, 23.96s/it]                                                       {'loss': 5.585, 'grad_norm': 0.5349538326263428, 'learning_rate': 0.0009659183673469389, 'epoch': 0.05}
  5%|â–Œ         | 267/5000 [1:46:51<31:30:14, 23.96s/it]  5%|â–Œ         | 268/5000 [1:47:15<31:09:06, 23.70s/it]                                                       {'loss': 5.4728, 'grad_norm': 0.5294502377510071, 'learning_rate': 0.0009657142857142858, 'epoch': 0.05}
  5%|â–Œ         | 268/5000 [1:47:15<31:09:06, 23.70s/it]  5%|â–Œ         | 269/5000 [1:47:39<31:20:23, 23.85s/it]                                                       {'loss': 5.5803, 'grad_norm': 0.5401564836502075, 'learning_rate': 0.0009655102040816327, 'epoch': 0.05}
  5%|â–Œ         | 269/5000 [1:47:39<31:20:23, 23.85s/it]  5%|â–Œ         | 270/5000 [1:48:00<30:29:45, 23.21s/it]                                                       {'loss': 5.5114, 'grad_norm': 0.5321093797683716, 'learning_rate': 0.0009653061224489796, 'epoch': 0.05}
  5%|â–Œ         | 270/5000 [1:48:00<30:29:45, 23.21s/it]  5%|â–Œ         | 271/5000 [1:48:24<30:40:24, 23.35s/it]                                                       {'loss': 5.5082, 'grad_norm': 0.5344474911689758, 'learning_rate': 0.0009651020408163266, 'epoch': 0.05}
  5%|â–Œ         | 271/5000 [1:48:24<30:40:24, 23.35s/it]  5%|â–Œ         | 272/5000 [1:48:47<30:23:29, 23.14s/it]                                                       {'loss': 5.5883, 'grad_norm': 0.5301332473754883, 'learning_rate': 0.0009648979591836735, 'epoch': 0.05}
  5%|â–Œ         | 272/5000 [1:48:47<30:23:29, 23.14s/it]  5%|â–Œ         | 273/5000 [1:49:13<31:37:32, 24.09s/it]                                                       {'loss': 5.6242, 'grad_norm': 0.5917647480964661, 'learning_rate': 0.0009646938775510204, 'epoch': 0.05}
  5%|â–Œ         | 273/5000 [1:49:13<31:37:32, 24.09s/it]  5%|â–Œ         | 274/5000 [1:49:39<32:26:35, 24.71s/it]                                                       {'loss': 5.5394, 'grad_norm': 0.5199373960494995, 'learning_rate': 0.0009644897959183674, 'epoch': 0.05}
  5%|â–Œ         | 274/5000 [1:49:39<32:26:35, 24.71s/it]  6%|â–Œ         | 275/5000 [1:50:03<32:06:29, 24.46s/it]                                                       {'loss': 5.5129, 'grad_norm': 0.5352385640144348, 'learning_rate': 0.0009642857142857143, 'epoch': 0.06}
  6%|â–Œ         | 275/5000 [1:50:03<32:06:29, 24.46s/it]  6%|â–Œ         | 276/5000 [1:50:31<33:32:20, 25.56s/it]                                                       {'loss': 5.5166, 'grad_norm': 0.5023183822631836, 'learning_rate': 0.0009640816326530612, 'epoch': 0.06}
  6%|â–Œ         | 276/5000 [1:50:31<33:32:20, 25.56s/it]  6%|â–Œ         | 277/5000 [1:50:55<32:55:58, 25.10s/it]                                                       {'loss': 5.4352, 'grad_norm': 0.5216600894927979, 'learning_rate': 0.0009638775510204081, 'epoch': 0.06}
  6%|â–Œ         | 277/5000 [1:50:55<32:55:58, 25.10s/it]  6%|â–Œ         | 278/5000 [1:51:18<32:00:49, 24.41s/it]                                                       {'loss': 5.4442, 'grad_norm': 0.5375169515609741, 'learning_rate': 0.0009636734693877551, 'epoch': 0.06}
  6%|â–Œ         | 278/5000 [1:51:18<32:00:49, 24.41s/it]  6%|â–Œ         | 279/5000 [1:51:42<31:46:12, 24.23s/it]                                                       {'loss': 5.4307, 'grad_norm': 0.5510470867156982, 'learning_rate': 0.0009634693877551021, 'epoch': 0.06}
  6%|â–Œ         | 279/5000 [1:51:42<31:46:12, 24.23s/it]  6%|â–Œ         | 280/5000 [1:52:06<31:41:54, 24.18s/it]                                                       {'loss': 5.4058, 'grad_norm': 0.5098979473114014, 'learning_rate': 0.000963265306122449, 'epoch': 0.06}
  6%|â–Œ         | 280/5000 [1:52:06<31:41:54, 24.18s/it]  6%|â–Œ         | 281/5000 [1:52:29<31:19:35, 23.90s/it]                                                       {'loss': 5.398, 'grad_norm': 0.5638568997383118, 'learning_rate': 0.000963061224489796, 'epoch': 0.06}
  6%|â–Œ         | 281/5000 [1:52:29<31:19:35, 23.90s/it]  6%|â–Œ         | 282/5000 [1:52:56<32:33:36, 24.84s/it]                                                       {'loss': 5.4386, 'grad_norm': 0.5580384731292725, 'learning_rate': 0.0009628571428571429, 'epoch': 0.06}
  6%|â–Œ         | 282/5000 [1:52:56<32:33:36, 24.84s/it]  6%|â–Œ         | 283/5000 [1:53:24<33:46:51, 25.78s/it]                                                       {'loss': 5.4033, 'grad_norm': 0.5213739275932312, 'learning_rate': 0.0009626530612244898, 'epoch': 0.06}
  6%|â–Œ         | 283/5000 [1:53:24<33:46:51, 25.78s/it]  6%|â–Œ         | 284/5000 [1:53:49<33:20:00, 25.45s/it]                                                       {'loss': 5.3558, 'grad_norm': 0.5103195905685425, 'learning_rate': 0.0009624489795918368, 'epoch': 0.06}
  6%|â–Œ         | 284/5000 [1:53:49<33:20:00, 25.45s/it]  6%|â–Œ         | 285/5000 [1:54:11<32:06:15, 24.51s/it]                                                       {'loss': 5.3375, 'grad_norm': 0.5198614597320557, 'learning_rate': 0.0009622448979591837, 'epoch': 0.06}
  6%|â–Œ         | 285/5000 [1:54:11<32:06:15, 24.51s/it]  6%|â–Œ         | 286/5000 [1:54:33<31:04:07, 23.73s/it]                                                       {'loss': 5.3979, 'grad_norm': 0.548955500125885, 'learning_rate': 0.0009620408163265306, 'epoch': 0.06}
  6%|â–Œ         | 286/5000 [1:54:33<31:04:07, 23.73s/it]  6%|â–Œ         | 287/5000 [1:54:58<31:30:11, 24.06s/it]                                                       {'loss': 5.3324, 'grad_norm': 0.5127163529396057, 'learning_rate': 0.0009618367346938776, 'epoch': 0.06}
  6%|â–Œ         | 287/5000 [1:54:58<31:30:11, 24.06s/it]  6%|â–Œ         | 288/5000 [1:55:28<33:40:01, 25.72s/it]                                                       {'loss': 5.3035, 'grad_norm': 0.512396514415741, 'learning_rate': 0.0009616326530612245, 'epoch': 0.06}
  6%|â–Œ         | 288/5000 [1:55:28<33:40:01, 25.72s/it]  6%|â–Œ         | 289/5000 [1:55:52<33:08:26, 25.33s/it]                                                       {'loss': 5.3372, 'grad_norm': 0.48610228300094604, 'learning_rate': 0.0009614285714285714, 'epoch': 0.06}
  6%|â–Œ         | 289/5000 [1:55:52<33:08:26, 25.33s/it]  6%|â–Œ         | 290/5000 [1:56:13<31:35:53, 24.15s/it]                                                       {'loss': 5.3019, 'grad_norm': 0.4727896749973297, 'learning_rate': 0.0009612244897959183, 'epoch': 0.06}
  6%|â–Œ         | 290/5000 [1:56:13<31:35:53, 24.15s/it]  6%|â–Œ         | 291/5000 [1:56:35<30:31:04, 23.33s/it]                                                       {'loss': 5.3294, 'grad_norm': 0.4799927771091461, 'learning_rate': 0.0009610204081632653, 'epoch': 0.06}
  6%|â–Œ         | 291/5000 [1:56:35<30:31:04, 23.33s/it]  6%|â–Œ         | 292/5000 [1:57:07<33:56:23, 25.95s/it]                                                       {'loss': 5.2867, 'grad_norm': 0.478182315826416, 'learning_rate': 0.0009608163265306122, 'epoch': 0.06}
  6%|â–Œ         | 292/5000 [1:57:07<33:56:23, 25.95s/it]  6%|â–Œ         | 293/5000 [1:57:33<33:55:33, 25.95s/it]                                                       {'loss': 5.3095, 'grad_norm': 0.45602092146873474, 'learning_rate': 0.0009606122448979591, 'epoch': 0.06}
  6%|â–Œ         | 293/5000 [1:57:33<33:55:33, 25.95s/it]  6%|â–Œ         | 294/5000 [1:57:57<33:05:39, 25.32s/it]                                                       {'loss': 5.2338, 'grad_norm': 0.42569002509117126, 'learning_rate': 0.0009604081632653062, 'epoch': 0.06}
  6%|â–Œ         | 294/5000 [1:57:57<33:05:39, 25.32s/it]  6%|â–Œ         | 295/5000 [1:58:23<33:38:34, 25.74s/it]                                                       {'loss': 5.2399, 'grad_norm': 0.45029106736183167, 'learning_rate': 0.0009602040816326531, 'epoch': 0.06}
  6%|â–Œ         | 295/5000 [1:58:23<33:38:34, 25.74s/it]  6%|â–Œ         | 296/5000 [1:58:48<33:06:02, 25.33s/it]                                                       {'loss': 5.254, 'grad_norm': 0.41836318373680115, 'learning_rate': 0.00096, 'epoch': 0.06}
  6%|â–Œ         | 296/5000 [1:58:48<33:06:02, 25.33s/it]  6%|â–Œ         | 297/5000 [1:59:09<31:33:09, 24.15s/it]                                                       {'loss': 5.2602, 'grad_norm': 0.4204164445400238, 'learning_rate': 0.000959795918367347, 'epoch': 0.06}
  6%|â–Œ         | 297/5000 [1:59:09<31:33:09, 24.15s/it]  6%|â–Œ         | 298/5000 [1:59:31<30:34:52, 23.41s/it]                                                       {'loss': 5.2337, 'grad_norm': 0.44031259417533875, 'learning_rate': 0.0009595918367346939, 'epoch': 0.06}
  6%|â–Œ         | 298/5000 [1:59:31<30:34:52, 23.41s/it]  6%|â–Œ         | 299/5000 [1:59:53<30:03:17, 23.02s/it]                                                       {'loss': 5.2, 'grad_norm': 0.4093644618988037, 'learning_rate': 0.0009593877551020408, 'epoch': 0.06}
  6%|â–Œ         | 299/5000 [1:59:53<30:03:17, 23.02s/it]  6%|â–Œ         | 300/5000 [2:00:17<30:23:20, 23.28s/it]                                                       {'loss': 5.2847, 'grad_norm': 0.3988659083843231, 'learning_rate': 0.0009591836734693877, 'epoch': 0.06}
  6%|â–Œ         | 300/5000 [2:00:17<30:23:20, 23.28s/it][2025-10-19 19:47:16,065] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 301/5000 [2:00:40<30:22:19, 23.27s/it]                                                       {'loss': 5.2469, 'grad_norm': 0.4002571702003479, 'learning_rate': 0.0009589795918367347, 'epoch': 0.06}
  6%|â–Œ         | 301/5000 [2:00:40<30:22:19, 23.27s/it]  6%|â–Œ         | 302/5000 [2:01:02<29:43:46, 22.78s/it]                                                       {'loss': 5.2961, 'grad_norm': 0.435299277305603, 'learning_rate': 0.0009587755102040816, 'epoch': 0.06}
  6%|â–Œ         | 302/5000 [2:01:02<29:43:46, 22.78s/it]  6%|â–Œ         | 303/5000 [2:01:26<30:18:07, 23.22s/it]                                                       {'loss': 5.1769, 'grad_norm': 0.37725532054901123, 'learning_rate': 0.0009585714285714285, 'epoch': 0.06}
  6%|â–Œ         | 303/5000 [2:01:26<30:18:07, 23.22s/it]  6%|â–Œ         | 304/5000 [2:01:48<30:00:34, 23.01s/it]                                                       {'loss': 5.2082, 'grad_norm': 0.3929838538169861, 'learning_rate': 0.0009583673469387755, 'epoch': 0.06}
  6%|â–Œ         | 304/5000 [2:01:48<30:00:34, 23.01s/it]  6%|â–Œ         | 305/5000 [2:02:14<30:59:59, 23.77s/it]                                                       {'loss': 5.2112, 'grad_norm': 0.3460402190685272, 'learning_rate': 0.0009581632653061225, 'epoch': 0.06}
  6%|â–Œ         | 305/5000 [2:02:14<30:59:59, 23.77s/it]  6%|â–Œ         | 306/5000 [2:02:40<31:40:55, 24.30s/it]                                                       {'loss': 5.2164, 'grad_norm': 0.34075361490249634, 'learning_rate': 0.0009579591836734694, 'epoch': 0.06}
  6%|â–Œ         | 306/5000 [2:02:40<31:40:55, 24.30s/it]  6%|â–Œ         | 307/5000 [2:03:02<30:54:46, 23.71s/it]                                                       {'loss': 5.1389, 'grad_norm': 0.34118857979774475, 'learning_rate': 0.0009577551020408164, 'epoch': 0.06}
  6%|â–Œ         | 307/5000 [2:03:02<30:54:46, 23.71s/it]  6%|â–Œ         | 308/5000 [2:03:27<31:20:36, 24.05s/it]                                                       {'loss': 5.1625, 'grad_norm': 0.337359756231308, 'learning_rate': 0.0009575510204081633, 'epoch': 0.06}
  6%|â–Œ         | 308/5000 [2:03:27<31:20:36, 24.05s/it]  6%|â–Œ         | 309/5000 [2:03:50<30:55:10, 23.73s/it]                                                       {'loss': 5.2287, 'grad_norm': 0.36290425062179565, 'learning_rate': 0.0009573469387755102, 'epoch': 0.06}
  6%|â–Œ         | 309/5000 [2:03:50<30:55:10, 23.73s/it]  6%|â–Œ         | 310/5000 [2:04:11<29:48:43, 22.88s/it]                                                       {'loss': 5.1462, 'grad_norm': 0.2809999883174896, 'learning_rate': 0.0009571428571428573, 'epoch': 0.06}
  6%|â–Œ         | 310/5000 [2:04:11<29:48:43, 22.88s/it]  6%|â–Œ         | 311/5000 [2:04:35<30:18:07, 23.26s/it]                                                       {'loss': 5.1687, 'grad_norm': 0.3167080879211426, 'learning_rate': 0.0009569387755102042, 'epoch': 0.06}
  6%|â–Œ         | 311/5000 [2:04:35<30:18:07, 23.26s/it]  6%|â–Œ         | 312/5000 [2:05:01<31:24:14, 24.12s/it]                                                       {'loss': 5.2218, 'grad_norm': 0.35362592339515686, 'learning_rate': 0.0009567346938775511, 'epoch': 0.06}
  6%|â–Œ         | 312/5000 [2:05:01<31:24:14, 24.12s/it]  6%|â–‹         | 313/5000 [2:05:26<31:37:19, 24.29s/it]                                                       {'loss': 5.1984, 'grad_norm': 0.33566293120384216, 'learning_rate': 0.000956530612244898, 'epoch': 0.06}
  6%|â–‹         | 313/5000 [2:05:26<31:37:19, 24.29s/it]  6%|â–‹         | 314/5000 [2:05:50<31:29:40, 24.20s/it]                                                       {'loss': 5.1572, 'grad_norm': 0.2976572513580322, 'learning_rate': 0.000956326530612245, 'epoch': 0.06}
  6%|â–‹         | 314/5000 [2:05:50<31:29:40, 24.20s/it]  6%|â–‹         | 315/5000 [2:06:13<31:19:49, 24.07s/it]                                                       {'loss': 5.137, 'grad_norm': 0.3151080012321472, 'learning_rate': 0.0009561224489795919, 'epoch': 0.06}
  6%|â–‹         | 315/5000 [2:06:13<31:19:49, 24.07s/it]  6%|â–‹         | 316/5000 [2:06:42<33:11:53, 25.52s/it]                                                       {'loss': 5.1625, 'grad_norm': 0.2897346019744873, 'learning_rate': 0.0009559183673469388, 'epoch': 0.06}
  6%|â–‹         | 316/5000 [2:06:42<33:11:53, 25.52s/it]  6%|â–‹         | 317/5000 [2:07:04<31:35:59, 24.29s/it]                                                       {'loss': 5.1228, 'grad_norm': 0.294126957654953, 'learning_rate': 0.0009557142857142858, 'epoch': 0.06}
  6%|â–‹         | 317/5000 [2:07:04<31:35:59, 24.29s/it]  6%|â–‹         | 318/5000 [2:07:25<30:17:49, 23.30s/it]                                                       {'loss': 5.0742, 'grad_norm': 0.25338393449783325, 'learning_rate': 0.0009555102040816327, 'epoch': 0.06}
  6%|â–‹         | 318/5000 [2:07:25<30:17:49, 23.30s/it]  6%|â–‹         | 319/5000 [2:07:48<30:29:44, 23.45s/it]                                                       {'loss': 5.1336, 'grad_norm': 0.27387648820877075, 'learning_rate': 0.0009553061224489796, 'epoch': 0.06}
  6%|â–‹         | 319/5000 [2:07:48<30:29:44, 23.45s/it]  6%|â–‹         | 320/5000 [2:08:12<30:27:50, 23.43s/it]                                                       {'loss': 5.0925, 'grad_norm': 0.26849058270454407, 'learning_rate': 0.0009551020408163265, 'epoch': 0.06}
  6%|â–‹         | 320/5000 [2:08:12<30:27:50, 23.43s/it]  6%|â–‹         | 321/5000 [2:08:34<30:04:21, 23.14s/it]                                                       {'loss': 5.1118, 'grad_norm': 0.2678435742855072, 'learning_rate': 0.0009548979591836735, 'epoch': 0.06}
  6%|â–‹         | 321/5000 [2:08:34<30:04:21, 23.14s/it]  6%|â–‹         | 322/5000 [2:08:56<29:29:39, 22.70s/it]                                                       {'loss': 5.1541, 'grad_norm': 0.3361685872077942, 'learning_rate': 0.0009546938775510204, 'epoch': 0.06}
  6%|â–‹         | 322/5000 [2:08:56<29:29:39, 22.70s/it]  6%|â–‹         | 323/5000 [2:09:20<30:08:21, 23.20s/it]                                                       {'loss': 5.1588, 'grad_norm': 0.2974489629268646, 'learning_rate': 0.0009544897959183673, 'epoch': 0.06}
  6%|â–‹         | 323/5000 [2:09:20<30:08:21, 23.20s/it]  6%|â–‹         | 324/5000 [2:09:45<30:30:46, 23.49s/it]                                                       {'loss': 5.0771, 'grad_norm': 0.23852626979351044, 'learning_rate': 0.0009542857142857143, 'epoch': 0.06}
  6%|â–‹         | 324/5000 [2:09:45<30:30:46, 23.49s/it]  6%|â–‹         | 325/5000 [2:10:06<29:37:47, 22.82s/it]                                                       {'loss': 5.1171, 'grad_norm': 0.2434852421283722, 'learning_rate': 0.0009540816326530613, 'epoch': 0.07}
  6%|â–‹         | 325/5000 [2:10:06<29:37:47, 22.82s/it]  7%|â–‹         | 326/5000 [2:10:28<29:21:21, 22.61s/it]                                                       {'loss': 5.0897, 'grad_norm': 0.2569545805454254, 'learning_rate': 0.0009538775510204082, 'epoch': 0.07}
  7%|â–‹         | 326/5000 [2:10:28<29:21:21, 22.61s/it]  7%|â–‹         | 327/5000 [2:10:51<29:36:28, 22.81s/it]                                                       {'loss': 5.0648, 'grad_norm': 0.27670466899871826, 'learning_rate': 0.0009536734693877552, 'epoch': 0.07}
  7%|â–‹         | 327/5000 [2:10:51<29:36:28, 22.81s/it]  7%|â–‹         | 328/5000 [2:11:13<29:21:17, 22.62s/it]                                                       {'loss': 5.0657, 'grad_norm': 0.2193652242422104, 'learning_rate': 0.0009534693877551021, 'epoch': 0.07}
  7%|â–‹         | 328/5000 [2:11:13<29:21:17, 22.62s/it]  7%|â–‹         | 329/5000 [2:11:35<29:09:43, 22.48s/it]                                                       {'loss': 5.1315, 'grad_norm': 0.24275647103786469, 'learning_rate': 0.000953265306122449, 'epoch': 0.07}
  7%|â–‹         | 329/5000 [2:11:35<29:09:43, 22.48s/it]  7%|â–‹         | 330/5000 [2:12:03<30:59:20, 23.89s/it]                                                       {'loss': 5.0709, 'grad_norm': 0.21657691895961761, 'learning_rate': 0.000953061224489796, 'epoch': 0.07}
  7%|â–‹         | 330/5000 [2:12:03<30:59:20, 23.89s/it]  7%|â–‹         | 331/5000 [2:12:27<31:02:31, 23.93s/it]                                                       {'loss': 5.1312, 'grad_norm': 0.23085279762744904, 'learning_rate': 0.0009528571428571429, 'epoch': 0.07}
  7%|â–‹         | 331/5000 [2:12:27<31:02:31, 23.93s/it]  7%|â–‹         | 332/5000 [2:12:51<31:16:19, 24.12s/it]                                                       {'loss': 5.1048, 'grad_norm': 0.2631871998310089, 'learning_rate': 0.0009526530612244898, 'epoch': 0.07}
  7%|â–‹         | 332/5000 [2:12:51<31:16:19, 24.12s/it]  7%|â–‹         | 333/5000 [2:13:18<32:07:45, 24.78s/it]                                                       {'loss': 5.0937, 'grad_norm': 0.220114067196846, 'learning_rate': 0.0009524489795918367, 'epoch': 0.07}
  7%|â–‹         | 333/5000 [2:13:18<32:07:45, 24.78s/it]  7%|â–‹         | 334/5000 [2:13:42<32:00:29, 24.70s/it]                                                       {'loss': 5.0818, 'grad_norm': 0.2133834958076477, 'learning_rate': 0.0009522448979591837, 'epoch': 0.07}
  7%|â–‹         | 334/5000 [2:13:42<32:00:29, 24.70s/it]  7%|â–‹         | 335/5000 [2:14:06<31:42:25, 24.47s/it]                                                       {'loss': 5.0992, 'grad_norm': 0.21658921241760254, 'learning_rate': 0.0009520408163265306, 'epoch': 0.07}
  7%|â–‹         | 335/5000 [2:14:06<31:42:25, 24.47s/it]  7%|â–‹         | 336/5000 [2:14:27<30:25:33, 23.48s/it]                                                       {'loss': 5.0874, 'grad_norm': 0.21759532392024994, 'learning_rate': 0.0009518367346938775, 'epoch': 0.07}
  7%|â–‹         | 336/5000 [2:14:27<30:25:33, 23.48s/it]  7%|â–‹         | 337/5000 [2:14:51<30:36:31, 23.63s/it]                                                       {'loss': 5.0881, 'grad_norm': 0.2537555992603302, 'learning_rate': 0.0009516326530612245, 'epoch': 0.07}
  7%|â–‹         | 337/5000 [2:14:51<30:36:31, 23.63s/it]  7%|â–‹         | 338/5000 [2:15:17<31:28:02, 24.30s/it]                                                       {'loss': 5.1633, 'grad_norm': 0.26747313141822815, 'learning_rate': 0.0009514285714285714, 'epoch': 0.07}
  7%|â–‹         | 338/5000 [2:15:17<31:28:02, 24.30s/it]  7%|â–‹         | 339/5000 [2:15:41<31:23:06, 24.24s/it]                                                       {'loss': 5.0693, 'grad_norm': 0.19941838085651398, 'learning_rate': 0.0009512244897959183, 'epoch': 0.07}
  7%|â–‹         | 339/5000 [2:15:41<31:23:06, 24.24s/it]  7%|â–‹         | 340/5000 [2:16:03<30:37:55, 23.66s/it]                                                       {'loss': 5.0658, 'grad_norm': 0.20467683672904968, 'learning_rate': 0.0009510204081632652, 'epoch': 0.07}
  7%|â–‹         | 340/5000 [2:16:03<30:37:55, 23.66s/it]  7%|â–‹         | 341/5000 [2:16:25<29:50:38, 23.06s/it]                                                       {'loss': 5.0462, 'grad_norm': 0.23048415780067444, 'learning_rate': 0.0009508163265306123, 'epoch': 0.07}
  7%|â–‹         | 341/5000 [2:16:25<29:50:38, 23.06s/it]  7%|â–‹         | 342/5000 [2:16:49<29:58:54, 23.17s/it]                                                       {'loss': 5.0553, 'grad_norm': 0.18715299665927887, 'learning_rate': 0.0009506122448979592, 'epoch': 0.07}
  7%|â–‹         | 342/5000 [2:16:49<29:58:54, 23.17s/it]  7%|â–‹         | 343/5000 [2:17:10<29:21:03, 22.69s/it]                                                       {'loss': 5.0264, 'grad_norm': 0.21299771964550018, 'learning_rate': 0.0009504081632653061, 'epoch': 0.07}
  7%|â–‹         | 343/5000 [2:17:10<29:21:03, 22.69s/it]  7%|â–‹         | 344/5000 [2:17:37<30:52:43, 23.88s/it]                                                       {'loss': 5.0606, 'grad_norm': 0.20732343196868896, 'learning_rate': 0.0009502040816326531, 'epoch': 0.07}
  7%|â–‹         | 344/5000 [2:17:37<30:52:43, 23.88s/it]  7%|â–‹         | 345/5000 [2:18:02<31:16:21, 24.19s/it]                                                       {'loss': 5.064, 'grad_norm': 0.18981409072875977, 'learning_rate': 0.00095, 'epoch': 0.07}
  7%|â–‹         | 345/5000 [2:18:02<31:16:21, 24.19s/it]  7%|â–‹         | 346/5000 [2:18:23<30:18:00, 23.44s/it]                                                       {'loss': 5.0651, 'grad_norm': 0.20292051136493683, 'learning_rate': 0.0009497959183673469, 'epoch': 0.07}
  7%|â–‹         | 346/5000 [2:18:23<30:18:00, 23.44s/it]  7%|â–‹         | 347/5000 [2:18:48<30:47:03, 23.82s/it]                                                       {'loss': 5.091, 'grad_norm': 0.23020607233047485, 'learning_rate': 0.0009495918367346939, 'epoch': 0.07}
  7%|â–‹         | 347/5000 [2:18:48<30:47:03, 23.82s/it]  7%|â–‹         | 348/5000 [2:19:10<30:11:06, 23.36s/it]                                                       {'loss': 5.0352, 'grad_norm': 0.18488451838493347, 'learning_rate': 0.0009493877551020408, 'epoch': 0.07}
  7%|â–‹         | 348/5000 [2:19:10<30:11:06, 23.36s/it]  7%|â–‹         | 349/5000 [2:19:34<30:18:25, 23.46s/it]                                                       {'loss': 5.0051, 'grad_norm': 0.21166950464248657, 'learning_rate': 0.0009491836734693877, 'epoch': 0.07}
  7%|â–‹         | 349/5000 [2:19:34<30:18:25, 23.46s/it]  7%|â–‹         | 350/5000 [2:19:58<30:30:22, 23.62s/it]                                                       {'loss': 5.047, 'grad_norm': 0.18205566704273224, 'learning_rate': 0.0009489795918367348, 'epoch': 0.07}
  7%|â–‹         | 350/5000 [2:19:58<30:30:22, 23.62s/it][2025-10-19 20:06:57,251] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 351/5000 [2:20:21<30:23:16, 23.53s/it]                                                       {'loss': 5.0878, 'grad_norm': 0.205132395029068, 'learning_rate': 0.0009487755102040817, 'epoch': 0.07}
  7%|â–‹         | 351/5000 [2:20:21<30:23:16, 23.53s/it]  7%|â–‹         | 352/5000 [2:20:46<30:55:32, 23.95s/it]                                                       {'loss': 5.0538, 'grad_norm': 0.1871640682220459, 'learning_rate': 0.0009485714285714286, 'epoch': 0.07}
  7%|â–‹         | 352/5000 [2:20:46<30:55:32, 23.95s/it]  7%|â–‹         | 353/5000 [2:21:13<32:01:28, 24.81s/it]                                                       {'loss': 5.0569, 'grad_norm': 0.2006804198026657, 'learning_rate': 0.0009483673469387755, 'epoch': 0.07}
  7%|â–‹         | 353/5000 [2:21:13<32:01:28, 24.81s/it]  7%|â–‹         | 354/5000 [2:21:35<30:49:29, 23.89s/it]                                                       {'loss': 5.0611, 'grad_norm': 0.16510699689388275, 'learning_rate': 0.0009481632653061225, 'epoch': 0.07}
  7%|â–‹         | 354/5000 [2:21:35<30:49:29, 23.89s/it]  7%|â–‹         | 355/5000 [2:21:56<29:37:27, 22.96s/it]                                                       {'loss': 5.0377, 'grad_norm': 0.1903902143239975, 'learning_rate': 0.0009479591836734694, 'epoch': 0.07}
  7%|â–‹         | 355/5000 [2:21:56<29:37:27, 22.96s/it]  7%|â–‹         | 356/5000 [2:22:19<29:53:16, 23.17s/it]                                                       {'loss': 5.0697, 'grad_norm': 0.32850220799446106, 'learning_rate': 0.0009477551020408164, 'epoch': 0.07}
  7%|â–‹         | 356/5000 [2:22:19<29:53:16, 23.17s/it]  7%|â–‹         | 357/5000 [2:22:43<30:11:50, 23.41s/it]                                                       {'loss': 5.0621, 'grad_norm': 0.18432919681072235, 'learning_rate': 0.0009475510204081634, 'epoch': 0.07}
  7%|â–‹         | 357/5000 [2:22:43<30:11:50, 23.41s/it]  7%|â–‹         | 358/5000 [2:23:05<29:40:51, 23.02s/it]                                                       {'loss': 4.9837, 'grad_norm': 0.166157066822052, 'learning_rate': 0.0009473469387755103, 'epoch': 0.07}
  7%|â–‹         | 358/5000 [2:23:05<29:40:51, 23.02s/it]  7%|â–‹         | 359/5000 [2:23:28<29:24:12, 22.81s/it]                                                       {'loss': 5.0225, 'grad_norm': 0.20456445217132568, 'learning_rate': 0.0009471428571428572, 'epoch': 0.07}
  7%|â–‹         | 359/5000 [2:23:28<29:24:12, 22.81s/it]  7%|â–‹         | 360/5000 [2:23:51<29:36:40, 22.97s/it]                                                       {'loss': 5.066, 'grad_norm': 0.17514006793498993, 'learning_rate': 0.0009469387755102042, 'epoch': 0.07}
  7%|â–‹         | 360/5000 [2:23:51<29:36:40, 22.97s/it]  7%|â–‹         | 361/5000 [2:24:18<31:18:39, 24.30s/it]                                                       {'loss': 5.055, 'grad_norm': 0.16493330895900726, 'learning_rate': 0.0009467346938775511, 'epoch': 0.07}
  7%|â–‹         | 361/5000 [2:24:18<31:18:39, 24.30s/it]  7%|â–‹         | 362/5000 [2:24:43<31:20:25, 24.33s/it]                                                       {'loss': 5.0182, 'grad_norm': 0.2171672135591507, 'learning_rate': 0.000946530612244898, 'epoch': 0.07}
  7%|â–‹         | 362/5000 [2:24:43<31:20:25, 24.33s/it]  7%|â–‹         | 363/5000 [2:25:06<30:58:33, 24.05s/it]                                                       {'loss': 5.0131, 'grad_norm': 0.21147103607654572, 'learning_rate': 0.0009463265306122449, 'epoch': 0.07}
  7%|â–‹         | 363/5000 [2:25:06<30:58:33, 24.05s/it]  7%|â–‹         | 364/5000 [2:25:27<29:35:57, 22.98s/it]                                                       {'loss': 5.0101, 'grad_norm': 0.15190599858760834, 'learning_rate': 0.0009461224489795919, 'epoch': 0.07}
  7%|â–‹         | 364/5000 [2:25:27<29:35:57, 22.98s/it]  7%|â–‹         | 365/5000 [2:25:53<30:57:49, 24.05s/it]                                                       {'loss': 5.0473, 'grad_norm': 0.2336573451757431, 'learning_rate': 0.0009459183673469388, 'epoch': 0.07}
  7%|â–‹         | 365/5000 [2:25:53<30:57:49, 24.05s/it]  7%|â–‹         | 366/5000 [2:26:27<34:41:17, 26.95s/it]                                                       {'loss': 5.0508, 'grad_norm': 0.1879369616508484, 'learning_rate': 0.0009457142857142857, 'epoch': 0.07}
  7%|â–‹         | 366/5000 [2:26:27<34:41:17, 26.95s/it]  7%|â–‹         | 367/5000 [2:26:51<33:31:31, 26.05s/it]                                                       {'loss': 5.0096, 'grad_norm': 0.1628275215625763, 'learning_rate': 0.0009455102040816327, 'epoch': 0.07}
  7%|â–‹         | 367/5000 [2:26:51<33:31:31, 26.05s/it]  7%|â–‹         | 368/5000 [2:27:14<32:29:07, 25.25s/it]                                                       {'loss': 4.9961, 'grad_norm': 0.1730533391237259, 'learning_rate': 0.0009453061224489796, 'epoch': 0.07}
  7%|â–‹         | 368/5000 [2:27:14<32:29:07, 25.25s/it]  7%|â–‹         | 369/5000 [2:27:36<31:17:49, 24.33s/it]                                                       {'loss': 5.0582, 'grad_norm': 0.16490621864795685, 'learning_rate': 0.0009451020408163265, 'epoch': 0.07}
  7%|â–‹         | 369/5000 [2:27:36<31:17:49, 24.33s/it]  7%|â–‹         | 370/5000 [2:27:59<30:27:48, 23.69s/it]                                                       {'loss': 5.0413, 'grad_norm': 0.2234056293964386, 'learning_rate': 0.0009448979591836734, 'epoch': 0.07}
  7%|â–‹         | 370/5000 [2:27:59<30:27:48, 23.69s/it]  7%|â–‹         | 371/5000 [2:28:21<29:52:09, 23.23s/it]                                                       {'loss': 5.0301, 'grad_norm': 0.16244055330753326, 'learning_rate': 0.0009446938775510204, 'epoch': 0.07}
  7%|â–‹         | 371/5000 [2:28:21<29:52:09, 23.23s/it]  7%|â–‹         | 372/5000 [2:28:43<29:24:36, 22.88s/it]                                                       {'loss': 5.0447, 'grad_norm': 0.18343332409858704, 'learning_rate': 0.0009444897959183674, 'epoch': 0.07}
  7%|â–‹         | 372/5000 [2:28:43<29:24:36, 22.88s/it]  7%|â–‹         | 373/5000 [2:29:07<29:46:45, 23.17s/it]                                                       {'loss': 5.025, 'grad_norm': 0.1601964682340622, 'learning_rate': 0.0009442857142857143, 'epoch': 0.07}
  7%|â–‹         | 373/5000 [2:29:07<29:46:45, 23.17s/it]  7%|â–‹         | 374/5000 [2:29:35<31:43:39, 24.69s/it]                                                       {'loss': 5.0117, 'grad_norm': 0.15590739250183105, 'learning_rate': 0.0009440816326530613, 'epoch': 0.07}
  7%|â–‹         | 374/5000 [2:29:35<31:43:39, 24.69s/it]  8%|â–Š         | 375/5000 [2:29:59<31:34:35, 24.58s/it]                                                       {'loss': 5.0112, 'grad_norm': 0.18176046013832092, 'learning_rate': 0.0009438775510204082, 'epoch': 0.07}
  8%|â–Š         | 375/5000 [2:29:59<31:34:35, 24.58s/it]  8%|â–Š         | 376/5000 [2:30:26<32:21:05, 25.19s/it]                                                       {'loss': 4.9842, 'grad_norm': 0.15306678414344788, 'learning_rate': 0.0009436734693877551, 'epoch': 0.08}
  8%|â–Š         | 376/5000 [2:30:26<32:21:05, 25.19s/it]  8%|â–Š         | 377/5000 [2:30:47<30:53:50, 24.06s/it]                                                       {'loss': 4.981, 'grad_norm': 0.1401280164718628, 'learning_rate': 0.0009434693877551021, 'epoch': 0.08}
  8%|â–Š         | 377/5000 [2:30:47<30:53:50, 24.06s/it]  8%|â–Š         | 378/5000 [2:31:12<31:00:43, 24.15s/it]                                                       {'loss': 5.0012, 'grad_norm': 0.15990892052650452, 'learning_rate': 0.000943265306122449, 'epoch': 0.08}
  8%|â–Š         | 378/5000 [2:31:12<31:00:43, 24.15s/it]  8%|â–Š         | 379/5000 [2:31:34<30:09:31, 23.50s/it]                                                       {'loss': 5.0538, 'grad_norm': 0.1816605180501938, 'learning_rate': 0.0009430612244897959, 'epoch': 0.08}
  8%|â–Š         | 379/5000 [2:31:34<30:09:31, 23.50s/it]  8%|â–Š         | 380/5000 [2:31:57<30:01:30, 23.40s/it]                                                       {'loss': 5.0207, 'grad_norm': 0.17161282896995544, 'learning_rate': 0.0009428571428571429, 'epoch': 0.08}
  8%|â–Š         | 380/5000 [2:31:57<30:01:30, 23.40s/it]  8%|â–Š         | 381/5000 [2:32:19<29:30:49, 23.00s/it]                                                       {'loss': 4.9964, 'grad_norm': 0.15834850072860718, 'learning_rate': 0.0009426530612244898, 'epoch': 0.08}
  8%|â–Š         | 381/5000 [2:32:19<29:30:49, 23.00s/it]  8%|â–Š         | 382/5000 [2:32:43<29:50:51, 23.27s/it]                                                       {'loss': 5.0407, 'grad_norm': 0.2772758901119232, 'learning_rate': 0.0009424489795918367, 'epoch': 0.08}
  8%|â–Š         | 382/5000 [2:32:43<29:50:51, 23.27s/it]  8%|â–Š         | 383/5000 [2:33:09<30:55:10, 24.11s/it]                                                       {'loss': 5.0259, 'grad_norm': 0.1339217722415924, 'learning_rate': 0.0009422448979591836, 'epoch': 0.08}
  8%|â–Š         | 383/5000 [2:33:09<30:55:10, 24.11s/it]  8%|â–Š         | 384/5000 [2:33:30<29:38:56, 23.12s/it]                                                       {'loss': 4.9936, 'grad_norm': 0.16941457986831665, 'learning_rate': 0.0009420408163265306, 'epoch': 0.08}
  8%|â–Š         | 384/5000 [2:33:30<29:38:56, 23.12s/it]  8%|â–Š         | 385/5000 [2:33:56<30:54:01, 24.10s/it]                                                       {'loss': 5.0384, 'grad_norm': 0.1845245063304901, 'learning_rate': 0.0009418367346938775, 'epoch': 0.08}
  8%|â–Š         | 385/5000 [2:33:56<30:54:01, 24.10s/it]  8%|â–Š         | 386/5000 [2:34:17<29:48:47, 23.26s/it]                                                       {'loss': 4.9954, 'grad_norm': 0.1390378475189209, 'learning_rate': 0.0009416326530612244, 'epoch': 0.08}
  8%|â–Š         | 386/5000 [2:34:17<29:48:47, 23.26s/it]  8%|â–Š         | 387/5000 [2:34:42<30:20:42, 23.68s/it]                                                       {'loss': 5.0037, 'grad_norm': 0.134634867310524, 'learning_rate': 0.0009414285714285715, 'epoch': 0.08}
  8%|â–Š         | 387/5000 [2:34:42<30:20:42, 23.68s/it]  8%|â–Š         | 388/5000 [2:35:03<29:22:49, 22.93s/it]                                                       {'loss': 4.9907, 'grad_norm': 0.1455029398202896, 'learning_rate': 0.0009412244897959184, 'epoch': 0.08}
  8%|â–Š         | 388/5000 [2:35:03<29:22:49, 22.93s/it]  8%|â–Š         | 389/5000 [2:35:27<29:52:06, 23.32s/it]                                                       {'loss': 5.0429, 'grad_norm': 0.1760018765926361, 'learning_rate': 0.0009410204081632653, 'epoch': 0.08}
  8%|â–Š         | 389/5000 [2:35:27<29:52:06, 23.32s/it]  8%|â–Š         | 390/5000 [2:35:53<30:50:20, 24.08s/it]                                                       {'loss': 5.0376, 'grad_norm': 0.1746876835823059, 'learning_rate': 0.0009408163265306123, 'epoch': 0.08}
  8%|â–Š         | 390/5000 [2:35:53<30:50:20, 24.08s/it]  8%|â–Š         | 391/5000 [2:36:15<30:05:36, 23.51s/it]                                                       {'loss': 5.0443, 'grad_norm': 0.17016461491584778, 'learning_rate': 0.0009406122448979592, 'epoch': 0.08}
  8%|â–Š         | 391/5000 [2:36:15<30:05:36, 23.51s/it]  8%|â–Š         | 392/5000 [2:36:41<30:54:18, 24.14s/it]                                                       {'loss': 5.0239, 'grad_norm': 0.15951530635356903, 'learning_rate': 0.0009404081632653061, 'epoch': 0.08}
  8%|â–Š         | 392/5000 [2:36:41<30:54:18, 24.14s/it]  8%|â–Š         | 393/5000 [2:37:03<29:55:37, 23.39s/it]                                                       {'loss': 4.9697, 'grad_norm': 0.1371789127588272, 'learning_rate': 0.000940204081632653, 'epoch': 0.08}
  8%|â–Š         | 393/5000 [2:37:03<29:55:37, 23.39s/it]  8%|â–Š         | 394/5000 [2:37:24<29:14:40, 22.86s/it]                                                       {'loss': 4.9863, 'grad_norm': 0.14642800390720367, 'learning_rate': 0.00094, 'epoch': 0.08}
  8%|â–Š         | 394/5000 [2:37:24<29:14:40, 22.86s/it]  8%|â–Š         | 395/5000 [2:37:47<29:20:09, 22.93s/it]                                                       {'loss': 5.0151, 'grad_norm': 0.15237945318222046, 'learning_rate': 0.000939795918367347, 'epoch': 0.08}
  8%|â–Š         | 395/5000 [2:37:47<29:20:09, 22.93s/it]  8%|â–Š         | 396/5000 [2:38:13<30:24:10, 23.77s/it]                                                       {'loss': 4.9559, 'grad_norm': 0.12122855335474014, 'learning_rate': 0.0009395918367346939, 'epoch': 0.08}
  8%|â–Š         | 396/5000 [2:38:13<30:24:10, 23.77s/it]  8%|â–Š         | 397/5000 [2:38:39<31:20:16, 24.51s/it]                                                       {'loss': 4.9693, 'grad_norm': 0.11824031174182892, 'learning_rate': 0.0009393877551020409, 'epoch': 0.08}
  8%|â–Š         | 397/5000 [2:38:39<31:20:16, 24.51s/it]  8%|â–Š         | 398/5000 [2:39:04<31:17:54, 24.48s/it]                                                       {'loss': 5.0144, 'grad_norm': 0.13505607843399048, 'learning_rate': 0.0009391836734693878, 'epoch': 0.08}
  8%|â–Š         | 398/5000 [2:39:04<31:17:54, 24.48s/it]  8%|â–Š         | 399/5000 [2:39:29<31:37:39, 24.75s/it]                                                       {'loss': 4.9642, 'grad_norm': 0.11672700196504593, 'learning_rate': 0.0009389795918367347, 'epoch': 0.08}
  8%|â–Š         | 399/5000 [2:39:29<31:37:39, 24.75s/it]  8%|â–Š         | 400/5000 [2:39:50<30:13:49, 23.66s/it]                                                       {'loss': 4.9738, 'grad_norm': 0.13312451541423798, 'learning_rate': 0.0009387755102040817, 'epoch': 0.08}
  8%|â–Š         | 400/5000 [2:39:50<30:13:49, 23.66s/it][2025-10-19 20:26:49,560] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 401/5000 [2:40:17<31:31:32, 24.68s/it]                                                       {'loss': 5.0112, 'grad_norm': 0.16100940108299255, 'learning_rate': 0.0009385714285714286, 'epoch': 0.08}
  8%|â–Š         | 401/5000 [2:40:17<31:31:32, 24.68s/it]  8%|â–Š         | 402/5000 [2:40:41<31:13:04, 24.44s/it]                                                       {'loss': 4.9906, 'grad_norm': 0.2195955514907837, 'learning_rate': 0.0009383673469387755, 'epoch': 0.08}
  8%|â–Š         | 402/5000 [2:40:41<31:13:04, 24.44s/it]  8%|â–Š         | 403/5000 [2:41:03<30:00:24, 23.50s/it]                                                       {'loss': 5.0066, 'grad_norm': 0.20467303693294525, 'learning_rate': 0.0009381632653061226, 'epoch': 0.08}
  8%|â–Š         | 403/5000 [2:41:03<30:00:24, 23.50s/it]  8%|â–Š         | 404/5000 [2:41:25<29:32:24, 23.14s/it]                                                       {'loss': 4.9729, 'grad_norm': 0.10146403312683105, 'learning_rate': 0.0009379591836734695, 'epoch': 0.08}
  8%|â–Š         | 404/5000 [2:41:25<29:32:24, 23.14s/it]  8%|â–Š         | 405/5000 [2:41:48<29:37:53, 23.22s/it]                                                       {'loss': 4.9858, 'grad_norm': 0.1816038340330124, 'learning_rate': 0.0009377551020408164, 'epoch': 0.08}
  8%|â–Š         | 405/5000 [2:41:48<29:37:53, 23.22s/it]  8%|â–Š         | 406/5000 [2:42:17<31:39:11, 24.80s/it]                                                       {'loss': 5.0082, 'grad_norm': 0.17202122509479523, 'learning_rate': 0.0009375510204081633, 'epoch': 0.08}
  8%|â–Š         | 406/5000 [2:42:17<31:39:11, 24.80s/it]  8%|â–Š         | 407/5000 [2:42:38<30:19:30, 23.77s/it]                                                       {'loss': 5.0042, 'grad_norm': 0.15736699104309082, 'learning_rate': 0.0009373469387755103, 'epoch': 0.08}
  8%|â–Š         | 407/5000 [2:42:38<30:19:30, 23.77s/it]  8%|â–Š         | 408/5000 [2:43:05<31:31:59, 24.72s/it]                                                       {'loss': 4.9998, 'grad_norm': 0.15588927268981934, 'learning_rate': 0.0009371428571428572, 'epoch': 0.08}
  8%|â–Š         | 408/5000 [2:43:05<31:31:59, 24.72s/it]  8%|â–Š         | 409/5000 [2:43:26<29:55:17, 23.46s/it]                                                       {'loss': 4.9659, 'grad_norm': 0.12826837599277496, 'learning_rate': 0.0009369387755102041, 'epoch': 0.08}
  8%|â–Š         | 409/5000 [2:43:26<29:55:17, 23.46s/it]  8%|â–Š         | 410/5000 [2:43:52<31:10:26, 24.45s/it]                                                       {'loss': 5.0457, 'grad_norm': 0.17806042730808258, 'learning_rate': 0.0009367346938775511, 'epoch': 0.08}
  8%|â–Š         | 410/5000 [2:43:52<31:10:26, 24.45s/it]  8%|â–Š         | 411/5000 [2:44:16<31:00:06, 24.32s/it]                                                       {'loss': 5.0118, 'grad_norm': 0.2415006160736084, 'learning_rate': 0.000936530612244898, 'epoch': 0.08}
  8%|â–Š         | 411/5000 [2:44:16<31:00:06, 24.32s/it]  8%|â–Š         | 412/5000 [2:44:39<30:10:59, 23.68s/it]                                                       {'loss': 4.9933, 'grad_norm': 0.13488835096359253, 'learning_rate': 0.0009363265306122449, 'epoch': 0.08}
  8%|â–Š         | 412/5000 [2:44:39<30:10:59, 23.68s/it]  8%|â–Š         | 413/5000 [2:45:00<29:14:14, 22.95s/it]                                                       {'loss': 4.9547, 'grad_norm': 0.12343719601631165, 'learning_rate': 0.0009361224489795918, 'epoch': 0.08}
  8%|â–Š         | 413/5000 [2:45:00<29:14:14, 22.95s/it]  8%|â–Š         | 414/5000 [2:45:21<28:34:26, 22.43s/it]                                                       {'loss': 4.9783, 'grad_norm': 0.12056264281272888, 'learning_rate': 0.0009359183673469388, 'epoch': 0.08}
  8%|â–Š         | 414/5000 [2:45:21<28:34:26, 22.43s/it]  8%|â–Š         | 415/5000 [2:45:45<29:07:40, 22.87s/it]                                                       {'loss': 5.0114, 'grad_norm': 0.14258424937725067, 'learning_rate': 0.0009357142857142857, 'epoch': 0.08}
  8%|â–Š         | 415/5000 [2:45:45<29:07:40, 22.87s/it]  8%|â–Š         | 416/5000 [2:46:12<30:37:38, 24.05s/it]                                                       {'loss': 4.9796, 'grad_norm': 0.1265740841627121, 'learning_rate': 0.0009355102040816326, 'epoch': 0.08}
  8%|â–Š         | 416/5000 [2:46:12<30:37:38, 24.05s/it]  8%|â–Š         | 417/5000 [2:46:36<30:42:11, 24.12s/it]                                                       {'loss': 5.0017, 'grad_norm': 0.2048185169696808, 'learning_rate': 0.0009353061224489796, 'epoch': 0.08}
  8%|â–Š         | 417/5000 [2:46:36<30:42:11, 24.12s/it]  8%|â–Š         | 418/5000 [2:46:57<29:29:26, 23.17s/it]                                                       {'loss': 5.0024, 'grad_norm': 0.14868766069412231, 'learning_rate': 0.0009351020408163266, 'epoch': 0.08}
  8%|â–Š         | 418/5000 [2:46:57<29:29:26, 23.17s/it]  8%|â–Š         | 419/5000 [2:47:23<30:36:10, 24.05s/it]                                                       {'loss': 4.9737, 'grad_norm': 0.11926103383302689, 'learning_rate': 0.0009348979591836735, 'epoch': 0.08}
  8%|â–Š         | 419/5000 [2:47:23<30:36:10, 24.05s/it]  8%|â–Š         | 420/5000 [2:47:48<30:50:30, 24.24s/it]                                                       {'loss': 4.9919, 'grad_norm': 0.1619342416524887, 'learning_rate': 0.0009346938775510205, 'epoch': 0.08}
  8%|â–Š         | 420/5000 [2:47:48<30:50:30, 24.24s/it]  8%|â–Š         | 421/5000 [2:48:09<29:41:26, 23.34s/it]                                                       {'loss': 4.9816, 'grad_norm': 0.15227742493152618, 'learning_rate': 0.0009344897959183674, 'epoch': 0.08}
  8%|â–Š         | 421/5000 [2:48:09<29:41:26, 23.34s/it]  8%|â–Š         | 422/5000 [2:48:38<31:40:10, 24.90s/it]                                                       {'loss': 5.0197, 'grad_norm': 0.14790302515029907, 'learning_rate': 0.0009342857142857143, 'epoch': 0.08}
  8%|â–Š         | 422/5000 [2:48:38<31:40:10, 24.90s/it]  8%|â–Š         | 423/5000 [2:49:06<33:05:22, 26.03s/it]                                                       {'loss': 4.9964, 'grad_norm': 0.1410030871629715, 'learning_rate': 0.0009340816326530612, 'epoch': 0.08}
  8%|â–Š         | 423/5000 [2:49:06<33:05:22, 26.03s/it]  8%|â–Š         | 424/5000 [2:49:30<32:13:38, 25.35s/it]                                                       {'loss': 4.9488, 'grad_norm': 0.10708735138177872, 'learning_rate': 0.0009338775510204082, 'epoch': 0.08}
  8%|â–Š         | 424/5000 [2:49:30<32:13:38, 25.35s/it]  8%|â–Š         | 425/5000 [2:50:00<33:56:30, 26.71s/it]                                                       {'loss': 5.0017, 'grad_norm': 0.11692824214696884, 'learning_rate': 0.0009336734693877551, 'epoch': 0.09}
  8%|â–Š         | 425/5000 [2:50:00<33:56:30, 26.71s/it]  9%|â–Š         | 426/5000 [2:50:28<34:18:00, 27.00s/it]                                                       {'loss': 4.9811, 'grad_norm': 0.10356064140796661, 'learning_rate': 0.000933469387755102, 'epoch': 0.09}
  9%|â–Š         | 426/5000 [2:50:28<34:18:00, 27.00s/it]  9%|â–Š         | 427/5000 [2:50:49<32:08:36, 25.30s/it]                                                       {'loss': 4.9709, 'grad_norm': 0.13116037845611572, 'learning_rate': 0.000933265306122449, 'epoch': 0.09}
  9%|â–Š         | 427/5000 [2:50:49<32:08:36, 25.30s/it]  9%|â–Š         | 428/5000 [2:51:14<32:14:56, 25.39s/it]                                                       {'loss': 4.9523, 'grad_norm': 0.10194238275289536, 'learning_rate': 0.0009330612244897959, 'epoch': 0.09}
  9%|â–Š         | 428/5000 [2:51:14<32:14:56, 25.39s/it]  9%|â–Š         | 429/5000 [2:51:38<31:32:28, 24.84s/it]                                                       {'loss': 4.9831, 'grad_norm': 0.1976218968629837, 'learning_rate': 0.0009328571428571428, 'epoch': 0.09}
  9%|â–Š         | 429/5000 [2:51:38<31:32:28, 24.84s/it]  9%|â–Š         | 430/5000 [2:52:03<31:44:45, 25.01s/it]                                                       {'loss': 5.0005, 'grad_norm': 0.13956978917121887, 'learning_rate': 0.0009326530612244898, 'epoch': 0.09}
  9%|â–Š         | 430/5000 [2:52:03<31:44:45, 25.01s/it]  9%|â–Š         | 431/5000 [2:52:26<30:56:42, 24.38s/it]                                                       {'loss': 4.9665, 'grad_norm': 0.1550287902355194, 'learning_rate': 0.0009324489795918367, 'epoch': 0.09}
  9%|â–Š         | 431/5000 [2:52:26<30:56:42, 24.38s/it]  9%|â–Š         | 432/5000 [2:52:53<31:39:53, 24.95s/it]                                                       {'loss': 4.9796, 'grad_norm': 0.13972137868404388, 'learning_rate': 0.0009322448979591836, 'epoch': 0.09}
  9%|â–Š         | 432/5000 [2:52:53<31:39:53, 24.95s/it]  9%|â–Š         | 433/5000 [2:53:14<30:23:59, 23.96s/it]                                                       {'loss': 4.9604, 'grad_norm': 0.14827384054660797, 'learning_rate': 0.0009320408163265305, 'epoch': 0.09}
  9%|â–Š         | 433/5000 [2:53:14<30:23:59, 23.96s/it]  9%|â–Š         | 434/5000 [2:53:37<29:52:55, 23.56s/it]                                                       {'loss': 4.9699, 'grad_norm': 0.19789576530456543, 'learning_rate': 0.0009318367346938776, 'epoch': 0.09}
  9%|â–Š         | 434/5000 [2:53:37<29:52:55, 23.56s/it]  9%|â–Š         | 435/5000 [2:54:00<29:52:55, 23.57s/it]                                                       {'loss': 4.991, 'grad_norm': 0.15499833226203918, 'learning_rate': 0.0009316326530612245, 'epoch': 0.09}
  9%|â–Š         | 435/5000 [2:54:00<29:52:55, 23.57s/it]  9%|â–Š         | 436/5000 [2:54:23<29:27:09, 23.23s/it]                                                       {'loss': 4.9893, 'grad_norm': 0.1288776695728302, 'learning_rate': 0.0009314285714285714, 'epoch': 0.09}
  9%|â–Š         | 436/5000 [2:54:23<29:27:09, 23.23s/it]  9%|â–Š         | 437/5000 [2:54:44<28:26:40, 22.44s/it]                                                       {'loss': 4.9688, 'grad_norm': 0.1509087234735489, 'learning_rate': 0.0009312244897959184, 'epoch': 0.09}
  9%|â–Š         | 437/5000 [2:54:44<28:26:40, 22.44s/it]  9%|â–‰         | 438/5000 [2:55:08<29:03:31, 22.93s/it]                                                       {'loss': 4.998, 'grad_norm': 0.12164633721113205, 'learning_rate': 0.0009310204081632653, 'epoch': 0.09}
  9%|â–‰         | 438/5000 [2:55:08<29:03:31, 22.93s/it]  9%|â–‰         | 439/5000 [2:55:30<28:52:39, 22.79s/it]                                                       {'loss': 4.9434, 'grad_norm': 0.1232718825340271, 'learning_rate': 0.0009308163265306122, 'epoch': 0.09}
  9%|â–‰         | 439/5000 [2:55:30<28:52:39, 22.79s/it]  9%|â–‰         | 440/5000 [2:55:53<28:46:35, 22.72s/it]                                                       {'loss': 4.9995, 'grad_norm': 0.2218555510044098, 'learning_rate': 0.0009306122448979592, 'epoch': 0.09}
  9%|â–‰         | 440/5000 [2:55:53<28:46:35, 22.72s/it]  9%|â–‰         | 441/5000 [2:56:18<29:50:38, 23.57s/it]                                                       {'loss': 5.0016, 'grad_norm': 0.15411753952503204, 'learning_rate': 0.0009304081632653062, 'epoch': 0.09}
  9%|â–‰         | 441/5000 [2:56:18<29:50:38, 23.57s/it]  9%|â–‰         | 442/5000 [2:56:40<29:13:48, 23.09s/it]                                                       {'loss': 4.9821, 'grad_norm': 0.16279618442058563, 'learning_rate': 0.0009302040816326531, 'epoch': 0.09}
  9%|â–‰         | 442/5000 [2:56:40<29:13:48, 23.09s/it]  9%|â–‰         | 443/5000 [2:57:02<28:42:59, 22.69s/it]                                                       {'loss': 4.948, 'grad_norm': 0.10540179163217545, 'learning_rate': 0.00093, 'epoch': 0.09}
  9%|â–‰         | 443/5000 [2:57:02<28:42:59, 22.69s/it]  9%|â–‰         | 444/5000 [2:57:30<30:44:03, 24.29s/it]                                                       {'loss': 4.9806, 'grad_norm': 0.11030115187168121, 'learning_rate': 0.000929795918367347, 'epoch': 0.09}
  9%|â–‰         | 444/5000 [2:57:30<30:44:03, 24.29s/it]  9%|â–‰         | 445/5000 [2:57:51<29:39:08, 23.44s/it]                                                       {'loss': 4.954, 'grad_norm': 0.12142212688922882, 'learning_rate': 0.0009295918367346939, 'epoch': 0.09}
  9%|â–‰         | 445/5000 [2:57:51<29:39:08, 23.44s/it]  9%|â–‰         | 446/5000 [2:58:16<29:57:00, 23.68s/it]                                                       {'loss': 4.9764, 'grad_norm': 0.1561078429222107, 'learning_rate': 0.0009293877551020408, 'epoch': 0.09}
  9%|â–‰         | 446/5000 [2:58:16<29:57:00, 23.68s/it]  9%|â–‰         | 447/5000 [2:58:41<30:45:52, 24.33s/it]                                                       {'loss': 4.9375, 'grad_norm': 0.09038520604372025, 'learning_rate': 0.0009291836734693878, 'epoch': 0.09}
  9%|â–‰         | 447/5000 [2:58:41<30:45:52, 24.33s/it]  9%|â–‰         | 448/5000 [2:59:06<30:51:13, 24.40s/it]                                                       {'loss': 4.9512, 'grad_norm': 0.1724969893693924, 'learning_rate': 0.0009289795918367347, 'epoch': 0.09}
  9%|â–‰         | 448/5000 [2:59:06<30:51:13, 24.40s/it]  9%|â–‰         | 449/5000 [2:59:27<29:33:38, 23.38s/it]                                                       {'loss': 4.9439, 'grad_norm': 0.09863235801458359, 'learning_rate': 0.0009287755102040817, 'epoch': 0.09}
  9%|â–‰         | 449/5000 [2:59:27<29:33:38, 23.38s/it]  9%|â–‰         | 450/5000 [2:59:51<29:53:42, 23.65s/it]                                                       {'loss': 4.9215, 'grad_norm': 0.08926242589950562, 'learning_rate': 0.0009285714285714287, 'epoch': 0.09}
  9%|â–‰         | 450/5000 [2:59:51<29:53:42, 23.65s/it][2025-10-19 20:46:50,529] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 451/5000 [3:00:17<30:36:15, 24.22s/it]                                                       {'loss': 4.9332, 'grad_norm': 0.0895267128944397, 'learning_rate': 0.0009283673469387756, 'epoch': 0.09}
  9%|â–‰         | 451/5000 [3:00:17<30:36:15, 24.22s/it]  9%|â–‰         | 452/5000 [3:00:49<33:27:42, 26.49s/it]                                                       {'loss': 4.9542, 'grad_norm': 0.10060038417577744, 'learning_rate': 0.0009281632653061225, 'epoch': 0.09}
  9%|â–‰         | 452/5000 [3:00:49<33:27:42, 26.49s/it]  9%|â–‰         | 453/5000 [3:01:10<31:37:37, 25.04s/it]                                                       {'loss': 4.9768, 'grad_norm': 0.15524569153785706, 'learning_rate': 0.0009279591836734695, 'epoch': 0.09}
  9%|â–‰         | 453/5000 [3:01:10<31:37:37, 25.04s/it]  9%|â–‰         | 454/5000 [3:01:34<31:02:30, 24.58s/it]                                                       {'loss': 4.9674, 'grad_norm': 0.19015678763389587, 'learning_rate': 0.0009277551020408164, 'epoch': 0.09}
  9%|â–‰         | 454/5000 [3:01:34<31:02:30, 24.58s/it]  9%|â–‰         | 455/5000 [3:02:01<32:11:58, 25.50s/it]                                                       {'loss': 4.9716, 'grad_norm': 0.14730077981948853, 'learning_rate': 0.0009275510204081633, 'epoch': 0.09}
  9%|â–‰         | 455/5000 [3:02:01<32:11:58, 25.50s/it]  9%|â–‰         | 456/5000 [3:02:25<31:37:50, 25.06s/it]                                                       {'loss': 4.9466, 'grad_norm': 0.12544536590576172, 'learning_rate': 0.0009273469387755102, 'epoch': 0.09}
  9%|â–‰         | 456/5000 [3:02:25<31:37:50, 25.06s/it]  9%|â–‰         | 457/5000 [3:02:47<30:09:58, 23.90s/it]                                                       {'loss': 4.975, 'grad_norm': 0.13594035804271698, 'learning_rate': 0.0009271428571428572, 'epoch': 0.09}
  9%|â–‰         | 457/5000 [3:02:47<30:09:58, 23.90s/it]  9%|â–‰         | 458/5000 [3:03:11<30:18:41, 24.02s/it]                                                       {'loss': 4.9496, 'grad_norm': 0.14517176151275635, 'learning_rate': 0.0009269387755102041, 'epoch': 0.09}
  9%|â–‰         | 458/5000 [3:03:11<30:18:41, 24.02s/it]  9%|â–‰         | 459/5000 [3:03:39<31:46:54, 25.20s/it]                                                       {'loss': 5.0027, 'grad_norm': 0.14677122235298157, 'learning_rate': 0.000926734693877551, 'epoch': 0.09}
  9%|â–‰         | 459/5000 [3:03:39<31:46:54, 25.20s/it]  9%|â–‰         | 460/5000 [3:04:03<31:15:14, 24.78s/it]                                                       {'loss': 5.0053, 'grad_norm': 0.17234952747821808, 'learning_rate': 0.000926530612244898, 'epoch': 0.09}
  9%|â–‰         | 460/5000 [3:04:03<31:15:14, 24.78s/it]  9%|â–‰         | 461/5000 [3:04:30<32:10:14, 25.52s/it]                                                       {'loss': 5.0047, 'grad_norm': 0.1863064169883728, 'learning_rate': 0.0009263265306122449, 'epoch': 0.09}
  9%|â–‰         | 461/5000 [3:04:30<32:10:14, 25.52s/it]  9%|â–‰         | 462/5000 [3:04:52<30:51:09, 24.48s/it]                                                       {'loss': 4.951, 'grad_norm': 0.09892190247774124, 'learning_rate': 0.0009261224489795918, 'epoch': 0.09}
  9%|â–‰         | 462/5000 [3:04:52<30:51:09, 24.48s/it]  9%|â–‰         | 463/5000 [3:05:19<31:39:35, 25.12s/it]                                                       {'loss': 4.9936, 'grad_norm': 0.19227388501167297, 'learning_rate': 0.0009259183673469388, 'epoch': 0.09}
  9%|â–‰         | 463/5000 [3:05:19<31:39:35, 25.12s/it]  9%|â–‰         | 464/5000 [3:05:42<30:58:30, 24.58s/it]                                                       {'loss': 4.9576, 'grad_norm': 0.13156725466251373, 'learning_rate': 0.0009257142857142857, 'epoch': 0.09}
  9%|â–‰         | 464/5000 [3:05:42<30:58:30, 24.58s/it]  9%|â–‰         | 465/5000 [3:06:04<30:10:32, 23.95s/it]                                                       {'loss': 4.9645, 'grad_norm': 0.13461168110370636, 'learning_rate': 0.0009255102040816327, 'epoch': 0.09}
  9%|â–‰         | 465/5000 [3:06:04<30:10:32, 23.95s/it]  9%|â–‰         | 466/5000 [3:06:31<31:03:33, 24.66s/it]                                                       {'loss': 4.9547, 'grad_norm': 0.10611710697412491, 'learning_rate': 0.0009253061224489796, 'epoch': 0.09}
  9%|â–‰         | 466/5000 [3:06:31<31:03:33, 24.66s/it]  9%|â–‰         | 467/5000 [3:06:57<31:50:06, 25.28s/it]                                                       {'loss': 4.971, 'grad_norm': 0.10394472628831863, 'learning_rate': 0.0009251020408163266, 'epoch': 0.09}
  9%|â–‰         | 467/5000 [3:06:57<31:50:06, 25.28s/it]  9%|â–‰         | 468/5000 [3:07:19<30:19:07, 24.08s/it]                                                       {'loss': 4.9388, 'grad_norm': 0.11180302500724792, 'learning_rate': 0.0009248979591836735, 'epoch': 0.09}
  9%|â–‰         | 468/5000 [3:07:19<30:19:07, 24.08s/it]  9%|â–‰         | 469/5000 [3:07:43<30:12:49, 24.01s/it]                                                       {'loss': 4.9752, 'grad_norm': 0.14900827407836914, 'learning_rate': 0.0009246938775510204, 'epoch': 0.09}
  9%|â–‰         | 469/5000 [3:07:43<30:12:49, 24.01s/it]  9%|â–‰         | 470/5000 [3:08:05<29:37:39, 23.55s/it]                                                       {'loss': 4.9595, 'grad_norm': 0.13475678861141205, 'learning_rate': 0.0009244897959183674, 'epoch': 0.09}
  9%|â–‰         | 470/5000 [3:08:05<29:37:39, 23.55s/it]  9%|â–‰         | 471/5000 [3:08:28<29:17:14, 23.28s/it]                                                       {'loss': 4.9589, 'grad_norm': 0.14875927567481995, 'learning_rate': 0.0009242857142857143, 'epoch': 0.09}
  9%|â–‰         | 471/5000 [3:08:28<29:17:14, 23.28s/it]  9%|â–‰         | 472/5000 [3:08:55<30:46:33, 24.47s/it]                                                       {'loss': 5.017, 'grad_norm': 0.14699609577655792, 'learning_rate': 0.0009240816326530612, 'epoch': 0.09}
  9%|â–‰         | 472/5000 [3:08:55<30:46:33, 24.47s/it]  9%|â–‰         | 473/5000 [3:09:16<29:39:31, 23.59s/it]                                                       {'loss': 4.9845, 'grad_norm': 0.18167996406555176, 'learning_rate': 0.0009238775510204082, 'epoch': 0.09}
  9%|â–‰         | 473/5000 [3:09:16<29:39:31, 23.59s/it]  9%|â–‰         | 474/5000 [3:09:43<30:46:04, 24.47s/it]                                                       {'loss': 4.9698, 'grad_norm': 0.22375433146953583, 'learning_rate': 0.0009236734693877551, 'epoch': 0.09}
  9%|â–‰         | 474/5000 [3:09:43<30:46:04, 24.47s/it] 10%|â–‰         | 475/5000 [3:10:07<30:24:33, 24.19s/it]                                                       {'loss': 4.9558, 'grad_norm': 0.1095845103263855, 'learning_rate': 0.000923469387755102, 'epoch': 0.1}
 10%|â–‰         | 475/5000 [3:10:07<30:24:33, 24.19s/it] 10%|â–‰         | 476/5000 [3:10:28<29:22:25, 23.37s/it]                                                       {'loss': 4.934, 'grad_norm': 0.1006941944360733, 'learning_rate': 0.0009232653061224489, 'epoch': 0.1}
 10%|â–‰         | 476/5000 [3:10:28<29:22:25, 23.37s/it] 10%|â–‰         | 477/5000 [3:10:56<30:59:45, 24.67s/it]                                                       {'loss': 4.9828, 'grad_norm': 0.12244568020105362, 'learning_rate': 0.0009230612244897959, 'epoch': 0.1}
 10%|â–‰         | 477/5000 [3:10:56<30:59:45, 24.67s/it] 10%|â–‰         | 478/5000 [3:11:20<30:42:54, 24.45s/it]                                                       {'loss': 4.9656, 'grad_norm': 0.13803529739379883, 'learning_rate': 0.0009228571428571428, 'epoch': 0.1}
 10%|â–‰         | 478/5000 [3:11:20<30:42:54, 24.45s/it] 10%|â–‰         | 479/5000 [3:11:41<29:35:08, 23.56s/it]                                                       {'loss': 4.964, 'grad_norm': 0.14514927566051483, 'learning_rate': 0.0009226530612244897, 'epoch': 0.1}
 10%|â–‰         | 479/5000 [3:11:41<29:35:08, 23.56s/it] 10%|â–‰         | 480/5000 [3:12:08<30:38:00, 24.40s/it]                                                       {'loss': 4.9533, 'grad_norm': 0.0935635194182396, 'learning_rate': 0.0009224489795918367, 'epoch': 0.1}
 10%|â–‰         | 480/5000 [3:12:08<30:38:00, 24.40s/it] 10%|â–‰         | 481/5000 [3:12:32<30:48:32, 24.54s/it]                                                       {'loss': 4.9665, 'grad_norm': 0.12027670443058014, 'learning_rate': 0.0009222448979591837, 'epoch': 0.1}
 10%|â–‰         | 481/5000 [3:12:32<30:48:32, 24.54s/it] 10%|â–‰         | 482/5000 [3:12:55<30:02:05, 23.93s/it]                                                       {'loss': 4.9499, 'grad_norm': 0.10470882058143616, 'learning_rate': 0.0009220408163265306, 'epoch': 0.1}
 10%|â–‰         | 482/5000 [3:12:55<30:02:05, 23.93s/it] 10%|â–‰         | 483/5000 [3:13:17<29:16:28, 23.33s/it]                                                       {'loss': 4.9308, 'grad_norm': 0.08603028953075409, 'learning_rate': 0.0009218367346938776, 'epoch': 0.1}
 10%|â–‰         | 483/5000 [3:13:17<29:16:28, 23.33s/it] 10%|â–‰         | 484/5000 [3:13:49<32:39:54, 26.04s/it]                                                       {'loss': 4.979, 'grad_norm': 0.1442112773656845, 'learning_rate': 0.0009216326530612245, 'epoch': 0.1}
 10%|â–‰         | 484/5000 [3:13:49<32:39:54, 26.04s/it] 10%|â–‰         | 485/5000 [3:14:13<31:55:26, 25.45s/it]                                                       {'loss': 4.9446, 'grad_norm': 0.10800067335367203, 'learning_rate': 0.0009214285714285714, 'epoch': 0.1}
 10%|â–‰         | 485/5000 [3:14:13<31:55:26, 25.45s/it] 10%|â–‰         | 486/5000 [3:14:38<31:32:39, 25.16s/it]                                                       {'loss': 4.9723, 'grad_norm': 0.1529552936553955, 'learning_rate': 0.0009212244897959185, 'epoch': 0.1}
 10%|â–‰         | 486/5000 [3:14:38<31:32:39, 25.16s/it] 10%|â–‰         | 487/5000 [3:15:01<30:54:23, 24.65s/it]                                                       {'loss': 4.9677, 'grad_norm': 0.11000058799982071, 'learning_rate': 0.0009210204081632654, 'epoch': 0.1}
 10%|â–‰         | 487/5000 [3:15:01<30:54:23, 24.65s/it] 10%|â–‰         | 488/5000 [3:15:22<29:29:45, 23.53s/it]                                                       {'loss': 4.937, 'grad_norm': 0.10203085839748383, 'learning_rate': 0.0009208163265306123, 'epoch': 0.1}
 10%|â–‰         | 488/5000 [3:15:22<29:29:45, 23.53s/it] 10%|â–‰         | 489/5000 [3:15:44<28:47:02, 22.97s/it]                                                       {'loss': 4.9383, 'grad_norm': 0.09157943725585938, 'learning_rate': 0.0009206122448979592, 'epoch': 0.1}
 10%|â–‰         | 489/5000 [3:15:44<28:47:02, 22.97s/it] 10%|â–‰         | 490/5000 [3:16:07<28:49:47, 23.01s/it]                                                       {'loss': 4.9484, 'grad_norm': 0.08994344621896744, 'learning_rate': 0.0009204081632653062, 'epoch': 0.1}
 10%|â–‰         | 490/5000 [3:16:07<28:49:47, 23.01s/it] 10%|â–‰         | 491/5000 [3:16:31<29:11:59, 23.31s/it]                                                       {'loss': 4.9835, 'grad_norm': 0.15802086889743805, 'learning_rate': 0.0009202040816326531, 'epoch': 0.1}
 10%|â–‰         | 491/5000 [3:16:31<29:11:59, 23.31s/it] 10%|â–‰         | 492/5000 [3:16:56<29:49:56, 23.82s/it]                                                       {'loss': 4.9125, 'grad_norm': 0.11510312557220459, 'learning_rate': 0.00092, 'epoch': 0.1}
 10%|â–‰         | 492/5000 [3:16:56<29:49:56, 23.82s/it] 10%|â–‰         | 493/5000 [3:17:17<28:52:00, 23.06s/it]                                                       {'loss': 4.9496, 'grad_norm': 0.1639009565114975, 'learning_rate': 0.000919795918367347, 'epoch': 0.1}
 10%|â–‰         | 493/5000 [3:17:17<28:52:00, 23.06s/it] 10%|â–‰         | 494/5000 [3:17:39<28:17:54, 22.61s/it]                                                       {'loss': 4.9431, 'grad_norm': 0.08324907720088959, 'learning_rate': 0.0009195918367346939, 'epoch': 0.1}
 10%|â–‰         | 494/5000 [3:17:39<28:17:54, 22.61s/it] 10%|â–‰         | 495/5000 [3:18:02<28:33:33, 22.82s/it]                                                       {'loss': 4.9319, 'grad_norm': 0.08100157976150513, 'learning_rate': 0.0009193877551020408, 'epoch': 0.1}
 10%|â–‰         | 495/5000 [3:18:02<28:33:33, 22.82s/it] 10%|â–‰         | 496/5000 [3:18:25<28:34:54, 22.85s/it]                                                       {'loss': 4.9608, 'grad_norm': 0.09255655109882355, 'learning_rate': 0.0009191836734693879, 'epoch': 0.1}
 10%|â–‰         | 496/5000 [3:18:25<28:34:54, 22.85s/it] 10%|â–‰         | 497/5000 [3:18:47<28:06:31, 22.47s/it]                                                       {'loss': 4.9441, 'grad_norm': 0.09850013256072998, 'learning_rate': 0.0009189795918367348, 'epoch': 0.1}
 10%|â–‰         | 497/5000 [3:18:47<28:06:31, 22.47s/it] 10%|â–‰         | 498/5000 [3:19:07<27:23:50, 21.91s/it]                                                       {'loss': 4.942, 'grad_norm': 0.09583885967731476, 'learning_rate': 0.0009187755102040817, 'epoch': 0.1}
 10%|â–‰         | 498/5000 [3:19:07<27:23:50, 21.91s/it] 10%|â–‰         | 499/5000 [3:19:29<27:10:46, 21.74s/it]                                                       {'loss': 4.947, 'grad_norm': 0.11763864010572433, 'learning_rate': 0.0009185714285714286, 'epoch': 0.1}
 10%|â–‰         | 499/5000 [3:19:29<27:10:46, 21.74s/it] 10%|â–ˆ         | 500/5000 [3:19:56<29:19:13, 23.46s/it]                                                       {'loss': 4.9526, 'grad_norm': 0.17286942899227142, 'learning_rate': 0.0009183673469387756, 'epoch': 0.1}
 10%|â–ˆ         | 500/5000 [3:19:56<29:19:13, 23.46s/it][2025-10-19 21:06:55,229] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 501/5000 [3:20:18<28:51:28, 23.09s/it]                                                       {'loss': 4.9599, 'grad_norm': 0.12219934910535812, 'learning_rate': 0.0009181632653061225, 'epoch': 0.1}
 10%|â–ˆ         | 501/5000 [3:20:18<28:51:28, 23.09s/it] 10%|â–ˆ         | 502/5000 [3:20:40<28:12:35, 22.58s/it]                                                       {'loss': 4.9399, 'grad_norm': 0.09200235456228256, 'learning_rate': 0.0009179591836734694, 'epoch': 0.1}
 10%|â–ˆ         | 502/5000 [3:20:40<28:12:35, 22.58s/it] 10%|â–ˆ         | 503/5000 [3:21:10<31:16:21, 25.03s/it]                                                       {'loss': 4.9865, 'grad_norm': 0.14256885647773743, 'learning_rate': 0.0009177551020408164, 'epoch': 0.1}
 10%|â–ˆ         | 503/5000 [3:21:10<31:16:21, 25.03s/it] 10%|â–ˆ         | 504/5000 [3:21:36<31:20:26, 25.09s/it]                                                       {'loss': 4.9632, 'grad_norm': 0.12083348631858826, 'learning_rate': 0.0009175510204081633, 'epoch': 0.1}
 10%|â–ˆ         | 504/5000 [3:21:36<31:20:26, 25.09s/it] 10%|â–ˆ         | 505/5000 [3:21:58<30:09:08, 24.15s/it]                                                       {'loss': 4.955, 'grad_norm': 0.21630282700061798, 'learning_rate': 0.0009173469387755102, 'epoch': 0.1}
 10%|â–ˆ         | 505/5000 [3:21:58<30:09:08, 24.15s/it] 10%|â–ˆ         | 506/5000 [3:22:24<31:05:46, 24.91s/it]                                                       {'loss': 4.9367, 'grad_norm': 0.11154809594154358, 'learning_rate': 0.0009171428571428571, 'epoch': 0.1}
 10%|â–ˆ         | 506/5000 [3:22:24<31:05:46, 24.91s/it] 10%|â–ˆ         | 507/5000 [3:22:48<30:38:05, 24.55s/it]                                                       {'loss': 4.9668, 'grad_norm': 0.12685781717300415, 'learning_rate': 0.0009169387755102041, 'epoch': 0.1}
 10%|â–ˆ         | 507/5000 [3:22:48<30:38:05, 24.55s/it] 10%|â–ˆ         | 508/5000 [3:23:13<30:49:44, 24.71s/it]                                                       {'loss': 4.9552, 'grad_norm': 0.1010071411728859, 'learning_rate': 0.000916734693877551, 'epoch': 0.1}
 10%|â–ˆ         | 508/5000 [3:23:13<30:49:44, 24.71s/it] 10%|â–ˆ         | 509/5000 [3:23:38<30:55:24, 24.79s/it]                                                       {'loss': 4.9388, 'grad_norm': 0.1835879385471344, 'learning_rate': 0.0009165306122448979, 'epoch': 0.1}
 10%|â–ˆ         | 509/5000 [3:23:38<30:55:24, 24.79s/it] 10%|â–ˆ         | 510/5000 [3:24:01<30:24:22, 24.38s/it]                                                       {'loss': 4.9584, 'grad_norm': 0.1728401929140091, 'learning_rate': 0.0009163265306122449, 'epoch': 0.1}
 10%|â–ˆ         | 510/5000 [3:24:01<30:24:22, 24.38s/it] 10%|â–ˆ         | 511/5000 [3:24:25<29:58:58, 24.05s/it]                                                       {'loss': 4.9682, 'grad_norm': 0.13912414014339447, 'learning_rate': 0.0009161224489795919, 'epoch': 0.1}
 10%|â–ˆ         | 511/5000 [3:24:25<29:58:58, 24.05s/it] 10%|â–ˆ         | 512/5000 [3:24:46<28:48:50, 23.11s/it]                                                       {'loss': 4.9207, 'grad_norm': 0.08907292783260345, 'learning_rate': 0.0009159183673469388, 'epoch': 0.1}
 10%|â–ˆ         | 512/5000 [3:24:46<28:48:50, 23.11s/it] 10%|â–ˆ         | 513/5000 [3:25:12<30:00:19, 24.07s/it]                                                       {'loss': 4.963, 'grad_norm': 0.15545791387557983, 'learning_rate': 0.0009157142857142858, 'epoch': 0.1}
 10%|â–ˆ         | 513/5000 [3:25:12<30:00:19, 24.07s/it] 10%|â–ˆ         | 514/5000 [3:25:37<30:14:42, 24.27s/it]                                                       {'loss': 4.9565, 'grad_norm': 0.11210370808839798, 'learning_rate': 0.0009155102040816327, 'epoch': 0.1}
 10%|â–ˆ         | 514/5000 [3:25:37<30:14:42, 24.27s/it] 10%|â–ˆ         | 515/5000 [3:26:04<31:19:40, 25.15s/it]                                                       {'loss': 4.9041, 'grad_norm': 0.08433163911104202, 'learning_rate': 0.0009153061224489796, 'epoch': 0.1}
 10%|â–ˆ         | 515/5000 [3:26:04<31:19:40, 25.15s/it] 10%|â–ˆ         | 516/5000 [3:26:26<30:12:53, 24.26s/it]                                                       {'loss': 4.9391, 'grad_norm': 0.1155962347984314, 'learning_rate': 0.0009151020408163266, 'epoch': 0.1}
 10%|â–ˆ         | 516/5000 [3:26:26<30:12:53, 24.26s/it] 10%|â–ˆ         | 517/5000 [3:26:52<30:47:27, 24.73s/it]                                                       {'loss': 4.9599, 'grad_norm': 0.15842333436012268, 'learning_rate': 0.0009148979591836735, 'epoch': 0.1}
 10%|â–ˆ         | 517/5000 [3:26:52<30:47:27, 24.73s/it] 10%|â–ˆ         | 518/5000 [3:27:16<30:41:07, 24.65s/it]                                                       {'loss': 4.9402, 'grad_norm': 0.1390237808227539, 'learning_rate': 0.0009146938775510204, 'epoch': 0.1}
 10%|â–ˆ         | 518/5000 [3:27:16<30:41:07, 24.65s/it] 10%|â–ˆ         | 519/5000 [3:27:38<29:38:56, 23.82s/it]                                                       {'loss': 4.9732, 'grad_norm': 0.1656748652458191, 'learning_rate': 0.0009144897959183673, 'epoch': 0.1}
 10%|â–ˆ         | 519/5000 [3:27:38<29:38:56, 23.82s/it] 10%|â–ˆ         | 520/5000 [3:28:05<30:34:01, 24.56s/it]                                                       {'loss': 4.9854, 'grad_norm': 0.12500974535942078, 'learning_rate': 0.0009142857142857143, 'epoch': 0.1}
 10%|â–ˆ         | 520/5000 [3:28:05<30:34:01, 24.56s/it] 10%|â–ˆ         | 521/5000 [3:28:26<29:29:31, 23.70s/it]                                                       {'loss': 4.9213, 'grad_norm': 0.10635410249233246, 'learning_rate': 0.0009140816326530612, 'epoch': 0.1}
 10%|â–ˆ         | 521/5000 [3:28:26<29:29:31, 23.70s/it] 10%|â–ˆ         | 522/5000 [3:28:55<31:21:34, 25.21s/it]                                                       {'loss': 4.9665, 'grad_norm': 0.11523140966892242, 'learning_rate': 0.0009138775510204081, 'epoch': 0.1}
 10%|â–ˆ         | 522/5000 [3:28:55<31:21:34, 25.21s/it] 10%|â–ˆ         | 523/5000 [3:29:16<29:44:36, 23.92s/it]                                                       {'loss': 4.9414, 'grad_norm': 0.10109829902648926, 'learning_rate': 0.0009136734693877551, 'epoch': 0.1}
 10%|â–ˆ         | 523/5000 [3:29:16<29:44:36, 23.92s/it] 10%|â–ˆ         | 524/5000 [3:29:38<29:11:52, 23.48s/it]                                                       {'loss': 4.9261, 'grad_norm': 0.07720180600881577, 'learning_rate': 0.000913469387755102, 'epoch': 0.1}
 10%|â–ˆ         | 524/5000 [3:29:38<29:11:52, 23.48s/it] 10%|â–ˆ         | 525/5000 [3:30:00<28:35:28, 23.00s/it]                                                       {'loss': 4.9341, 'grad_norm': 0.08209261298179626, 'learning_rate': 0.0009132653061224489, 'epoch': 0.1}
 10%|â–ˆ         | 525/5000 [3:30:00<28:35:28, 23.00s/it] 11%|â–ˆ         | 526/5000 [3:30:21<27:51:41, 22.42s/it]                                                       {'loss': 4.9463, 'grad_norm': 0.19674693048000336, 'learning_rate': 0.0009130612244897958, 'epoch': 0.11}
 11%|â–ˆ         | 526/5000 [3:30:21<27:51:41, 22.42s/it] 11%|â–ˆ         | 527/5000 [3:30:43<27:46:59, 22.36s/it]                                                       {'loss': 4.9694, 'grad_norm': 0.12811625003814697, 'learning_rate': 0.0009128571428571429, 'epoch': 0.11}
 11%|â–ˆ         | 527/5000 [3:30:43<27:46:59, 22.36s/it] 11%|â–ˆ         | 528/5000 [3:31:09<29:00:04, 23.35s/it]                                                       {'loss': 4.9167, 'grad_norm': 0.08593860268592834, 'learning_rate': 0.0009126530612244898, 'epoch': 0.11}
 11%|â–ˆ         | 528/5000 [3:31:09<29:00:04, 23.35s/it] 11%|â–ˆ         | 529/5000 [3:31:40<31:44:31, 25.56s/it]                                                       {'loss': 4.9598, 'grad_norm': 0.15504923462867737, 'learning_rate': 0.0009124489795918367, 'epoch': 0.11}
 11%|â–ˆ         | 529/5000 [3:31:40<31:44:31, 25.56s/it] 11%|â–ˆ         | 530/5000 [3:32:04<31:11:20, 25.12s/it]                                                       {'loss': 4.9166, 'grad_norm': 0.09288579970598221, 'learning_rate': 0.0009122448979591837, 'epoch': 0.11}
 11%|â–ˆ         | 530/5000 [3:32:04<31:11:20, 25.12s/it] 11%|â–ˆ         | 531/5000 [3:32:32<32:27:24, 26.15s/it]                                                       {'loss': 4.9411, 'grad_norm': 0.1582786589860916, 'learning_rate': 0.0009120408163265306, 'epoch': 0.11}
 11%|â–ˆ         | 531/5000 [3:32:32<32:27:24, 26.15s/it] 11%|â–ˆ         | 532/5000 [3:32:57<31:45:48, 25.59s/it]                                                       {'loss': 4.9637, 'grad_norm': 0.11562806367874146, 'learning_rate': 0.0009118367346938776, 'epoch': 0.11}
 11%|â–ˆ         | 532/5000 [3:32:57<31:45:48, 25.59s/it] 11%|â–ˆ         | 533/5000 [3:33:20<30:56:00, 24.93s/it]                                                       {'loss': 4.9211, 'grad_norm': 0.08114545792341232, 'learning_rate': 0.0009116326530612246, 'epoch': 0.11}
 11%|â–ˆ         | 533/5000 [3:33:20<30:56:00, 24.93s/it] 11%|â–ˆ         | 534/5000 [3:33:44<30:29:23, 24.58s/it]                                                       {'loss': 4.9665, 'grad_norm': 0.10985448956489563, 'learning_rate': 0.0009114285714285715, 'epoch': 0.11}
 11%|â–ˆ         | 534/5000 [3:33:44<30:29:23, 24.58s/it] 11%|â–ˆ         | 535/5000 [3:34:10<31:06:30, 25.08s/it]                                                       {'loss': 4.9356, 'grad_norm': 0.09784576296806335, 'learning_rate': 0.0009112244897959184, 'epoch': 0.11}
 11%|â–ˆ         | 535/5000 [3:34:10<31:06:30, 25.08s/it] 11%|â–ˆ         | 536/5000 [3:34:35<30:54:32, 24.93s/it]                                                       {'loss': 4.943, 'grad_norm': 0.19699811935424805, 'learning_rate': 0.0009110204081632654, 'epoch': 0.11}
 11%|â–ˆ         | 536/5000 [3:34:35<30:54:32, 24.93s/it] 11%|â–ˆ         | 537/5000 [3:34:58<30:12:45, 24.37s/it]                                                       {'loss': 4.9347, 'grad_norm': 0.08515164256095886, 'learning_rate': 0.0009108163265306123, 'epoch': 0.11}
 11%|â–ˆ         | 537/5000 [3:34:58<30:12:45, 24.37s/it] 11%|â–ˆ         | 538/5000 [3:35:22<30:06:16, 24.29s/it]                                                       {'loss': 4.9338, 'grad_norm': 0.10259360074996948, 'learning_rate': 0.0009106122448979592, 'epoch': 0.11}
 11%|â–ˆ         | 538/5000 [3:35:22<30:06:16, 24.29s/it] 11%|â–ˆ         | 539/5000 [3:35:47<30:23:31, 24.53s/it]                                                       {'loss': 4.9604, 'grad_norm': 0.09254222363233566, 'learning_rate': 0.0009104081632653061, 'epoch': 0.11}
 11%|â–ˆ         | 539/5000 [3:35:47<30:23:31, 24.53s/it] 11%|â–ˆ         | 540/5000 [3:36:12<30:29:32, 24.61s/it]                                                       {'loss': 4.9537, 'grad_norm': 0.1476507931947708, 'learning_rate': 0.0009102040816326531, 'epoch': 0.11}
 11%|â–ˆ         | 540/5000 [3:36:12<30:29:32, 24.61s/it] 11%|â–ˆ         | 541/5000 [3:36:36<30:27:27, 24.59s/it]                                                       {'loss': 4.9472, 'grad_norm': 0.08699022233486176, 'learning_rate': 0.00091, 'epoch': 0.11}
 11%|â–ˆ         | 541/5000 [3:36:36<30:27:27, 24.59s/it] 11%|â–ˆ         | 542/5000 [3:37:00<30:01:22, 24.24s/it]                                                       {'loss': 4.9392, 'grad_norm': 0.10629689693450928, 'learning_rate': 0.000909795918367347, 'epoch': 0.11}
 11%|â–ˆ         | 542/5000 [3:37:00<30:01:22, 24.24s/it] 11%|â–ˆ         | 543/5000 [3:37:21<28:44:54, 23.22s/it]                                                       {'loss': 4.9245, 'grad_norm': 0.09142410755157471, 'learning_rate': 0.000909591836734694, 'epoch': 0.11}
 11%|â–ˆ         | 543/5000 [3:37:21<28:44:54, 23.22s/it] 11%|â–ˆ         | 544/5000 [3:37:45<29:00:01, 23.43s/it]                                                       {'loss': 4.9151, 'grad_norm': 0.0947398915886879, 'learning_rate': 0.0009093877551020409, 'epoch': 0.11}
 11%|â–ˆ         | 544/5000 [3:37:45<29:00:01, 23.43s/it] 11%|â–ˆ         | 545/5000 [3:38:09<29:32:01, 23.87s/it]                                                       {'loss': 4.9653, 'grad_norm': 0.12109732627868652, 'learning_rate': 0.0009091836734693878, 'epoch': 0.11}
 11%|â–ˆ         | 545/5000 [3:38:09<29:32:01, 23.87s/it] 11%|â–ˆ         | 546/5000 [3:38:31<28:49:27, 23.30s/it]                                                       {'loss': 4.9562, 'grad_norm': 0.16730834543704987, 'learning_rate': 0.0009089795918367348, 'epoch': 0.11}
 11%|â–ˆ         | 546/5000 [3:38:31<28:49:27, 23.30s/it] 11%|â–ˆ         | 547/5000 [3:38:54<28:33:53, 23.09s/it]                                                       {'loss': 4.971, 'grad_norm': 0.1411069631576538, 'learning_rate': 0.0009087755102040817, 'epoch': 0.11}
 11%|â–ˆ         | 547/5000 [3:38:54<28:33:53, 23.09s/it] 11%|â–ˆ         | 548/5000 [3:39:16<28:06:41, 22.73s/it]                                                       {'loss': 4.9281, 'grad_norm': 0.11614557355642319, 'learning_rate': 0.0009085714285714286, 'epoch': 0.11}
 11%|â–ˆ         | 548/5000 [3:39:16<28:06:41, 22.73s/it] 11%|â–ˆ         | 549/5000 [3:39:45<30:29:10, 24.66s/it]                                                       {'loss': 4.9305, 'grad_norm': 0.09737332165241241, 'learning_rate': 0.0009083673469387755, 'epoch': 0.11}
 11%|â–ˆ         | 549/5000 [3:39:45<30:29:10, 24.66s/it] 11%|â–ˆ         | 550/5000 [3:40:09<30:09:58, 24.40s/it]                                                       {'loss': 4.9276, 'grad_norm': 0.08001061528921127, 'learning_rate': 0.0009081632653061225, 'epoch': 0.11}
 11%|â–ˆ         | 550/5000 [3:40:09<30:09:58, 24.40s/it][2025-10-19 21:27:08,089] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 11%|â–ˆ         | 551/5000 [3:40:32<29:40:07, 24.01s/it]                                                       {'loss': 4.9116, 'grad_norm': 0.08217249065637589, 'learning_rate': 0.0009079591836734694, 'epoch': 0.11}
 11%|â–ˆ         | 551/5000 [3:40:32<29:40:07, 24.01s/it] 11%|â–ˆ         | 552/5000 [3:40:53<28:34:13, 23.12s/it]                                                       {'loss': 4.9565, 'grad_norm': 0.13651198148727417, 'learning_rate': 0.0009077551020408163, 'epoch': 0.11}
 11%|â–ˆ         | 552/5000 [3:40:53<28:34:13, 23.12s/it] 11%|â–ˆ         | 553/5000 [3:41:17<28:57:11, 23.44s/it]                                                       {'loss': 4.9302, 'grad_norm': 0.1039719358086586, 'learning_rate': 0.0009075510204081633, 'epoch': 0.11}
 11%|â–ˆ         | 553/5000 [3:41:17<28:57:11, 23.44s/it] 11%|â–ˆ         | 554/5000 [3:41:39<28:26:02, 23.02s/it]                                                       {'loss': 4.9108, 'grad_norm': 0.07358666509389877, 'learning_rate': 0.0009073469387755102, 'epoch': 0.11}
 11%|â–ˆ         | 554/5000 [3:41:39<28:26:02, 23.02s/it] 11%|â–ˆ         | 555/5000 [3:42:02<28:28:22, 23.06s/it]                                                       {'loss': 4.9407, 'grad_norm': 0.2206910103559494, 'learning_rate': 0.0009071428571428571, 'epoch': 0.11}
 11%|â–ˆ         | 555/5000 [3:42:02<28:28:22, 23.06s/it] 11%|â–ˆ         | 556/5000 [3:42:26<28:37:51, 23.19s/it]                                                       {'loss': 4.936, 'grad_norm': 0.08354800194501877, 'learning_rate': 0.000906938775510204, 'epoch': 0.11}
 11%|â–ˆ         | 556/5000 [3:42:26<28:37:51, 23.19s/it] 11%|â–ˆ         | 557/5000 [3:42:52<29:50:59, 24.19s/it]                                                       {'loss': 4.9518, 'grad_norm': 0.2143908143043518, 'learning_rate': 0.000906734693877551, 'epoch': 0.11}
 11%|â–ˆ         | 557/5000 [3:42:52<29:50:59, 24.19s/it] 11%|â–ˆ         | 558/5000 [3:43:19<30:45:43, 24.93s/it]                                                       {'loss': 4.9502, 'grad_norm': 0.10661882907152176, 'learning_rate': 0.000906530612244898, 'epoch': 0.11}
 11%|â–ˆ         | 558/5000 [3:43:19<30:45:43, 24.93s/it] 11%|â–ˆ         | 559/5000 [3:43:43<30:15:44, 24.53s/it]                                                       {'loss': 4.9338, 'grad_norm': 0.17227979004383087, 'learning_rate': 0.000906326530612245, 'epoch': 0.11}
 11%|â–ˆ         | 559/5000 [3:43:43<30:15:44, 24.53s/it] 11%|â–ˆ         | 560/5000 [3:44:09<31:00:41, 25.14s/it]                                                       {'loss': 4.9303, 'grad_norm': 0.11251203715801239, 'learning_rate': 0.0009061224489795919, 'epoch': 0.11}
 11%|â–ˆ         | 560/5000 [3:44:09<31:00:41, 25.14s/it] 11%|â–ˆ         | 561/5000 [3:44:34<30:43:29, 24.92s/it]                                                       {'loss': 4.9262, 'grad_norm': 0.10550488531589508, 'learning_rate': 0.0009059183673469388, 'epoch': 0.11}
 11%|â–ˆ         | 561/5000 [3:44:34<30:43:29, 24.92s/it] 11%|â–ˆ         | 562/5000 [3:44:57<30:13:56, 24.52s/it]                                                       {'loss': 4.9271, 'grad_norm': 0.09816933423280716, 'learning_rate': 0.0009057142857142857, 'epoch': 0.11}
 11%|â–ˆ         | 562/5000 [3:44:57<30:13:56, 24.52s/it] 11%|â–ˆâ–        | 563/5000 [3:45:21<30:01:45, 24.36s/it]                                                       {'loss': 4.916, 'grad_norm': 0.1021428108215332, 'learning_rate': 0.0009055102040816327, 'epoch': 0.11}
 11%|â–ˆâ–        | 563/5000 [3:45:21<30:01:45, 24.36s/it] 11%|â–ˆâ–        | 564/5000 [3:45:43<29:02:31, 23.57s/it]                                                       {'loss': 4.9179, 'grad_norm': 0.08247624337673187, 'learning_rate': 0.0009053061224489796, 'epoch': 0.11}
 11%|â–ˆâ–        | 564/5000 [3:45:43<29:02:31, 23.57s/it] 11%|â–ˆâ–        | 565/5000 [3:46:10<30:29:21, 24.75s/it]                                                       {'loss': 4.9383, 'grad_norm': 0.10148543119430542, 'learning_rate': 0.0009051020408163265, 'epoch': 0.11}
 11%|â–ˆâ–        | 565/5000 [3:46:10<30:29:21, 24.75s/it] 11%|â–ˆâ–        | 566/5000 [3:46:45<34:11:34, 27.76s/it]                                                       {'loss': 4.991, 'grad_norm': 0.14295782148838043, 'learning_rate': 0.0009048979591836735, 'epoch': 0.11}
 11%|â–ˆâ–        | 566/5000 [3:46:45<34:11:34, 27.76s/it] 11%|â–ˆâ–        | 567/5000 [3:47:11<33:33:28, 27.25s/it]                                                       {'loss': 4.9261, 'grad_norm': 0.10245560109615326, 'learning_rate': 0.0009046938775510204, 'epoch': 0.11}
 11%|â–ˆâ–        | 567/5000 [3:47:11<33:33:28, 27.25s/it] 11%|â–ˆâ–        | 568/5000 [3:47:38<33:17:31, 27.04s/it]                                                       {'loss': 4.9349, 'grad_norm': 0.11084383726119995, 'learning_rate': 0.0009044897959183673, 'epoch': 0.11}
 11%|â–ˆâ–        | 568/5000 [3:47:38<33:17:31, 27.04s/it] 11%|â–ˆâ–        | 569/5000 [3:48:00<31:39:59, 25.73s/it]                                                       {'loss': 4.9138, 'grad_norm': 0.08705850690603256, 'learning_rate': 0.0009042857142857142, 'epoch': 0.11}
 11%|â–ˆâ–        | 569/5000 [3:48:00<31:39:59, 25.73s/it] 11%|â–ˆâ–        | 570/5000 [3:48:30<33:11:22, 26.97s/it]                                                       {'loss': 4.93, 'grad_norm': 0.08089043945074081, 'learning_rate': 0.0009040816326530612, 'epoch': 0.11}
 11%|â–ˆâ–        | 570/5000 [3:48:30<33:11:22, 26.97s/it] 11%|â–ˆâ–        | 571/5000 [3:48:53<31:35:08, 25.67s/it]                                                       {'loss': 4.9061, 'grad_norm': 0.08427290618419647, 'learning_rate': 0.0009038775510204081, 'epoch': 0.11}
 11%|â–ˆâ–        | 571/5000 [3:48:53<31:35:08, 25.67s/it] 11%|â–ˆâ–        | 572/5000 [3:49:14<29:53:25, 24.30s/it]                                                       {'loss': 4.8972, 'grad_norm': 0.07070159167051315, 'learning_rate': 0.000903673469387755, 'epoch': 0.11}
 11%|â–ˆâ–        | 572/5000 [3:49:14<29:53:25, 24.30s/it] 11%|â–ˆâ–        | 573/5000 [3:49:38<29:51:00, 24.27s/it]                                                       {'loss': 4.9205, 'grad_norm': 0.07090736925601959, 'learning_rate': 0.000903469387755102, 'epoch': 0.11}
 11%|â–ˆâ–        | 573/5000 [3:49:38<29:51:00, 24.27s/it] 11%|â–ˆâ–        | 574/5000 [3:50:01<29:08:58, 23.71s/it]                                                       {'loss': 4.9095, 'grad_norm': 0.08343417942523956, 'learning_rate': 0.000903265306122449, 'epoch': 0.11}
 11%|â–ˆâ–        | 574/5000 [3:50:01<29:08:58, 23.71s/it] 12%|â–ˆâ–        | 575/5000 [3:50:27<30:00:31, 24.41s/it]                                                       {'loss': 4.919, 'grad_norm': 0.08727795630693436, 'learning_rate': 0.0009030612244897959, 'epoch': 0.12}
 12%|â–ˆâ–        | 575/5000 [3:50:27<30:00:31, 24.41s/it] 12%|â–ˆâ–        | 576/5000 [3:50:48<28:49:20, 23.45s/it]                                                       {'loss': 4.918, 'grad_norm': 0.14420312643051147, 'learning_rate': 0.0009028571428571429, 'epoch': 0.12}
 12%|â–ˆâ–        | 576/5000 [3:50:48<28:49:20, 23.45s/it] 12%|â–ˆâ–        | 577/5000 [3:51:20<31:50:44, 25.92s/it]                                                       {'loss': 4.9501, 'grad_norm': 0.15229789912700653, 'learning_rate': 0.0009026530612244899, 'epoch': 0.12}
 12%|â–ˆâ–        | 577/5000 [3:51:20<31:50:44, 25.92s/it] 12%|â–ˆâ–        | 578/5000 [3:51:42<30:38:06, 24.94s/it]                                                       {'loss': 4.9007, 'grad_norm': 0.10681356489658356, 'learning_rate': 0.0009024489795918368, 'epoch': 0.12}
 12%|â–ˆâ–        | 578/5000 [3:51:42<30:38:06, 24.94s/it] 12%|â–ˆâ–        | 579/5000 [3:52:06<30:03:42, 24.48s/it]                                                       {'loss': 4.9426, 'grad_norm': 0.09557320922613144, 'learning_rate': 0.0009022448979591838, 'epoch': 0.12}
 12%|â–ˆâ–        | 579/5000 [3:52:06<30:03:42, 24.48s/it] 12%|â–ˆâ–        | 580/5000 [3:52:31<30:11:18, 24.59s/it]                                                       {'loss': 4.9447, 'grad_norm': 0.09897882491350174, 'learning_rate': 0.0009020408163265307, 'epoch': 0.12}
 12%|â–ˆâ–        | 580/5000 [3:52:31<30:11:18, 24.59s/it] 12%|â–ˆâ–        | 581/5000 [3:52:55<30:00:15, 24.44s/it]                                                       {'loss': 4.9119, 'grad_norm': 0.08345989882946014, 'learning_rate': 0.0009018367346938776, 'epoch': 0.12}
 12%|â–ˆâ–        | 581/5000 [3:52:55<30:00:15, 24.44s/it] 12%|â–ˆâ–        | 582/5000 [3:53:18<29:39:56, 24.17s/it]                                                       {'loss': 4.9068, 'grad_norm': 0.08639546483755112, 'learning_rate': 0.0009016326530612245, 'epoch': 0.12}
 12%|â–ˆâ–        | 582/5000 [3:53:18<29:39:56, 24.17s/it] 12%|â–ˆâ–        | 583/5000 [3:53:39<28:19:33, 23.09s/it]                                                       {'loss': 4.8906, 'grad_norm': 0.0629357174038887, 'learning_rate': 0.0009014285714285715, 'epoch': 0.12}
 12%|â–ˆâ–        | 583/5000 [3:53:39<28:19:33, 23.09s/it] 12%|â–ˆâ–        | 584/5000 [3:54:01<27:58:09, 22.80s/it]                                                       {'loss': 4.9401, 'grad_norm': 0.11063919216394424, 'learning_rate': 0.0009012244897959184, 'epoch': 0.12}
 12%|â–ˆâ–        | 584/5000 [3:54:01<27:58:09, 22.80s/it] 12%|â–ˆâ–        | 585/5000 [3:54:22<27:20:26, 22.29s/it]                                                       {'loss': 4.9203, 'grad_norm': 0.08597597479820251, 'learning_rate': 0.0009010204081632653, 'epoch': 0.12}
 12%|â–ˆâ–        | 585/5000 [3:54:22<27:20:26, 22.29s/it] 12%|â–ˆâ–        | 586/5000 [3:54:45<27:26:42, 22.38s/it]                                                       {'loss': 4.9615, 'grad_norm': 0.10467306524515152, 'learning_rate': 0.0009008163265306123, 'epoch': 0.12}
 12%|â–ˆâ–        | 586/5000 [3:54:45<27:26:42, 22.38s/it] 12%|â–ˆâ–        | 587/5000 [3:55:08<27:53:31, 22.75s/it]                                                       {'loss': 4.9144, 'grad_norm': 0.06403801590204239, 'learning_rate': 0.0009006122448979592, 'epoch': 0.12}
 12%|â–ˆâ–        | 587/5000 [3:55:08<27:53:31, 22.75s/it] 12%|â–ˆâ–        | 588/5000 [3:55:29<27:14:49, 22.23s/it]                                                       {'loss': 4.915, 'grad_norm': 0.10651343315839767, 'learning_rate': 0.0009004081632653061, 'epoch': 0.12}
 12%|â–ˆâ–        | 588/5000 [3:55:29<27:14:49, 22.23s/it] 12%|â–ˆâ–        | 589/5000 [3:55:52<27:30:38, 22.45s/it]                                                       {'loss': 4.9152, 'grad_norm': 0.09063087403774261, 'learning_rate': 0.0009002040816326532, 'epoch': 0.12}
 12%|â–ˆâ–        | 589/5000 [3:55:52<27:30:38, 22.45s/it] 12%|â–ˆâ–        | 590/5000 [3:56:16<28:11:03, 23.01s/it]                                                       {'loss': 4.9123, 'grad_norm': 0.08483538776636124, 'learning_rate': 0.0009000000000000001, 'epoch': 0.12}
 12%|â–ˆâ–        | 590/5000 [3:56:16<28:11:03, 23.01s/it] 12%|â–ˆâ–        | 591/5000 [3:56:43<29:17:29, 23.92s/it]                                                       {'loss': 4.923, 'grad_norm': 0.07714253664016724, 'learning_rate': 0.000899795918367347, 'epoch': 0.12}
 12%|â–ˆâ–        | 591/5000 [3:56:43<29:17:29, 23.92s/it] 12%|â–ˆâ–        | 592/5000 [3:57:07<29:39:03, 24.22s/it]                                                       {'loss': 4.9455, 'grad_norm': 0.10845331847667694, 'learning_rate': 0.0008995918367346939, 'epoch': 0.12}
 12%|â–ˆâ–        | 592/5000 [3:57:07<29:39:03, 24.22s/it] 12%|â–ˆâ–        | 593/5000 [3:57:32<29:40:19, 24.24s/it]                                                       {'loss': 4.9097, 'grad_norm': 0.07688180357217789, 'learning_rate': 0.0008993877551020409, 'epoch': 0.12}
 12%|â–ˆâ–        | 593/5000 [3:57:32<29:40:19, 24.24s/it] 12%|â–ˆâ–        | 594/5000 [3:57:53<28:42:57, 23.46s/it]                                                       {'loss': 4.9529, 'grad_norm': 0.19352702796459198, 'learning_rate': 0.0008991836734693878, 'epoch': 0.12}
 12%|â–ˆâ–        | 594/5000 [3:57:53<28:42:57, 23.46s/it] 12%|â–ˆâ–        | 595/5000 [3:58:14<27:38:11, 22.59s/it]                                                       {'loss': 4.9012, 'grad_norm': 0.07827416062355042, 'learning_rate': 0.0008989795918367347, 'epoch': 0.12}
 12%|â–ˆâ–        | 595/5000 [3:58:14<27:38:11, 22.59s/it] 12%|â–ˆâ–        | 596/5000 [3:58:37<27:53:44, 22.80s/it]                                                       {'loss': 4.9013, 'grad_norm': 0.09096778184175491, 'learning_rate': 0.0008987755102040817, 'epoch': 0.12}
 12%|â–ˆâ–        | 596/5000 [3:58:37<27:53:44, 22.80s/it] 12%|â–ˆâ–        | 597/5000 [3:59:00<27:51:48, 22.78s/it]                                                       {'loss': 4.9135, 'grad_norm': 0.07084883749485016, 'learning_rate': 0.0008985714285714286, 'epoch': 0.12}
 12%|â–ˆâ–        | 597/5000 [3:59:00<27:51:48, 22.78s/it] 12%|â–ˆâ–        | 598/5000 [3:59:28<29:40:15, 24.27s/it]                                                       {'loss': 4.9224, 'grad_norm': 0.08307613432407379, 'learning_rate': 0.0008983673469387755, 'epoch': 0.12}
 12%|â–ˆâ–        | 598/5000 [3:59:28<29:40:15, 24.27s/it] 12%|â–ˆâ–        | 599/5000 [3:59:53<29:59:42, 24.54s/it]                                                       {'loss': 4.9025, 'grad_norm': 0.07862824201583862, 'learning_rate': 0.0008981632653061224, 'epoch': 0.12}
 12%|â–ˆâ–        | 599/5000 [3:59:53<29:59:42, 24.54s/it] 12%|â–ˆâ–        | 600/5000 [4:00:16<29:33:09, 24.18s/it]                                                       {'loss': 4.8975, 'grad_norm': 0.10017555952072144, 'learning_rate': 0.0008979591836734694, 'epoch': 0.12}
 12%|â–ˆâ–        | 600/5000 [4:00:16<29:33:09, 24.18s/it][2025-10-19 21:47:15,451] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 12%|â–ˆâ–        | 601/5000 [4:00:45<31:05:01, 25.44s/it]                                                       {'loss': 4.9206, 'grad_norm': 0.0727192834019661, 'learning_rate': 0.0008977551020408163, 'epoch': 0.12}
 12%|â–ˆâ–        | 601/5000 [4:00:45<31:05:01, 25.44s/it] 12%|â–ˆâ–        | 602/5000 [4:01:11<31:32:29, 25.82s/it]                                                       {'loss': 4.9356, 'grad_norm': 0.1115373745560646, 'learning_rate': 0.0008975510204081632, 'epoch': 0.12}
 12%|â–ˆâ–        | 602/5000 [4:01:11<31:32:29, 25.82s/it] 12%|â–ˆâ–        | 603/5000 [4:01:36<31:07:46, 25.49s/it]                                                       {'loss': 4.9172, 'grad_norm': 0.09879469126462936, 'learning_rate': 0.0008973469387755102, 'epoch': 0.12}
 12%|â–ˆâ–        | 603/5000 [4:01:36<31:07:46, 25.49s/it] 12%|â–ˆâ–        | 604/5000 [4:02:06<32:44:51, 26.82s/it]                                                       {'loss': 4.9495, 'grad_norm': 0.11970565468072891, 'learning_rate': 0.0008971428571428572, 'epoch': 0.12}
 12%|â–ˆâ–        | 604/5000 [4:02:06<32:44:51, 26.82s/it] 12%|â–ˆâ–        | 605/5000 [4:02:38<34:32:15, 28.29s/it]                                                       {'loss': 4.9729, 'grad_norm': 0.22648847103118896, 'learning_rate': 0.0008969387755102041, 'epoch': 0.12}
 12%|â–ˆâ–        | 605/5000 [4:02:38<34:32:15, 28.29s/it] 12%|â–ˆâ–        | 606/5000 [4:03:01<32:52:13, 26.93s/it]                                                       {'loss': 4.9279, 'grad_norm': 0.0724809318780899, 'learning_rate': 0.0008967346938775511, 'epoch': 0.12}
 12%|â–ˆâ–        | 606/5000 [4:03:01<32:52:13, 26.93s/it] 12%|â–ˆâ–        | 607/5000 [4:03:25<31:45:09, 26.02s/it]                                                       {'loss': 4.9163, 'grad_norm': 0.07768464833498001, 'learning_rate': 0.000896530612244898, 'epoch': 0.12}
 12%|â–ˆâ–        | 607/5000 [4:03:25<31:45:09, 26.02s/it] 12%|â–ˆâ–        | 608/5000 [4:03:51<31:41:35, 25.98s/it]                                                       {'loss': 4.9074, 'grad_norm': 0.09768714755773544, 'learning_rate': 0.0008963265306122449, 'epoch': 0.12}
 12%|â–ˆâ–        | 608/5000 [4:03:51<31:41:35, 25.98s/it] 12%|â–ˆâ–        | 609/5000 [4:04:16<31:06:48, 25.51s/it]                                                       {'loss': 4.9257, 'grad_norm': 0.09491576254367828, 'learning_rate': 0.0008961224489795919, 'epoch': 0.12}
 12%|â–ˆâ–        | 609/5000 [4:04:16<31:06:48, 25.51s/it] 12%|â–ˆâ–        | 610/5000 [4:04:40<30:44:43, 25.21s/it]                                                       {'loss': 4.9002, 'grad_norm': 0.12193150818347931, 'learning_rate': 0.0008959183673469388, 'epoch': 0.12}
 12%|â–ˆâ–        | 610/5000 [4:04:40<30:44:43, 25.21s/it] 12%|â–ˆâ–        | 611/5000 [4:05:02<29:21:18, 24.08s/it]                                                       {'loss': 4.921, 'grad_norm': 0.11350274831056595, 'learning_rate': 0.0008957142857142857, 'epoch': 0.12}
 12%|â–ˆâ–        | 611/5000 [4:05:02<29:21:18, 24.08s/it] 12%|â–ˆâ–        | 612/5000 [4:05:23<28:13:01, 23.15s/it]                                                       {'loss': 4.9225, 'grad_norm': 0.18534468114376068, 'learning_rate': 0.0008955102040816326, 'epoch': 0.12}
 12%|â–ˆâ–        | 612/5000 [4:05:23<28:13:01, 23.15s/it] 12%|â–ˆâ–        | 613/5000 [4:05:46<28:24:59, 23.32s/it]                                                       {'loss': 4.9084, 'grad_norm': 0.10620582848787308, 'learning_rate': 0.0008953061224489796, 'epoch': 0.12}
 12%|â–ˆâ–        | 613/5000 [4:05:46<28:24:59, 23.32s/it] 12%|â–ˆâ–        | 614/5000 [4:06:08<27:47:32, 22.81s/it]                                                       {'loss': 4.9013, 'grad_norm': 0.09363585710525513, 'learning_rate': 0.0008951020408163265, 'epoch': 0.12}
 12%|â–ˆâ–        | 614/5000 [4:06:08<27:47:32, 22.81s/it] 12%|â–ˆâ–        | 615/5000 [4:06:30<27:25:17, 22.51s/it]                                                       {'loss': 4.9156, 'grad_norm': 0.09377965331077576, 'learning_rate': 0.0008948979591836734, 'epoch': 0.12}
 12%|â–ˆâ–        | 615/5000 [4:06:30<27:25:17, 22.51s/it] 12%|â–ˆâ–        | 616/5000 [4:06:54<28:02:56, 23.03s/it]                                                       {'loss': 4.9345, 'grad_norm': 0.15645602345466614, 'learning_rate': 0.0008946938775510204, 'epoch': 0.12}
 12%|â–ˆâ–        | 616/5000 [4:06:54<28:02:56, 23.03s/it] 12%|â–ˆâ–        | 617/5000 [4:07:22<29:51:13, 24.52s/it]                                                       {'loss': 4.9632, 'grad_norm': 0.09760717302560806, 'learning_rate': 0.0008944897959183673, 'epoch': 0.12}
 12%|â–ˆâ–        | 617/5000 [4:07:22<29:51:13, 24.52s/it] 12%|â–ˆâ–        | 618/5000 [4:07:50<31:18:56, 25.73s/it]                                                       {'loss': 4.91, 'grad_norm': 0.08591920137405396, 'learning_rate': 0.0008942857142857142, 'epoch': 0.12}
 12%|â–ˆâ–        | 618/5000 [4:07:50<31:18:56, 25.73s/it] 12%|â–ˆâ–        | 619/5000 [4:08:22<33:23:07, 27.43s/it]                                                       {'loss': 4.9625, 'grad_norm': 0.21217294037342072, 'learning_rate': 0.0008940816326530611, 'epoch': 0.12}
 12%|â–ˆâ–        | 619/5000 [4:08:22<33:23:07, 27.43s/it] 12%|â–ˆâ–        | 620/5000 [4:08:47<32:41:26, 26.87s/it]                                                       {'loss': 4.9504, 'grad_norm': 0.10884800553321838, 'learning_rate': 0.0008938775510204082, 'epoch': 0.12}
 12%|â–ˆâ–        | 620/5000 [4:08:47<32:41:26, 26.87s/it] 12%|â–ˆâ–        | 621/5000 [4:09:15<32:49:00, 26.98s/it]                                                       {'loss': 4.9269, 'grad_norm': 0.10894300788640976, 'learning_rate': 0.0008936734693877551, 'epoch': 0.12}
 12%|â–ˆâ–        | 621/5000 [4:09:15<32:49:00, 26.98s/it] 12%|â–ˆâ–        | 622/5000 [4:09:38<31:26:04, 25.85s/it]                                                       {'loss': 4.9225, 'grad_norm': 0.0773736834526062, 'learning_rate': 0.0008934693877551021, 'epoch': 0.12}
 12%|â–ˆâ–        | 622/5000 [4:09:38<31:26:04, 25.85s/it] 12%|â–ˆâ–        | 623/5000 [4:10:12<34:28:42, 28.36s/it]                                                       {'loss': 4.9256, 'grad_norm': 0.11386959254741669, 'learning_rate': 0.0008932653061224491, 'epoch': 0.12}
 12%|â–ˆâ–        | 623/5000 [4:10:12<34:28:42, 28.36s/it] 12%|â–ˆâ–        | 624/5000 [4:10:33<31:55:56, 26.27s/it]                                                       {'loss': 4.9325, 'grad_norm': 0.10537362098693848, 'learning_rate': 0.000893061224489796, 'epoch': 0.12}
 12%|â–ˆâ–        | 624/5000 [4:10:34<31:55:56, 26.27s/it] 12%|â–ˆâ–Ž        | 625/5000 [4:10:57<30:53:21, 25.42s/it]                                                       {'loss': 4.9334, 'grad_norm': 0.07673190534114838, 'learning_rate': 0.0008928571428571429, 'epoch': 0.12}
 12%|â–ˆâ–Ž        | 625/5000 [4:10:57<30:53:21, 25.42s/it] 13%|â–ˆâ–Ž        | 626/5000 [4:11:21<30:19:54, 24.96s/it]                                                       {'loss': 4.921, 'grad_norm': 0.07835136353969574, 'learning_rate': 0.0008926530612244899, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 626/5000 [4:11:21<30:19:54, 24.96s/it] 13%|â–ˆâ–Ž        | 627/5000 [4:11:49<31:34:26, 25.99s/it]                                                       {'loss': 4.9578, 'grad_norm': 0.11604320257902145, 'learning_rate': 0.0008924489795918368, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 627/5000 [4:11:49<31:34:26, 25.99s/it] 13%|â–ˆâ–Ž        | 628/5000 [4:12:14<31:15:53, 25.74s/it]                                                       {'loss': 4.9184, 'grad_norm': 0.10595760494470596, 'learning_rate': 0.0008922448979591837, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 628/5000 [4:12:14<31:15:53, 25.74s/it] 13%|â–ˆâ–Ž        | 629/5000 [4:12:41<31:40:22, 26.09s/it]                                                       {'loss': 4.9237, 'grad_norm': 0.08030658960342407, 'learning_rate': 0.0008920408163265307, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 629/5000 [4:12:41<31:40:22, 26.09s/it] 13%|â–ˆâ–Ž        | 630/5000 [4:13:05<30:58:48, 25.52s/it]                                                       {'loss': 4.9028, 'grad_norm': 0.06585343182086945, 'learning_rate': 0.0008918367346938776, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 630/5000 [4:13:05<30:58:48, 25.52s/it] 13%|â–ˆâ–Ž        | 631/5000 [4:13:29<30:03:56, 24.77s/it]                                                       {'loss': 4.9147, 'grad_norm': 0.10775618255138397, 'learning_rate': 0.0008916326530612245, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 631/5000 [4:13:29<30:03:56, 24.77s/it] 13%|â–ˆâ–Ž        | 632/5000 [4:13:50<28:49:28, 23.76s/it]                                                       {'loss': 4.8985, 'grad_norm': 0.060377903282642365, 'learning_rate': 0.0008914285714285714, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 632/5000 [4:13:50<28:49:28, 23.76s/it] 13%|â–ˆâ–Ž        | 633/5000 [4:14:15<29:09:38, 24.04s/it]                                                       {'loss': 4.905, 'grad_norm': 0.09770495444536209, 'learning_rate': 0.0008912244897959184, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 633/5000 [4:14:15<29:09:38, 24.04s/it] 13%|â–ˆâ–Ž        | 634/5000 [4:14:39<29:20:11, 24.19s/it]                                                       {'loss': 4.8991, 'grad_norm': 0.06663107126951218, 'learning_rate': 0.0008910204081632653, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 634/5000 [4:14:39<29:20:11, 24.19s/it] 13%|â–ˆâ–Ž        | 635/5000 [4:15:14<33:05:29, 27.29s/it]                                                       {'loss': 4.9149, 'grad_norm': 0.0963655561208725, 'learning_rate': 0.0008908163265306123, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 635/5000 [4:15:14<33:05:29, 27.29s/it] 13%|â–ˆâ–Ž        | 636/5000 [4:15:38<31:52:46, 26.30s/it]                                                       {'loss': 4.9177, 'grad_norm': 0.06904753297567368, 'learning_rate': 0.0008906122448979593, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 636/5000 [4:15:38<31:52:46, 26.30s/it] 13%|â–ˆâ–Ž        | 637/5000 [4:16:02<31:05:25, 25.65s/it]                                                       {'loss': 4.8904, 'grad_norm': 0.10653998702764511, 'learning_rate': 0.0008904081632653062, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 637/5000 [4:16:02<31:05:25, 25.65s/it] 13%|â–ˆâ–Ž        | 638/5000 [4:16:24<29:52:08, 24.65s/it]                                                       {'loss': 4.933, 'grad_norm': 0.09711712598800659, 'learning_rate': 0.0008902040816326531, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 638/5000 [4:16:24<29:52:08, 24.65s/it] 13%|â–ˆâ–Ž        | 639/5000 [4:16:52<31:06:12, 25.68s/it]                                                       {'loss': 4.9359, 'grad_norm': 0.1077500507235527, 'learning_rate': 0.0008900000000000001, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 639/5000 [4:16:52<31:06:12, 25.68s/it] 13%|â–ˆâ–Ž        | 640/5000 [4:17:13<29:20:57, 24.23s/it]                                                       {'loss': 4.9359, 'grad_norm': 0.1019505113363266, 'learning_rate': 0.000889795918367347, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 640/5000 [4:17:13<29:20:57, 24.23s/it] 13%|â–ˆâ–Ž        | 641/5000 [4:17:37<29:07:53, 24.06s/it]                                                       {'loss': 4.9026, 'grad_norm': 0.08252502977848053, 'learning_rate': 0.0008895918367346939, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 641/5000 [4:17:37<29:07:53, 24.06s/it] 13%|â–ˆâ–Ž        | 642/5000 [4:18:01<29:06:43, 24.05s/it]                                                       {'loss': 4.9271, 'grad_norm': 0.0990193635225296, 'learning_rate': 0.0008893877551020408, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 642/5000 [4:18:01<29:06:43, 24.05s/it] 13%|â–ˆâ–Ž        | 643/5000 [4:18:26<29:25:26, 24.31s/it]                                                       {'loss': 4.9344, 'grad_norm': 0.11422397196292877, 'learning_rate': 0.0008891836734693878, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 643/5000 [4:18:26<29:25:26, 24.31s/it] 13%|â–ˆâ–Ž        | 644/5000 [4:18:48<28:38:29, 23.67s/it]                                                       {'loss': 4.9037, 'grad_norm': 0.0771130919456482, 'learning_rate': 0.0008889795918367347, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 644/5000 [4:18:48<28:38:29, 23.67s/it] 13%|â–ˆâ–Ž        | 645/5000 [4:19:13<29:04:49, 24.04s/it]                                                       {'loss': 4.9313, 'grad_norm': 0.07280094176530838, 'learning_rate': 0.0008887755102040816, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 645/5000 [4:19:13<29:04:49, 24.04s/it] 13%|â–ˆâ–Ž        | 646/5000 [4:19:34<28:08:14, 23.26s/it]                                                       {'loss': 4.9181, 'grad_norm': 0.09904951602220535, 'learning_rate': 0.0008885714285714286, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 646/5000 [4:19:34<28:08:14, 23.26s/it] 13%|â–ˆâ–Ž        | 647/5000 [4:19:58<28:17:17, 23.39s/it]                                                       {'loss': 4.903, 'grad_norm': 0.07597321271896362, 'learning_rate': 0.0008883673469387755, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 647/5000 [4:19:58<28:17:17, 23.39s/it] 13%|â–ˆâ–Ž        | 648/5000 [4:20:22<28:29:55, 23.57s/it]                                                       {'loss': 4.9073, 'grad_norm': 0.06719189137220383, 'learning_rate': 0.0008881632653061224, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 648/5000 [4:20:22<28:29:55, 23.57s/it] 13%|â–ˆâ–Ž        | 649/5000 [4:20:55<32:05:44, 26.56s/it]                                                       {'loss': 4.9802, 'grad_norm': 0.14191904664039612, 'learning_rate': 0.0008879591836734694, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 649/5000 [4:20:55<32:05:44, 26.56s/it] 13%|â–ˆâ–Ž        | 650/5000 [4:21:20<31:24:11, 25.99s/it]                                                       {'loss': 4.9496, 'grad_norm': 0.0837864950299263, 'learning_rate': 0.0008877551020408163, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 650/5000 [4:21:20<31:24:11, 25.99s/it][2025-10-19 22:08:19,282] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 651/5000 [4:21:48<32:16:24, 26.72s/it]                                                       {'loss': 4.9228, 'grad_norm': 0.07190515100955963, 'learning_rate': 0.0008875510204081633, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 651/5000 [4:21:48<32:16:24, 26.72s/it] 13%|â–ˆâ–Ž        | 652/5000 [4:22:10<30:15:16, 25.05s/it]                                                       {'loss': 4.9075, 'grad_norm': 0.06262596696615219, 'learning_rate': 0.0008873469387755102, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 652/5000 [4:22:10<30:15:16, 25.05s/it] 13%|â–ˆâ–Ž        | 653/5000 [4:22:34<29:55:22, 24.78s/it]                                                       {'loss': 4.9136, 'grad_norm': 0.07312171906232834, 'learning_rate': 0.0008871428571428572, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 653/5000 [4:22:34<29:55:22, 24.78s/it] 13%|â–ˆâ–Ž        | 654/5000 [4:22:55<28:37:19, 23.71s/it]                                                       {'loss': 4.9037, 'grad_norm': 0.06063695251941681, 'learning_rate': 0.0008869387755102041, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 654/5000 [4:22:55<28:37:19, 23.71s/it] 13%|â–ˆâ–Ž        | 655/5000 [4:23:16<27:45:24, 23.00s/it]                                                       {'loss': 4.9038, 'grad_norm': 0.06677855551242828, 'learning_rate': 0.000886734693877551, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 655/5000 [4:23:16<27:45:24, 23.00s/it] 13%|â–ˆâ–Ž        | 656/5000 [4:23:40<27:55:55, 23.15s/it]                                                       {'loss': 4.9199, 'grad_norm': 0.12944763898849487, 'learning_rate': 0.000886530612244898, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 656/5000 [4:23:40<27:55:55, 23.15s/it] 13%|â–ˆâ–Ž        | 657/5000 [4:24:04<28:14:57, 23.42s/it]                                                       {'loss': 4.9204, 'grad_norm': 0.2006172388792038, 'learning_rate': 0.0008863265306122449, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 657/5000 [4:24:04<28:14:57, 23.42s/it] 13%|â–ˆâ–Ž        | 658/5000 [4:24:27<28:14:08, 23.41s/it]                                                       {'loss': 4.932, 'grad_norm': 0.10517048835754395, 'learning_rate': 0.0008861224489795918, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 658/5000 [4:24:27<28:14:08, 23.41s/it] 13%|â–ˆâ–Ž        | 659/5000 [4:24:54<29:17:47, 24.30s/it]                                                       {'loss': 4.9161, 'grad_norm': 0.07324279099702835, 'learning_rate': 0.0008859183673469388, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 659/5000 [4:24:54<29:17:47, 24.30s/it] 13%|â–ˆâ–Ž        | 660/5000 [4:25:15<28:17:25, 23.47s/it]                                                       {'loss': 4.9384, 'grad_norm': 0.0829756036400795, 'learning_rate': 0.0008857142857142857, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 660/5000 [4:25:15<28:17:25, 23.47s/it] 13%|â–ˆâ–Ž        | 661/5000 [4:25:39<28:26:03, 23.59s/it]                                                       {'loss': 4.9275, 'grad_norm': 0.10793563723564148, 'learning_rate': 0.0008855102040816326, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 661/5000 [4:25:39<28:26:03, 23.59s/it] 13%|â–ˆâ–Ž        | 662/5000 [4:26:03<28:33:21, 23.70s/it]                                                       {'loss': 4.9031, 'grad_norm': 0.07400237023830414, 'learning_rate': 0.0008853061224489795, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 662/5000 [4:26:03<28:33:21, 23.70s/it] 13%|â–ˆâ–Ž        | 663/5000 [4:26:27<28:33:14, 23.70s/it]                                                       {'loss': 4.9218, 'grad_norm': 0.11794018000364304, 'learning_rate': 0.0008851020408163265, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 663/5000 [4:26:27<28:33:14, 23.70s/it] 13%|â–ˆâ–Ž        | 664/5000 [4:26:48<27:43:02, 23.01s/it]                                                       {'loss': 4.9169, 'grad_norm': 0.1531132161617279, 'learning_rate': 0.0008848979591836734, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 664/5000 [4:26:48<27:43:02, 23.01s/it] 13%|â–ˆâ–Ž        | 665/5000 [4:27:10<27:15:28, 22.64s/it]                                                       {'loss': 4.9055, 'grad_norm': 0.08198831230401993, 'learning_rate': 0.0008846938775510203, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 665/5000 [4:27:10<27:15:28, 22.64s/it] 13%|â–ˆâ–Ž        | 666/5000 [4:27:38<29:06:14, 24.18s/it]                                                       {'loss': 4.9076, 'grad_norm': 0.07262110710144043, 'learning_rate': 0.0008844897959183673, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 666/5000 [4:27:38<29:06:14, 24.18s/it] 13%|â–ˆâ–Ž        | 667/5000 [4:28:02<29:03:33, 24.14s/it]                                                       {'loss': 4.9025, 'grad_norm': 0.058727603405714035, 'learning_rate': 0.0008842857142857143, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 667/5000 [4:28:02<29:03:33, 24.14s/it] 13%|â–ˆâ–Ž        | 668/5000 [4:28:22<27:47:31, 23.10s/it]                                                       {'loss': 4.9125, 'grad_norm': 0.0840541198849678, 'learning_rate': 0.0008840816326530613, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 668/5000 [4:28:22<27:47:31, 23.10s/it] 13%|â–ˆâ–Ž        | 669/5000 [4:28:46<28:07:53, 23.38s/it]                                                       {'loss': 4.9251, 'grad_norm': 0.12710066139698029, 'learning_rate': 0.0008838775510204083, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 669/5000 [4:28:46<28:07:53, 23.38s/it] 13%|â–ˆâ–Ž        | 670/5000 [4:29:11<28:26:45, 23.65s/it]                                                       {'loss': 4.926, 'grad_norm': 0.08816239982843399, 'learning_rate': 0.0008836734693877552, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 670/5000 [4:29:11<28:26:45, 23.65s/it] 13%|â–ˆâ–Ž        | 671/5000 [4:29:35<28:34:53, 23.77s/it]                                                       {'loss': 4.8933, 'grad_norm': 0.06893311440944672, 'learning_rate': 0.0008834693877551021, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 671/5000 [4:29:35<28:34:53, 23.77s/it] 13%|â–ˆâ–Ž        | 672/5000 [4:29:59<28:54:33, 24.05s/it]                                                       {'loss': 4.9121, 'grad_norm': 0.06597521901130676, 'learning_rate': 0.000883265306122449, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 672/5000 [4:29:59<28:54:33, 24.05s/it] 13%|â–ˆâ–Ž        | 673/5000 [4:30:30<31:13:10, 25.97s/it]                                                       {'loss': 4.9291, 'grad_norm': 0.15304681658744812, 'learning_rate': 0.000883061224489796, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 673/5000 [4:30:30<31:13:10, 25.97s/it] 13%|â–ˆâ–Ž        | 674/5000 [4:30:55<30:50:35, 25.67s/it]                                                       {'loss': 4.9037, 'grad_norm': 0.10873005539178848, 'learning_rate': 0.0008828571428571429, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 674/5000 [4:30:55<30:50:35, 25.67s/it] 14%|â–ˆâ–Ž        | 675/5000 [4:31:19<30:25:12, 25.32s/it]                                                       {'loss': 4.9196, 'grad_norm': 0.09608127176761627, 'learning_rate': 0.0008826530612244898, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 675/5000 [4:31:19<30:25:12, 25.32s/it] 14%|â–ˆâ–Ž        | 676/5000 [4:31:43<29:56:15, 24.92s/it]                                                       {'loss': 4.9118, 'grad_norm': 0.1102016270160675, 'learning_rate': 0.0008824489795918368, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 676/5000 [4:31:43<29:56:15, 24.92s/it] 14%|â–ˆâ–Ž        | 677/5000 [4:32:08<29:49:28, 24.84s/it]                                                       {'loss': 4.9567, 'grad_norm': 0.20351633429527283, 'learning_rate': 0.0008822448979591837, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 677/5000 [4:32:08<29:49:28, 24.84s/it] 14%|â–ˆâ–Ž        | 678/5000 [4:32:32<29:39:16, 24.70s/it]                                                       {'loss': 4.9019, 'grad_norm': 0.08359967917203903, 'learning_rate': 0.0008820408163265306, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 678/5000 [4:32:32<29:39:16, 24.70s/it] 14%|â–ˆâ–Ž        | 679/5000 [4:32:54<28:38:03, 23.86s/it]                                                       {'loss': 4.8841, 'grad_norm': 0.05686722323298454, 'learning_rate': 0.0008818367346938776, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 679/5000 [4:32:54<28:38:03, 23.86s/it] 14%|â–ˆâ–Ž        | 680/5000 [4:33:23<30:22:58, 25.32s/it]                                                       {'loss': 4.9257, 'grad_norm': 0.12107615917921066, 'learning_rate': 0.0008816326530612245, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 680/5000 [4:33:23<30:22:58, 25.32s/it] 14%|â–ˆâ–Ž        | 681/5000 [4:33:44<28:42:21, 23.93s/it]                                                       {'loss': 4.9102, 'grad_norm': 0.12773007154464722, 'learning_rate': 0.0008814285714285714, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 681/5000 [4:33:44<28:42:21, 23.93s/it] 14%|â–ˆâ–Ž        | 682/5000 [4:34:08<28:41:52, 23.93s/it]                                                       {'loss': 4.9311, 'grad_norm': 0.14101162552833557, 'learning_rate': 0.0008812244897959185, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 682/5000 [4:34:08<28:41:52, 23.93s/it] 14%|â–ˆâ–Ž        | 683/5000 [4:34:32<28:55:12, 24.12s/it]                                                       {'loss': 4.89, 'grad_norm': 0.05404040962457657, 'learning_rate': 0.0008810204081632654, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 683/5000 [4:34:32<28:55:12, 24.12s/it] 14%|â–ˆâ–Ž        | 684/5000 [4:34:58<29:35:05, 24.68s/it]                                                       {'loss': 4.9286, 'grad_norm': 0.12226767838001251, 'learning_rate': 0.0008808163265306123, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 684/5000 [4:34:58<29:35:05, 24.68s/it] 14%|â–ˆâ–Ž        | 685/5000 [4:35:21<29:05:07, 24.27s/it]                                                       {'loss': 4.9285, 'grad_norm': 0.0909288227558136, 'learning_rate': 0.0008806122448979592, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 685/5000 [4:35:21<29:05:07, 24.27s/it] 14%|â–ˆâ–Ž        | 686/5000 [4:35:45<28:54:21, 24.12s/it]                                                       {'loss': 4.9089, 'grad_norm': 0.06880486011505127, 'learning_rate': 0.0008804081632653062, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 686/5000 [4:35:45<28:54:21, 24.12s/it] 14%|â–ˆâ–Ž        | 687/5000 [4:36:11<29:27:34, 24.59s/it]                                                       {'loss': 4.9031, 'grad_norm': 0.06539811193943024, 'learning_rate': 0.0008802040816326531, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 687/5000 [4:36:11<29:27:34, 24.59s/it] 14%|â–ˆâ–        | 688/5000 [4:36:33<28:29:14, 23.78s/it]                                                       {'loss': 4.9411, 'grad_norm': 0.09361103177070618, 'learning_rate': 0.00088, 'epoch': 0.14}
 14%|â–ˆâ–        | 688/5000 [4:36:33<28:29:14, 23.78s/it] 14%|â–ˆâ–        | 689/5000 [4:36:54<27:36:03, 23.05s/it]                                                       {'loss': 4.9021, 'grad_norm': 0.05711385980248451, 'learning_rate': 0.000879795918367347, 'epoch': 0.14}
 14%|â–ˆâ–        | 689/5000 [4:36:54<27:36:03, 23.05s/it] 14%|â–ˆâ–        | 690/5000 [4:37:15<26:45:16, 22.35s/it]                                                       {'loss': 4.9482, 'grad_norm': 0.11397076398134232, 'learning_rate': 0.0008795918367346939, 'epoch': 0.14}
 14%|â–ˆâ–        | 690/5000 [4:37:15<26:45:16, 22.35s/it] 14%|â–ˆâ–        | 691/5000 [4:37:38<27:05:05, 22.63s/it]                                                       {'loss': 4.9039, 'grad_norm': 0.08097072690725327, 'learning_rate': 0.0008793877551020408, 'epoch': 0.14}
 14%|â–ˆâ–        | 691/5000 [4:37:38<27:05:05, 22.63s/it] 14%|â–ˆâ–        | 692/5000 [4:37:59<26:28:06, 22.12s/it]                                                       {'loss': 4.9162, 'grad_norm': 0.1030532717704773, 'learning_rate': 0.0008791836734693877, 'epoch': 0.14}
 14%|â–ˆâ–        | 692/5000 [4:37:59<26:28:06, 22.12s/it] 14%|â–ˆâ–        | 693/5000 [4:38:28<28:46:38, 24.05s/it]                                                       {'loss': 4.9137, 'grad_norm': 0.07038604468107224, 'learning_rate': 0.0008789795918367347, 'epoch': 0.14}
 14%|â–ˆâ–        | 693/5000 [4:38:28<28:46:38, 24.05s/it] 14%|â–ˆâ–        | 694/5000 [4:38:49<27:40:19, 23.14s/it]                                                       {'loss': 4.8948, 'grad_norm': 0.05606134235858917, 'learning_rate': 0.0008787755102040816, 'epoch': 0.14}
 14%|â–ˆâ–        | 694/5000 [4:38:49<27:40:19, 23.14s/it] 14%|â–ˆâ–        | 695/5000 [4:39:11<27:18:18, 22.83s/it]                                                       {'loss': 4.9038, 'grad_norm': 0.09327135235071182, 'learning_rate': 0.0008785714285714285, 'epoch': 0.14}
 14%|â–ˆâ–        | 695/5000 [4:39:11<27:18:18, 22.83s/it] 14%|â–ˆâ–        | 696/5000 [4:39:39<29:09:39, 24.39s/it]                                                       {'loss': 4.9377, 'grad_norm': 0.18327704071998596, 'learning_rate': 0.0008783673469387755, 'epoch': 0.14}
 14%|â–ˆâ–        | 696/5000 [4:39:39<29:09:39, 24.39s/it] 14%|â–ˆâ–        | 697/5000 [4:40:05<29:51:43, 24.98s/it]                                                       {'loss': 4.9063, 'grad_norm': 0.09151630848646164, 'learning_rate': 0.0008781632653061224, 'epoch': 0.14}
 14%|â–ˆâ–        | 697/5000 [4:40:05<29:51:43, 24.98s/it] 14%|â–ˆâ–        | 698/5000 [4:40:26<28:22:32, 23.75s/it]                                                       {'loss': 4.9066, 'grad_norm': 0.054919108748435974, 'learning_rate': 0.0008779591836734694, 'epoch': 0.14}
 14%|â–ˆâ–        | 698/5000 [4:40:26<28:22:32, 23.75s/it] 14%|â–ˆâ–        | 699/5000 [4:40:48<27:46:56, 23.25s/it]                                                       {'loss': 4.909, 'grad_norm': 0.07914984226226807, 'learning_rate': 0.0008777551020408164, 'epoch': 0.14}
 14%|â–ˆâ–        | 699/5000 [4:40:48<27:46:56, 23.25s/it] 14%|â–ˆâ–        | 700/5000 [4:41:16<29:24:26, 24.62s/it]                                                       {'loss': 4.9156, 'grad_norm': 0.12446065247058868, 'learning_rate': 0.0008775510204081633, 'epoch': 0.14}
 14%|â–ˆâ–        | 700/5000 [4:41:16<29:24:26, 24.62s/it][2025-10-19 22:28:15,149] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 14%|â–ˆâ–        | 701/5000 [4:41:39<28:58:11, 24.26s/it]                                                       {'loss': 4.8907, 'grad_norm': 0.05847727879881859, 'learning_rate': 0.0008773469387755102, 'epoch': 0.14}
 14%|â–ˆâ–        | 701/5000 [4:41:39<28:58:11, 24.26s/it] 14%|â–ˆâ–        | 702/5000 [4:42:04<29:06:31, 24.38s/it]                                                       {'loss': 4.9057, 'grad_norm': 0.060665301978588104, 'learning_rate': 0.0008771428571428572, 'epoch': 0.14}
 14%|â–ˆâ–        | 702/5000 [4:42:04<29:06:31, 24.38s/it] 14%|â–ˆâ–        | 703/5000 [4:42:27<28:44:46, 24.08s/it]                                                       {'loss': 4.9154, 'grad_norm': 0.10853035748004913, 'learning_rate': 0.0008769387755102041, 'epoch': 0.14}
 14%|â–ˆâ–        | 703/5000 [4:42:27<28:44:46, 24.08s/it] 14%|â–ˆâ–        | 704/5000 [4:42:56<30:27:46, 25.53s/it]                                                       {'loss': 4.9138, 'grad_norm': 0.15591461956501007, 'learning_rate': 0.000876734693877551, 'epoch': 0.14}
 14%|â–ˆâ–        | 704/5000 [4:42:56<30:27:46, 25.53s/it] 14%|â–ˆâ–        | 705/5000 [4:43:18<29:08:58, 24.43s/it]                                                       {'loss': 4.9067, 'grad_norm': 0.11519792675971985, 'learning_rate': 0.0008765306122448979, 'epoch': 0.14}
 14%|â–ˆâ–        | 705/5000 [4:43:18<29:08:58, 24.43s/it] 14%|â–ˆâ–        | 706/5000 [4:43:43<29:10:52, 24.47s/it]                                                       {'loss': 4.9038, 'grad_norm': 0.062403708696365356, 'learning_rate': 0.0008763265306122449, 'epoch': 0.14}
 14%|â–ˆâ–        | 706/5000 [4:43:43<29:10:52, 24.47s/it] 14%|â–ˆâ–        | 707/5000 [4:44:04<28:10:58, 23.63s/it]                                                       {'loss': 4.9198, 'grad_norm': 0.08054672181606293, 'learning_rate': 0.0008761224489795918, 'epoch': 0.14}
 14%|â–ˆâ–        | 707/5000 [4:44:04<28:10:58, 23.63s/it] 14%|â–ˆâ–        | 708/5000 [4:44:28<28:10:40, 23.63s/it]                                                       {'loss': 4.8908, 'grad_norm': 0.060157209634780884, 'learning_rate': 0.0008759183673469387, 'epoch': 0.14}
 14%|â–ˆâ–        | 708/5000 [4:44:28<28:10:40, 23.63s/it] 14%|â–ˆâ–        | 709/5000 [4:44:52<28:18:17, 23.75s/it]                                                       {'loss': 4.9048, 'grad_norm': 0.0954187884926796, 'learning_rate': 0.0008757142857142857, 'epoch': 0.14}
 14%|â–ˆâ–        | 709/5000 [4:44:52<28:18:17, 23.75s/it] 14%|â–ˆâ–        | 710/5000 [4:45:13<27:08:20, 22.77s/it]                                                       {'loss': 4.9015, 'grad_norm': 0.05636601522564888, 'learning_rate': 0.0008755102040816326, 'epoch': 0.14}
 14%|â–ˆâ–        | 710/5000 [4:45:13<27:08:20, 22.77s/it] 14%|â–ˆâ–        | 711/5000 [4:45:34<26:36:42, 22.34s/it]                                                       {'loss': 4.9157, 'grad_norm': 0.07076210528612137, 'learning_rate': 0.0008753061224489795, 'epoch': 0.14}
 14%|â–ˆâ–        | 711/5000 [4:45:34<26:36:42, 22.34s/it] 14%|â–ˆâ–        | 712/5000 [4:45:55<26:04:07, 21.89s/it]                                                       {'loss': 4.9096, 'grad_norm': 0.14514732360839844, 'learning_rate': 0.0008751020408163264, 'epoch': 0.14}
 14%|â–ˆâ–        | 712/5000 [4:45:55<26:04:07, 21.89s/it] 14%|â–ˆâ–        | 713/5000 [4:46:17<26:03:56, 21.89s/it]                                                       {'loss': 4.9239, 'grad_norm': 0.08829184621572495, 'learning_rate': 0.0008748979591836736, 'epoch': 0.14}
 14%|â–ˆâ–        | 713/5000 [4:46:17<26:03:56, 21.89s/it] 14%|â–ˆâ–        | 714/5000 [4:46:40<26:27:14, 22.22s/it]                                                       {'loss': 4.8885, 'grad_norm': 0.12764787673950195, 'learning_rate': 0.0008746938775510205, 'epoch': 0.14}
 14%|â–ˆâ–        | 714/5000 [4:46:40<26:27:14, 22.22s/it] 14%|â–ˆâ–        | 715/5000 [4:47:04<27:06:51, 22.78s/it]                                                       {'loss': 4.8947, 'grad_norm': 0.05038133263587952, 'learning_rate': 0.0008744897959183675, 'epoch': 0.14}
 14%|â–ˆâ–        | 715/5000 [4:47:04<27:06:51, 22.78s/it] 14%|â–ˆâ–        | 716/5000 [4:47:28<27:37:38, 23.22s/it]                                                       {'loss': 4.9161, 'grad_norm': 0.07352488487958908, 'learning_rate': 0.0008742857142857144, 'epoch': 0.14}
 14%|â–ˆâ–        | 716/5000 [4:47:28<27:37:38, 23.22s/it] 14%|â–ˆâ–        | 717/5000 [4:47:52<27:57:26, 23.50s/it]                                                       {'loss': 4.9177, 'grad_norm': 0.08555874228477478, 'learning_rate': 0.0008740816326530613, 'epoch': 0.14}
 14%|â–ˆâ–        | 717/5000 [4:47:52<27:57:26, 23.50s/it] 14%|â–ˆâ–        | 718/5000 [4:48:13<27:08:01, 22.81s/it]                                                       {'loss': 4.8989, 'grad_norm': 0.048950910568237305, 'learning_rate': 0.0008738775510204082, 'epoch': 0.14}
 14%|â–ˆâ–        | 718/5000 [4:48:13<27:08:01, 22.81s/it] 14%|â–ˆâ–        | 719/5000 [4:48:39<28:01:06, 23.56s/it]                                                       {'loss': 4.9073, 'grad_norm': 0.17882384359836578, 'learning_rate': 0.0008736734693877552, 'epoch': 0.14}
 14%|â–ˆâ–        | 719/5000 [4:48:39<28:01:06, 23.56s/it] 14%|â–ˆâ–        | 720/5000 [4:49:00<27:24:11, 23.05s/it]                                                       {'loss': 4.895, 'grad_norm': 0.07113339751958847, 'learning_rate': 0.0008734693877551021, 'epoch': 0.14}
 14%|â–ˆâ–        | 720/5000 [4:49:00<27:24:11, 23.05s/it] 14%|â–ˆâ–        | 721/5000 [4:49:25<28:01:16, 23.57s/it]                                                       {'loss': 4.8924, 'grad_norm': 0.05153452605009079, 'learning_rate': 0.000873265306122449, 'epoch': 0.14}
 14%|â–ˆâ–        | 721/5000 [4:49:25<28:01:16, 23.57s/it] 14%|â–ˆâ–        | 722/5000 [4:49:49<27:54:48, 23.49s/it]                                                       {'loss': 4.9177, 'grad_norm': 0.0783480703830719, 'learning_rate': 0.000873061224489796, 'epoch': 0.14}
 14%|â–ˆâ–        | 722/5000 [4:49:49<27:54:48, 23.49s/it] 14%|â–ˆâ–        | 723/5000 [4:50:11<27:41:51, 23.31s/it]                                                       {'loss': 4.9123, 'grad_norm': 0.06937233358621597, 'learning_rate': 0.0008728571428571429, 'epoch': 0.14}
 14%|â–ˆâ–        | 723/5000 [4:50:11<27:41:51, 23.31s/it] 14%|â–ˆâ–        | 724/5000 [4:50:43<30:41:14, 25.84s/it]                                                       {'loss': 4.9173, 'grad_norm': 0.08994775265455246, 'learning_rate': 0.0008726530612244898, 'epoch': 0.14}
 14%|â–ˆâ–        | 724/5000 [4:50:43<30:41:14, 25.84s/it] 14%|â–ˆâ–        | 725/5000 [4:51:10<30:58:03, 26.08s/it]                                                       {'loss': 4.9111, 'grad_norm': 0.11924664676189423, 'learning_rate': 0.0008724489795918367, 'epoch': 0.14}
 14%|â–ˆâ–        | 725/5000 [4:51:10<30:58:03, 26.08s/it] 15%|â–ˆâ–        | 726/5000 [4:51:31<29:04:46, 24.49s/it]                                                       {'loss': 4.8975, 'grad_norm': 0.053263526409864426, 'learning_rate': 0.0008722448979591837, 'epoch': 0.15}
 15%|â–ˆâ–        | 726/5000 [4:51:31<29:04:46, 24.49s/it] 15%|â–ˆâ–        | 727/5000 [4:51:55<29:06:07, 24.52s/it]                                                       {'loss': 4.9074, 'grad_norm': 0.10548534989356995, 'learning_rate': 0.0008720408163265306, 'epoch': 0.15}
 15%|â–ˆâ–        | 727/5000 [4:51:55<29:06:07, 24.52s/it] 15%|â–ˆâ–        | 728/5000 [4:52:22<30:02:37, 25.32s/it]                                                       {'loss': 4.9321, 'grad_norm': 0.11981890350580215, 'learning_rate': 0.0008718367346938776, 'epoch': 0.15}
 15%|â–ˆâ–        | 728/5000 [4:52:22<30:02:37, 25.32s/it] 15%|â–ˆâ–        | 729/5000 [4:52:48<30:17:39, 25.53s/it]                                                       {'loss': 4.9396, 'grad_norm': 0.1125425398349762, 'learning_rate': 0.0008716326530612246, 'epoch': 0.15}
 15%|â–ˆâ–        | 729/5000 [4:52:48<30:17:39, 25.53s/it] 15%|â–ˆâ–        | 730/5000 [4:53:12<29:29:23, 24.86s/it]                                                       {'loss': 4.9009, 'grad_norm': 0.08798398822546005, 'learning_rate': 0.0008714285714285715, 'epoch': 0.15}
 15%|â–ˆâ–        | 730/5000 [4:53:12<29:29:23, 24.86s/it] 15%|â–ˆâ–        | 731/5000 [4:53:35<29:01:55, 24.48s/it]                                                       {'loss': 4.893, 'grad_norm': 0.0564444400370121, 'learning_rate': 0.0008712244897959184, 'epoch': 0.15}
 15%|â–ˆâ–        | 731/5000 [4:53:35<29:01:55, 24.48s/it] 15%|â–ˆâ–        | 732/5000 [4:54:08<32:05:20, 27.07s/it]                                                       {'loss': 4.9426, 'grad_norm': 0.08700717985630035, 'learning_rate': 0.0008710204081632654, 'epoch': 0.15}
 15%|â–ˆâ–        | 732/5000 [4:54:08<32:05:20, 27.07s/it] 15%|â–ˆâ–        | 733/5000 [4:54:30<30:04:54, 25.38s/it]                                                       {'loss': 4.913, 'grad_norm': 0.058543816208839417, 'learning_rate': 0.0008708163265306123, 'epoch': 0.15}
 15%|â–ˆâ–        | 733/5000 [4:54:30<30:04:54, 25.38s/it] 15%|â–ˆâ–        | 734/5000 [4:54:52<28:58:04, 24.45s/it]                                                       {'loss': 4.9015, 'grad_norm': 0.04921390861272812, 'learning_rate': 0.0008706122448979592, 'epoch': 0.15}
 15%|â–ˆâ–        | 734/5000 [4:54:52<28:58:04, 24.45s/it] 15%|â–ˆâ–        | 735/5000 [4:55:14<27:55:03, 23.56s/it]                                                       {'loss': 4.9209, 'grad_norm': 0.08037681877613068, 'learning_rate': 0.0008704081632653061, 'epoch': 0.15}
 15%|â–ˆâ–        | 735/5000 [4:55:14<27:55:03, 23.56s/it] 15%|â–ˆâ–        | 736/5000 [4:55:37<27:45:25, 23.43s/it]                                                       {'loss': 4.9068, 'grad_norm': 0.10226224362850189, 'learning_rate': 0.0008702040816326531, 'epoch': 0.15}
 15%|â–ˆâ–        | 736/5000 [4:55:37<27:45:25, 23.43s/it] 15%|â–ˆâ–        | 737/5000 [4:56:01<28:05:49, 23.73s/it]                                                       {'loss': 4.9259, 'grad_norm': 0.12412501871585846, 'learning_rate': 0.00087, 'epoch': 0.15}
 15%|â–ˆâ–        | 737/5000 [4:56:01<28:05:49, 23.73s/it] 15%|â–ˆâ–        | 738/5000 [4:56:30<29:54:28, 25.26s/it]                                                       {'loss': 4.93, 'grad_norm': 0.0985238254070282, 'learning_rate': 0.0008697959183673469, 'epoch': 0.15}
 15%|â–ˆâ–        | 738/5000 [4:56:30<29:54:28, 25.26s/it] 15%|â–ˆâ–        | 739/5000 [4:56:56<30:08:14, 25.46s/it]                                                       {'loss': 4.9059, 'grad_norm': 0.06427233666181564, 'learning_rate': 0.0008695918367346939, 'epoch': 0.15}
 15%|â–ˆâ–        | 739/5000 [4:56:56<30:08:14, 25.46s/it] 15%|â–ˆâ–        | 740/5000 [4:57:17<28:44:23, 24.29s/it]                                                       {'loss': 4.8901, 'grad_norm': 0.08016267418861389, 'learning_rate': 0.0008693877551020408, 'epoch': 0.15}
 15%|â–ˆâ–        | 740/5000 [4:57:17<28:44:23, 24.29s/it] 15%|â–ˆâ–        | 741/5000 [4:57:43<29:15:57, 24.74s/it]                                                       {'loss': 4.9249, 'grad_norm': 0.13572503626346588, 'learning_rate': 0.0008691836734693877, 'epoch': 0.15}
 15%|â–ˆâ–        | 741/5000 [4:57:43<29:15:57, 24.74s/it] 15%|â–ˆâ–        | 742/5000 [4:58:09<29:40:11, 25.08s/it]                                                       {'loss': 4.8996, 'grad_norm': 0.06388791650533676, 'learning_rate': 0.0008689795918367347, 'epoch': 0.15}
 15%|â–ˆâ–        | 742/5000 [4:58:09<29:40:11, 25.08s/it] 15%|â–ˆâ–        | 743/5000 [4:58:30<28:08:23, 23.80s/it]                                                       {'loss': 4.9175, 'grad_norm': 0.17020080983638763, 'learning_rate': 0.0008687755102040816, 'epoch': 0.15}
 15%|â–ˆâ–        | 743/5000 [4:58:30<28:08:23, 23.80s/it] 15%|â–ˆâ–        | 744/5000 [4:59:00<30:22:12, 25.69s/it]                                                       {'loss': 4.8994, 'grad_norm': 0.06006050482392311, 'learning_rate': 0.0008685714285714286, 'epoch': 0.15}
 15%|â–ˆâ–        | 744/5000 [4:59:00<30:22:12, 25.69s/it] 15%|â–ˆâ–        | 745/5000 [4:59:31<32:24:02, 27.41s/it]                                                       {'loss': 4.9188, 'grad_norm': 0.10769142955541611, 'learning_rate': 0.0008683673469387755, 'epoch': 0.15}
 15%|â–ˆâ–        | 745/5000 [4:59:31<32:24:02, 27.41s/it] 15%|â–ˆâ–        | 746/5000 [4:59:53<30:20:34, 25.68s/it]                                                       {'loss': 4.8926, 'grad_norm': 0.13284222781658173, 'learning_rate': 0.0008681632653061225, 'epoch': 0.15}
 15%|â–ˆâ–        | 746/5000 [4:59:53<30:20:34, 25.68s/it] 15%|â–ˆâ–        | 747/5000 [5:00:14<28:38:49, 24.25s/it]                                                       {'loss': 4.8966, 'grad_norm': 0.09935276210308075, 'learning_rate': 0.0008679591836734694, 'epoch': 0.15}
 15%|â–ˆâ–        | 747/5000 [5:00:14<28:38:49, 24.25s/it] 15%|â–ˆâ–        | 748/5000 [5:00:35<27:32:20, 23.32s/it]                                                       {'loss': 4.899, 'grad_norm': 0.052657537162303925, 'learning_rate': 0.0008677551020408163, 'epoch': 0.15}
 15%|â–ˆâ–        | 748/5000 [5:00:35<27:32:20, 23.32s/it] 15%|â–ˆâ–        | 749/5000 [5:01:01<28:15:37, 23.93s/it]                                                       {'loss': 4.9116, 'grad_norm': 0.17184169590473175, 'learning_rate': 0.0008675510204081633, 'epoch': 0.15}
 15%|â–ˆâ–        | 749/5000 [5:01:01<28:15:37, 23.93s/it] 15%|â–ˆâ–Œ        | 750/5000 [5:01:30<30:16:13, 25.64s/it]                                                       {'loss': 4.9078, 'grad_norm': 0.05791599303483963, 'learning_rate': 0.0008673469387755102, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 750/5000 [5:01:30<30:16:13, 25.64s/it][2025-10-19 22:48:29,389] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 15%|â–ˆâ–Œ        | 751/5000 [5:01:55<29:58:21, 25.39s/it]                                                       {'loss': 4.8941, 'grad_norm': 0.07121933996677399, 'learning_rate': 0.0008671428571428571, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 751/5000 [5:01:55<29:58:21, 25.39s/it] 15%|â–ˆâ–Œ        | 752/5000 [5:02:23<30:58:56, 26.26s/it]                                                       {'loss': 4.8987, 'grad_norm': 0.055114585906267166, 'learning_rate': 0.0008669387755102041, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 752/5000 [5:02:23<30:58:56, 26.26s/it] 15%|â–ˆâ–Œ        | 753/5000 [5:02:47<30:02:20, 25.46s/it]                                                       {'loss': 4.8955, 'grad_norm': 0.12313085794448853, 'learning_rate': 0.000866734693877551, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 753/5000 [5:02:47<30:02:20, 25.46s/it] 15%|â–ˆâ–Œ        | 754/5000 [5:03:13<30:10:13, 25.58s/it]                                                       {'loss': 4.9108, 'grad_norm': 0.05216658487915993, 'learning_rate': 0.0008665306122448979, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 754/5000 [5:03:13<30:10:13, 25.58s/it] 15%|â–ˆâ–Œ        | 755/5000 [5:03:37<29:40:42, 25.17s/it]                                                       {'loss': 4.8903, 'grad_norm': 0.045524872839450836, 'learning_rate': 0.0008663265306122448, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 755/5000 [5:03:37<29:40:42, 25.17s/it] 15%|â–ˆâ–Œ        | 756/5000 [5:04:01<29:23:09, 24.93s/it]                                                       {'loss': 4.9111, 'grad_norm': 0.06663332134485245, 'learning_rate': 0.0008661224489795918, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 756/5000 [5:04:01<29:23:09, 24.93s/it] 15%|â–ˆâ–Œ        | 757/5000 [5:04:24<28:30:39, 24.19s/it]                                                       {'loss': 4.8968, 'grad_norm': 0.08945297449827194, 'learning_rate': 0.0008659183673469387, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 757/5000 [5:04:24<28:30:39, 24.19s/it] 15%|â–ˆâ–Œ        | 758/5000 [5:04:48<28:31:09, 24.20s/it]                                                       {'loss': 4.8926, 'grad_norm': 0.046731650829315186, 'learning_rate': 0.0008657142857142857, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 758/5000 [5:04:48<28:31:09, 24.20s/it] 15%|â–ˆâ–Œ        | 759/5000 [5:05:09<27:24:52, 23.27s/it]                                                       {'loss': 4.8894, 'grad_norm': 0.048146434128284454, 'learning_rate': 0.0008655102040816328, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 759/5000 [5:05:09<27:24:52, 23.27s/it] 15%|â–ˆâ–Œ        | 760/5000 [5:05:33<27:46:38, 23.58s/it]                                                       {'loss': 4.9173, 'grad_norm': 0.08337568491697311, 'learning_rate': 0.0008653061224489797, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 760/5000 [5:05:33<27:46:38, 23.58s/it] 15%|â–ˆâ–Œ        | 761/5000 [5:06:01<29:02:27, 24.66s/it]                                                       {'loss': 4.9032, 'grad_norm': 0.061358317732810974, 'learning_rate': 0.0008651020408163266, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 761/5000 [5:06:01<29:02:27, 24.66s/it] 15%|â–ˆâ–Œ        | 762/5000 [5:06:35<32:24:41, 27.53s/it]                                                       {'loss': 4.9115, 'grad_norm': 0.09416870027780533, 'learning_rate': 0.0008648979591836736, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 762/5000 [5:06:35<32:24:41, 27.53s/it] 15%|â–ˆâ–Œ        | 763/5000 [5:06:59<31:06:20, 26.43s/it]                                                       {'loss': 4.9056, 'grad_norm': 0.12819825112819672, 'learning_rate': 0.0008646938775510205, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 763/5000 [5:06:59<31:06:20, 26.43s/it] 15%|â–ˆâ–Œ        | 764/5000 [5:07:20<29:16:10, 24.88s/it]                                                       {'loss': 4.8764, 'grad_norm': 0.0456220917403698, 'learning_rate': 0.0008644897959183674, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 764/5000 [5:07:20<29:16:10, 24.88s/it] 15%|â–ˆâ–Œ        | 765/5000 [5:07:44<28:58:51, 24.64s/it]                                                       {'loss': 4.9133, 'grad_norm': 0.11316832154989243, 'learning_rate': 0.0008642857142857144, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 765/5000 [5:07:44<28:58:51, 24.64s/it] 15%|â–ˆâ–Œ        | 766/5000 [5:08:06<28:08:30, 23.93s/it]                                                       {'loss': 4.9023, 'grad_norm': 0.1044868752360344, 'learning_rate': 0.0008640816326530613, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 766/5000 [5:08:06<28:08:30, 23.93s/it] 15%|â–ˆâ–Œ        | 767/5000 [5:08:32<28:38:44, 24.36s/it]                                                       {'loss': 4.8971, 'grad_norm': 0.06467802822589874, 'learning_rate': 0.0008638775510204082, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 767/5000 [5:08:32<28:38:44, 24.36s/it] 15%|â–ˆâ–Œ        | 768/5000 [5:08:56<28:28:28, 24.22s/it]                                                       {'loss': 4.8991, 'grad_norm': 0.05188358947634697, 'learning_rate': 0.0008636734693877551, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 768/5000 [5:08:56<28:28:28, 24.22s/it] 15%|â–ˆâ–Œ        | 769/5000 [5:09:20<28:25:52, 24.19s/it]                                                       {'loss': 4.9227, 'grad_norm': 0.07507387548685074, 'learning_rate': 0.0008634693877551021, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 769/5000 [5:09:20<28:25:52, 24.19s/it] 15%|â–ˆâ–Œ        | 770/5000 [5:09:48<29:44:50, 25.32s/it]                                                       {'loss': 4.9294, 'grad_norm': 0.11624173074960709, 'learning_rate': 0.000863265306122449, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 770/5000 [5:09:48<29:44:50, 25.32s/it] 15%|â–ˆâ–Œ        | 771/5000 [5:10:09<28:23:47, 24.17s/it]                                                       {'loss': 4.8999, 'grad_norm': 0.07515939325094223, 'learning_rate': 0.0008630612244897959, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 771/5000 [5:10:09<28:23:47, 24.17s/it] 15%|â–ˆâ–Œ        | 772/5000 [5:10:37<29:39:05, 25.25s/it]                                                       {'loss': 4.8969, 'grad_norm': 0.06116744875907898, 'learning_rate': 0.0008628571428571429, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 772/5000 [5:10:37<29:39:05, 25.25s/it] 15%|â–ˆâ–Œ        | 773/5000 [5:11:04<30:17:52, 25.80s/it]                                                       {'loss': 4.9245, 'grad_norm': 0.11307741701602936, 'learning_rate': 0.0008626530612244898, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 773/5000 [5:11:04<30:17:52, 25.80s/it] 15%|â–ˆâ–Œ        | 774/5000 [5:11:36<32:22:13, 27.58s/it]                                                       {'loss': 4.9386, 'grad_norm': 0.10636187344789505, 'learning_rate': 0.0008624489795918367, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 774/5000 [5:11:36<32:22:13, 27.58s/it] 16%|â–ˆâ–Œ        | 775/5000 [5:11:57<30:08:01, 25.68s/it]                                                       {'loss': 4.8982, 'grad_norm': 0.07096758484840393, 'learning_rate': 0.0008622448979591838, 'epoch': 0.15}
 16%|â–ˆâ–Œ        | 775/5000 [5:11:57<30:08:01, 25.68s/it] 16%|â–ˆâ–Œ        | 776/5000 [5:12:18<28:28:12, 24.26s/it]                                                       {'loss': 4.885, 'grad_norm': 0.07712242752313614, 'learning_rate': 0.0008620408163265307, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 776/5000 [5:12:18<28:28:12, 24.26s/it] 16%|â–ˆâ–Œ        | 777/5000 [5:12:42<28:24:18, 24.21s/it]                                                       {'loss': 4.897, 'grad_norm': 0.1005023792386055, 'learning_rate': 0.0008618367346938776, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 777/5000 [5:12:42<28:24:18, 24.21s/it] 16%|â–ˆâ–Œ        | 778/5000 [5:13:04<27:30:08, 23.45s/it]                                                       {'loss': 4.915, 'grad_norm': 0.0651477575302124, 'learning_rate': 0.0008616326530612245, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 778/5000 [5:13:04<27:30:08, 23.45s/it] 16%|â–ˆâ–Œ        | 779/5000 [5:13:27<27:17:51, 23.28s/it]                                                       {'loss': 4.9012, 'grad_norm': 0.05473775044083595, 'learning_rate': 0.0008614285714285715, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 779/5000 [5:13:27<27:17:51, 23.28s/it] 16%|â–ˆâ–Œ        | 780/5000 [5:13:48<26:30:24, 22.61s/it]                                                       {'loss': 4.9035, 'grad_norm': 0.07595542073249817, 'learning_rate': 0.0008612244897959184, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 780/5000 [5:13:48<26:30:24, 22.61s/it] 16%|â–ˆâ–Œ        | 781/5000 [5:14:15<28:16:38, 24.13s/it]                                                       {'loss': 4.892, 'grad_norm': 0.04849899932742119, 'learning_rate': 0.0008610204081632653, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 781/5000 [5:14:15<28:16:38, 24.13s/it] 16%|â–ˆâ–Œ        | 782/5000 [5:14:37<27:32:42, 23.51s/it]                                                       {'loss': 4.8854, 'grad_norm': 0.06235918402671814, 'learning_rate': 0.0008608163265306123, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 782/5000 [5:14:37<27:32:42, 23.51s/it] 16%|â–ˆâ–Œ        | 783/5000 [5:15:05<28:52:58, 24.66s/it]                                                       {'loss': 4.9025, 'grad_norm': 0.21057526767253876, 'learning_rate': 0.0008606122448979592, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 783/5000 [5:15:05<28:52:58, 24.66s/it] 16%|â–ˆâ–Œ        | 784/5000 [5:15:27<28:06:08, 24.00s/it]                                                       {'loss': 4.8978, 'grad_norm': 0.05406690761446953, 'learning_rate': 0.0008604081632653061, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 784/5000 [5:15:27<28:06:08, 24.00s/it] 16%|â–ˆâ–Œ        | 785/5000 [5:15:51<27:58:10, 23.89s/it]                                                       {'loss': 4.8819, 'grad_norm': 0.06737448275089264, 'learning_rate': 0.000860204081632653, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 785/5000 [5:15:51<27:58:10, 23.89s/it] 16%|â–ˆâ–Œ        | 786/5000 [5:16:18<29:09:33, 24.91s/it]                                                       {'loss': 4.8989, 'grad_norm': 0.06544456630945206, 'learning_rate': 0.00086, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 786/5000 [5:16:18<29:09:33, 24.91s/it] 16%|â–ˆâ–Œ        | 787/5000 [5:16:41<28:36:01, 24.44s/it]                                                       {'loss': 4.8962, 'grad_norm': 0.1011861115694046, 'learning_rate': 0.0008597959183673469, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 787/5000 [5:16:41<28:36:01, 24.44s/it] 16%|â–ˆâ–Œ        | 788/5000 [5:17:03<27:32:14, 23.54s/it]                                                       {'loss': 4.8979, 'grad_norm': 0.09931346774101257, 'learning_rate': 0.0008595918367346938, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 788/5000 [5:17:03<27:32:14, 23.54s/it] 16%|â–ˆâ–Œ        | 789/5000 [5:17:25<27:08:17, 23.20s/it]                                                       {'loss': 4.9106, 'grad_norm': 0.15631882846355438, 'learning_rate': 0.0008593877551020408, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 789/5000 [5:17:25<27:08:17, 23.20s/it] 16%|â–ˆâ–Œ        | 790/5000 [5:17:49<27:12:04, 23.26s/it]                                                       {'loss': 4.8809, 'grad_norm': 0.06845775991678238, 'learning_rate': 0.0008591836734693877, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 790/5000 [5:17:49<27:12:04, 23.26s/it] 16%|â–ˆâ–Œ        | 791/5000 [5:18:14<28:00:47, 23.96s/it]                                                       {'loss': 4.8859, 'grad_norm': 0.06948339194059372, 'learning_rate': 0.0008589795918367347, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 791/5000 [5:18:14<28:00:47, 23.96s/it] 16%|â–ˆâ–Œ        | 792/5000 [5:18:35<27:02:20, 23.13s/it]                                                       {'loss': 4.9026, 'grad_norm': 0.05485737323760986, 'learning_rate': 0.0008587755102040817, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 792/5000 [5:18:35<27:02:20, 23.13s/it] 16%|â–ˆâ–Œ        | 793/5000 [5:18:57<26:29:07, 22.66s/it]                                                       {'loss': 4.8995, 'grad_norm': 0.06371916830539703, 'learning_rate': 0.0008585714285714286, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 793/5000 [5:18:57<26:29:07, 22.66s/it] 16%|â–ˆâ–Œ        | 794/5000 [5:19:19<26:10:07, 22.40s/it]                                                       {'loss': 4.8988, 'grad_norm': 0.1412688046693802, 'learning_rate': 0.0008583673469387755, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 794/5000 [5:19:19<26:10:07, 22.40s/it] 16%|â–ˆâ–Œ        | 795/5000 [5:19:40<25:55:21, 22.19s/it]                                                       {'loss': 4.9096, 'grad_norm': 0.06410545855760574, 'learning_rate': 0.0008581632653061225, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 795/5000 [5:19:40<25:55:21, 22.19s/it] 16%|â–ˆâ–Œ        | 796/5000 [5:20:06<27:11:46, 23.29s/it]                                                       {'loss': 4.896, 'grad_norm': 0.05613543093204498, 'learning_rate': 0.0008579591836734694, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 796/5000 [5:20:06<27:11:46, 23.29s/it] 16%|â–ˆâ–Œ        | 797/5000 [5:20:28<26:31:59, 22.73s/it]                                                       {'loss': 4.9307, 'grad_norm': 0.13500259816646576, 'learning_rate': 0.0008577551020408163, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 797/5000 [5:20:28<26:31:59, 22.73s/it] 16%|â–ˆâ–Œ        | 798/5000 [5:20:52<26:54:28, 23.05s/it]                                                       {'loss': 4.8851, 'grad_norm': 0.04944261536002159, 'learning_rate': 0.0008575510204081632, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 798/5000 [5:20:52<26:54:28, 23.05s/it] 16%|â–ˆâ–Œ        | 799/5000 [5:21:16<27:13:32, 23.33s/it]                                                       {'loss': 4.8923, 'grad_norm': 0.07244915515184402, 'learning_rate': 0.0008573469387755102, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 799/5000 [5:21:16<27:13:32, 23.33s/it] 16%|â–ˆâ–Œ        | 800/5000 [5:21:37<26:43:57, 22.91s/it]                                                       {'loss': 4.9003, 'grad_norm': 0.0518469512462616, 'learning_rate': 0.0008571428571428571, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 800/5000 [5:21:37<26:43:57, 22.91s/it][2025-10-19 23:08:36,706] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 16%|â–ˆâ–Œ        | 801/5000 [5:22:03<27:36:14, 23.67s/it]                                                       {'loss': 4.8954, 'grad_norm': 0.07171246409416199, 'learning_rate': 0.000856938775510204, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 801/5000 [5:22:03<27:36:14, 23.67s/it] 16%|â–ˆâ–Œ        | 802/5000 [5:22:27<27:44:27, 23.79s/it]                                                       {'loss': 4.8934, 'grad_norm': 0.06510716676712036, 'learning_rate': 0.000856734693877551, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 802/5000 [5:22:27<27:44:27, 23.79s/it] 16%|â–ˆâ–Œ        | 803/5000 [5:22:50<27:36:29, 23.68s/it]                                                       {'loss': 4.914, 'grad_norm': 0.10312014818191528, 'learning_rate': 0.0008565306122448979, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 803/5000 [5:22:50<27:36:29, 23.68s/it] 16%|â–ˆâ–Œ        | 804/5000 [5:23:13<27:05:47, 23.25s/it]                                                       {'loss': 4.8878, 'grad_norm': 0.05312661454081535, 'learning_rate': 0.0008563265306122449, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 804/5000 [5:23:13<27:05:47, 23.25s/it] 16%|â–ˆâ–Œ        | 805/5000 [5:23:37<27:21:33, 23.48s/it]                                                       {'loss': 4.9144, 'grad_norm': 0.08320577442646027, 'learning_rate': 0.0008561224489795919, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 805/5000 [5:23:37<27:21:33, 23.48s/it] 16%|â–ˆâ–Œ        | 806/5000 [5:24:01<27:43:44, 23.80s/it]                                                       {'loss': 4.9063, 'grad_norm': 0.20553915202617645, 'learning_rate': 0.0008559183673469389, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 806/5000 [5:24:01<27:43:44, 23.80s/it] 16%|â–ˆâ–Œ        | 807/5000 [5:24:22<26:33:48, 22.81s/it]                                                       {'loss': 4.8957, 'grad_norm': 0.07366848737001419, 'learning_rate': 0.0008557142857142858, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 807/5000 [5:24:22<26:33:48, 22.81s/it] 16%|â–ˆâ–Œ        | 808/5000 [5:24:47<27:16:01, 23.42s/it]                                                       {'loss': 4.8842, 'grad_norm': 0.09454616904258728, 'learning_rate': 0.0008555102040816328, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 808/5000 [5:24:47<27:16:01, 23.42s/it] 16%|â–ˆâ–Œ        | 809/5000 [5:25:12<27:49:16, 23.90s/it]                                                       {'loss': 4.9002, 'grad_norm': 0.07702410221099854, 'learning_rate': 0.0008553061224489797, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 809/5000 [5:25:12<27:49:16, 23.90s/it] 16%|â–ˆâ–Œ        | 810/5000 [5:25:33<26:51:46, 23.08s/it]                                                       {'loss': 4.8759, 'grad_norm': 0.031982142478227615, 'learning_rate': 0.0008551020408163266, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 810/5000 [5:25:33<26:51:46, 23.08s/it] 16%|â–ˆâ–Œ        | 811/5000 [5:25:57<27:09:24, 23.34s/it]                                                       {'loss': 4.8999, 'grad_norm': 0.09612065553665161, 'learning_rate': 0.0008548979591836735, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 811/5000 [5:25:57<27:09:24, 23.34s/it] 16%|â–ˆâ–Œ        | 812/5000 [5:26:22<27:55:28, 24.00s/it]                                                       {'loss': 4.9171, 'grad_norm': 0.10085882991552353, 'learning_rate': 0.0008546938775510205, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 812/5000 [5:26:22<27:55:28, 24.00s/it] 16%|â–ˆâ–‹        | 813/5000 [5:26:47<28:15:18, 24.29s/it]                                                       {'loss': 4.9207, 'grad_norm': 0.09900107234716415, 'learning_rate': 0.0008544897959183674, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 813/5000 [5:26:47<28:15:18, 24.29s/it] 16%|â–ˆâ–‹        | 814/5000 [5:27:08<27:06:17, 23.31s/it]                                                       {'loss': 4.8854, 'grad_norm': 0.04330173507332802, 'learning_rate': 0.0008542857142857143, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 814/5000 [5:27:08<27:06:17, 23.31s/it] 16%|â–ˆâ–‹        | 815/5000 [5:27:29<26:13:30, 22.56s/it]                                                       {'loss': 4.8877, 'grad_norm': 0.06822085380554199, 'learning_rate': 0.0008540816326530613, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 815/5000 [5:27:29<26:13:30, 22.56s/it] 16%|â–ˆâ–‹        | 816/5000 [5:27:51<26:00:53, 22.38s/it]                                                       {'loss': 4.8969, 'grad_norm': 0.06498636305332184, 'learning_rate': 0.0008538775510204082, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 816/5000 [5:27:51<26:00:53, 22.38s/it] 16%|â–ˆâ–‹        | 817/5000 [5:28:21<28:40:21, 24.68s/it]                                                       {'loss': 4.914, 'grad_norm': 0.09698227792978287, 'learning_rate': 0.0008536734693877551, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 817/5000 [5:28:21<28:40:21, 24.68s/it] 16%|â–ˆâ–‹        | 818/5000 [5:28:44<28:12:15, 24.28s/it]                                                       {'loss': 4.8916, 'grad_norm': 0.06288707256317139, 'learning_rate': 0.000853469387755102, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 818/5000 [5:28:44<28:12:15, 24.28s/it] 16%|â–ˆâ–‹        | 819/5000 [5:29:06<27:12:53, 23.43s/it]                                                       {'loss': 4.8906, 'grad_norm': 0.05136506259441376, 'learning_rate': 0.000853265306122449, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 819/5000 [5:29:06<27:12:53, 23.43s/it] 16%|â–ˆâ–‹        | 820/5000 [5:29:34<28:51:46, 24.86s/it]                                                       {'loss': 4.8819, 'grad_norm': 0.06864919513463974, 'learning_rate': 0.0008530612244897959, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 820/5000 [5:29:34<28:51:46, 24.86s/it] 16%|â–ˆâ–‹        | 821/5000 [5:29:59<28:44:06, 24.75s/it]                                                       {'loss': 4.9334, 'grad_norm': 0.08753324300050735, 'learning_rate': 0.0008528571428571428, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 821/5000 [5:29:59<28:44:06, 24.75s/it] 16%|â–ˆâ–‹        | 822/5000 [5:30:23<28:33:38, 24.61s/it]                                                       {'loss': 4.8854, 'grad_norm': 0.08466564118862152, 'learning_rate': 0.0008526530612244899, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 822/5000 [5:30:23<28:33:38, 24.61s/it] 16%|â–ˆâ–‹        | 823/5000 [5:30:44<27:13:06, 23.46s/it]                                                       {'loss': 4.9014, 'grad_norm': 0.14517168700695038, 'learning_rate': 0.0008524489795918368, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 823/5000 [5:30:44<27:13:06, 23.46s/it] 16%|â–ˆâ–‹        | 824/5000 [5:31:06<26:53:59, 23.19s/it]                                                       {'loss': 4.8895, 'grad_norm': 0.0444282665848732, 'learning_rate': 0.0008522448979591837, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 824/5000 [5:31:06<26:53:59, 23.19s/it] 16%|â–ˆâ–‹        | 825/5000 [5:31:30<27:09:57, 23.42s/it]                                                       {'loss': 4.8967, 'grad_norm': 0.07590265572071075, 'learning_rate': 0.0008520408163265307, 'epoch': 0.17}
 16%|â–ˆâ–‹        | 825/5000 [5:31:30<27:09:57, 23.42s/it] 17%|â–ˆâ–‹        | 826/5000 [5:31:53<26:58:51, 23.27s/it]                                                       {'loss': 4.8888, 'grad_norm': 0.06677570939064026, 'learning_rate': 0.0008518367346938776, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 826/5000 [5:31:53<26:58:51, 23.27s/it] 17%|â–ˆâ–‹        | 827/5000 [5:32:15<26:32:12, 22.89s/it]                                                       {'loss': 4.901, 'grad_norm': 0.07339920848608017, 'learning_rate': 0.0008516326530612245, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 827/5000 [5:32:15<26:32:12, 22.89s/it] 17%|â–ˆâ–‹        | 828/5000 [5:32:39<27:03:05, 23.34s/it]                                                       {'loss': 4.9209, 'grad_norm': 0.10909951478242874, 'learning_rate': 0.0008514285714285714, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 828/5000 [5:32:39<27:03:05, 23.34s/it] 17%|â–ˆâ–‹        | 829/5000 [5:33:01<26:20:00, 22.73s/it]                                                       {'loss': 4.8829, 'grad_norm': 0.0838637426495552, 'learning_rate': 0.0008512244897959184, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 829/5000 [5:33:01<26:20:00, 22.73s/it] 17%|â–ˆâ–‹        | 830/5000 [5:33:25<26:53:47, 23.22s/it]                                                       {'loss': 4.9135, 'grad_norm': 0.05998879298567772, 'learning_rate': 0.0008510204081632653, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 830/5000 [5:33:25<26:53:47, 23.22s/it] 17%|â–ˆâ–‹        | 831/5000 [5:33:47<26:33:38, 22.94s/it]                                                       {'loss': 4.9189, 'grad_norm': 0.08844213932752609, 'learning_rate': 0.0008508163265306122, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 831/5000 [5:33:47<26:33:38, 22.94s/it] 17%|â–ˆâ–‹        | 832/5000 [5:34:11<26:47:18, 23.14s/it]                                                       {'loss': 4.9115, 'grad_norm': 0.06745041161775589, 'learning_rate': 0.0008506122448979592, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 832/5000 [5:34:11<26:47:18, 23.14s/it] 17%|â–ˆâ–‹        | 833/5000 [5:34:35<27:05:20, 23.40s/it]                                                       {'loss': 4.9012, 'grad_norm': 0.05680065602064133, 'learning_rate': 0.0008504081632653061, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 833/5000 [5:34:35<27:05:20, 23.40s/it] 17%|â–ˆâ–‹        | 834/5000 [5:35:00<27:45:53, 23.99s/it]                                                       {'loss': 4.8917, 'grad_norm': 0.10410377383232117, 'learning_rate': 0.000850204081632653, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 834/5000 [5:35:00<27:45:53, 23.99s/it] 17%|â–ˆâ–‹        | 835/5000 [5:35:26<28:14:11, 24.41s/it]                                                       {'loss': 4.902, 'grad_norm': 0.11952695995569229, 'learning_rate': 0.00085, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 835/5000 [5:35:26<28:14:11, 24.41s/it] 17%|â–ˆâ–‹        | 836/5000 [5:35:49<27:54:34, 24.13s/it]                                                       {'loss': 4.8947, 'grad_norm': 0.06283009052276611, 'learning_rate': 0.0008497959183673469, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 836/5000 [5:35:49<27:54:34, 24.13s/it] 17%|â–ˆâ–‹        | 837/5000 [5:36:11<27:13:07, 23.54s/it]                                                       {'loss': 4.8959, 'grad_norm': 0.06039859727025032, 'learning_rate': 0.0008495918367346939, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 837/5000 [5:36:11<27:13:07, 23.54s/it] 17%|â–ˆâ–‹        | 838/5000 [5:36:35<27:22:25, 23.68s/it]                                                       {'loss': 4.9039, 'grad_norm': 0.07637622207403183, 'learning_rate': 0.0008493877551020408, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 838/5000 [5:36:35<27:22:25, 23.68s/it] 17%|â–ˆâ–‹        | 839/5000 [5:37:02<28:25:31, 24.59s/it]                                                       {'loss': 4.897, 'grad_norm': 0.06582590192556381, 'learning_rate': 0.0008491836734693878, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 839/5000 [5:37:02<28:25:31, 24.59s/it] 17%|â–ˆâ–‹        | 840/5000 [5:37:27<28:21:43, 24.54s/it]                                                       {'loss': 4.9061, 'grad_norm': 0.06478196382522583, 'learning_rate': 0.0008489795918367347, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 840/5000 [5:37:27<28:21:43, 24.54s/it] 17%|â–ˆâ–‹        | 841/5000 [5:37:54<29:24:35, 25.46s/it]                                                       {'loss': 4.9123, 'grad_norm': 0.11752129346132278, 'learning_rate': 0.0008487755102040816, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 841/5000 [5:37:54<29:24:35, 25.46s/it] 17%|â–ˆâ–‹        | 842/5000 [5:38:20<29:24:12, 25.46s/it]                                                       {'loss': 4.8856, 'grad_norm': 0.04458083212375641, 'learning_rate': 0.0008485714285714286, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 842/5000 [5:38:20<29:24:12, 25.46s/it] 17%|â–ˆâ–‹        | 843/5000 [5:38:45<29:28:35, 25.53s/it]                                                       {'loss': 4.8979, 'grad_norm': 0.08017774671316147, 'learning_rate': 0.0008483673469387755, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 843/5000 [5:38:45<29:28:35, 25.53s/it] 17%|â–ˆâ–‹        | 844/5000 [5:39:15<31:05:08, 26.93s/it]                                                       {'loss': 4.883, 'grad_norm': 0.044341396540403366, 'learning_rate': 0.0008481632653061224, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 844/5000 [5:39:15<31:05:08, 26.93s/it] 17%|â–ˆâ–‹        | 845/5000 [5:39:42<30:59:19, 26.85s/it]                                                       {'loss': 4.9158, 'grad_norm': 0.10754558444023132, 'learning_rate': 0.0008479591836734694, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 845/5000 [5:39:42<30:59:19, 26.85s/it] 17%|â–ˆâ–‹        | 846/5000 [5:40:04<29:14:09, 25.34s/it]                                                       {'loss': 4.9068, 'grad_norm': 0.09080352634191513, 'learning_rate': 0.0008477551020408163, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 846/5000 [5:40:04<29:14:09, 25.34s/it] 17%|â–ˆâ–‹        | 847/5000 [5:40:31<29:44:57, 25.79s/it]                                                       {'loss': 4.9108, 'grad_norm': 0.15112371742725372, 'learning_rate': 0.0008475510204081632, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 847/5000 [5:40:31<29:44:57, 25.79s/it] 17%|â–ˆâ–‹        | 848/5000 [5:40:53<28:26:57, 24.67s/it]                                                       {'loss': 4.8805, 'grad_norm': 0.05086536332964897, 'learning_rate': 0.0008473469387755101, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 848/5000 [5:40:53<28:26:57, 24.67s/it] 17%|â–ˆâ–‹        | 849/5000 [5:41:20<29:17:03, 25.40s/it]                                                       {'loss': 4.8939, 'grad_norm': 0.05794043466448784, 'learning_rate': 0.0008471428571428572, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 849/5000 [5:41:20<29:17:03, 25.40s/it] 17%|â–ˆâ–‹        | 850/5000 [5:41:47<29:45:08, 25.81s/it]                                                       {'loss': 4.8893, 'grad_norm': 0.04931343346834183, 'learning_rate': 0.0008469387755102041, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 850/5000 [5:41:47<29:45:08, 25.81s/it][2025-10-19 23:28:45,929] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 17%|â–ˆâ–‹        | 851/5000 [5:42:14<30:13:04, 26.22s/it]                                                       {'loss': 4.8994, 'grad_norm': 0.05786234512925148, 'learning_rate': 0.000846734693877551, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 851/5000 [5:42:14<30:13:04, 26.22s/it] 17%|â–ˆâ–‹        | 852/5000 [5:42:36<28:42:53, 24.92s/it]                                                       {'loss': 4.8955, 'grad_norm': 0.1205674484372139, 'learning_rate': 0.0008465306122448981, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 852/5000 [5:42:36<28:42:53, 24.92s/it] 17%|â–ˆâ–‹        | 853/5000 [5:42:57<27:24:02, 23.79s/it]                                                       {'loss': 4.8916, 'grad_norm': 0.08755405247211456, 'learning_rate': 0.000846326530612245, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 853/5000 [5:42:57<27:24:02, 23.79s/it] 17%|â–ˆâ–‹        | 854/5000 [5:43:24<28:29:06, 24.73s/it]                                                       {'loss': 4.8969, 'grad_norm': 0.07134738564491272, 'learning_rate': 0.0008461224489795919, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 854/5000 [5:43:24<28:29:06, 24.73s/it] 17%|â–ˆâ–‹        | 855/5000 [5:43:49<28:43:49, 24.95s/it]                                                       {'loss': 4.9155, 'grad_norm': 0.11801128089427948, 'learning_rate': 0.0008459183673469389, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 855/5000 [5:43:49<28:43:49, 24.95s/it] 17%|â–ˆâ–‹        | 856/5000 [5:44:14<28:42:51, 24.94s/it]                                                       {'loss': 4.9071, 'grad_norm': 0.13453659415245056, 'learning_rate': 0.0008457142857142858, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 856/5000 [5:44:14<28:42:51, 24.94s/it] 17%|â–ˆâ–‹        | 857/5000 [5:44:41<29:24:36, 25.56s/it]                                                       {'loss': 4.8938, 'grad_norm': 0.13465477526187897, 'learning_rate': 0.0008455102040816327, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 857/5000 [5:44:41<29:24:36, 25.56s/it] 17%|â–ˆâ–‹        | 858/5000 [5:45:06<29:17:12, 25.45s/it]                                                       {'loss': 4.8873, 'grad_norm': 0.07175406068563461, 'learning_rate': 0.0008453061224489797, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 858/5000 [5:45:06<29:17:12, 25.45s/it] 17%|â–ˆâ–‹        | 859/5000 [5:45:34<29:54:39, 26.00s/it]                                                       {'loss': 4.9059, 'grad_norm': 0.11741422861814499, 'learning_rate': 0.0008451020408163266, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 859/5000 [5:45:34<29:54:39, 26.00s/it] 17%|â–ˆâ–‹        | 860/5000 [5:45:57<28:57:00, 25.17s/it]                                                       {'loss': 4.8942, 'grad_norm': 0.07934026420116425, 'learning_rate': 0.0008448979591836735, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 860/5000 [5:45:57<28:57:00, 25.17s/it] 17%|â–ˆâ–‹        | 861/5000 [5:46:21<28:41:05, 24.95s/it]                                                       {'loss': 4.8875, 'grad_norm': 0.05408686026930809, 'learning_rate': 0.0008446938775510204, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 861/5000 [5:46:21<28:41:05, 24.95s/it]