==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/train.log
W1021 00:48:25.573000 128671711295296 torch/distributed/run.py:779] 
W1021 00:48:25.573000 128671711295296 torch/distributed/run.py:779] *****************************************
W1021 00:48:25.573000 128671711295296 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1021 00:48:25.573000 128671711295296 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-21 00:48:34,682] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.97it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251021_004835-27o4eeix
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/27o4eeix
[2025-10-21 00:48:36,593] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.95it/s]
[2025-10-21 00:48:37,231] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-21 00:48:37,236] INFO [src.utils:19] Loading lora adapter from TailTokenDetachPrefixWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-21 00:48:45,892] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-21 00:48:47,547] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-21 00:48:47,547] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-21 00:48:51,772] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-21 00:48:51,772] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-21 00:48:52,614] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-21 00:48:52,615] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-21 00:48:52,615] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-21 00:48:52,617] INFO [src.utils:19] ==================================================
[2025-10-21 00:48:52,618] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-21 00:48:52,619] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:48:52,620] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:48:52,621] INFO [src.utils:19] ==================================================
[2025-10-21 00:48:54,385] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:48:54,385] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:48:54,386] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:48:54,386] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:48:54,386] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:48:54,386] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:48:54,386] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:48:54,386] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:48:54,386] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:48:54,387] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:48:54,387] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:48:54,387] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:48:54,388] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:48:54,388] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:48:54,394] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-21 00:48:54,396] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1021 00:48:57.395197883 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1021 00:48:57.445456187 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:42:50,  4.03s/it]                                                  {'loss': 48.8187, 'grad_norm': 1105.453125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:42:50,  4.03s/it]  0%|          | 2/6000 [00:06<5:17:08,  3.17s/it]                                                  {'loss': 46.0145, 'grad_norm': 1351.8187255859375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:17:08,  3.17s/it]  0%|          | 3/6000 [00:09<4:55:30,  2.96s/it]                                                  {'loss': 45.6821, 'grad_norm': 1339.3243408203125, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:55:30,  2.96s/it]  0%|          | 4/6000 [00:11<4:43:20,  2.84s/it]                                                  {'loss': 44.8137, 'grad_norm': 1244.635498046875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:11<4:43:20,  2.84s/it]  0%|          | 5/6000 [00:14<4:37:22,  2.78s/it]                                                  {'loss': 45.3615, 'grad_norm': 1320.796630859375, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:37:22,  2.78s/it]  0%|          | 6/6000 [00:17<4:32:34,  2.73s/it]                                                  {'loss': 45.352, 'grad_norm': 1237.3663330078125, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:32:34,  2.73s/it]  0%|          | 7/6000 [00:19<4:28:34,  2.69s/it]                                                  {'loss': 43.4114, 'grad_norm': 1304.3621826171875, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:19<4:28:34,  2.69s/it]  0%|          | 8/6000 [00:22<4:24:12,  2.65s/it]                                                  {'loss': 43.6141, 'grad_norm': 1291.39013671875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:24:12,  2.65s/it]  0%|          | 9/6000 [00:25<4:25:38,  2.66s/it]                                                  {'loss': 34.2382, 'grad_norm': 1501.5391845703125, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:25:38,  2.66s/it]  0%|          | 10/6000 [00:27<4:23:07,  2.64s/it]                                                   {'loss': 37.2604, 'grad_norm': 1575.6419677734375, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:27<4:23:07,  2.64s/it]  0%|          | 11/6000 [00:30<4:31:12,  2.72s/it]                                                   {'loss': 38.5538, 'grad_norm': 2056.947021484375, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:30<4:31:12,  2.72s/it]  0%|          | 12/6000 [00:33<4:33:24,  2.74s/it]                                                   {'loss': 32.6887, 'grad_norm': 2548.945556640625, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:33<4:33:24,  2.74s/it]  0%|          | 13/6000 [00:36<4:30:28,  2.71s/it]                                                   {'loss': 27.8522, 'grad_norm': 2920.062744140625, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:30:28,  2.71s/it]  0%|          | 14/6000 [00:38<4:33:03,  2.74s/it]                                                   {'loss': 26.5995, 'grad_norm': 2883.93115234375, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:38<4:33:03,  2.74s/it]  0%|          | 15/6000 [00:41<4:30:36,  2.71s/it]                                                   {'loss': 19.887, 'grad_norm': 1491.1689453125, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:41<4:30:36,  2.71s/it]  0%|          | 16/6000 [00:44<4:28:17,  2.69s/it]                                                   {'loss': 22.1304, 'grad_norm': 1495.740234375, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:44<4:28:17,  2.69s/it]  0%|          | 17/6000 [00:46<4:27:16,  2.68s/it]                                                   {'loss': 20.0229, 'grad_norm': 1246.18505859375, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:46<4:27:16,  2.68s/it]  0%|          | 18/6000 [00:49<4:26:52,  2.68s/it]                                                   {'loss': 14.3255, 'grad_norm': 1312.56201171875, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:49<4:26:52,  2.68s/it]  0%|          | 19/6000 [00:52<4:26:11,  2.67s/it]                                                   {'loss': 15.2423, 'grad_norm': 1088.000732421875, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:52<4:26:11,  2.67s/it]  0%|          | 20/6000 [00:54<4:25:57,  2.67s/it]                                                   {'loss': 15.9987, 'grad_norm': 1070.306396484375, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:54<4:25:57,  2.67s/it]  0%|          | 21/6000 [00:57<4:29:09,  2.70s/it]                                                   {'loss': 11.5636, 'grad_norm': 877.6091918945312, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:57<4:29:09,  2.70s/it]  0%|          | 22/6000 [01:00<4:28:58,  2.70s/it]                                                   {'loss': 10.8808, 'grad_norm': 448.40447998046875, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:00<4:28:58,  2.70s/it]  0%|          | 23/6000 [01:02<4:27:15,  2.68s/it]                                                   {'loss': 9.9076, 'grad_norm': 423.50811767578125, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:02<4:27:15,  2.68s/it]  0%|          | 24/6000 [01:05<4:28:44,  2.70s/it]                                                   {'loss': 6.0001, 'grad_norm': 195.8194580078125, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:05<4:28:44,  2.70s/it]  0%|          | 25/6000 [01:08<4:28:18,  2.69s/it]                                                   {'loss': 7.3553, 'grad_norm': 374.0245361328125, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:08<4:28:18,  2.69s/it]  0%|          | 26/6000 [01:11<4:28:46,  2.70s/it]                                                   {'loss': 6.9701, 'grad_norm': 477.0765380859375, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:11<4:28:46,  2.70s/it]  0%|          | 27/6000 [01:13<4:28:12,  2.69s/it]                                                   {'loss': 4.9146, 'grad_norm': 464.1723937988281, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:13<4:28:12,  2.69s/it]  0%|          | 28/6000 [01:17<4:53:02,  2.94s/it]                                                   {'loss': 3.6009, 'grad_norm': 181.7191619873047, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:17<4:53:02,  2.94s/it]  0%|          | 29/6000 [01:19<4:41:20,  2.83s/it]                                                   {'loss': 3.3068, 'grad_norm': 97.66641235351562, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:19<4:41:20,  2.83s/it]  0%|          | 30/6000 [01:22<4:36:12,  2.78s/it]                                                   {'loss': 3.145, 'grad_norm': 70.04513549804688, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:22<4:36:12,  2.78s/it]  1%|          | 31/6000 [01:25<4:32:25,  2.74s/it]                                                   {'loss': 3.0397, 'grad_norm': 64.98693084716797, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:25<4:32:25,  2.74s/it]  1%|          | 32/6000 [01:27<4:29:34,  2.71s/it]                                                   {'loss': 3.1434, 'grad_norm': 67.11238098144531, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:27<4:29:34,  2.71s/it]  1%|          | 33/6000 [01:30<4:28:11,  2.70s/it]                                                   {'loss': 3.0705, 'grad_norm': 60.300758361816406, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:30<4:28:11,  2.70s/it]  1%|          | 34/6000 [01:33<4:27:07,  2.69s/it]                                                   {'loss': 3.1713, 'grad_norm': 47.312828063964844, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:33<4:27:07,  2.69s/it]  1%|          | 35/6000 [01:35<4:27:45,  2.69s/it]                                                   {'loss': 2.8845, 'grad_norm': 23.327425003051758, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:35<4:27:45,  2.69s/it]  1%|          | 36/6000 [01:38<4:25:10,  2.67s/it]                                                   {'loss': 2.9556, 'grad_norm': 32.184139251708984, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:38<4:25:10,  2.67s/it]  1%|          | 37/6000 [01:41<4:25:16,  2.67s/it]                                                   {'loss': 2.8869, 'grad_norm': 25.256999969482422, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:41<4:25:16,  2.67s/it]  1%|          | 38/6000 [01:43<4:24:25,  2.66s/it]                                                   {'loss': 2.8481, 'grad_norm': 19.787080764770508, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:43<4:24:25,  2.66s/it]  1%|          | 39/6000 [01:46<4:24:15,  2.66s/it]                                                   {'loss': 2.9267, 'grad_norm': 23.210784912109375, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:46<4:24:15,  2.66s/it]  1%|          | 40/6000 [01:49<4:25:12,  2.67s/it]                                                   {'loss': 3.1513, 'grad_norm': 36.82062911987305, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:49<4:25:12,  2.67s/it]  1%|          | 41/6000 [01:51<4:24:17,  2.66s/it]                                                   {'loss': 2.8252, 'grad_norm': 12.778738021850586, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:51<4:24:17,  2.66s/it]  1%|          | 42/6000 [01:54<4:25:44,  2.68s/it]                                                   {'loss': 2.8303, 'grad_norm': 13.065743446350098, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:54<4:25:44,  2.68s/it]  1%|          | 43/6000 [01:58<4:54:48,  2.97s/it]                                                   {'loss': 2.8756, 'grad_norm': 35.043033599853516, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [01:58<4:54:48,  2.97s/it]  1%|          | 44/6000 [02:01<4:59:38,  3.02s/it]                                                   {'loss': 2.8178, 'grad_norm': 13.50464916229248, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:01<4:59:38,  3.02s/it]  1%|          | 45/6000 [02:03<4:49:47,  2.92s/it]                                                   {'loss': 2.8157, 'grad_norm': 7.719387531280518, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:03<4:49:47,  2.92s/it]  1%|          | 46/6000 [02:06<4:45:01,  2.87s/it]                                                   {'loss': 2.7878, 'grad_norm': 6.55045747756958, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:06<4:45:01,  2.87s/it]  1%|          | 47/6000 [02:09<4:39:29,  2.82s/it]                                                   {'loss': 2.8029, 'grad_norm': 7.764191627502441, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:09<4:39:29,  2.82s/it]  1%|          | 48/6000 [02:12<4:37:30,  2.80s/it]                                                   {'loss': 2.7879, 'grad_norm': 5.205328464508057, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:12<4:37:30,  2.80s/it]  1%|          | 49/6000 [02:14<4:32:43,  2.75s/it]                                                   {'loss': 2.7896, 'grad_norm': 4.2573933601379395, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:14<4:32:43,  2.75s/it]  1%|          | 50/6000 [02:17<4:34:14,  2.77s/it]                                                   {'loss': 2.7891, 'grad_norm': 4.972419738769531, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:17<4:34:14,  2.77s/it][2025-10-21 00:51:12,108] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:22<5:29:37,  3.32s/it]                                                   {'loss': 2.8016, 'grad_norm': 4.392867088317871, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:22<5:29:37,  3.32s/it]  1%|          | 52/6000 [02:24<5:08:37,  3.11s/it]                                                   {'loss': 2.7887, 'grad_norm': 4.5692362785339355, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:24<5:08:37,  3.11s/it]  1%|          | 53/6000 [02:27<4:56:50,  2.99s/it]                                                   {'loss': 2.8728, 'grad_norm': 4.538926124572754, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:27<4:56:50,  2.99s/it]  1%|          | 54/6000 [02:30<4:45:44,  2.88s/it]                                                   {'loss': 2.799, 'grad_norm': 4.812840461730957, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:30<4:45:44,  2.88s/it]  1%|          | 55/6000 [02:32<4:38:30,  2.81s/it]                                                   {'loss': 2.7962, 'grad_norm': 4.6822099685668945, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:32<4:38:30,  2.81s/it]  1%|          | 56/6000 [02:35<4:35:25,  2.78s/it]                                                   {'loss': 2.802, 'grad_norm': 5.80979585647583, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:35<4:35:25,  2.78s/it]  1%|          | 57/6000 [02:38<4:32:16,  2.75s/it]                                                   {'loss': 2.7944, 'grad_norm': 4.073761940002441, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:38<4:32:16,  2.75s/it]  1%|          | 58/6000 [02:40<4:29:15,  2.72s/it]                                                   {'loss': 2.7818, 'grad_norm': 3.799055814743042, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:40<4:29:15,  2.72s/it]  1%|          | 59/6000 [02:43<4:26:26,  2.69s/it]                                                   {'loss': 2.7836, 'grad_norm': 3.2557740211486816, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:43<4:26:26,  2.69s/it]  1%|          | 60/6000 [02:45<4:23:25,  2.66s/it]                                                   {'loss': 2.7832, 'grad_norm': 3.9308037757873535, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:45<4:23:25,  2.66s/it]  1%|          | 61/6000 [02:48<4:20:53,  2.64s/it]                                                   {'loss': 2.8138, 'grad_norm': 4.976950168609619, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:48<4:20:53,  2.64s/it]  1%|          | 62/6000 [02:51<4:21:23,  2.64s/it]                                                   {'loss': 2.7968, 'grad_norm': 2.7839877605438232, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:51<4:21:23,  2.64s/it]  1%|          | 63/6000 [02:53<4:22:06,  2.65s/it]                                                   {'loss': 2.7745, 'grad_norm': 3.9469120502471924, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:53<4:22:06,  2.65s/it]  1%|          | 64/6000 [02:56<4:21:23,  2.64s/it]                                                   {'loss': 2.7756, 'grad_norm': 3.058454990386963, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [02:56<4:21:23,  2.64s/it]  1%|          | 65/6000 [02:59<4:20:51,  2.64s/it]                                                   {'loss': 2.7927, 'grad_norm': 3.7883567810058594, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [02:59<4:20:51,  2.64s/it]  1%|          | 66/6000 [03:02<4:33:54,  2.77s/it]                                                   {'loss': 2.7819, 'grad_norm': 3.069809675216675, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:02<4:33:54,  2.77s/it]  1%|          | 67/6000 [03:04<4:31:16,  2.74s/it]                                                   {'loss': 2.7957, 'grad_norm': 3.0069169998168945, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:04<4:31:16,  2.74s/it]  1%|          | 68/6000 [03:07<4:27:33,  2.71s/it]                                                   {'loss': 2.7885, 'grad_norm': 5.552429676055908, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:07<4:27:33,  2.71s/it]  1%|          | 69/6000 [03:10<4:27:00,  2.70s/it]                                                   {'loss': 2.7877, 'grad_norm': 4.11251163482666, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:10<4:27:00,  2.70s/it]  1%|          | 70/6000 [03:12<4:27:28,  2.71s/it]                                                   {'loss': 2.8009, 'grad_norm': 6.888373851776123, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:12<4:27:28,  2.71s/it]  1%|          | 71/6000 [03:15<4:24:30,  2.68s/it]                                                   {'loss': 2.785, 'grad_norm': 4.946927547454834, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:15<4:24:30,  2.68s/it]  1%|          | 72/6000 [03:18<4:34:43,  2.78s/it]                                                   {'loss': 2.7807, 'grad_norm': 9.393721580505371, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:18<4:34:43,  2.78s/it]  1%|          | 73/6000 [03:21<4:40:28,  2.84s/it]                                                   {'loss': 2.8441, 'grad_norm': 5.492393970489502, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:21<4:40:28,  2.84s/it]  1%|          | 74/6000 [03:24<4:33:58,  2.77s/it]                                                   {'loss': 2.7743, 'grad_norm': 4.204822063446045, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:24<4:33:58,  2.77s/it]  1%|â–         | 75/6000 [03:26<4:30:38,  2.74s/it]                                                   {'loss': 2.7886, 'grad_norm': 8.710296630859375, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:26<4:30:38,  2.74s/it]  1%|â–         | 76/6000 [03:29<4:32:06,  2.76s/it]                                                   {'loss': 2.7944, 'grad_norm': 3.3408870697021484, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:29<4:32:06,  2.76s/it]  1%|â–         | 77/6000 [03:32<4:30:40,  2.74s/it]                                                   {'loss': 2.8488, 'grad_norm': 2.669114112854004, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:32<4:30:40,  2.74s/it]  1%|â–         | 78/6000 [03:35<4:39:19,  2.83s/it]                                                   {'loss': 2.777, 'grad_norm': 3.703016519546509, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:35<4:39:19,  2.83s/it]  1%|â–         | 79/6000 [03:38<4:35:43,  2.79s/it]                                                   {'loss': 2.7857, 'grad_norm': 4.346187114715576, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:38<4:35:43,  2.79s/it]  1%|â–         | 80/6000 [03:40<4:39:41,  2.83s/it]                                                   {'loss': 2.7915, 'grad_norm': 13.209434509277344, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:41<4:39:41,  2.83s/it]  1%|â–         | 81/6000 [03:43<4:34:05,  2.78s/it]                                                   {'loss': 2.7755, 'grad_norm': 4.264941215515137, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:43<4:34:05,  2.78s/it]  1%|â–         | 82/6000 [03:46<4:32:09,  2.76s/it]                                                   {'loss': 2.7811, 'grad_norm': 10.535388946533203, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:46<4:32:09,  2.76s/it]  1%|â–         | 83/6000 [03:48<4:28:20,  2.72s/it]                                                   {'loss': 2.7953, 'grad_norm': 3.1858763694763184, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:49<4:28:20,  2.72s/it]  1%|â–         | 84/6000 [03:51<4:28:13,  2.72s/it]                                                   {'loss': 2.7788, 'grad_norm': 5.404037952423096, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:51<4:28:13,  2.72s/it]  1%|â–         | 85/6000 [03:54<4:36:28,  2.80s/it]                                                   {'loss': 2.7817, 'grad_norm': 5.618320465087891, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:54<4:36:28,  2.80s/it]  1%|â–         | 86/6000 [03:57<4:31:39,  2.76s/it]                                                   {'loss': 2.8038, 'grad_norm': 3.705240249633789, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [03:57<4:31:39,  2.76s/it]  1%|â–         | 87/6000 [03:59<4:26:58,  2.71s/it]                                                   {'loss': 2.7892, 'grad_norm': 4.208827972412109, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [03:59<4:26:58,  2.71s/it]  1%|â–         | 88/6000 [04:02<4:25:03,  2.69s/it]                                                   {'loss': 2.8053, 'grad_norm': 8.263352394104004, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:02<4:25:03,  2.69s/it]  1%|â–         | 89/6000 [04:05<4:21:28,  2.65s/it]                                                   {'loss': 2.7829, 'grad_norm': 4.445395469665527, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:05<4:21:28,  2.65s/it]  2%|â–         | 90/6000 [04:07<4:21:56,  2.66s/it]                                                   {'loss': 2.8205, 'grad_norm': 2.8158516883850098, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:07<4:21:56,  2.66s/it]  2%|â–         | 91/6000 [04:10<4:22:37,  2.67s/it]                                                   {'loss': 2.7659, 'grad_norm': 4.967577934265137, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:10<4:22:37,  2.67s/it]  2%|â–         | 92/6000 [04:13<4:33:30,  2.78s/it]                                                   {'loss': 2.7739, 'grad_norm': 3.6626498699188232, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:13<4:33:30,  2.78s/it]  2%|â–         | 93/6000 [04:16<4:34:34,  2.79s/it]                                                   {'loss': 2.7719, 'grad_norm': 4.445727825164795, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:16<4:34:34,  2.79s/it]  2%|â–         | 94/6000 [04:18<4:29:19,  2.74s/it]                                                   {'loss': 2.7937, 'grad_norm': 2.9486191272735596, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:18<4:29:19,  2.74s/it]  2%|â–         | 95/6000 [04:21<4:27:07,  2.71s/it]                                                   {'loss': 2.7776, 'grad_norm': 2.579800844192505, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:21<4:27:07,  2.71s/it]  2%|â–         | 96/6000 [04:24<4:25:42,  2.70s/it]                                                   {'loss': 2.7731, 'grad_norm': 2.7726175785064697, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:24<4:25:42,  2.70s/it]  2%|â–         | 97/6000 [04:26<4:24:51,  2.69s/it]                                                   {'loss': 2.7777, 'grad_norm': 4.854724407196045, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:26<4:24:51,  2.69s/it]  2%|â–         | 98/6000 [04:29<4:24:08,  2.69s/it]                                                   {'loss': 2.7804, 'grad_norm': 4.9044880867004395, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:29<4:24:08,  2.69s/it]  2%|â–         | 99/6000 [04:32<4:21:20,  2.66s/it]                                                   {'loss': 2.769, 'grad_norm': 3.657961845397949, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:32<4:21:20,  2.66s/it]  2%|â–         | 100/6000 [04:34<4:20:43,  2.65s/it]                                                    {'loss': 2.7623, 'grad_norm': 4.3081955909729, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:34<4:20:43,  2.65s/it][2025-10-21 00:53:29,490] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:40<5:44:09,  3.50s/it]                                                    {'loss': 2.8158, 'grad_norm': 13.981045722961426, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:40<5:44:09,  3.50s/it]  2%|â–         | 102/6000 [04:43<5:20:20,  3.26s/it]                                                    {'loss': 2.7861, 'grad_norm': 3.9930951595306396, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:43<5:20:20,  3.26s/it]  2%|â–         | 103/6000 [04:45<5:03:32,  3.09s/it]                                                    {'loss': 2.7823, 'grad_norm': 4.901896953582764, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:45<5:03:32,  3.09s/it]  2%|â–         | 104/6000 [04:48<4:57:37,  3.03s/it]                                                    {'loss': 2.7678, 'grad_norm': 3.855379819869995, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:48<4:57:37,  3.03s/it]  2%|â–         | 105/6000 [04:51<4:49:35,  2.95s/it]                                                    {'loss': 2.7959, 'grad_norm': 3.7150814533233643, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:51<4:49:35,  2.95s/it]  2%|â–         | 106/6000 [04:54<4:44:52,  2.90s/it]                                                    {'loss': 2.7761, 'grad_norm': 1.9534988403320312, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:54<4:44:52,  2.90s/it]  2%|â–         | 107/6000 [04:56<4:37:17,  2.82s/it]                                                    {'loss': 2.7892, 'grad_norm': 2.7457435131073, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [04:56<4:37:17,  2.82s/it]  2%|â–         | 108/6000 [04:59<4:36:41,  2.82s/it]                                                    {'loss': 2.7819, 'grad_norm': 2.326510429382324, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [04:59<4:36:41,  2.82s/it]  2%|â–         | 109/6000 [05:02<4:44:37,  2.90s/it]                                                    {'loss': 2.8242, 'grad_norm': 2.052945375442505, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:02<4:44:37,  2.90s/it]  2%|â–         | 110/6000 [05:05<4:37:41,  2.83s/it]                                                    {'loss': 2.845, 'grad_norm': 2.1903162002563477, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:05<4:37:41,  2.83s/it]  2%|â–         | 111/6000 [05:08<4:36:13,  2.81s/it]                                                    {'loss': 2.774, 'grad_norm': 2.244886636734009, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:08<4:36:13,  2.81s/it]  2%|â–         | 112/6000 [05:11<4:48:00,  2.93s/it]                                                    {'loss': 2.7835, 'grad_norm': 2.2792770862579346, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:11<4:48:00,  2.93s/it]  2%|â–         | 113/6000 [05:14<4:39:42,  2.85s/it]                                                    {'loss': 2.7701, 'grad_norm': 1.8679587841033936, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:14<4:39:42,  2.85s/it]  2%|â–         | 114/6000 [05:16<4:35:53,  2.81s/it]                                                    {'loss': 2.7811, 'grad_norm': 1.8104767799377441, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:16<4:35:53,  2.81s/it]  2%|â–         | 115/6000 [05:19<4:31:15,  2.77s/it]                                                    {'loss': 2.8419, 'grad_norm': 2.518541097640991, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:19<4:31:15,  2.77s/it]  2%|â–         | 116/6000 [05:22<4:27:28,  2.73s/it]                                                    {'loss': 2.7853, 'grad_norm': 2.9528825283050537, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:22<4:27:28,  2.73s/it]  2%|â–         | 117/6000 [05:24<4:25:48,  2.71s/it]                                                    {'loss': 2.9126, 'grad_norm': 2.0036323070526123, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:24<4:25:48,  2.71s/it]  2%|â–         | 118/6000 [05:27<4:40:35,  2.86s/it]                                                    {'loss': 2.7738, 'grad_norm': 2.4813742637634277, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:27<4:40:35,  2.86s/it]  2%|â–         | 119/6000 [05:30<4:33:09,  2.79s/it]                                                    {'loss': 2.759, 'grad_norm': 3.2596659660339355, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:30<4:33:09,  2.79s/it]  2%|â–         | 120/6000 [05:33<4:29:38,  2.75s/it]                                                    {'loss': 2.7757, 'grad_norm': 2.9681873321533203, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:33<4:29:38,  2.75s/it]  2%|â–         | 121/6000 [05:35<4:28:14,  2.74s/it]                                                    {'loss': 2.7611, 'grad_norm': 2.671037435531616, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:35<4:28:14,  2.74s/it]  2%|â–         | 122/6000 [05:38<4:29:08,  2.75s/it]                                                    {'loss': 2.7844, 'grad_norm': 2.9029345512390137, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:38<4:29:08,  2.75s/it]  2%|â–         | 123/6000 [05:41<4:25:23,  2.71s/it]                                                    {'loss': 2.7824, 'grad_norm': 3.7565343379974365, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:41<4:25:23,  2.71s/it]  2%|â–         | 124/6000 [05:43<4:21:54,  2.67s/it]                                                    {'loss': 2.8233, 'grad_norm': 3.9394047260284424, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:43<4:21:54,  2.67s/it]  2%|â–         | 125/6000 [05:46<4:27:32,  2.73s/it]                                                    {'loss': 2.9566, 'grad_norm': 6.121943950653076, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:46<4:27:32,  2.73s/it]  2%|â–         | 126/6000 [05:49<4:27:37,  2.73s/it]                                                    {'loss': 2.7584, 'grad_norm': 4.373027801513672, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:49<4:27:37,  2.73s/it]  2%|â–         | 127/6000 [05:52<4:29:19,  2.75s/it]                                                    {'loss': 2.7784, 'grad_norm': 3.3736064434051514, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:52<4:29:19,  2.75s/it]  2%|â–         | 128/6000 [05:54<4:25:16,  2.71s/it]                                                    {'loss': 2.7907, 'grad_norm': 3.64262056350708, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [05:54<4:25:16,  2.71s/it]  2%|â–         | 129/6000 [05:57<4:24:20,  2.70s/it]                                                    {'loss': 2.8627, 'grad_norm': 2.9444620609283447, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [05:57<4:24:20,  2.70s/it]  2%|â–         | 130/6000 [06:00<4:23:52,  2.70s/it]                                                    {'loss': 2.779, 'grad_norm': 2.9485883712768555, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:00<4:23:52,  2.70s/it]  2%|â–         | 131/6000 [06:02<4:22:40,  2.69s/it]                                                    {'loss': 2.7872, 'grad_norm': 2.7764251232147217, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:02<4:22:40,  2.69s/it]  2%|â–         | 132/6000 [06:05<4:20:42,  2.67s/it]                                                    {'loss': 2.7921, 'grad_norm': 3.5725579261779785, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:05<4:20:42,  2.67s/it]  2%|â–         | 133/6000 [06:08<4:20:28,  2.66s/it]                                                    {'loss': 2.7696, 'grad_norm': 2.6292834281921387, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:08<4:20:28,  2.66s/it]  2%|â–         | 134/6000 [06:11<4:26:02,  2.72s/it]                                                    {'loss': 2.7628, 'grad_norm': 2.7218971252441406, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:11<4:26:02,  2.72s/it]  2%|â–         | 135/6000 [06:13<4:23:41,  2.70s/it]                                                    {'loss': 2.7922, 'grad_norm': 2.7532896995544434, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:13<4:23:41,  2.70s/it]  2%|â–         | 136/6000 [06:16<4:21:26,  2.68s/it]                                                    {'loss': 2.7931, 'grad_norm': 2.944986343383789, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:16<4:21:26,  2.68s/it]  2%|â–         | 137/6000 [06:19<4:37:17,  2.84s/it]                                                    {'loss': 2.7562, 'grad_norm': 4.9966325759887695, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:19<4:37:17,  2.84s/it]  2%|â–         | 138/6000 [06:22<4:30:15,  2.77s/it]                                                    {'loss': 2.7814, 'grad_norm': 3.3310885429382324, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:22<4:30:15,  2.77s/it]  2%|â–         | 139/6000 [06:24<4:29:38,  2.76s/it]                                                    {'loss': 2.8302, 'grad_norm': 4.08573579788208, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:24<4:29:38,  2.76s/it]  2%|â–         | 140/6000 [06:27<4:36:50,  2.83s/it]                                                    {'loss': 2.7709, 'grad_norm': 4.108180999755859, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:27<4:36:50,  2.83s/it]  2%|â–         | 141/6000 [06:30<4:34:44,  2.81s/it]                                                    {'loss': 2.7535, 'grad_norm': 4.113409519195557, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:30<4:34:44,  2.81s/it]  2%|â–         | 142/6000 [06:33<4:27:39,  2.74s/it]                                                    {'loss': 2.7812, 'grad_norm': 3.669316291809082, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:33<4:27:39,  2.74s/it]  2%|â–         | 143/6000 [06:35<4:23:23,  2.70s/it]                                                    {'loss': 2.7519, 'grad_norm': 6.542207717895508, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:35<4:23:23,  2.70s/it]  2%|â–         | 144/6000 [06:38<4:22:18,  2.69s/it]                                                    {'loss': 2.7666, 'grad_norm': 4.134267330169678, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:38<4:22:18,  2.69s/it]  2%|â–         | 145/6000 [06:41<4:20:05,  2.67s/it]                                                    {'loss': 2.8747, 'grad_norm': 11.802007675170898, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:41<4:20:05,  2.67s/it]  2%|â–         | 146/6000 [06:43<4:21:17,  2.68s/it]                                                    {'loss': 2.8796, 'grad_norm': 10.706439018249512, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:43<4:21:17,  2.68s/it]  2%|â–         | 147/6000 [06:46<4:19:57,  2.66s/it]                                                    {'loss': 2.7782, 'grad_norm': 7.0871500968933105, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:46<4:19:57,  2.66s/it]  2%|â–         | 148/6000 [06:49<4:19:59,  2.67s/it]                                                    {'loss': 2.7427, 'grad_norm': 6.061687469482422, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:49<4:19:59,  2.67s/it]  2%|â–         | 149/6000 [06:51<4:19:32,  2.66s/it]                                                    {'loss': 2.7345, 'grad_norm': 10.782829284667969, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [06:51<4:19:32,  2.66s/it]  2%|â–Ž         | 150/6000 [06:54<4:18:25,  2.65s/it]                                                    {'loss': 2.7865, 'grad_norm': 8.358757019042969, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [06:54<4:18:25,  2.65s/it][2025-10-21 00:55:49,028] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [06:59<5:23:26,  3.32s/it]                                                    {'loss': 2.8265, 'grad_norm': 6.519837379455566, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [06:59<5:23:26,  3.32s/it]  3%|â–Ž         | 152/6000 [07:01<5:03:34,  3.11s/it]                                                    {'loss': 2.7882, 'grad_norm': 6.234210014343262, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:01<5:03:34,  3.11s/it]  3%|â–Ž         | 153/6000 [07:04<4:50:33,  2.98s/it]                                                    {'loss': 2.7933, 'grad_norm': 3.99847412109375, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:04<4:50:33,  2.98s/it]  3%|â–Ž         | 154/6000 [07:07<4:44:32,  2.92s/it]                                                    {'loss': 2.7668, 'grad_norm': 3.484602689743042, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:07<4:44:32,  2.92s/it]  3%|â–Ž         | 155/6000 [07:10<4:39:39,  2.87s/it]                                                    {'loss': 2.7838, 'grad_norm': 3.3229236602783203, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:10<4:39:39,  2.87s/it]  3%|â–Ž         | 156/6000 [07:12<4:36:13,  2.84s/it]                                                    {'loss': 2.8401, 'grad_norm': 4.277785301208496, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:12<4:36:13,  2.84s/it]  3%|â–Ž         | 157/6000 [07:16<4:44:36,  2.92s/it]                                                    {'loss': 2.758, 'grad_norm': 3.835747480392456, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:16<4:44:36,  2.92s/it]  3%|â–Ž         | 158/6000 [07:18<4:35:26,  2.83s/it]                                                    {'loss': 2.7851, 'grad_norm': 3.6900885105133057, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:18<4:35:26,  2.83s/it]  3%|â–Ž         | 159/6000 [07:21<4:30:38,  2.78s/it]                                                    {'loss': 2.7762, 'grad_norm': 3.1410908699035645, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:21<4:30:38,  2.78s/it]  3%|â–Ž         | 160/6000 [07:24<4:43:30,  2.91s/it]                                                    {'loss': 2.7941, 'grad_norm': 3.0330493450164795, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:24<4:43:30,  2.91s/it]  3%|â–Ž         | 161/6000 [07:27<4:40:25,  2.88s/it]                                                    {'loss': 2.7753, 'grad_norm': 3.2311465740203857, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:27<4:40:25,  2.88s/it]  3%|â–Ž         | 162/6000 [07:29<4:33:11,  2.81s/it]                                                    {'loss': 2.7873, 'grad_norm': 3.635568857192993, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:30<4:33:11,  2.81s/it]  3%|â–Ž         | 163/6000 [07:33<4:45:23,  2.93s/it]                                                    {'loss': 2.7569, 'grad_norm': 3.5766537189483643, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:33<4:45:23,  2.93s/it]  3%|â–Ž         | 164/6000 [07:35<4:36:48,  2.85s/it]                                                    {'loss': 2.8973, 'grad_norm': 4.268078804016113, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:35<4:36:48,  2.85s/it]  3%|â–Ž         | 165/6000 [07:38<4:33:41,  2.81s/it]                                                    {'loss': 2.7502, 'grad_norm': 4.888232707977295, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:38<4:33:41,  2.81s/it]  3%|â–Ž         | 166/6000 [07:41<4:29:37,  2.77s/it]                                                    {'loss': 2.7652, 'grad_norm': 4.027377605438232, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:41<4:29:37,  2.77s/it]  3%|â–Ž         | 167/6000 [07:43<4:27:29,  2.75s/it]                                                    {'loss': 2.7948, 'grad_norm': 6.404426574707031, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:43<4:27:29,  2.75s/it]  3%|â–Ž         | 168/6000 [07:46<4:26:53,  2.75s/it]                                                    {'loss': 2.832, 'grad_norm': 3.861626625061035, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:46<4:26:53,  2.75s/it]  3%|â–Ž         | 169/6000 [07:49<4:36:53,  2.85s/it]                                                    {'loss': 2.847, 'grad_norm': 5.944636821746826, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:49<4:36:53,  2.85s/it]  3%|â–Ž         | 170/6000 [07:52<4:29:25,  2.77s/it]                                                    {'loss': 2.7946, 'grad_norm': 5.171166896820068, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [07:52<4:29:25,  2.77s/it]  3%|â–Ž         | 171/6000 [07:55<4:26:04,  2.74s/it]                                                    {'loss': 2.7599, 'grad_norm': 3.97186541557312, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [07:55<4:26:04,  2.74s/it]  3%|â–Ž         | 172/6000 [07:57<4:27:36,  2.76s/it]                                                    {'loss': 2.8158, 'grad_norm': 4.07172966003418, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [07:57<4:27:36,  2.76s/it]  3%|â–Ž         | 173/6000 [08:00<4:25:10,  2.73s/it]                                                    {'loss': 2.7966, 'grad_norm': 3.8677542209625244, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:00<4:25:10,  2.73s/it]  3%|â–Ž         | 174/6000 [08:03<4:42:29,  2.91s/it]                                                    {'loss': 2.7757, 'grad_norm': 14.103415489196777, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:03<4:42:29,  2.91s/it]  3%|â–Ž         | 175/6000 [08:06<4:37:11,  2.86s/it]                                                    {'loss': 2.7709, 'grad_norm': 3.176133394241333, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:06<4:37:11,  2.86s/it]  3%|â–Ž         | 176/6000 [08:09<4:40:10,  2.89s/it]                                                    {'loss': 2.7763, 'grad_norm': 3.1427412033081055, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:09<4:40:10,  2.89s/it]  3%|â–Ž         | 177/6000 [08:12<4:33:58,  2.82s/it]                                                    {'loss': 2.7934, 'grad_norm': 3.494532346725464, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:12<4:33:58,  2.82s/it]  3%|â–Ž         | 178/6000 [08:14<4:31:02,  2.79s/it]                                                    {'loss': 2.7829, 'grad_norm': 2.8428382873535156, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:14<4:31:02,  2.79s/it]  3%|â–Ž         | 179/6000 [08:17<4:28:51,  2.77s/it]                                                    {'loss': 2.7961, 'grad_norm': 1.8198292255401611, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:17<4:28:51,  2.77s/it]  3%|â–Ž         | 180/6000 [08:20<4:29:08,  2.77s/it]                                                    {'loss': 2.7886, 'grad_norm': 1.8361036777496338, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:20<4:29:08,  2.77s/it]  3%|â–Ž         | 181/6000 [08:23<4:25:39,  2.74s/it]                                                    {'loss': 2.8147, 'grad_norm': 2.0437748432159424, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:23<4:25:39,  2.74s/it]  3%|â–Ž         | 182/6000 [08:25<4:22:26,  2.71s/it]                                                    {'loss': 2.8177, 'grad_norm': 2.035181999206543, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:25<4:22:26,  2.71s/it]  3%|â–Ž         | 183/6000 [08:28<4:19:47,  2.68s/it]                                                    {'loss': 2.7798, 'grad_norm': 2.8686559200286865, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:28<4:19:47,  2.68s/it]  3%|â–Ž         | 184/6000 [08:30<4:18:41,  2.67s/it]                                                    {'loss': 2.7763, 'grad_norm': 2.5300700664520264, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:30<4:18:41,  2.67s/it]  3%|â–Ž         | 185/6000 [08:33<4:19:18,  2.68s/it]                                                    {'loss': 2.7826, 'grad_norm': 1.9186347723007202, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:33<4:19:18,  2.68s/it]  3%|â–Ž         | 186/6000 [08:36<4:19:07,  2.67s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.8245021104812622, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:36<4:19:07,  2.67s/it]  3%|â–Ž         | 187/6000 [08:38<4:17:52,  2.66s/it]                                                    {'loss': 2.791, 'grad_norm': 4.133584499359131, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:38<4:17:52,  2.66s/it]  3%|â–Ž         | 188/6000 [08:41<4:16:30,  2.65s/it]                                                    {'loss': 2.7867, 'grad_norm': 2.9901509284973145, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:41<4:16:30,  2.65s/it]  3%|â–Ž         | 189/6000 [08:44<4:17:19,  2.66s/it]                                                    {'loss': 2.7887, 'grad_norm': 2.7857117652893066, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:44<4:17:19,  2.66s/it]  3%|â–Ž         | 190/6000 [08:46<4:14:55,  2.63s/it]                                                    {'loss': 2.7873, 'grad_norm': 2.6552467346191406, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:46<4:14:55,  2.63s/it]  3%|â–Ž         | 191/6000 [08:49<4:16:58,  2.65s/it]                                                    {'loss': 2.7805, 'grad_norm': 2.9583802223205566, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [08:49<4:16:58,  2.65s/it]  3%|â–Ž         | 192/6000 [08:52<4:33:55,  2.83s/it]                                                    {'loss': 2.891, 'grad_norm': 3.4184725284576416, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [08:52<4:33:55,  2.83s/it]  3%|â–Ž         | 193/6000 [08:55<4:38:06,  2.87s/it]                                                    {'loss': 2.7705, 'grad_norm': 3.1242969036102295, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [08:55<4:38:06,  2.87s/it]  3%|â–Ž         | 194/6000 [08:58<4:30:16,  2.79s/it]                                                    {'loss': 2.7796, 'grad_norm': 3.125500440597534, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [08:58<4:30:16,  2.79s/it]  3%|â–Ž         | 195/6000 [09:01<4:27:21,  2.76s/it]                                                    {'loss': 2.841, 'grad_norm': 3.971494436264038, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:01<4:27:21,  2.76s/it]  3%|â–Ž         | 196/6000 [09:03<4:25:07,  2.74s/it]                                                    {'loss': 2.7887, 'grad_norm': 2.6106107234954834, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:03<4:25:07,  2.74s/it]  3%|â–Ž         | 197/6000 [09:06<4:27:55,  2.77s/it]                                                    {'loss': 2.787, 'grad_norm': 1.9072624444961548, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:06<4:27:55,  2.77s/it]  3%|â–Ž         | 198/6000 [09:09<4:28:15,  2.77s/it]                                                    {'loss': 2.7906, 'grad_norm': 2.349855661392212, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:09<4:28:15,  2.77s/it]  3%|â–Ž         | 199/6000 [09:12<4:25:22,  2.74s/it]                                                    {'loss': 2.7661, 'grad_norm': 2.4394352436065674, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:12<4:25:22,  2.74s/it]  3%|â–Ž         | 200/6000 [09:15<4:37:46,  2.87s/it]                                                    {'loss': 2.7559, 'grad_norm': 2.425515651702881, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:15<4:37:46,  2.87s/it][2025-10-21 00:58:09,814] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:19<5:27:37,  3.39s/it]                                                    {'loss': 2.7894, 'grad_norm': 2.944797992706299, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:19<5:27:37,  3.39s/it]  3%|â–Ž         | 202/6000 [09:22<5:06:28,  3.17s/it]                                                    {'loss': 2.7918, 'grad_norm': 3.2142653465270996, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:22<5:06:28,  3.17s/it]  3%|â–Ž         | 203/6000 [09:25<4:51:44,  3.02s/it]                                                    {'loss': 2.8037, 'grad_norm': 3.1406807899475098, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:25<4:51:44,  3.02s/it]  3%|â–Ž         | 204/6000 [09:27<4:44:27,  2.94s/it]                                                    {'loss': 2.7929, 'grad_norm': 3.67545747756958, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:27<4:44:27,  2.94s/it]  3%|â–Ž         | 205/6000 [09:30<4:33:40,  2.83s/it]                                                    {'loss': 2.7638, 'grad_norm': 3.2295138835906982, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:30<4:33:40,  2.83s/it]  3%|â–Ž         | 206/6000 [09:33<4:27:48,  2.77s/it]                                                    {'loss': 2.8335, 'grad_norm': 2.9197919368743896, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:33<4:27:48,  2.77s/it]  3%|â–Ž         | 207/6000 [09:35<4:22:45,  2.72s/it]                                                    {'loss': 2.8669, 'grad_norm': 3.0173370838165283, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:35<4:22:45,  2.72s/it]  3%|â–Ž         | 208/6000 [09:38<4:20:02,  2.69s/it]                                                    {'loss': 2.8062, 'grad_norm': 3.4837124347686768, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:38<4:20:02,  2.69s/it]  3%|â–Ž         | 209/6000 [09:41<4:19:27,  2.69s/it]                                                    {'loss': 2.7657, 'grad_norm': 2.7326481342315674, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:41<4:19:27,  2.69s/it]  4%|â–Ž         | 210/6000 [09:44<4:30:35,  2.80s/it]                                                    {'loss': 2.7642, 'grad_norm': 2.680500030517578, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:44<4:30:35,  2.80s/it]  4%|â–Ž         | 211/6000 [09:46<4:33:05,  2.83s/it]                                                    {'loss': 2.801, 'grad_norm': 2.662241220474243, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [09:46<4:33:05,  2.83s/it]  4%|â–Ž         | 212/6000 [09:50<4:40:26,  2.91s/it]                                                    {'loss': 2.7723, 'grad_norm': 2.3557441234588623, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [09:50<4:40:26,  2.91s/it]  4%|â–Ž         | 213/6000 [09:52<4:35:28,  2.86s/it]                                                    {'loss': 2.7877, 'grad_norm': 2.515655994415283, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [09:52<4:35:28,  2.86s/it]  4%|â–Ž         | 214/6000 [09:55<4:28:59,  2.79s/it]                                                    {'loss': 2.7676, 'grad_norm': 1.8180235624313354, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [09:55<4:28:59,  2.79s/it]  4%|â–Ž         | 215/6000 [09:58<4:24:32,  2.74s/it]                                                    {'loss': 2.8101, 'grad_norm': 2.257451295852661, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [09:58<4:24:32,  2.74s/it]  4%|â–Ž         | 216/6000 [10:00<4:22:48,  2.73s/it]                                                    {'loss': 2.7807, 'grad_norm': 1.9969443082809448, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:00<4:22:48,  2.73s/it]  4%|â–Ž         | 217/6000 [10:03<4:25:30,  2.75s/it]                                                    {'loss': 2.8166, 'grad_norm': 1.693257451057434, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:03<4:25:30,  2.75s/it]  4%|â–Ž         | 218/6000 [10:06<4:27:11,  2.77s/it]                                                    {'loss': 2.7908, 'grad_norm': 1.8884443044662476, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:06<4:27:11,  2.77s/it]  4%|â–Ž         | 219/6000 [10:08<4:22:06,  2.72s/it]                                                    {'loss': 2.8337, 'grad_norm': 1.6849157810211182, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:09<4:22:06,  2.72s/it]  4%|â–Ž         | 220/6000 [10:11<4:19:48,  2.70s/it]                                                    {'loss': 2.7683, 'grad_norm': 1.6240158081054688, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:11<4:19:48,  2.70s/it]  4%|â–Ž         | 221/6000 [10:14<4:20:44,  2.71s/it]                                                    {'loss': 2.7971, 'grad_norm': 2.6607444286346436, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:14<4:20:44,  2.71s/it]  4%|â–Ž         | 222/6000 [10:17<4:19:44,  2.70s/it]                                                    {'loss': 2.779, 'grad_norm': 2.785144567489624, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:17<4:19:44,  2.70s/it]  4%|â–Ž         | 223/6000 [10:19<4:18:41,  2.69s/it]                                                    {'loss': 2.7552, 'grad_norm': 2.200350284576416, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:19<4:18:41,  2.69s/it]  4%|â–Ž         | 224/6000 [10:22<4:15:35,  2.66s/it]                                                    {'loss': 2.8437, 'grad_norm': 2.6398580074310303, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:22<4:15:35,  2.66s/it]  4%|â–         | 225/6000 [10:25<4:17:27,  2.67s/it]                                                    {'loss': 2.7752, 'grad_norm': 1.7077832221984863, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:25<4:17:27,  2.67s/it]  4%|â–         | 226/6000 [10:28<4:31:46,  2.82s/it]                                                    {'loss': 2.7751, 'grad_norm': 2.671924591064453, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:28<4:31:46,  2.82s/it]  4%|â–         | 227/6000 [10:30<4:25:28,  2.76s/it]                                                    {'loss': 2.8237, 'grad_norm': 1.3351291418075562, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:30<4:25:28,  2.76s/it]  4%|â–         | 228/6000 [10:33<4:24:31,  2.75s/it]                                                    {'loss': 2.7841, 'grad_norm': 1.8563687801361084, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:33<4:24:31,  2.75s/it]  4%|â–         | 229/6000 [10:36<4:20:51,  2.71s/it]                                                    {'loss': 2.7866, 'grad_norm': 1.9064104557037354, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:36<4:20:51,  2.71s/it]  4%|â–         | 230/6000 [10:39<4:26:26,  2.77s/it]                                                    {'loss': 2.7952, 'grad_norm': 2.0237066745758057, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:39<4:26:26,  2.77s/it]  4%|â–         | 231/6000 [10:41<4:22:56,  2.73s/it]                                                    {'loss': 2.7813, 'grad_norm': 2.2441155910491943, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:41<4:22:56,  2.73s/it]  4%|â–         | 232/6000 [10:44<4:20:19,  2.71s/it]                                                    {'loss': 2.7745, 'grad_norm': 1.767046332359314, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [10:44<4:20:19,  2.71s/it]  4%|â–         | 233/6000 [10:47<4:29:46,  2.81s/it]                                                    {'loss': 2.7909, 'grad_norm': 2.796856164932251, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [10:47<4:29:46,  2.81s/it]  4%|â–         | 234/6000 [10:50<4:35:05,  2.86s/it]                                                    {'loss': 2.7553, 'grad_norm': 2.052241325378418, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [10:50<4:35:05,  2.86s/it]  4%|â–         | 235/6000 [10:52<4:27:32,  2.78s/it]                                                    {'loss': 2.8498, 'grad_norm': 1.4645657539367676, 'learning_rate': 4.885593220338983e-05, 'epoch': 0.04}
  4%|â–         | 235/6000 [10:52<4:27:32,  2.78s/it]  4%|â–         | 236/6000 [10:55<4:25:00,  2.76s/it]                                                    {'loss': 2.8077, 'grad_norm': 2.383708953857422, 'learning_rate': 4.884745762711865e-05, 'epoch': 0.04}
  4%|â–         | 236/6000 [10:55<4:25:00,  2.76s/it]  4%|â–         | 237/6000 [10:58<4:21:43,  2.72s/it]                                                    {'loss': 2.7805, 'grad_norm': 2.564185857772827, 'learning_rate': 4.883898305084746e-05, 'epoch': 0.04}
  4%|â–         | 237/6000 [10:58<4:21:43,  2.72s/it]  4%|â–         | 238/6000 [11:01<4:32:03,  2.83s/it]                                                    {'loss': 2.8363, 'grad_norm': 1.6780836582183838, 'learning_rate': 4.883050847457628e-05, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:01<4:32:03,  2.83s/it]  4%|â–         | 239/6000 [11:04<4:29:06,  2.80s/it]                                                    {'loss': 2.7889, 'grad_norm': 2.485140800476074, 'learning_rate': 4.882203389830508e-05, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:04<4:29:06,  2.80s/it]  4%|â–         | 240/6000 [11:06<4:25:47,  2.77s/it]                                                    {'loss': 2.7876, 'grad_norm': 1.5347754955291748, 'learning_rate': 4.88135593220339e-05, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:06<4:25:47,  2.77s/it]  4%|â–         | 241/6000 [11:09<4:22:24,  2.73s/it]                                                    {'loss': 2.7742, 'grad_norm': 2.086608409881592, 'learning_rate': 4.880508474576271e-05, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:09<4:22:24,  2.73s/it]  4%|â–         | 242/6000 [11:12<4:20:03,  2.71s/it]                                                    {'loss': 2.7935, 'grad_norm': 2.2183966636657715, 'learning_rate': 4.879661016949153e-05, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:12<4:20:03,  2.71s/it]  4%|â–         | 243/6000 [11:15<4:27:27,  2.79s/it]                                                    {'loss': 2.7714, 'grad_norm': 1.7876673936843872, 'learning_rate': 4.878813559322034e-05, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:15<4:27:27,  2.79s/it]  4%|â–         | 244/6000 [11:17<4:23:10,  2.74s/it]                                                    {'loss': 2.7892, 'grad_norm': 2.5856449604034424, 'learning_rate': 4.877966101694916e-05, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:17<4:23:10,  2.74s/it]  4%|â–         | 245/6000 [11:20<4:22:07,  2.73s/it]                                                    {'loss': 2.7807, 'grad_norm': 1.687009334564209, 'learning_rate': 4.877118644067797e-05, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:20<4:22:07,  2.73s/it]  4%|â–         | 246/6000 [11:23<4:23:14,  2.74s/it]                                                    {'loss': 2.7802, 'grad_norm': 1.9210025072097778, 'learning_rate': 4.876271186440678e-05, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:23<4:23:14,  2.74s/it]  4%|â–         | 247/6000 [11:25<4:19:55,  2.71s/it]                                                    {'loss': 2.7874, 'grad_norm': 2.002854108810425, 'learning_rate': 4.8754237288135593e-05, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:26<4:19:55,  2.71s/it]  4%|â–         | 248/6000 [11:28<4:26:09,  2.78s/it]                                                    {'loss': 2.7682, 'grad_norm': 1.7212929725646973, 'learning_rate': 4.8745762711864405e-05, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:28<4:26:09,  2.78s/it]  4%|â–         | 249/6000 [11:31<4:22:21,  2.74s/it]                                                    {'loss': 2.7859, 'grad_norm': 2.0295121669769287, 'learning_rate': 4.873728813559322e-05, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:31<4:22:21,  2.74s/it]  4%|â–         | 250/6000 [11:34<4:18:46,  2.70s/it]                                                    {'loss': 2.7951, 'grad_norm': 1.6245312690734863, 'learning_rate': 4.8728813559322034e-05, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:34<4:18:46,  2.70s/it][2025-10-21 01:00:28,639] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [11:38<5:10:21,  3.24s/it]                                                    {'loss': 2.7903, 'grad_norm': 1.7964375019073486, 'learning_rate': 4.872033898305085e-05, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:38<5:10:21,  3.24s/it]  4%|â–         | 252/6000 [11:41<4:58:33,  3.12s/it]                                                    {'loss': 2.8135, 'grad_norm': 1.778808355331421, 'learning_rate': 4.8711864406779663e-05, 'epoch': 0.04}
  4%|â–         | 252/6000 [11:41<4:58:33,  3.12s/it]  4%|â–         | 253/6000 [11:44<4:45:43,  2.98s/it]                                                    {'loss': 2.7994, 'grad_norm': 1.5020921230316162, 'learning_rate': 4.8703389830508475e-05, 'epoch': 0.04}
  4%|â–         | 253/6000 [11:44<4:45:43,  2.98s/it]  4%|â–         | 254/6000 [11:46<4:39:19,  2.92s/it]                                                    {'loss': 2.7581, 'grad_norm': 1.8283687829971313, 'learning_rate': 4.8694915254237286e-05, 'epoch': 0.04}
  4%|â–         | 254/6000 [11:46<4:39:19,  2.92s/it]  4%|â–         | 255/6000 [11:49<4:30:37,  2.83s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.6259891986846924, 'learning_rate': 4.8686440677966104e-05, 'epoch': 0.04}
  4%|â–         | 255/6000 [11:49<4:30:37,  2.83s/it]  4%|â–         | 256/6000 [11:52<4:37:59,  2.90s/it]                                                    {'loss': 2.775, 'grad_norm': 1.7311896085739136, 'learning_rate': 4.8677966101694915e-05, 'epoch': 0.04}
  4%|â–         | 256/6000 [11:52<4:37:59,  2.90s/it]  4%|â–         | 257/6000 [11:55<4:29:46,  2.82s/it]                                                    {'loss': 2.7698, 'grad_norm': 1.5562628507614136, 'learning_rate': 4.8669491525423733e-05, 'epoch': 0.04}
  4%|â–         | 257/6000 [11:55<4:29:46,  2.82s/it]  4%|â–         | 258/6000 [11:57<4:24:46,  2.77s/it]                                                    {'loss': 2.7784, 'grad_norm': 2.766838312149048, 'learning_rate': 4.8661016949152545e-05, 'epoch': 0.04}
  4%|â–         | 258/6000 [11:57<4:24:46,  2.77s/it]  4%|â–         | 259/6000 [12:00<4:20:28,  2.72s/it]                                                    {'loss': 2.7749, 'grad_norm': 3.1017401218414307, 'learning_rate': 4.865254237288136e-05, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:00<4:20:28,  2.72s/it]  4%|â–         | 260/6000 [12:02<4:17:11,  2.69s/it]                                                    {'loss': 2.7778, 'grad_norm': 2.77960205078125, 'learning_rate': 4.8644067796610174e-05, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:03<4:17:11,  2.69s/it]  4%|â–         | 261/6000 [12:05<4:20:45,  2.73s/it]                                                    {'loss': 2.7975, 'grad_norm': 3.391494035720825, 'learning_rate': 4.8635593220338985e-05, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:05<4:20:45,  2.73s/it]  4%|â–         | 262/6000 [12:08<4:20:28,  2.72s/it]                                                    {'loss': 2.7651, 'grad_norm': 2.7381436824798584, 'learning_rate': 4.86271186440678e-05, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:08<4:20:28,  2.72s/it]  4%|â–         | 263/6000 [12:11<4:28:33,  2.81s/it]                                                    {'loss': 2.8431, 'grad_norm': 2.381389856338501, 'learning_rate': 4.8618644067796615e-05, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:11<4:28:33,  2.81s/it]  4%|â–         | 264/6000 [12:14<4:23:33,  2.76s/it]                                                    {'loss': 2.7691, 'grad_norm': 3.0625977516174316, 'learning_rate': 4.8610169491525426e-05, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:14<4:23:33,  2.76s/it]  4%|â–         | 265/6000 [12:17<4:26:28,  2.79s/it]                                                    {'loss': 2.7922, 'grad_norm': 3.2052767276763916, 'learning_rate': 4.8601694915254244e-05, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:17<4:26:28,  2.79s/it]  4%|â–         | 266/6000 [12:19<4:27:06,  2.79s/it]                                                    {'loss': 2.8075, 'grad_norm': 2.9584240913391113, 'learning_rate': 4.8593220338983055e-05, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:19<4:27:06,  2.79s/it]  4%|â–         | 267/6000 [12:22<4:22:38,  2.75s/it]                                                    {'loss': 2.7685, 'grad_norm': 2.9227476119995117, 'learning_rate': 4.858474576271187e-05, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:22<4:22:38,  2.75s/it]  4%|â–         | 268/6000 [12:25<4:21:26,  2.74s/it]                                                    {'loss': 2.8359, 'grad_norm': 2.972036361694336, 'learning_rate': 4.857627118644068e-05, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:25<4:21:26,  2.74s/it]  4%|â–         | 269/6000 [12:27<4:23:19,  2.76s/it]                                                    {'loss': 2.7834, 'grad_norm': 4.797426700592041, 'learning_rate': 4.856779661016949e-05, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:27<4:23:19,  2.76s/it]  4%|â–         | 270/6000 [12:30<4:20:44,  2.73s/it]                                                    {'loss': 2.792, 'grad_norm': 3.9337096214294434, 'learning_rate': 4.855932203389831e-05, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:30<4:20:44,  2.73s/it]  5%|â–         | 271/6000 [12:33<4:18:10,  2.70s/it]                                                    {'loss': 2.7648, 'grad_norm': 3.769953489303589, 'learning_rate': 4.855084745762712e-05, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:33<4:18:10,  2.70s/it]  5%|â–         | 272/6000 [12:35<4:16:53,  2.69s/it]                                                    {'loss': 2.7615, 'grad_norm': 3.7747802734375, 'learning_rate': 4.8542372881355937e-05, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:35<4:16:53,  2.69s/it]  5%|â–         | 273/6000 [12:38<4:15:25,  2.68s/it]                                                    {'loss': 2.7888, 'grad_norm': 4.198321342468262, 'learning_rate': 4.853389830508475e-05, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:38<4:15:25,  2.68s/it]  5%|â–         | 274/6000 [12:41<4:24:18,  2.77s/it]                                                    {'loss': 2.7583, 'grad_norm': 4.129866600036621, 'learning_rate': 4.8525423728813566e-05, 'epoch': 0.05}
  5%|â–         | 274/6000 [12:41<4:24:18,  2.77s/it]  5%|â–         | 275/6000 [12:44<4:21:29,  2.74s/it]                                                    {'loss': 2.7867, 'grad_norm': 3.878772735595703, 'learning_rate': 4.851694915254237e-05, 'epoch': 0.05}
  5%|â–         | 275/6000 [12:44<4:21:29,  2.74s/it]  5%|â–         | 276/6000 [12:46<4:19:56,  2.72s/it]                                                    {'loss': 2.8888, 'grad_norm': 4.7917938232421875, 'learning_rate': 4.850847457627119e-05, 'epoch': 0.05}
  5%|â–         | 276/6000 [12:46<4:19:56,  2.72s/it]  5%|â–         | 277/6000 [12:49<4:16:13,  2.69s/it]                                                    {'loss': 2.7931, 'grad_norm': 4.483028411865234, 'learning_rate': 4.85e-05, 'epoch': 0.05}
  5%|â–         | 277/6000 [12:49<4:16:13,  2.69s/it]  5%|â–         | 278/6000 [12:52<4:15:33,  2.68s/it]                                                    {'loss': 2.7634, 'grad_norm': 3.9188506603240967, 'learning_rate': 4.849152542372882e-05, 'epoch': 0.05}
  5%|â–         | 278/6000 [12:52<4:15:33,  2.68s/it]  5%|â–         | 279/6000 [12:54<4:16:28,  2.69s/it]                                                    {'loss': 2.779, 'grad_norm': 4.395587921142578, 'learning_rate': 4.848305084745763e-05, 'epoch': 0.05}
  5%|â–         | 279/6000 [12:54<4:16:28,  2.69s/it]  5%|â–         | 280/6000 [12:57<4:15:57,  2.68s/it]                                                    {'loss': 2.7645, 'grad_norm': 3.416156768798828, 'learning_rate': 4.847457627118645e-05, 'epoch': 0.05}
  5%|â–         | 280/6000 [12:57<4:15:57,  2.68s/it]  5%|â–         | 281/6000 [13:00<4:16:14,  2.69s/it]                                                    {'loss': 2.7812, 'grad_norm': 4.545618534088135, 'learning_rate': 4.846610169491526e-05, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:00<4:16:14,  2.69s/it]  5%|â–         | 282/6000 [13:03<4:26:16,  2.79s/it]                                                    {'loss': 2.7925, 'grad_norm': 3.540522575378418, 'learning_rate': 4.845762711864407e-05, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:03<4:26:16,  2.79s/it]  5%|â–         | 283/6000 [13:06<4:31:06,  2.85s/it]                                                    {'loss': 2.8203, 'grad_norm': 4.901656150817871, 'learning_rate': 4.844915254237288e-05, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:06<4:31:06,  2.85s/it]  5%|â–         | 284/6000 [13:09<4:44:34,  2.99s/it]                                                    {'loss': 2.7822, 'grad_norm': 2.8215062618255615, 'learning_rate': 4.84406779661017e-05, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:09<4:44:34,  2.99s/it]  5%|â–         | 285/6000 [13:12<4:40:41,  2.95s/it]                                                    {'loss': 2.7906, 'grad_norm': 2.9036705493927, 'learning_rate': 4.843220338983051e-05, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:12<4:40:41,  2.95s/it]  5%|â–         | 286/6000 [13:15<4:32:02,  2.86s/it]                                                    {'loss': 2.7705, 'grad_norm': 2.6231038570404053, 'learning_rate': 4.842372881355933e-05, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:15<4:32:02,  2.86s/it]  5%|â–         | 287/6000 [13:17<4:28:54,  2.82s/it]                                                    {'loss': 2.8037, 'grad_norm': 2.439927101135254, 'learning_rate': 4.841525423728814e-05, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:17<4:28:54,  2.82s/it]  5%|â–         | 288/6000 [13:20<4:24:28,  2.78s/it]                                                    {'loss': 2.7858, 'grad_norm': 3.363952875137329, 'learning_rate': 4.840677966101695e-05, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:20<4:24:28,  2.78s/it]  5%|â–         | 289/6000 [13:23<4:25:52,  2.79s/it]                                                    {'loss': 2.7627, 'grad_norm': 2.7328312397003174, 'learning_rate': 4.839830508474576e-05, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:23<4:25:52,  2.79s/it]  5%|â–         | 290/6000 [13:26<4:22:13,  2.76s/it]                                                    {'loss': 2.7986, 'grad_norm': 6.541264533996582, 'learning_rate': 4.8389830508474574e-05, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:26<4:22:13,  2.76s/it]  5%|â–         | 291/6000 [13:28<4:18:16,  2.71s/it]                                                    {'loss': 2.7953, 'grad_norm': 27.314008712768555, 'learning_rate': 4.838135593220339e-05, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:28<4:18:16,  2.71s/it]  5%|â–         | 292/6000 [13:31<4:16:48,  2.70s/it]                                                    {'loss': 2.7767, 'grad_norm': 10.534708023071289, 'learning_rate': 4.83728813559322e-05, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:31<4:16:48,  2.70s/it]  5%|â–         | 293/6000 [13:33<4:14:02,  2.67s/it]                                                    {'loss': 2.8469, 'grad_norm': 5.760343551635742, 'learning_rate': 4.836440677966102e-05, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:33<4:14:02,  2.67s/it]  5%|â–         | 294/6000 [13:36<4:14:04,  2.67s/it]                                                    {'loss': 2.7975, 'grad_norm': 12.313959121704102, 'learning_rate': 4.835593220338983e-05, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:36<4:14:04,  2.67s/it]  5%|â–         | 295/6000 [13:39<4:13:33,  2.67s/it]                                                    {'loss': 2.7885, 'grad_norm': 5.16951847076416, 'learning_rate': 4.834745762711865e-05, 'epoch': 0.05}
  5%|â–         | 295/6000 [13:39<4:13:33,  2.67s/it]  5%|â–         | 296/6000 [13:42<4:18:05,  2.71s/it]                                                    {'loss': 2.7734, 'grad_norm': 4.9669294357299805, 'learning_rate': 4.833898305084746e-05, 'epoch': 0.05}
  5%|â–         | 296/6000 [13:42<4:18:05,  2.71s/it]  5%|â–         | 297/6000 [13:44<4:16:56,  2.70s/it]                                                    {'loss': 2.7885, 'grad_norm': 3.74135684967041, 'learning_rate': 4.833050847457627e-05, 'epoch': 0.05}
  5%|â–         | 297/6000 [13:44<4:16:56,  2.70s/it]  5%|â–         | 298/6000 [13:47<4:20:53,  2.75s/it]                                                    {'loss': 2.7745, 'grad_norm': 1.483178734779358, 'learning_rate': 4.8322033898305084e-05, 'epoch': 0.05}
  5%|â–         | 298/6000 [13:47<4:20:53,  2.75s/it]  5%|â–         | 299/6000 [13:51<4:41:56,  2.97s/it]                                                    {'loss': 2.7902, 'grad_norm': 2.071578025817871, 'learning_rate': 4.83135593220339e-05, 'epoch': 0.05}
  5%|â–         | 299/6000 [13:51<4:41:56,  2.97s/it]  5%|â–Œ         | 300/6000 [13:53<4:33:29,  2.88s/it]                                                    {'loss': 2.7718, 'grad_norm': 1.479352355003357, 'learning_rate': 4.8305084745762714e-05, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [13:53<4:33:29,  2.88s/it][2025-10-21 01:02:48,356] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [13:58<5:27:55,  3.45s/it]                                                    {'loss': 2.8068, 'grad_norm': 2.0055129528045654, 'learning_rate': 4.829661016949153e-05, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [13:58<5:27:55,  3.45s/it]  5%|â–Œ         | 302/6000 [14:01<5:05:36,  3.22s/it]                                                    {'loss': 2.7745, 'grad_norm': 1.6313194036483765, 'learning_rate': 4.828813559322034e-05, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:01<5:05:36,  3.22s/it]  5%|â–Œ         | 303/6000 [14:03<4:51:58,  3.07s/it]                                                    {'loss': 2.7614, 'grad_norm': 1.9082963466644287, 'learning_rate': 4.8279661016949154e-05, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:03<4:51:58,  3.07s/it]  5%|â–Œ         | 304/6000 [14:06<4:40:24,  2.95s/it]                                                    {'loss': 2.789, 'grad_norm': 2.3255016803741455, 'learning_rate': 4.8271186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:06<4:40:24,  2.95s/it]  5%|â–Œ         | 305/6000 [14:09<4:29:41,  2.84s/it]                                                    {'loss': 2.7769, 'grad_norm': 2.500779867172241, 'learning_rate': 4.8262711864406784e-05, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:09<4:29:41,  2.84s/it]  5%|â–Œ         | 306/6000 [14:12<4:28:34,  2.83s/it]                                                    {'loss': 2.7901, 'grad_norm': 3.287662982940674, 'learning_rate': 4.8254237288135595e-05, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:12<4:28:34,  2.83s/it]  5%|â–Œ         | 307/6000 [14:14<4:23:08,  2.77s/it]                                                    {'loss': 2.7903, 'grad_norm': 1.9086439609527588, 'learning_rate': 4.824576271186441e-05, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:14<4:23:08,  2.77s/it]  5%|â–Œ         | 308/6000 [14:17<4:20:34,  2.75s/it]                                                    {'loss': 2.8054, 'grad_norm': 2.4474387168884277, 'learning_rate': 4.8237288135593224e-05, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:17<4:20:34,  2.75s/it]  5%|â–Œ         | 309/6000 [14:20<4:21:24,  2.76s/it]                                                    {'loss': 2.7722, 'grad_norm': 2.8139662742614746, 'learning_rate': 4.8228813559322036e-05, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:20<4:21:24,  2.76s/it]  5%|â–Œ         | 310/6000 [14:22<4:18:29,  2.73s/it]                                                    {'loss': 2.8096, 'grad_norm': 4.04144811630249, 'learning_rate': 4.822033898305085e-05, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:22<4:18:29,  2.73s/it]  5%|â–Œ         | 311/6000 [14:25<4:15:32,  2.70s/it]                                                    {'loss': 2.7783, 'grad_norm': 2.547779083251953, 'learning_rate': 4.821186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:25<4:15:32,  2.70s/it]  5%|â–Œ         | 312/6000 [14:28<4:14:23,  2.68s/it]                                                    {'loss': 2.7845, 'grad_norm': 6.248089790344238, 'learning_rate': 4.8203389830508476e-05, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:28<4:14:23,  2.68s/it]  5%|â–Œ         | 313/6000 [14:30<4:15:05,  2.69s/it]                                                    {'loss': 2.8466, 'grad_norm': 2.5435779094696045, 'learning_rate': 4.819491525423729e-05, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:30<4:15:05,  2.69s/it]  5%|â–Œ         | 314/6000 [14:33<4:14:27,  2.69s/it]                                                    {'loss': 2.8062, 'grad_norm': 4.379367828369141, 'learning_rate': 4.8186440677966105e-05, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:33<4:14:27,  2.69s/it]  5%|â–Œ         | 315/6000 [14:36<4:11:33,  2.66s/it]                                                    {'loss': 2.7799, 'grad_norm': 1.7665692567825317, 'learning_rate': 4.817796610169492e-05, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:36<4:11:33,  2.66s/it]  5%|â–Œ         | 316/6000 [14:38<4:10:55,  2.65s/it]                                                    {'loss': 2.7727, 'grad_norm': 4.179628849029541, 'learning_rate': 4.8169491525423735e-05, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [14:38<4:10:55,  2.65s/it]  5%|â–Œ         | 317/6000 [14:41<4:15:11,  2.69s/it]                                                    {'loss': 2.8719, 'grad_norm': 3.432493209838867, 'learning_rate': 4.8161016949152546e-05, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [14:41<4:15:11,  2.69s/it]  5%|â–Œ         | 318/6000 [14:44<4:22:54,  2.78s/it]                                                    {'loss': 2.7556, 'grad_norm': 3.1484808921813965, 'learning_rate': 4.815254237288136e-05, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [14:44<4:22:54,  2.78s/it]  5%|â–Œ         | 319/6000 [14:47<4:19:01,  2.74s/it]                                                    {'loss': 2.7524, 'grad_norm': 6.88181734085083, 'learning_rate': 4.814406779661017e-05, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [14:47<4:19:01,  2.74s/it]  5%|â–Œ         | 320/6000 [14:49<4:18:33,  2.73s/it]                                                    {'loss': 2.7483, 'grad_norm': 5.756705284118652, 'learning_rate': 4.813559322033899e-05, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [14:49<4:18:33,  2.73s/it]  5%|â–Œ         | 321/6000 [14:52<4:27:55,  2.83s/it]                                                    {'loss': 2.7804, 'grad_norm': 6.159782886505127, 'learning_rate': 4.81271186440678e-05, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [14:52<4:27:55,  2.83s/it]  5%|â–Œ         | 322/6000 [14:55<4:23:09,  2.78s/it]                                                    {'loss': 2.7736, 'grad_norm': 15.004870414733887, 'learning_rate': 4.8118644067796616e-05, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [14:55<4:23:09,  2.78s/it]  5%|â–Œ         | 323/6000 [14:58<4:36:11,  2.92s/it]                                                    {'loss': 2.8407, 'grad_norm': 36.81072235107422, 'learning_rate': 4.811016949152543e-05, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [14:58<4:36:11,  2.92s/it]  5%|â–Œ         | 324/6000 [15:01<4:27:41,  2.83s/it]                                                    {'loss': 2.8641, 'grad_norm': 35.38233184814453, 'learning_rate': 4.810169491525424e-05, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:01<4:27:41,  2.83s/it]  5%|â–Œ         | 325/6000 [15:03<4:21:27,  2.76s/it]                                                    {'loss': 2.7456, 'grad_norm': 14.540940284729004, 'learning_rate': 4.809322033898305e-05, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:03<4:21:27,  2.76s/it]  5%|â–Œ         | 326/6000 [15:06<4:18:17,  2.73s/it]                                                    {'loss': 2.8187, 'grad_norm': 6.278031826019287, 'learning_rate': 4.808474576271187e-05, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:06<4:18:17,  2.73s/it]  5%|â–Œ         | 327/6000 [15:09<4:14:07,  2.69s/it]                                                    {'loss': 2.7841, 'grad_norm': 4.260417938232422, 'learning_rate': 4.807627118644068e-05, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:09<4:14:07,  2.69s/it]  5%|â–Œ         | 328/6000 [15:11<4:13:09,  2.68s/it]                                                    {'loss': 2.7759, 'grad_norm': 3.946002244949341, 'learning_rate': 4.80677966101695e-05, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:11<4:13:09,  2.68s/it]  5%|â–Œ         | 329/6000 [15:14<4:12:51,  2.68s/it]                                                    {'loss': 2.7883, 'grad_norm': 1.6696282625198364, 'learning_rate': 4.805932203389831e-05, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:14<4:12:51,  2.68s/it]  6%|â–Œ         | 330/6000 [15:17<4:12:06,  2.67s/it]                                                    {'loss': 2.7948, 'grad_norm': 2.059410572052002, 'learning_rate': 4.805084745762712e-05, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:17<4:12:06,  2.67s/it]  6%|â–Œ         | 331/6000 [15:19<4:12:51,  2.68s/it]                                                    {'loss': 2.7736, 'grad_norm': 1.437360167503357, 'learning_rate': 4.804237288135594e-05, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:19<4:12:51,  2.68s/it]  6%|â–Œ         | 332/6000 [15:22<4:11:20,  2.66s/it]                                                    {'loss': 2.7712, 'grad_norm': 1.411620855331421, 'learning_rate': 4.803389830508474e-05, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:22<4:11:20,  2.66s/it]  6%|â–Œ         | 333/6000 [15:25<4:15:48,  2.71s/it]                                                    {'loss': 2.7834, 'grad_norm': 1.420652985572815, 'learning_rate': 4.802542372881356e-05, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:25<4:15:48,  2.71s/it]  6%|â–Œ         | 334/6000 [15:27<4:14:19,  2.69s/it]                                                    {'loss': 2.7757, 'grad_norm': 1.2943249940872192, 'learning_rate': 4.801694915254237e-05, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:28<4:14:19,  2.69s/it]  6%|â–Œ         | 335/6000 [15:30<4:14:48,  2.70s/it]                                                    {'loss': 2.7965, 'grad_norm': 1.4657742977142334, 'learning_rate': 4.800847457627119e-05, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [15:30<4:14:48,  2.70s/it]  6%|â–Œ         | 336/6000 [15:33<4:11:47,  2.67s/it]                                                    {'loss': 2.7778, 'grad_norm': 1.192427396774292, 'learning_rate': 4.8e-05, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [15:33<4:11:47,  2.67s/it]  6%|â–Œ         | 337/6000 [15:36<4:15:43,  2.71s/it]                                                    {'loss': 2.7806, 'grad_norm': 1.1161221265792847, 'learning_rate': 4.799152542372882e-05, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [15:36<4:15:43,  2.71s/it]  6%|â–Œ         | 338/6000 [15:38<4:13:45,  2.69s/it]                                                    {'loss': 2.7789, 'grad_norm': 1.24203360080719, 'learning_rate': 4.798305084745763e-05, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [15:38<4:13:45,  2.69s/it]  6%|â–Œ         | 339/6000 [15:41<4:13:34,  2.69s/it]                                                    {'loss': 2.7764, 'grad_norm': 0.9374775290489197, 'learning_rate': 4.797457627118644e-05, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [15:41<4:13:34,  2.69s/it]  6%|â–Œ         | 340/6000 [15:44<4:19:19,  2.75s/it]                                                    {'loss': 2.7667, 'grad_norm': 1.2135001420974731, 'learning_rate': 4.796610169491525e-05, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [15:44<4:19:19,  2.75s/it]  6%|â–Œ         | 341/6000 [15:46<4:14:52,  2.70s/it]                                                    {'loss': 2.7762, 'grad_norm': 1.3145272731781006, 'learning_rate': 4.795762711864407e-05, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [15:46<4:14:52,  2.70s/it]  6%|â–Œ         | 342/6000 [15:49<4:13:17,  2.69s/it]                                                    {'loss': 2.8061, 'grad_norm': 0.8520150184631348, 'learning_rate': 4.794915254237288e-05, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [15:49<4:13:17,  2.69s/it]  6%|â–Œ         | 343/6000 [15:52<4:29:04,  2.85s/it]                                                    {'loss': 2.8176, 'grad_norm': 1.110528826713562, 'learning_rate': 4.79406779661017e-05, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [15:52<4:29:04,  2.85s/it]  6%|â–Œ         | 344/6000 [15:55<4:24:02,  2.80s/it]                                                    {'loss': 2.7905, 'grad_norm': 1.0730702877044678, 'learning_rate': 4.793220338983051e-05, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [15:55<4:24:02,  2.80s/it]  6%|â–Œ         | 345/6000 [15:58<4:18:30,  2.74s/it]                                                    {'loss': 2.8144, 'grad_norm': 1.180295705795288, 'learning_rate': 4.792372881355933e-05, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [15:58<4:18:30,  2.74s/it]  6%|â–Œ         | 346/6000 [16:00<4:16:06,  2.72s/it]                                                    {'loss': 2.8139, 'grad_norm': 1.1931449174880981, 'learning_rate': 4.7915254237288134e-05, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:00<4:16:06,  2.72s/it]  6%|â–Œ         | 347/6000 [16:03<4:13:43,  2.69s/it]                                                    {'loss': 2.7846, 'grad_norm': 0.9352107048034668, 'learning_rate': 4.790677966101695e-05, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:03<4:13:43,  2.69s/it]  6%|â–Œ         | 348/6000 [16:06<4:15:25,  2.71s/it]                                                    {'loss': 2.7863, 'grad_norm': 1.3440121412277222, 'learning_rate': 4.7898305084745764e-05, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:06<4:15:25,  2.71s/it]  6%|â–Œ         | 349/6000 [16:08<4:12:27,  2.68s/it]                                                    {'loss': 2.7815, 'grad_norm': 1.586548089981079, 'learning_rate': 4.788983050847458e-05, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:08<4:12:27,  2.68s/it]  6%|â–Œ         | 350/6000 [16:11<4:11:52,  2.67s/it]                                                    {'loss': 2.7849, 'grad_norm': 1.366516351699829, 'learning_rate': 4.788135593220339e-05, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:11<4:11:52,  2.67s/it][2025-10-21 01:05:06,023] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [16:16<5:14:13,  3.34s/it]                                                    {'loss': 2.7537, 'grad_norm': 1.8813906908035278, 'learning_rate': 4.7872881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:16<5:14:13,  3.34s/it]  6%|â–Œ         | 352/6000 [16:18<4:53:34,  3.12s/it]                                                    {'loss': 2.7843, 'grad_norm': 1.5811820030212402, 'learning_rate': 4.786440677966102e-05, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:18<4:53:34,  3.12s/it]  6%|â–Œ         | 353/6000 [16:21<4:38:27,  2.96s/it]                                                    {'loss': 2.816, 'grad_norm': 1.479760766029358, 'learning_rate': 4.7855932203389834e-05, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:21<4:38:27,  2.96s/it]  6%|â–Œ         | 354/6000 [16:24<4:30:03,  2.87s/it]                                                    {'loss': 2.7839, 'grad_norm': 1.5848243236541748, 'learning_rate': 4.7847457627118645e-05, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:24<4:30:03,  2.87s/it]  6%|â–Œ         | 355/6000 [16:26<4:24:09,  2.81s/it]                                                    {'loss': 2.8144, 'grad_norm': 2.068758726119995, 'learning_rate': 4.7838983050847456e-05, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:26<4:24:09,  2.81s/it]  6%|â–Œ         | 356/6000 [16:29<4:26:28,  2.83s/it]                                                    {'loss': 2.7784, 'grad_norm': 2.852590799331665, 'learning_rate': 4.7830508474576274e-05, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [16:29<4:26:28,  2.83s/it]  6%|â–Œ         | 357/6000 [16:32<4:21:47,  2.78s/it]                                                    {'loss': 2.8041, 'grad_norm': 1.9094452857971191, 'learning_rate': 4.7822033898305086e-05, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [16:32<4:21:47,  2.78s/it]  6%|â–Œ         | 358/6000 [16:35<4:17:24,  2.74s/it]                                                    {'loss': 2.7788, 'grad_norm': 2.076270580291748, 'learning_rate': 4.7813559322033904e-05, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [16:35<4:17:24,  2.74s/it]  6%|â–Œ         | 359/6000 [16:37<4:15:24,  2.72s/it]                                                    {'loss': 2.7754, 'grad_norm': 1.7912602424621582, 'learning_rate': 4.7805084745762715e-05, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [16:37<4:15:24,  2.72s/it]  6%|â–Œ         | 360/6000 [16:40<4:14:49,  2.71s/it]                                                    {'loss': 2.799, 'grad_norm': 2.2083303928375244, 'learning_rate': 4.7796610169491526e-05, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [16:40<4:14:49,  2.71s/it]  6%|â–Œ         | 361/6000 [16:42<4:12:17,  2.68s/it]                                                    {'loss': 2.7771, 'grad_norm': 2.3300204277038574, 'learning_rate': 4.778813559322034e-05, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [16:43<4:12:17,  2.68s/it]  6%|â–Œ         | 362/6000 [16:45<4:09:40,  2.66s/it]                                                    {'loss': 2.7727, 'grad_norm': 1.9979114532470703, 'learning_rate': 4.7779661016949156e-05, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [16:45<4:09:40,  2.66s/it]  6%|â–Œ         | 363/6000 [16:48<4:08:13,  2.64s/it]                                                    {'loss': 2.808, 'grad_norm': 2.471468687057495, 'learning_rate': 4.777118644067797e-05, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [16:48<4:08:13,  2.64s/it]  6%|â–Œ         | 364/6000 [16:50<4:12:01,  2.68s/it]                                                    {'loss': 2.7861, 'grad_norm': 2.161485195159912, 'learning_rate': 4.7762711864406785e-05, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [16:50<4:12:01,  2.68s/it]  6%|â–Œ         | 365/6000 [16:53<4:09:32,  2.66s/it]                                                    {'loss': 2.7772, 'grad_norm': 2.366727352142334, 'learning_rate': 4.7754237288135596e-05, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [16:53<4:09:32,  2.66s/it]  6%|â–Œ         | 366/6000 [16:56<4:09:36,  2.66s/it]                                                    {'loss': 2.7971, 'grad_norm': 2.061830997467041, 'learning_rate': 4.7745762711864414e-05, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [16:56<4:09:36,  2.66s/it]  6%|â–Œ         | 367/6000 [16:58<4:09:34,  2.66s/it]                                                    {'loss': 2.767, 'grad_norm': 1.7128621339797974, 'learning_rate': 4.773728813559322e-05, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [16:58<4:09:34,  2.66s/it]  6%|â–Œ         | 368/6000 [17:02<4:22:41,  2.80s/it]                                                    {'loss': 2.7643, 'grad_norm': 2.3396108150482178, 'learning_rate': 4.772881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [17:02<4:22:41,  2.80s/it]  6%|â–Œ         | 369/6000 [17:04<4:18:54,  2.76s/it]                                                    {'loss': 2.8027, 'grad_norm': 1.7163479328155518, 'learning_rate': 4.772033898305085e-05, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [17:04<4:18:54,  2.76s/it]  6%|â–Œ         | 370/6000 [17:07<4:17:17,  2.74s/it]                                                    {'loss': 2.7606, 'grad_norm': 1.6931015253067017, 'learning_rate': 4.7711864406779666e-05, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [17:07<4:17:17,  2.74s/it]  6%|â–Œ         | 371/6000 [17:10<4:13:41,  2.70s/it]                                                    {'loss': 2.7644, 'grad_norm': 1.7423018217086792, 'learning_rate': 4.770338983050848e-05, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [17:10<4:13:41,  2.70s/it]  6%|â–Œ         | 372/6000 [17:12<4:14:20,  2.71s/it]                                                    {'loss': 2.7903, 'grad_norm': 1.8782823085784912, 'learning_rate': 4.769491525423729e-05, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [17:12<4:14:20,  2.71s/it]  6%|â–Œ         | 373/6000 [17:15<4:12:57,  2.70s/it]                                                    {'loss': 2.9099, 'grad_norm': 1.2438594102859497, 'learning_rate': 4.768644067796611e-05, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [17:15<4:12:57,  2.70s/it]  6%|â–Œ         | 374/6000 [17:18<4:12:17,  2.69s/it]                                                    {'loss': 2.7671, 'grad_norm': 1.9724434614181519, 'learning_rate': 4.767796610169492e-05, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [17:18<4:12:17,  2.69s/it]  6%|â–‹         | 375/6000 [17:20<4:11:27,  2.68s/it]                                                    {'loss': 2.7986, 'grad_norm': 2.375725507736206, 'learning_rate': 4.766949152542373e-05, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [17:20<4:11:27,  2.68s/it]  6%|â–‹         | 376/6000 [17:23<4:09:08,  2.66s/it]                                                    {'loss': 2.7866, 'grad_norm': 2.0864462852478027, 'learning_rate': 4.766101694915254e-05, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [17:23<4:09:08,  2.66s/it]  6%|â–‹         | 377/6000 [17:25<4:08:20,  2.65s/it]                                                    {'loss': 2.8378, 'grad_norm': 2.717637538909912, 'learning_rate': 4.765254237288136e-05, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [17:25<4:08:20,  2.65s/it]  6%|â–‹         | 378/6000 [17:28<4:10:38,  2.67s/it]                                                    {'loss': 2.7639, 'grad_norm': 2.346940517425537, 'learning_rate': 4.764406779661017e-05, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [17:28<4:10:38,  2.67s/it]  6%|â–‹         | 379/6000 [17:31<4:11:38,  2.69s/it]                                                    {'loss': 2.765, 'grad_norm': 2.425177812576294, 'learning_rate': 4.763559322033899e-05, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [17:31<4:11:38,  2.69s/it]  6%|â–‹         | 380/6000 [17:34<4:20:28,  2.78s/it]                                                    {'loss': 2.7848, 'grad_norm': 1.949808120727539, 'learning_rate': 4.76271186440678e-05, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [17:34<4:20:28,  2.78s/it]  6%|â–‹         | 381/6000 [17:37<4:15:48,  2.73s/it]                                                    {'loss': 2.7986, 'grad_norm': 2.5841360092163086, 'learning_rate': 4.761864406779661e-05, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [17:37<4:15:48,  2.73s/it]  6%|â–‹         | 382/6000 [17:39<4:14:39,  2.72s/it]                                                    {'loss': 2.7719, 'grad_norm': 1.821962833404541, 'learning_rate': 4.761016949152542e-05, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [17:39<4:14:39,  2.72s/it]  6%|â–‹         | 383/6000 [17:42<4:13:40,  2.71s/it]                                                    {'loss': 2.8158, 'grad_norm': 1.9030201435089111, 'learning_rate': 4.760169491525424e-05, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [17:42<4:13:40,  2.71s/it]  6%|â–‹         | 384/6000 [17:45<4:11:21,  2.69s/it]                                                    {'loss': 2.7768, 'grad_norm': 2.196085214614868, 'learning_rate': 4.759322033898305e-05, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [17:45<4:11:21,  2.69s/it]  6%|â–‹         | 385/6000 [17:48<4:22:13,  2.80s/it]                                                    {'loss': 2.7681, 'grad_norm': 1.792952299118042, 'learning_rate': 4.758474576271187e-05, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [17:48<4:22:13,  2.80s/it]  6%|â–‹         | 386/6000 [17:50<4:24:32,  2.83s/it]                                                    {'loss': 2.791, 'grad_norm': 1.9145407676696777, 'learning_rate': 4.757627118644068e-05, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [17:51<4:24:32,  2.83s/it]  6%|â–‹         | 387/6000 [17:54<4:31:31,  2.90s/it]                                                    {'loss': 2.7602, 'grad_norm': 2.517850875854492, 'learning_rate': 4.75677966101695e-05, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [17:54<4:31:31,  2.90s/it]  6%|â–‹         | 388/6000 [17:56<4:29:25,  2.88s/it]                                                    {'loss': 2.8082, 'grad_norm': 1.506872296333313, 'learning_rate': 4.755932203389831e-05, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [17:56<4:29:25,  2.88s/it]  6%|â–‹         | 389/6000 [17:59<4:23:19,  2.82s/it]                                                    {'loss': 2.7834, 'grad_norm': 1.8121711015701294, 'learning_rate': 4.755084745762712e-05, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [17:59<4:23:19,  2.82s/it]  6%|â–‹         | 390/6000 [18:02<4:20:36,  2.79s/it]                                                    {'loss': 2.7828, 'grad_norm': 1.849089503288269, 'learning_rate': 4.754237288135593e-05, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [18:02<4:20:36,  2.79s/it]  7%|â–‹         | 391/6000 [18:04<4:15:01,  2.73s/it]                                                    {'loss': 2.7658, 'grad_norm': 1.908433198928833, 'learning_rate': 4.7533898305084744e-05, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [18:04<4:15:01,  2.73s/it]  7%|â–‹         | 392/6000 [18:07<4:14:47,  2.73s/it]                                                    {'loss': 2.7689, 'grad_norm': 1.9423772096633911, 'learning_rate': 4.752542372881356e-05, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [18:07<4:14:47,  2.73s/it]  7%|â–‹         | 393/6000 [18:10<4:12:41,  2.70s/it]                                                    {'loss': 2.7766, 'grad_norm': 2.320220947265625, 'learning_rate': 4.751694915254237e-05, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [18:10<4:12:41,  2.70s/it]  7%|â–‹         | 394/6000 [18:12<4:13:05,  2.71s/it]                                                    {'loss': 2.7897, 'grad_norm': 2.223334550857544, 'learning_rate': 4.750847457627119e-05, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [18:12<4:13:05,  2.71s/it]  7%|â–‹         | 395/6000 [18:15<4:14:18,  2.72s/it]                                                    {'loss': 2.7734, 'grad_norm': 2.2666032314300537, 'learning_rate': 4.75e-05, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [18:15<4:14:18,  2.72s/it]  7%|â–‹         | 396/6000 [18:18<4:12:26,  2.70s/it]                                                    {'loss': 2.7954, 'grad_norm': 2.7774901390075684, 'learning_rate': 4.7491525423728814e-05, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [18:18<4:12:26,  2.70s/it]  7%|â–‹         | 397/6000 [18:21<4:21:18,  2.80s/it]                                                    {'loss': 2.7699, 'grad_norm': 2.4056432247161865, 'learning_rate': 4.7483050847457625e-05, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [18:21<4:21:18,  2.80s/it]  7%|â–‹         | 398/6000 [18:24<4:17:18,  2.76s/it]                                                    {'loss': 2.7658, 'grad_norm': 2.5702459812164307, 'learning_rate': 4.747457627118644e-05, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [18:24<4:17:18,  2.76s/it]  7%|â–‹         | 399/6000 [18:26<4:16:26,  2.75s/it]                                                    {'loss': 2.7702, 'grad_norm': 2.6142232418060303, 'learning_rate': 4.7466101694915255e-05, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [18:26<4:16:26,  2.75s/it]  7%|â–‹         | 400/6000 [18:29<4:13:46,  2.72s/it]                                                    {'loss': 2.7724, 'grad_norm': 1.8344995975494385, 'learning_rate': 4.745762711864407e-05, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [18:29<4:13:46,  2.72s/it][2025-10-21 01:07:24,044] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [18:33<5:04:44,  3.27s/it]                                                    {'loss': 2.763, 'grad_norm': 2.0340168476104736, 'learning_rate': 4.7449152542372884e-05, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [18:33<5:04:44,  3.27s/it]  7%|â–‹         | 402/6000 [18:36<4:46:41,  3.07s/it]                                                    {'loss': 2.7735, 'grad_norm': 2.300015449523926, 'learning_rate': 4.74406779661017e-05, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [18:36<4:46:41,  3.07s/it]  7%|â–‹         | 403/6000 [18:39<4:35:58,  2.96s/it]                                                    {'loss': 2.8105, 'grad_norm': 5.405674457550049, 'learning_rate': 4.7432203389830506e-05, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [18:39<4:35:58,  2.96s/it]  7%|â–‹         | 404/6000 [18:42<4:30:13,  2.90s/it]                                                    {'loss': 2.7271, 'grad_norm': 4.263745307922363, 'learning_rate': 4.7423728813559325e-05, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [18:42<4:30:13,  2.90s/it]  7%|â–‹         | 405/6000 [18:44<4:23:32,  2.83s/it]                                                    {'loss': 2.7659, 'grad_norm': 4.58840274810791, 'learning_rate': 4.7415254237288136e-05, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [18:44<4:23:32,  2.83s/it]  7%|â–‹         | 406/6000 [18:47<4:21:03,  2.80s/it]                                                    {'loss': 2.8074, 'grad_norm': 5.200864791870117, 'learning_rate': 4.7406779661016954e-05, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [18:47<4:21:03,  2.80s/it]  7%|â–‹         | 407/6000 [18:50<4:19:22,  2.78s/it]                                                    {'loss': 2.7191, 'grad_norm': 5.020241737365723, 'learning_rate': 4.7398305084745765e-05, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [18:50<4:19:22,  2.78s/it]  7%|â–‹         | 408/6000 [18:53<4:35:19,  2.95s/it]                                                    {'loss': 2.8647, 'grad_norm': 7.288946151733398, 'learning_rate': 4.738983050847458e-05, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [18:53<4:35:19,  2.95s/it]  7%|â–‹         | 409/6000 [18:56<4:29:56,  2.90s/it]                                                    {'loss': 2.7822, 'grad_norm': 3.79345703125, 'learning_rate': 4.7381355932203395e-05, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [18:56<4:29:56,  2.90s/it]  7%|â–‹         | 410/6000 [18:58<4:21:53,  2.81s/it]                                                    {'loss': 2.7903, 'grad_norm': 4.213449478149414, 'learning_rate': 4.7372881355932206e-05, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [18:58<4:21:53,  2.81s/it]  7%|â–‹         | 411/6000 [19:02<4:37:00,  2.97s/it]                                                    {'loss': 2.8469, 'grad_norm': 5.995479106903076, 'learning_rate': 4.736440677966102e-05, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [19:02<4:37:00,  2.97s/it]  7%|â–‹         | 412/6000 [19:04<4:28:09,  2.88s/it]                                                    {'loss': 2.7921, 'grad_norm': 3.4683308601379395, 'learning_rate': 4.735593220338983e-05, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [19:04<4:28:09,  2.88s/it]  7%|â–‹         | 413/6000 [19:07<4:27:11,  2.87s/it]                                                    {'loss': 2.7643, 'grad_norm': 2.52297306060791, 'learning_rate': 4.7347457627118646e-05, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [19:07<4:27:11,  2.87s/it]  7%|â–‹         | 414/6000 [19:10<4:21:18,  2.81s/it]                                                    {'loss': 2.8005, 'grad_norm': 1.6613218784332275, 'learning_rate': 4.733898305084746e-05, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [19:10<4:21:18,  2.81s/it]  7%|â–‹         | 415/6000 [19:13<4:21:54,  2.81s/it]                                                    {'loss': 2.7651, 'grad_norm': 1.9126518964767456, 'learning_rate': 4.7330508474576276e-05, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [19:13<4:21:54,  2.81s/it]  7%|â–‹         | 416/6000 [19:15<4:17:23,  2.77s/it]                                                    {'loss': 2.8054, 'grad_norm': 1.5058950185775757, 'learning_rate': 4.732203389830509e-05, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [19:15<4:17:23,  2.77s/it]  7%|â–‹         | 417/6000 [19:18<4:16:41,  2.76s/it]                                                    {'loss': 2.8227, 'grad_norm': 2.588216543197632, 'learning_rate': 4.73135593220339e-05, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [19:18<4:16:41,  2.76s/it]  7%|â–‹         | 418/6000 [19:21<4:19:22,  2.79s/it]                                                    {'loss': 2.7995, 'grad_norm': 1.7004908323287964, 'learning_rate': 4.730508474576271e-05, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [19:21<4:19:22,  2.79s/it]  7%|â–‹         | 419/6000 [19:24<4:28:04,  2.88s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.5984402894973755, 'learning_rate': 4.729661016949153e-05, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [19:24<4:28:04,  2.88s/it]  7%|â–‹         | 420/6000 [19:27<4:23:48,  2.84s/it]                                                    {'loss': 2.7813, 'grad_norm': 2.090062379837036, 'learning_rate': 4.728813559322034e-05, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [19:27<4:23:48,  2.84s/it]  7%|â–‹         | 421/6000 [19:30<4:20:32,  2.80s/it]                                                    {'loss': 2.7666, 'grad_norm': 1.7127935886383057, 'learning_rate': 4.727966101694916e-05, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [19:30<4:20:32,  2.80s/it]  7%|â–‹         | 422/6000 [19:32<4:15:48,  2.75s/it]                                                    {'loss': 2.7979, 'grad_norm': 1.6032997369766235, 'learning_rate': 4.727118644067797e-05, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [19:32<4:15:48,  2.75s/it]  7%|â–‹         | 423/6000 [19:35<4:13:07,  2.72s/it]                                                    {'loss': 2.7586, 'grad_norm': 1.7073522806167603, 'learning_rate': 4.7262711864406786e-05, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [19:35<4:13:07,  2.72s/it]  7%|â–‹         | 424/6000 [19:38<4:11:53,  2.71s/it]                                                    {'loss': 2.7556, 'grad_norm': 1.887643575668335, 'learning_rate': 4.72542372881356e-05, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [19:38<4:11:53,  2.71s/it]  7%|â–‹         | 425/6000 [19:40<4:10:34,  2.70s/it]                                                    {'loss': 2.7702, 'grad_norm': 1.9669265747070312, 'learning_rate': 4.724576271186441e-05, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [19:40<4:10:34,  2.70s/it]  7%|â–‹         | 426/6000 [19:43<4:08:34,  2.68s/it]                                                    {'loss': 2.7692, 'grad_norm': 2.2883968353271484, 'learning_rate': 4.723728813559322e-05, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [19:43<4:08:34,  2.68s/it]  7%|â–‹         | 427/6000 [19:45<4:07:31,  2.66s/it]                                                    {'loss': 2.7504, 'grad_norm': 3.242030143737793, 'learning_rate': 4.722881355932204e-05, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [19:45<4:07:31,  2.66s/it]  7%|â–‹         | 428/6000 [19:48<4:09:00,  2.68s/it]                                                    {'loss': 2.8302, 'grad_norm': 3.973980665206909, 'learning_rate': 4.722033898305085e-05, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [19:48<4:09:00,  2.68s/it]  7%|â–‹         | 429/6000 [19:51<4:08:25,  2.68s/it]                                                    {'loss': 2.808, 'grad_norm': 4.198992729187012, 'learning_rate': 4.721186440677967e-05, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [19:51<4:08:25,  2.68s/it]  7%|â–‹         | 430/6000 [19:54<4:07:49,  2.67s/it]                                                    {'loss': 2.8489, 'grad_norm': 3.4473955631256104, 'learning_rate': 4.720338983050848e-05, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [19:54<4:07:49,  2.67s/it]  7%|â–‹         | 431/6000 [19:56<4:06:43,  2.66s/it]                                                    {'loss': 2.792, 'grad_norm': 3.002664566040039, 'learning_rate': 4.719491525423729e-05, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [19:56<4:06:43,  2.66s/it]  7%|â–‹         | 432/6000 [19:59<4:05:53,  2.65s/it]                                                    {'loss': 2.8242, 'grad_norm': 3.0204198360443115, 'learning_rate': 4.71864406779661e-05, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [19:59<4:05:53,  2.65s/it]  7%|â–‹         | 433/6000 [20:01<4:04:11,  2.63s/it]                                                    {'loss': 2.787, 'grad_norm': 2.855801582336426, 'learning_rate': 4.717796610169491e-05, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [20:01<4:04:11,  2.63s/it]  7%|â–‹         | 434/6000 [20:04<4:05:09,  2.64s/it]                                                    {'loss': 2.7872, 'grad_norm': 3.7275609970092773, 'learning_rate': 4.716949152542373e-05, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [20:04<4:05:09,  2.64s/it]  7%|â–‹         | 435/6000 [20:07<4:06:16,  2.66s/it]                                                    {'loss': 2.7555, 'grad_norm': 1.9550859928131104, 'learning_rate': 4.716101694915254e-05, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [20:07<4:06:16,  2.66s/it]  7%|â–‹         | 436/6000 [20:09<4:06:46,  2.66s/it]                                                    {'loss': 2.8049, 'grad_norm': 1.6399693489074707, 'learning_rate': 4.715254237288136e-05, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [20:09<4:06:46,  2.66s/it]  7%|â–‹         | 437/6000 [20:12<4:08:35,  2.68s/it]                                                    {'loss': 2.812, 'grad_norm': 1.7594094276428223, 'learning_rate': 4.714406779661017e-05, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [20:12<4:08:35,  2.68s/it]  7%|â–‹         | 438/6000 [20:15<4:10:27,  2.70s/it]                                                    {'loss': 2.7751, 'grad_norm': 1.3473566770553589, 'learning_rate': 4.713559322033898e-05, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [20:15<4:10:27,  2.70s/it]  7%|â–‹         | 439/6000 [20:18<4:08:38,  2.68s/it]                                                    {'loss': 2.7972, 'grad_norm': 1.030077338218689, 'learning_rate': 4.7127118644067794e-05, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [20:18<4:08:38,  2.68s/it]  7%|â–‹         | 440/6000 [20:20<4:09:47,  2.70s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.4790393114089966, 'learning_rate': 4.711864406779661e-05, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [20:20<4:09:47,  2.70s/it]  7%|â–‹         | 441/6000 [20:23<4:18:01,  2.78s/it]                                                    {'loss': 2.7793, 'grad_norm': 1.5478124618530273, 'learning_rate': 4.7110169491525423e-05, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [20:23<4:18:01,  2.78s/it]  7%|â–‹         | 442/6000 [20:26<4:22:57,  2.84s/it]                                                    {'loss': 2.7769, 'grad_norm': 1.4645355939865112, 'learning_rate': 4.710169491525424e-05, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [20:26<4:22:57,  2.84s/it]  7%|â–‹         | 443/6000 [20:29<4:18:10,  2.79s/it]                                                    {'loss': 2.8273, 'grad_norm': 1.0861330032348633, 'learning_rate': 4.709322033898305e-05, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [20:29<4:18:10,  2.79s/it]  7%|â–‹         | 444/6000 [20:32<4:14:41,  2.75s/it]                                                    {'loss': 2.8336, 'grad_norm': 1.3466300964355469, 'learning_rate': 4.708474576271187e-05, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [20:32<4:14:41,  2.75s/it]  7%|â–‹         | 445/6000 [20:34<4:13:55,  2.74s/it]                                                    {'loss': 2.7812, 'grad_norm': 1.2821667194366455, 'learning_rate': 4.707627118644068e-05, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [20:34<4:13:55,  2.74s/it]  7%|â–‹         | 446/6000 [20:37<4:11:21,  2.72s/it]                                                    {'loss': 2.7755, 'grad_norm': 1.0002304315567017, 'learning_rate': 4.7067796610169493e-05, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [20:37<4:11:21,  2.72s/it]  7%|â–‹         | 447/6000 [20:40<4:08:53,  2.69s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.9540722370147705, 'learning_rate': 4.7059322033898305e-05, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [20:40<4:08:53,  2.69s/it]  7%|â–‹         | 448/6000 [20:42<4:08:40,  2.69s/it]                                                    {'loss': 2.7776, 'grad_norm': 0.94204181432724, 'learning_rate': 4.705084745762712e-05, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [20:42<4:08:40,  2.69s/it]  7%|â–‹         | 449/6000 [20:45<4:07:27,  2.67s/it]                                                    {'loss': 2.7619, 'grad_norm': 1.1794489622116089, 'learning_rate': 4.7042372881355934e-05, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [20:45<4:07:27,  2.67s/it]  8%|â–Š         | 450/6000 [20:48<4:09:06,  2.69s/it]                                                    {'loss': 2.771, 'grad_norm': 1.0528171062469482, 'learning_rate': 4.703389830508475e-05, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [20:48<4:09:06,  2.69s/it][2025-10-21 01:09:42,704] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 451/6000 [20:52<5:01:10,  3.26s/it]                                                    {'loss': 2.7845, 'grad_norm': 1.374833106994629, 'learning_rate': 4.702542372881356e-05, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [20:52<5:01:10,  3.26s/it]  8%|â–Š         | 452/6000 [20:55<4:52:57,  3.17s/it]                                                    {'loss': 2.789, 'grad_norm': 0.8225618600845337, 'learning_rate': 4.7016949152542375e-05, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [20:55<4:52:57,  3.17s/it]  8%|â–Š         | 453/6000 [20:58<4:39:01,  3.02s/it]                                                    {'loss': 2.7699, 'grad_norm': 0.9059927463531494, 'learning_rate': 4.7008474576271186e-05, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [20:58<4:39:01,  3.02s/it]  8%|â–Š         | 454/6000 [21:00<4:28:50,  2.91s/it]                                                    {'loss': 2.8504, 'grad_norm': 0.8921670913696289, 'learning_rate': 4.7e-05, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [21:00<4:28:50,  2.91s/it]  8%|â–Š         | 455/6000 [21:03<4:19:37,  2.81s/it]                                                    {'loss': 2.7652, 'grad_norm': 0.861894428730011, 'learning_rate': 4.6991525423728815e-05, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [21:03<4:19:37,  2.81s/it]  8%|â–Š         | 456/6000 [21:06<4:15:36,  2.77s/it]                                                    {'loss': 2.7797, 'grad_norm': 0.8570438623428345, 'learning_rate': 4.6983050847457627e-05, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [21:06<4:15:36,  2.77s/it]  8%|â–Š         | 457/6000 [21:08<4:11:48,  2.73s/it]                                                    {'loss': 2.8016, 'grad_norm': 0.8406614065170288, 'learning_rate': 4.6974576271186445e-05, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [21:08<4:11:48,  2.73s/it]  8%|â–Š         | 458/6000 [21:11<4:08:46,  2.69s/it]                                                    {'loss': 2.8048, 'grad_norm': 0.8862305283546448, 'learning_rate': 4.6966101694915256e-05, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [21:11<4:08:46,  2.69s/it]  8%|â–Š         | 459/6000 [21:14<4:07:37,  2.68s/it]                                                    {'loss': 2.7751, 'grad_norm': 1.000169038772583, 'learning_rate': 4.6957627118644074e-05, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [21:14<4:07:37,  2.68s/it]  8%|â–Š         | 460/6000 [21:16<4:06:25,  2.67s/it]                                                    {'loss': 2.7851, 'grad_norm': 0.8876963257789612, 'learning_rate': 4.694915254237288e-05, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [21:16<4:06:25,  2.67s/it]  8%|â–Š         | 461/6000 [21:19<4:05:36,  2.66s/it]                                                    {'loss': 2.8001, 'grad_norm': 1.3577948808670044, 'learning_rate': 4.6940677966101697e-05, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [21:19<4:05:36,  2.66s/it]  8%|â–Š         | 462/6000 [21:21<4:03:22,  2.64s/it]                                                    {'loss': 2.7801, 'grad_norm': 1.036658525466919, 'learning_rate': 4.693220338983051e-05, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [21:21<4:03:22,  2.64s/it]  8%|â–Š         | 463/6000 [21:24<4:10:09,  2.71s/it]                                                    {'loss': 2.7773, 'grad_norm': 1.0736900568008423, 'learning_rate': 4.6923728813559326e-05, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [21:24<4:10:09,  2.71s/it]  8%|â–Š         | 464/6000 [21:27<4:07:42,  2.68s/it]                                                    {'loss': 2.7837, 'grad_norm': 1.332423448562622, 'learning_rate': 4.691525423728814e-05, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [21:27<4:07:42,  2.68s/it]  8%|â–Š         | 465/6000 [21:30<4:11:10,  2.72s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.2498384714126587, 'learning_rate': 4.6906779661016955e-05, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [21:30<4:11:10,  2.72s/it]  8%|â–Š         | 466/6000 [21:32<4:09:41,  2.71s/it]                                                    {'loss': 2.7808, 'grad_norm': 1.4377671480178833, 'learning_rate': 4.6898305084745767e-05, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [21:32<4:09:41,  2.71s/it]  8%|â–Š         | 467/6000 [21:35<4:08:16,  2.69s/it]                                                    {'loss': 2.7959, 'grad_norm': 1.618837833404541, 'learning_rate': 4.688983050847458e-05, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [21:35<4:08:16,  2.69s/it]  8%|â–Š         | 468/6000 [21:38<4:08:41,  2.70s/it]                                                    {'loss': 2.7716, 'grad_norm': 1.4377671480178833, 'learning_rate': 4.688135593220339e-05, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [21:38<4:08:41,  2.70s/it]  8%|â–Š         | 469/6000 [21:40<4:06:17,  2.67s/it]                                                    {'loss': 2.7762, 'grad_norm': 1.2557393312454224, 'learning_rate': 4.687288135593221e-05, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [21:40<4:06:17,  2.67s/it]  8%|â–Š         | 470/6000 [21:43<4:05:20,  2.66s/it]                                                    {'loss': 2.7705, 'grad_norm': 1.462759256362915, 'learning_rate': 4.686440677966102e-05, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [21:43<4:05:20,  2.66s/it]  8%|â–Š         | 471/6000 [21:46<4:02:06,  2.63s/it]                                                    {'loss': 2.8546, 'grad_norm': 2.709836721420288, 'learning_rate': 4.6855932203389837e-05, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [21:46<4:02:06,  2.63s/it]  8%|â–Š         | 472/6000 [21:48<4:02:33,  2.63s/it]                                                    {'loss': 2.804, 'grad_norm': 1.0307890176773071, 'learning_rate': 4.684745762711865e-05, 'epoch': 0.08}
  8%|â–Š         | 472/6000 [21:48<4:02:33,  2.63s/it]  8%|â–Š         | 473/6000 [21:51<4:02:26,  2.63s/it]                                                    {'loss': 2.7749, 'grad_norm': 1.1245332956314087, 'learning_rate': 4.6838983050847466e-05, 'epoch': 0.08}
  8%|â–Š         | 473/6000 [21:51<4:02:26,  2.63s/it]  8%|â–Š         | 474/6000 [21:54<4:02:22,  2.63s/it]                                                    {'loss': 2.768, 'grad_norm': 1.1924185752868652, 'learning_rate': 4.683050847457627e-05, 'epoch': 0.08}
  8%|â–Š         | 474/6000 [21:54<4:02:22,  2.63s/it]  8%|â–Š         | 475/6000 [21:56<4:03:27,  2.64s/it]                                                    {'loss': 2.7797, 'grad_norm': 1.208371639251709, 'learning_rate': 4.682203389830508e-05, 'epoch': 0.08}
  8%|â–Š         | 475/6000 [21:56<4:03:27,  2.64s/it]  8%|â–Š         | 476/6000 [21:59<4:02:54,  2.64s/it]                                                    {'loss': 2.7897, 'grad_norm': 1.090126633644104, 'learning_rate': 4.68135593220339e-05, 'epoch': 0.08}
  8%|â–Š         | 476/6000 [21:59<4:02:54,  2.64s/it]  8%|â–Š         | 477/6000 [22:01<4:03:22,  2.64s/it]                                                    {'loss': 2.7679, 'grad_norm': 1.106128215789795, 'learning_rate': 4.680508474576271e-05, 'epoch': 0.08}
  8%|â–Š         | 477/6000 [22:01<4:03:22,  2.64s/it]  8%|â–Š         | 478/6000 [22:04<4:03:21,  2.64s/it]                                                    {'loss': 2.7827, 'grad_norm': 1.3380533456802368, 'learning_rate': 4.679661016949153e-05, 'epoch': 0.08}
  8%|â–Š         | 478/6000 [22:04<4:03:21,  2.64s/it]  8%|â–Š         | 479/6000 [22:07<4:02:32,  2.64s/it]                                                    {'loss': 2.788, 'grad_norm': 1.2883435487747192, 'learning_rate': 4.678813559322034e-05, 'epoch': 0.08}
  8%|â–Š         | 479/6000 [22:07<4:02:32,  2.64s/it]  8%|â–Š         | 480/6000 [22:09<4:01:51,  2.63s/it]                                                    {'loss': 2.773, 'grad_norm': 1.0169932842254639, 'learning_rate': 4.677966101694916e-05, 'epoch': 0.08}
  8%|â–Š         | 480/6000 [22:09<4:01:51,  2.63s/it]  8%|â–Š         | 481/6000 [22:12<4:05:07,  2.66s/it]                                                    {'loss': 2.767, 'grad_norm': 1.0671778917312622, 'learning_rate': 4.677118644067797e-05, 'epoch': 0.08}
  8%|â–Š         | 481/6000 [22:12<4:05:07,  2.66s/it]  8%|â–Š         | 482/6000 [22:15<4:06:47,  2.68s/it]                                                    {'loss': 2.7683, 'grad_norm': 1.3035385608673096, 'learning_rate': 4.676271186440678e-05, 'epoch': 0.08}
  8%|â–Š         | 482/6000 [22:15<4:06:47,  2.68s/it]  8%|â–Š         | 483/6000 [22:18<4:06:55,  2.69s/it]                                                    {'loss': 2.7958, 'grad_norm': 0.9130746126174927, 'learning_rate': 4.675423728813559e-05, 'epoch': 0.08}
  8%|â–Š         | 483/6000 [22:18<4:06:55,  2.69s/it]  8%|â–Š         | 484/6000 [22:20<4:10:18,  2.72s/it]                                                    {'loss': 2.773, 'grad_norm': 1.2121580839157104, 'learning_rate': 4.674576271186441e-05, 'epoch': 0.08}
  8%|â–Š         | 484/6000 [22:20<4:10:18,  2.72s/it]  8%|â–Š         | 485/6000 [22:23<4:22:10,  2.85s/it]                                                    {'loss': 2.7564, 'grad_norm': 2.4139115810394287, 'learning_rate': 4.673728813559322e-05, 'epoch': 0.08}
  8%|â–Š         | 485/6000 [22:23<4:22:10,  2.85s/it]  8%|â–Š         | 486/6000 [22:26<4:17:27,  2.80s/it]                                                    {'loss': 2.789, 'grad_norm': 1.0700621604919434, 'learning_rate': 4.672881355932204e-05, 'epoch': 0.08}
  8%|â–Š         | 486/6000 [22:26<4:17:27,  2.80s/it]  8%|â–Š         | 487/6000 [22:29<4:17:14,  2.80s/it]                                                    {'loss': 2.7853, 'grad_norm': 1.2668734788894653, 'learning_rate': 4.672033898305085e-05, 'epoch': 0.08}
  8%|â–Š         | 487/6000 [22:29<4:17:14,  2.80s/it]  8%|â–Š         | 488/6000 [22:32<4:17:14,  2.80s/it]                                                    {'loss': 2.7743, 'grad_norm': 1.335493803024292, 'learning_rate': 4.671186440677966e-05, 'epoch': 0.08}
  8%|â–Š         | 488/6000 [22:32<4:17:14,  2.80s/it]  8%|â–Š         | 489/6000 [22:35<4:16:27,  2.79s/it]                                                    {'loss': 2.7564, 'grad_norm': 1.7975034713745117, 'learning_rate': 4.6703389830508474e-05, 'epoch': 0.08}
  8%|â–Š         | 489/6000 [22:35<4:16:27,  2.79s/it]  8%|â–Š         | 490/6000 [22:37<4:14:13,  2.77s/it]                                                    {'loss': 2.8199, 'grad_norm': 1.3134825229644775, 'learning_rate': 4.669491525423729e-05, 'epoch': 0.08}
  8%|â–Š         | 490/6000 [22:37<4:14:13,  2.77s/it]  8%|â–Š         | 491/6000 [22:40<4:14:10,  2.77s/it]                                                    {'loss': 2.7683, 'grad_norm': 1.823805570602417, 'learning_rate': 4.66864406779661e-05, 'epoch': 0.08}
  8%|â–Š         | 491/6000 [22:40<4:14:10,  2.77s/it]  8%|â–Š         | 492/6000 [22:43<4:09:55,  2.72s/it]                                                    {'loss': 2.8124, 'grad_norm': 1.7374017238616943, 'learning_rate': 4.667796610169492e-05, 'epoch': 0.08}
  8%|â–Š         | 492/6000 [22:43<4:09:55,  2.72s/it]  8%|â–Š         | 493/6000 [22:45<4:11:42,  2.74s/it]                                                    {'loss': 2.7764, 'grad_norm': 2.0690433979034424, 'learning_rate': 4.666949152542373e-05, 'epoch': 0.08}
  8%|â–Š         | 493/6000 [22:45<4:11:42,  2.74s/it]  8%|â–Š         | 494/6000 [22:48<4:09:20,  2.72s/it]                                                    {'loss': 2.7664, 'grad_norm': 1.7860000133514404, 'learning_rate': 4.666101694915255e-05, 'epoch': 0.08}
  8%|â–Š         | 494/6000 [22:48<4:09:20,  2.72s/it]  8%|â–Š         | 495/6000 [22:51<4:10:09,  2.73s/it]                                                    {'loss': 2.7692, 'grad_norm': 2.142930030822754, 'learning_rate': 4.6652542372881355e-05, 'epoch': 0.08}
  8%|â–Š         | 495/6000 [22:51<4:10:09,  2.73s/it]  8%|â–Š         | 496/6000 [22:53<4:08:02,  2.70s/it]                                                    {'loss': 2.7517, 'grad_norm': 2.148308515548706, 'learning_rate': 4.6644067796610166e-05, 'epoch': 0.08}
  8%|â–Š         | 496/6000 [22:53<4:08:02,  2.70s/it]  8%|â–Š         | 497/6000 [22:56<4:06:10,  2.68s/it]                                                    {'loss': 2.7834, 'grad_norm': 3.312488317489624, 'learning_rate': 4.6635593220338984e-05, 'epoch': 0.08}
  8%|â–Š         | 497/6000 [22:56<4:06:10,  2.68s/it]  8%|â–Š         | 498/6000 [22:59<4:06:26,  2.69s/it]                                                    {'loss': 2.8059, 'grad_norm': 4.19139289855957, 'learning_rate': 4.6627118644067795e-05, 'epoch': 0.08}
  8%|â–Š         | 498/6000 [22:59<4:06:26,  2.69s/it]  8%|â–Š         | 499/6000 [23:01<4:06:24,  2.69s/it]                                                    {'loss': 2.7681, 'grad_norm': 2.9320998191833496, 'learning_rate': 4.6618644067796614e-05, 'epoch': 0.08}
  8%|â–Š         | 499/6000 [23:02<4:06:24,  2.69s/it]  8%|â–Š         | 500/6000 [23:04<4:07:24,  2.70s/it]                                                    {'loss': 2.7715, 'grad_norm': 3.390103578567505, 'learning_rate': 4.6610169491525425e-05, 'epoch': 0.08}
  8%|â–Š         | 500/6000 [23:04<4:07:24,  2.70s/it][2025-10-21 01:11:59,322] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 501/6000 [23:09<4:58:28,  3.26s/it]                                                    {'loss': 2.7896, 'grad_norm': 2.4825899600982666, 'learning_rate': 4.660169491525424e-05, 'epoch': 0.08}
  8%|â–Š         | 501/6000 [23:09<4:58:28,  3.26s/it]  8%|â–Š         | 502/6000 [23:12<4:46:16,  3.12s/it]                                                    {'loss': 2.7654, 'grad_norm': 3.186894655227661, 'learning_rate': 4.6593220338983054e-05, 'epoch': 0.08}
  8%|â–Š         | 502/6000 [23:12<4:46:16,  3.12s/it]  8%|â–Š         | 503/6000 [23:14<4:37:36,  3.03s/it]                                                    {'loss': 2.8443, 'grad_norm': 5.4251861572265625, 'learning_rate': 4.6584745762711865e-05, 'epoch': 0.08}
  8%|â–Š         | 503/6000 [23:14<4:37:36,  3.03s/it]  8%|â–Š         | 504/6000 [23:17<4:26:45,  2.91s/it]                                                    {'loss': 2.79, 'grad_norm': 4.055916786193848, 'learning_rate': 4.657627118644068e-05, 'epoch': 0.08}
  8%|â–Š         | 504/6000 [23:17<4:26:45,  2.91s/it]  8%|â–Š         | 505/6000 [23:20<4:20:43,  2.85s/it]                                                    {'loss': 2.8259, 'grad_norm': 3.9882736206054688, 'learning_rate': 4.6567796610169495e-05, 'epoch': 0.08}
  8%|â–Š         | 505/6000 [23:20<4:20:43,  2.85s/it]  8%|â–Š         | 506/6000 [23:23<4:26:36,  2.91s/it]                                                    {'loss': 2.7516, 'grad_norm': 3.165039539337158, 'learning_rate': 4.6559322033898306e-05, 'epoch': 0.08}
  8%|â–Š         | 506/6000 [23:23<4:26:36,  2.91s/it]  8%|â–Š         | 507/6000 [23:26<4:20:50,  2.85s/it]                                                    {'loss': 2.7896, 'grad_norm': 2.7963058948516846, 'learning_rate': 4.6550847457627124e-05, 'epoch': 0.08}
  8%|â–Š         | 507/6000 [23:26<4:20:50,  2.85s/it]  8%|â–Š         | 508/6000 [23:28<4:14:10,  2.78s/it]                                                    {'loss': 2.7793, 'grad_norm': 2.597128391265869, 'learning_rate': 4.6542372881355935e-05, 'epoch': 0.08}
  8%|â–Š         | 508/6000 [23:28<4:14:10,  2.78s/it]  8%|â–Š         | 509/6000 [23:31<4:09:53,  2.73s/it]                                                    {'loss': 2.8343, 'grad_norm': 2.651639461517334, 'learning_rate': 4.653389830508475e-05, 'epoch': 0.08}
  8%|â–Š         | 509/6000 [23:31<4:09:53,  2.73s/it]  8%|â–Š         | 510/6000 [23:33<4:09:43,  2.73s/it]                                                    {'loss': 2.7524, 'grad_norm': 2.4182465076446533, 'learning_rate': 4.652542372881356e-05, 'epoch': 0.09}
  8%|â–Š         | 510/6000 [23:33<4:09:43,  2.73s/it]  9%|â–Š         | 511/6000 [23:36<4:09:03,  2.72s/it]                                                    {'loss': 2.7324, 'grad_norm': 2.9628376960754395, 'learning_rate': 4.6516949152542376e-05, 'epoch': 0.09}
  9%|â–Š         | 511/6000 [23:36<4:09:03,  2.72s/it]  9%|â–Š         | 512/6000 [23:39<4:07:13,  2.70s/it]                                                    {'loss': 2.7877, 'grad_norm': 2.533979892730713, 'learning_rate': 4.650847457627119e-05, 'epoch': 0.09}
  9%|â–Š         | 512/6000 [23:39<4:07:13,  2.70s/it]  9%|â–Š         | 513/6000 [23:42<4:11:55,  2.75s/it]                                                    {'loss': 2.7422, 'grad_norm': 2.59287428855896, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.09}
  9%|â–Š         | 513/6000 [23:42<4:11:55,  2.75s/it]  9%|â–Š         | 514/6000 [23:44<4:11:20,  2.75s/it]                                                    {'loss': 2.8036, 'grad_norm': 3.8955960273742676, 'learning_rate': 4.649152542372882e-05, 'epoch': 0.09}
  9%|â–Š         | 514/6000 [23:44<4:11:20,  2.75s/it]  9%|â–Š         | 515/6000 [23:47<4:08:31,  2.72s/it]                                                    {'loss': 2.7565, 'grad_norm': 2.569249153137207, 'learning_rate': 4.6483050847457635e-05, 'epoch': 0.09}
  9%|â–Š         | 515/6000 [23:47<4:08:31,  2.72s/it]  9%|â–Š         | 516/6000 [23:50<4:13:52,  2.78s/it]                                                    {'loss': 2.7696, 'grad_norm': 2.589543342590332, 'learning_rate': 4.6474576271186446e-05, 'epoch': 0.09}
  9%|â–Š         | 516/6000 [23:50<4:13:52,  2.78s/it]  9%|â–Š         | 517/6000 [23:53<4:12:02,  2.76s/it]                                                    {'loss': 2.7698, 'grad_norm': 2.7241172790527344, 'learning_rate': 4.646610169491525e-05, 'epoch': 0.09}
  9%|â–Š         | 517/6000 [23:53<4:12:02,  2.76s/it]  9%|â–Š         | 518/6000 [23:55<4:08:11,  2.72s/it]                                                    {'loss': 2.7938, 'grad_norm': 3.160048246383667, 'learning_rate': 4.645762711864407e-05, 'epoch': 0.09}
  9%|â–Š         | 518/6000 [23:55<4:08:11,  2.72s/it]  9%|â–Š         | 519/6000 [23:58<4:06:27,  2.70s/it]                                                    {'loss': 2.766, 'grad_norm': 2.3817427158355713, 'learning_rate': 4.644915254237288e-05, 'epoch': 0.09}
  9%|â–Š         | 519/6000 [23:58<4:06:27,  2.70s/it]  9%|â–Š         | 520/6000 [24:01<4:10:57,  2.75s/it]                                                    {'loss': 2.7891, 'grad_norm': 2.742525577545166, 'learning_rate': 4.64406779661017e-05, 'epoch': 0.09}
  9%|â–Š         | 520/6000 [24:01<4:10:57,  2.75s/it]  9%|â–Š         | 521/6000 [24:03<4:06:48,  2.70s/it]                                                    {'loss': 2.7706, 'grad_norm': 2.0389037132263184, 'learning_rate': 4.643220338983051e-05, 'epoch': 0.09}
  9%|â–Š         | 521/6000 [24:03<4:06:48,  2.70s/it]  9%|â–Š         | 522/6000 [24:06<4:06:04,  2.70s/it]                                                    {'loss': 2.7727, 'grad_norm': 2.053647518157959, 'learning_rate': 4.642372881355933e-05, 'epoch': 0.09}
  9%|â–Š         | 522/6000 [24:06<4:06:04,  2.70s/it]  9%|â–Š         | 523/6000 [24:09<4:07:35,  2.71s/it]                                                    {'loss': 2.795, 'grad_norm': 2.395219087600708, 'learning_rate': 4.641525423728814e-05, 'epoch': 0.09}
  9%|â–Š         | 523/6000 [24:09<4:07:35,  2.71s/it]  9%|â–Š         | 524/6000 [24:12<4:05:34,  2.69s/it]                                                    {'loss': 2.8458, 'grad_norm': 1.4429999589920044, 'learning_rate': 4.640677966101695e-05, 'epoch': 0.09}
  9%|â–Š         | 524/6000 [24:12<4:05:34,  2.69s/it]  9%|â–‰         | 525/6000 [24:14<4:04:07,  2.68s/it]                                                    {'loss': 2.7766, 'grad_norm': 1.8038409948349, 'learning_rate': 4.639830508474576e-05, 'epoch': 0.09}
  9%|â–‰         | 525/6000 [24:14<4:04:07,  2.68s/it]  9%|â–‰         | 526/6000 [24:17<4:04:23,  2.68s/it]                                                    {'loss': 2.8329, 'grad_norm': 1.9776310920715332, 'learning_rate': 4.638983050847458e-05, 'epoch': 0.09}
  9%|â–‰         | 526/6000 [24:17<4:04:23,  2.68s/it]  9%|â–‰         | 527/6000 [24:20<4:05:07,  2.69s/it]                                                    {'loss': 2.7733, 'grad_norm': 1.5400135517120361, 'learning_rate': 4.638135593220339e-05, 'epoch': 0.09}
  9%|â–‰         | 527/6000 [24:20<4:05:07,  2.69s/it]  9%|â–‰         | 528/6000 [24:22<4:07:19,  2.71s/it]                                                    {'loss': 2.7712, 'grad_norm': 1.6028904914855957, 'learning_rate': 4.637288135593221e-05, 'epoch': 0.09}
  9%|â–‰         | 528/6000 [24:22<4:07:19,  2.71s/it]  9%|â–‰         | 529/6000 [24:25<4:04:59,  2.69s/it]                                                    {'loss': 2.7769, 'grad_norm': 1.490794062614441, 'learning_rate': 4.636440677966102e-05, 'epoch': 0.09}
  9%|â–‰         | 529/6000 [24:25<4:04:59,  2.69s/it]  9%|â–‰         | 530/6000 [24:28<4:05:46,  2.70s/it]                                                    {'loss': 2.7834, 'grad_norm': 1.3131667375564575, 'learning_rate': 4.635593220338984e-05, 'epoch': 0.09}
  9%|â–‰         | 530/6000 [24:28<4:05:46,  2.70s/it]  9%|â–‰         | 531/6000 [24:30<4:04:31,  2.68s/it]                                                    {'loss': 2.7573, 'grad_norm': 1.5792932510375977, 'learning_rate': 4.634745762711864e-05, 'epoch': 0.09}
  9%|â–‰         | 531/6000 [24:30<4:04:31,  2.68s/it]  9%|â–‰         | 532/6000 [24:33<4:04:29,  2.68s/it]                                                    {'loss': 2.7668, 'grad_norm': 1.6754603385925293, 'learning_rate': 4.633898305084746e-05, 'epoch': 0.09}
  9%|â–‰         | 532/6000 [24:33<4:04:29,  2.68s/it]  9%|â–‰         | 533/6000 [24:36<4:01:27,  2.65s/it]                                                    {'loss': 2.7864, 'grad_norm': 1.4764971733093262, 'learning_rate': 4.633050847457627e-05, 'epoch': 0.09}
  9%|â–‰         | 533/6000 [24:36<4:01:27,  2.65s/it]  9%|â–‰         | 534/6000 [24:38<4:02:09,  2.66s/it]                                                    {'loss': 2.7919, 'grad_norm': 1.7811628580093384, 'learning_rate': 4.632203389830509e-05, 'epoch': 0.09}
  9%|â–‰         | 534/6000 [24:38<4:02:09,  2.66s/it]  9%|â–‰         | 535/6000 [24:41<4:03:16,  2.67s/it]                                                    {'loss': 2.7979, 'grad_norm': 1.7827985286712646, 'learning_rate': 4.63135593220339e-05, 'epoch': 0.09}
  9%|â–‰         | 535/6000 [24:41<4:03:16,  2.67s/it]  9%|â–‰         | 536/6000 [24:44<4:03:48,  2.68s/it]                                                    {'loss': 2.7528, 'grad_norm': 1.5327792167663574, 'learning_rate': 4.630508474576272e-05, 'epoch': 0.09}
  9%|â–‰         | 536/6000 [24:44<4:03:48,  2.68s/it]  9%|â–‰         | 537/6000 [24:46<4:02:00,  2.66s/it]                                                    {'loss': 2.7757, 'grad_norm': 1.2024074792861938, 'learning_rate': 4.629661016949153e-05, 'epoch': 0.09}
  9%|â–‰         | 537/6000 [24:46<4:02:00,  2.66s/it]  9%|â–‰         | 538/6000 [24:49<4:02:03,  2.66s/it]                                                    {'loss': 2.778, 'grad_norm': 1.530151605606079, 'learning_rate': 4.628813559322034e-05, 'epoch': 0.09}
  9%|â–‰         | 538/6000 [24:49<4:02:03,  2.66s/it]  9%|â–‰         | 539/6000 [24:52<4:01:39,  2.66s/it]                                                    {'loss': 2.779, 'grad_norm': 1.6621301174163818, 'learning_rate': 4.627966101694915e-05, 'epoch': 0.09}
  9%|â–‰         | 539/6000 [24:52<4:01:39,  2.66s/it]  9%|â–‰         | 540/6000 [24:54<4:01:37,  2.66s/it]                                                    {'loss': 2.7944, 'grad_norm': 2.4806711673736572, 'learning_rate': 4.6271186440677964e-05, 'epoch': 0.09}
  9%|â–‰         | 540/6000 [24:54<4:01:37,  2.66s/it]  9%|â–‰         | 541/6000 [24:57<4:04:14,  2.68s/it]                                                    {'loss': 2.7882, 'grad_norm': 2.897019624710083, 'learning_rate': 4.626271186440678e-05, 'epoch': 0.09}
  9%|â–‰         | 541/6000 [24:57<4:04:14,  2.68s/it]  9%|â–‰         | 542/6000 [25:00<4:03:17,  2.67s/it]                                                    {'loss': 2.7268, 'grad_norm': 2.7604095935821533, 'learning_rate': 4.6254237288135594e-05, 'epoch': 0.09}
  9%|â–‰         | 542/6000 [25:00<4:03:17,  2.67s/it]  9%|â–‰         | 543/6000 [25:02<4:07:12,  2.72s/it]                                                    {'loss': 2.7629, 'grad_norm': 2.4227941036224365, 'learning_rate': 4.624576271186441e-05, 'epoch': 0.09}
  9%|â–‰         | 543/6000 [25:02<4:07:12,  2.72s/it]  9%|â–‰         | 544/6000 [25:05<4:06:03,  2.71s/it]                                                    {'loss': 2.7837, 'grad_norm': 1.918567180633545, 'learning_rate': 4.623728813559322e-05, 'epoch': 0.09}
  9%|â–‰         | 544/6000 [25:05<4:06:03,  2.71s/it]  9%|â–‰         | 545/6000 [25:08<4:06:33,  2.71s/it]                                                    {'loss': 2.7646, 'grad_norm': 2.285240888595581, 'learning_rate': 4.6228813559322034e-05, 'epoch': 0.09}
  9%|â–‰         | 545/6000 [25:08<4:06:33,  2.71s/it]  9%|â–‰         | 546/6000 [25:11<4:05:55,  2.71s/it]                                                    {'loss': 2.786, 'grad_norm': 3.3710715770721436, 'learning_rate': 4.6220338983050846e-05, 'epoch': 0.09}
  9%|â–‰         | 546/6000 [25:11<4:05:55,  2.71s/it]  9%|â–‰         | 547/6000 [25:13<4:08:33,  2.73s/it]                                                    {'loss': 2.7932, 'grad_norm': 2.224146604537964, 'learning_rate': 4.6211864406779664e-05, 'epoch': 0.09}
  9%|â–‰         | 547/6000 [25:13<4:08:33,  2.73s/it]  9%|â–‰         | 548/6000 [25:16<4:07:28,  2.72s/it]                                                    {'loss': 2.7589, 'grad_norm': 2.7291603088378906, 'learning_rate': 4.6203389830508475e-05, 'epoch': 0.09}
  9%|â–‰         | 548/6000 [25:16<4:07:28,  2.72s/it]  9%|â–‰         | 549/6000 [25:19<4:06:08,  2.71s/it]                                                    {'loss': 2.7831, 'grad_norm': 2.7484092712402344, 'learning_rate': 4.619491525423729e-05, 'epoch': 0.09}
  9%|â–‰         | 549/6000 [25:19<4:06:08,  2.71s/it]  9%|â–‰         | 550/6000 [25:21<4:05:12,  2.70s/it]                                                    {'loss': 2.7694, 'grad_norm': 2.460935592651367, 'learning_rate': 4.6186440677966104e-05, 'epoch': 0.09}
  9%|â–‰         | 550/6000 [25:21<4:05:12,  2.70s/it][2025-10-21 01:14:16,490] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 551/6000 [25:27<5:21:25,  3.54s/it]                                                    {'loss': 2.771, 'grad_norm': 3.0392792224884033, 'learning_rate': 4.617796610169492e-05, 'epoch': 0.09}
  9%|â–‰         | 551/6000 [25:27<5:21:25,  3.54s/it]  9%|â–‰         | 552/6000 [25:30<4:58:12,  3.28s/it]                                                    {'loss': 2.84, 'grad_norm': 6.089651584625244, 'learning_rate': 4.6169491525423734e-05, 'epoch': 0.09}
  9%|â–‰         | 552/6000 [25:30<4:58:12,  3.28s/it]  9%|â–‰         | 553/6000 [25:32<4:43:41,  3.12s/it]                                                    {'loss': 2.7707, 'grad_norm': 2.941664218902588, 'learning_rate': 4.6161016949152545e-05, 'epoch': 0.09}
  9%|â–‰         | 553/6000 [25:32<4:43:41,  3.12s/it]  9%|â–‰         | 554/6000 [25:35<4:30:19,  2.98s/it]                                                    {'loss': 2.7711, 'grad_norm': 2.2815089225769043, 'learning_rate': 4.6152542372881356e-05, 'epoch': 0.09}
  9%|â–‰         | 554/6000 [25:35<4:30:19,  2.98s/it]  9%|â–‰         | 555/6000 [25:38<4:20:29,  2.87s/it]                                                    {'loss': 2.7565, 'grad_norm': 2.6968934535980225, 'learning_rate': 4.6144067796610174e-05, 'epoch': 0.09}
  9%|â–‰         | 555/6000 [25:38<4:20:29,  2.87s/it]  9%|â–‰         | 556/6000 [25:40<4:13:44,  2.80s/it]                                                    {'loss': 2.7723, 'grad_norm': 2.7699573040008545, 'learning_rate': 4.6135593220338986e-05, 'epoch': 0.09}
  9%|â–‰         | 556/6000 [25:40<4:13:44,  2.80s/it]  9%|â–‰         | 557/6000 [25:43<4:10:50,  2.77s/it]                                                    {'loss': 2.7711, 'grad_norm': 3.655733585357666, 'learning_rate': 4.6127118644067804e-05, 'epoch': 0.09}
  9%|â–‰         | 557/6000 [25:43<4:10:50,  2.77s/it]  9%|â–‰         | 558/6000 [25:46<4:11:46,  2.78s/it]                                                    {'loss': 2.7668, 'grad_norm': 2.532015562057495, 'learning_rate': 4.6118644067796615e-05, 'epoch': 0.09}
  9%|â–‰         | 558/6000 [25:46<4:11:46,  2.78s/it]  9%|â–‰         | 559/6000 [25:49<4:12:22,  2.78s/it]                                                    {'loss': 2.741, 'grad_norm': 2.7918171882629395, 'learning_rate': 4.6110169491525426e-05, 'epoch': 0.09}
  9%|â–‰         | 559/6000 [25:49<4:12:22,  2.78s/it]  9%|â–‰         | 560/6000 [25:51<4:07:48,  2.73s/it]                                                    {'loss': 2.76, 'grad_norm': 2.2944140434265137, 'learning_rate': 4.610169491525424e-05, 'epoch': 0.09}
  9%|â–‰         | 560/6000 [25:51<4:07:48,  2.73s/it]  9%|â–‰         | 561/6000 [25:54<4:17:56,  2.85s/it]                                                    {'loss': 2.6925, 'grad_norm': 6.553783893585205, 'learning_rate': 4.609322033898305e-05, 'epoch': 0.09}
  9%|â–‰         | 561/6000 [25:54<4:17:56,  2.85s/it]  9%|â–‰         | 562/6000 [25:57<4:10:31,  2.76s/it]                                                    {'loss': 2.8387, 'grad_norm': 3.177438497543335, 'learning_rate': 4.608474576271187e-05, 'epoch': 0.09}
  9%|â–‰         | 562/6000 [25:57<4:10:31,  2.76s/it]  9%|â–‰         | 563/6000 [25:59<4:08:11,  2.74s/it]                                                    {'loss': 2.7602, 'grad_norm': 4.019383907318115, 'learning_rate': 4.607627118644068e-05, 'epoch': 0.09}
  9%|â–‰         | 563/6000 [25:59<4:08:11,  2.74s/it]  9%|â–‰         | 564/6000 [26:02<4:06:53,  2.73s/it]                                                    {'loss': 2.7687, 'grad_norm': 14.013716697692871, 'learning_rate': 4.6067796610169496e-05, 'epoch': 0.09}
  9%|â–‰         | 564/6000 [26:02<4:06:53,  2.73s/it]  9%|â–‰         | 565/6000 [26:05<4:11:30,  2.78s/it]                                                    {'loss': 2.7842, 'grad_norm': 13.636056900024414, 'learning_rate': 4.605932203389831e-05, 'epoch': 0.09}
  9%|â–‰         | 565/6000 [26:05<4:11:30,  2.78s/it]  9%|â–‰         | 566/6000 [26:08<4:07:22,  2.73s/it]                                                    {'loss': 2.7546, 'grad_norm': 3.7754013538360596, 'learning_rate': 4.605084745762712e-05, 'epoch': 0.09}
  9%|â–‰         | 566/6000 [26:08<4:07:22,  2.73s/it]  9%|â–‰         | 567/6000 [26:11<4:14:53,  2.81s/it]                                                    {'loss': 2.7628, 'grad_norm': 4.877734661102295, 'learning_rate': 4.604237288135593e-05, 'epoch': 0.09}
  9%|â–‰         | 567/6000 [26:11<4:14:53,  2.81s/it]  9%|â–‰         | 568/6000 [26:13<4:10:45,  2.77s/it]                                                    {'loss': 2.851, 'grad_norm': 22.860240936279297, 'learning_rate': 4.603389830508475e-05, 'epoch': 0.09}
  9%|â–‰         | 568/6000 [26:13<4:10:45,  2.77s/it]  9%|â–‰         | 569/6000 [26:16<4:09:11,  2.75s/it]                                                    {'loss': 2.7476, 'grad_norm': 16.94725227355957, 'learning_rate': 4.602542372881356e-05, 'epoch': 0.09}
  9%|â–‰         | 569/6000 [26:16<4:09:11,  2.75s/it] 10%|â–‰         | 570/6000 [26:19<4:07:32,  2.74s/it]                                                    {'loss': 2.8055, 'grad_norm': 6.287203311920166, 'learning_rate': 4.601694915254238e-05, 'epoch': 0.1}
 10%|â–‰         | 570/6000 [26:19<4:07:32,  2.74s/it] 10%|â–‰         | 571/6000 [26:23<4:39:42,  3.09s/it]                                                    {'loss': 2.7365, 'grad_norm': 9.78100299835205, 'learning_rate': 4.600847457627119e-05, 'epoch': 0.1}
 10%|â–‰         | 571/6000 [26:23<4:39:42,  3.09s/it] 10%|â–‰         | 572/6000 [26:25<4:28:28,  2.97s/it]                                                    {'loss': 2.8097, 'grad_norm': 3.581993818283081, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.1}
 10%|â–‰         | 572/6000 [26:25<4:28:28,  2.97s/it] 10%|â–‰         | 573/6000 [26:28<4:20:49,  2.88s/it]                                                    {'loss': 2.8236, 'grad_norm': 5.670806407928467, 'learning_rate': 4.599152542372882e-05, 'epoch': 0.1}
 10%|â–‰         | 573/6000 [26:28<4:20:49,  2.88s/it] 10%|â–‰         | 574/6000 [26:31<4:16:04,  2.83s/it]                                                    {'loss': 2.7683, 'grad_norm': 3.704538345336914, 'learning_rate': 4.598305084745763e-05, 'epoch': 0.1}
 10%|â–‰         | 574/6000 [26:31<4:16:04,  2.83s/it] 10%|â–‰         | 575/6000 [26:33<4:12:28,  2.79s/it]                                                    {'loss': 2.7664, 'grad_norm': 2.678154706954956, 'learning_rate': 4.597457627118644e-05, 'epoch': 0.1}
 10%|â–‰         | 575/6000 [26:33<4:12:28,  2.79s/it] 10%|â–‰         | 576/6000 [26:36<4:13:35,  2.81s/it]                                                    {'loss': 2.7891, 'grad_norm': 3.9688222408294678, 'learning_rate': 4.596610169491526e-05, 'epoch': 0.1}
 10%|â–‰         | 576/6000 [26:36<4:13:35,  2.81s/it] 10%|â–‰         | 577/6000 [26:39<4:16:57,  2.84s/it]                                                    {'loss': 2.7647, 'grad_norm': 3.255295515060425, 'learning_rate': 4.595762711864407e-05, 'epoch': 0.1}
 10%|â–‰         | 577/6000 [26:39<4:16:57,  2.84s/it] 10%|â–‰         | 578/6000 [26:43<4:28:41,  2.97s/it]                                                    {'loss': 2.7738, 'grad_norm': 2.860145330429077, 'learning_rate': 4.594915254237288e-05, 'epoch': 0.1}
 10%|â–‰         | 578/6000 [26:43<4:28:41,  2.97s/it] 10%|â–‰         | 579/6000 [26:46<4:33:28,  3.03s/it]                                                    {'loss': 2.7585, 'grad_norm': 2.664731979370117, 'learning_rate': 4.59406779661017e-05, 'epoch': 0.1}
 10%|â–‰         | 579/6000 [26:46<4:33:28,  3.03s/it] 10%|â–‰         | 580/6000 [26:48<4:25:07,  2.94s/it]                                                    {'loss': 2.7714, 'grad_norm': 2.2508645057678223, 'learning_rate': 4.593220338983051e-05, 'epoch': 0.1}
 10%|â–‰         | 580/6000 [26:48<4:25:07,  2.94s/it] 10%|â–‰         | 581/6000 [26:52<4:39:01,  3.09s/it]                                                    {'loss': 2.8187, 'grad_norm': 4.092830657958984, 'learning_rate': 4.592372881355932e-05, 'epoch': 0.1}
 10%|â–‰         | 581/6000 [26:52<4:39:01,  3.09s/it] 10%|â–‰         | 582/6000 [26:55<4:51:41,  3.23s/it]                                                    {'loss': 2.7621, 'grad_norm': 2.53243088722229, 'learning_rate': 4.591525423728813e-05, 'epoch': 0.1}
 10%|â–‰         | 582/6000 [26:55<4:51:41,  3.23s/it] 10%|â–‰         | 583/6000 [26:58<4:38:32,  3.09s/it]                                                    {'loss': 2.7741, 'grad_norm': 1.6866523027420044, 'learning_rate': 4.590677966101695e-05, 'epoch': 0.1}
 10%|â–‰         | 583/6000 [26:58<4:38:32,  3.09s/it] 10%|â–‰         | 584/6000 [27:01<4:27:35,  2.96s/it]                                                    {'loss': 2.8283, 'grad_norm': 1.4570403099060059, 'learning_rate': 4.589830508474576e-05, 'epoch': 0.1}
 10%|â–‰         | 584/6000 [27:01<4:27:35,  2.96s/it] 10%|â–‰         | 585/6000 [27:04<4:32:13,  3.02s/it]                                                    {'loss': 2.7897, 'grad_norm': 2.739356756210327, 'learning_rate': 4.588983050847458e-05, 'epoch': 0.1}
 10%|â–‰         | 585/6000 [27:04<4:32:13,  3.02s/it] 10%|â–‰         | 586/6000 [27:07<4:21:49,  2.90s/it]                                                    {'loss': 2.7804, 'grad_norm': 1.4992138147354126, 'learning_rate': 4.588135593220339e-05, 'epoch': 0.1}
 10%|â–‰         | 586/6000 [27:07<4:21:49,  2.90s/it] 10%|â–‰         | 587/6000 [27:09<4:13:15,  2.81s/it]                                                    {'loss': 2.8079, 'grad_norm': 1.2306135892868042, 'learning_rate': 4.587288135593221e-05, 'epoch': 0.1}
 10%|â–‰         | 587/6000 [27:09<4:13:15,  2.81s/it] 10%|â–‰         | 588/6000 [27:12<4:14:36,  2.82s/it]                                                    {'loss': 2.7841, 'grad_norm': 1.3314049243927002, 'learning_rate': 4.5864406779661014e-05, 'epoch': 0.1}
 10%|â–‰         | 588/6000 [27:12<4:14:36,  2.82s/it] 10%|â–‰         | 589/6000 [27:15<4:11:29,  2.79s/it]                                                    {'loss': 2.7843, 'grad_norm': 1.109541416168213, 'learning_rate': 4.585593220338983e-05, 'epoch': 0.1}
 10%|â–‰         | 589/6000 [27:15<4:11:29,  2.79s/it] 10%|â–‰         | 590/6000 [27:18<4:10:58,  2.78s/it]                                                    {'loss': 2.7783, 'grad_norm': 1.178414225578308, 'learning_rate': 4.5847457627118644e-05, 'epoch': 0.1}
 10%|â–‰         | 590/6000 [27:18<4:10:58,  2.78s/it] 10%|â–‰         | 591/6000 [27:20<4:06:46,  2.74s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.8833004236221313, 'learning_rate': 4.583898305084746e-05, 'epoch': 0.1}
 10%|â–‰         | 591/6000 [27:20<4:06:46,  2.74s/it] 10%|â–‰         | 592/6000 [27:23<4:04:28,  2.71s/it]                                                    {'loss': 2.7804, 'grad_norm': 0.7743247747421265, 'learning_rate': 4.583050847457627e-05, 'epoch': 0.1}
 10%|â–‰         | 592/6000 [27:23<4:04:28,  2.71s/it] 10%|â–‰         | 593/6000 [27:25<4:03:18,  2.70s/it]                                                    {'loss': 2.7777, 'grad_norm': 1.2722631692886353, 'learning_rate': 4.582203389830509e-05, 'epoch': 0.1}
 10%|â–‰         | 593/6000 [27:25<4:03:18,  2.70s/it] 10%|â–‰         | 594/6000 [27:28<4:05:04,  2.72s/it]                                                    {'loss': 2.7839, 'grad_norm': 1.2558008432388306, 'learning_rate': 4.58135593220339e-05, 'epoch': 0.1}
 10%|â–‰         | 594/6000 [27:28<4:05:04,  2.72s/it] 10%|â–‰         | 595/6000 [27:31<4:12:57,  2.81s/it]                                                    {'loss': 2.7789, 'grad_norm': 1.1320912837982178, 'learning_rate': 4.5805084745762714e-05, 'epoch': 0.1}
 10%|â–‰         | 595/6000 [27:31<4:12:57,  2.81s/it] 10%|â–‰         | 596/6000 [27:35<4:34:05,  3.04s/it]                                                    {'loss': 2.7461, 'grad_norm': 1.5322974920272827, 'learning_rate': 4.5796610169491525e-05, 'epoch': 0.1}
 10%|â–‰         | 596/6000 [27:35<4:34:05,  3.04s/it] 10%|â–‰         | 597/6000 [27:37<4:21:11,  2.90s/it]                                                    {'loss': 2.7877, 'grad_norm': 0.9875246286392212, 'learning_rate': 4.578813559322034e-05, 'epoch': 0.1}
 10%|â–‰         | 597/6000 [27:37<4:21:11,  2.90s/it] 10%|â–‰         | 598/6000 [27:40<4:16:03,  2.84s/it]                                                    {'loss': 2.7853, 'grad_norm': 1.3073527812957764, 'learning_rate': 4.5779661016949154e-05, 'epoch': 0.1}
 10%|â–‰         | 598/6000 [27:40<4:16:03,  2.84s/it] 10%|â–‰         | 599/6000 [27:43<4:09:26,  2.77s/it]                                                    {'loss': 2.7917, 'grad_norm': 0.9275081753730774, 'learning_rate': 4.5771186440677966e-05, 'epoch': 0.1}
 10%|â–‰         | 599/6000 [27:43<4:09:26,  2.77s/it] 10%|â–ˆ         | 600/6000 [27:45<4:06:44,  2.74s/it]                                                    {'loss': 2.8083, 'grad_norm': 1.18867826461792, 'learning_rate': 4.5762711864406784e-05, 'epoch': 0.1}
 10%|â–ˆ         | 600/6000 [27:45<4:06:44,  2.74s/it][2025-10-21 01:16:40,517] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 601/6000 [27:50<4:58:46,  3.32s/it]                                                    {'loss': 2.7757, 'grad_norm': 1.4556429386138916, 'learning_rate': 4.5754237288135595e-05, 'epoch': 0.1}
 10%|â–ˆ         | 601/6000 [27:50<4:58:46,  3.32s/it] 10%|â–ˆ         | 602/6000 [27:53<4:41:18,  3.13s/it]                                                    {'loss': 2.7779, 'grad_norm': 1.5677530765533447, 'learning_rate': 4.5745762711864406e-05, 'epoch': 0.1}
 10%|â–ˆ         | 602/6000 [27:53<4:41:18,  3.13s/it] 10%|â–ˆ         | 603/6000 [27:55<4:28:14,  2.98s/it]                                                    {'loss': 2.8478, 'grad_norm': 2.5232982635498047, 'learning_rate': 4.573728813559322e-05, 'epoch': 0.1}
 10%|â–ˆ         | 603/6000 [27:55<4:28:14,  2.98s/it] 10%|â–ˆ         | 604/6000 [27:59<4:35:57,  3.07s/it]                                                    {'loss': 2.7782, 'grad_norm': 1.299893856048584, 'learning_rate': 4.5728813559322036e-05, 'epoch': 0.1}
 10%|â–ˆ         | 604/6000 [27:59<4:35:57,  3.07s/it] 10%|â–ˆ         | 605/6000 [28:01<4:23:35,  2.93s/it]                                                    {'loss': 2.7884, 'grad_norm': 1.5374014377593994, 'learning_rate': 4.572033898305085e-05, 'epoch': 0.1}
 10%|â–ˆ         | 605/6000 [28:01<4:23:35,  2.93s/it] 10%|â–ˆ         | 606/6000 [28:04<4:21:01,  2.90s/it]                                                    {'loss': 2.7677, 'grad_norm': 1.9505479335784912, 'learning_rate': 4.5711864406779665e-05, 'epoch': 0.1}
 10%|â–ˆ         | 606/6000 [28:04<4:21:01,  2.90s/it] 10%|â–ˆ         | 607/6000 [28:07<4:14:48,  2.83s/it]                                                    {'loss': 2.8067, 'grad_norm': 2.1019389629364014, 'learning_rate': 4.5703389830508476e-05, 'epoch': 0.1}
 10%|â–ˆ         | 607/6000 [28:07<4:14:48,  2.83s/it] 10%|â–ˆ         | 608/6000 [28:10<4:14:07,  2.83s/it]                                                    {'loss': 2.7577, 'grad_norm': 1.7852147817611694, 'learning_rate': 4.5694915254237294e-05, 'epoch': 0.1}
 10%|â–ˆ         | 608/6000 [28:10<4:14:07,  2.83s/it] 10%|â–ˆ         | 609/6000 [28:12<4:09:32,  2.78s/it]                                                    {'loss': 2.7992, 'grad_norm': 2.078037977218628, 'learning_rate': 4.5686440677966106e-05, 'epoch': 0.1}
 10%|â–ˆ         | 609/6000 [28:12<4:09:32,  2.78s/it] 10%|â–ˆ         | 610/6000 [28:15<4:05:48,  2.74s/it]                                                    {'loss': 2.7586, 'grad_norm': 1.7977553606033325, 'learning_rate': 4.567796610169492e-05, 'epoch': 0.1}
 10%|â–ˆ         | 610/6000 [28:15<4:05:48,  2.74s/it] 10%|â–ˆ         | 611/6000 [28:18<4:03:21,  2.71s/it]                                                    {'loss': 2.8262, 'grad_norm': 1.7465614080429077, 'learning_rate': 4.566949152542373e-05, 'epoch': 0.1}
 10%|â–ˆ         | 611/6000 [28:18<4:03:21,  2.71s/it] 10%|â–ˆ         | 612/6000 [28:20<4:05:06,  2.73s/it]                                                    {'loss': 2.7715, 'grad_norm': 2.065546751022339, 'learning_rate': 4.5661016949152546e-05, 'epoch': 0.1}
 10%|â–ˆ         | 612/6000 [28:20<4:05:06,  2.73s/it] 10%|â–ˆ         | 613/6000 [28:23<4:04:52,  2.73s/it]                                                    {'loss': 2.8317, 'grad_norm': 2.601452589035034, 'learning_rate': 4.565254237288136e-05, 'epoch': 0.1}
 10%|â–ˆ         | 613/6000 [28:23<4:04:52,  2.73s/it] 10%|â–ˆ         | 614/6000 [28:26<4:01:31,  2.69s/it]                                                    {'loss': 2.7574, 'grad_norm': 1.6121816635131836, 'learning_rate': 4.5644067796610176e-05, 'epoch': 0.1}
 10%|â–ˆ         | 614/6000 [28:26<4:01:31,  2.69s/it] 10%|â–ˆ         | 615/6000 [28:28<4:01:33,  2.69s/it]                                                    {'loss': 2.7846, 'grad_norm': 2.423196792602539, 'learning_rate': 4.563559322033899e-05, 'epoch': 0.1}
 10%|â–ˆ         | 615/6000 [28:28<4:01:33,  2.69s/it] 10%|â–ˆ         | 616/6000 [28:31<4:00:24,  2.68s/it]                                                    {'loss': 2.7581, 'grad_norm': 1.7545254230499268, 'learning_rate': 4.56271186440678e-05, 'epoch': 0.1}
 10%|â–ˆ         | 616/6000 [28:31<4:00:24,  2.68s/it] 10%|â–ˆ         | 617/6000 [28:34<4:01:31,  2.69s/it]                                                    {'loss': 2.7772, 'grad_norm': 2.4291679859161377, 'learning_rate': 4.561864406779661e-05, 'epoch': 0.1}
 10%|â–ˆ         | 617/6000 [28:34<4:01:31,  2.69s/it] 10%|â–ˆ         | 618/6000 [28:36<4:00:39,  2.68s/it]                                                    {'loss': 2.8014, 'grad_norm': 2.627261161804199, 'learning_rate': 4.561016949152543e-05, 'epoch': 0.1}
 10%|â–ˆ         | 618/6000 [28:36<4:00:39,  2.68s/it] 10%|â–ˆ         | 619/6000 [28:39<3:58:57,  2.66s/it]                                                    {'loss': 2.7843, 'grad_norm': 1.5661309957504272, 'learning_rate': 4.560169491525424e-05, 'epoch': 0.1}
 10%|â–ˆ         | 619/6000 [28:39<3:58:57,  2.66s/it] 10%|â–ˆ         | 620/6000 [28:42<3:58:09,  2.66s/it]                                                    {'loss': 2.7856, 'grad_norm': 1.9389195442199707, 'learning_rate': 4.559322033898305e-05, 'epoch': 0.1}
 10%|â–ˆ         | 620/6000 [28:42<3:58:09,  2.66s/it] 10%|â–ˆ         | 621/6000 [28:44<3:55:54,  2.63s/it]                                                    {'loss': 2.8352, 'grad_norm': 1.2439476251602173, 'learning_rate': 4.558474576271187e-05, 'epoch': 0.1}
 10%|â–ˆ         | 621/6000 [28:44<3:55:54,  2.63s/it] 10%|â–ˆ         | 622/6000 [28:47<3:55:02,  2.62s/it]                                                    {'loss': 2.7778, 'grad_norm': 1.4999104738235474, 'learning_rate': 4.557627118644068e-05, 'epoch': 0.1}
 10%|â–ˆ         | 622/6000 [28:47<3:55:02,  2.62s/it] 10%|â–ˆ         | 623/6000 [28:49<3:54:35,  2.62s/it]                                                    {'loss': 2.7827, 'grad_norm': 1.6844249963760376, 'learning_rate': 4.556779661016949e-05, 'epoch': 0.1}
 10%|â–ˆ         | 623/6000 [28:49<3:54:35,  2.62s/it] 10%|â–ˆ         | 624/6000 [28:52<3:59:35,  2.67s/it]                                                    {'loss': 2.7634, 'grad_norm': 1.6349188089370728, 'learning_rate': 4.55593220338983e-05, 'epoch': 0.1}
 10%|â–ˆ         | 624/6000 [28:52<3:59:35,  2.67s/it] 10%|â–ˆ         | 625/6000 [28:55<4:03:13,  2.72s/it]                                                    {'loss': 2.7991, 'grad_norm': 1.8455816507339478, 'learning_rate': 4.555084745762712e-05, 'epoch': 0.1}
 10%|â–ˆ         | 625/6000 [28:55<4:03:13,  2.72s/it] 10%|â–ˆ         | 626/6000 [28:58<4:01:44,  2.70s/it]                                                    {'loss': 2.7943, 'grad_norm': 2.1247782707214355, 'learning_rate': 4.554237288135593e-05, 'epoch': 0.1}
 10%|â–ˆ         | 626/6000 [28:58<4:01:44,  2.70s/it] 10%|â–ˆ         | 627/6000 [29:01<4:20:26,  2.91s/it]                                                    {'loss': 2.7727, 'grad_norm': 2.9231371879577637, 'learning_rate': 4.553389830508475e-05, 'epoch': 0.1}
 10%|â–ˆ         | 627/6000 [29:01<4:20:26,  2.91s/it] 10%|â–ˆ         | 628/6000 [29:04<4:15:25,  2.85s/it]                                                    {'loss': 2.7613, 'grad_norm': 1.9623533487319946, 'learning_rate': 4.552542372881356e-05, 'epoch': 0.1}
 10%|â–ˆ         | 628/6000 [29:04<4:15:25,  2.85s/it] 10%|â–ˆ         | 629/6000 [29:07<4:10:49,  2.80s/it]                                                    {'loss': 2.7448, 'grad_norm': 2.264794111251831, 'learning_rate': 4.551694915254238e-05, 'epoch': 0.1}
 10%|â–ˆ         | 629/6000 [29:07<4:10:49,  2.80s/it] 10%|â–ˆ         | 630/6000 [29:09<4:09:00,  2.78s/it]                                                    {'loss': 2.7722, 'grad_norm': 2.1670284271240234, 'learning_rate': 4.550847457627119e-05, 'epoch': 0.1}
 10%|â–ˆ         | 630/6000 [29:09<4:09:00,  2.78s/it] 11%|â–ˆ         | 631/6000 [29:12<4:05:51,  2.75s/it]                                                    {'loss': 2.7294, 'grad_norm': 3.9566917419433594, 'learning_rate': 4.55e-05, 'epoch': 0.11}
 11%|â–ˆ         | 631/6000 [29:12<4:05:51,  2.75s/it] 11%|â–ˆ         | 632/6000 [29:15<4:04:57,  2.74s/it]                                                    {'loss': 2.8842, 'grad_norm': 4.671815872192383, 'learning_rate': 4.549152542372881e-05, 'epoch': 0.11}
 11%|â–ˆ         | 632/6000 [29:15<4:04:57,  2.74s/it] 11%|â–ˆ         | 633/6000 [29:17<4:04:36,  2.73s/it]                                                    {'loss': 2.7877, 'grad_norm': 3.4448180198669434, 'learning_rate': 4.548305084745763e-05, 'epoch': 0.11}
 11%|â–ˆ         | 633/6000 [29:17<4:04:36,  2.73s/it] 11%|â–ˆ         | 634/6000 [29:20<4:12:46,  2.83s/it]                                                    {'loss': 2.7746, 'grad_norm': 9.007501602172852, 'learning_rate': 4.547457627118644e-05, 'epoch': 0.11}
 11%|â–ˆ         | 634/6000 [29:20<4:12:46,  2.83s/it] 11%|â–ˆ         | 635/6000 [29:23<4:11:24,  2.81s/it]                                                    {'loss': 2.7714, 'grad_norm': 6.192893028259277, 'learning_rate': 4.546610169491526e-05, 'epoch': 0.11}
 11%|â–ˆ         | 635/6000 [29:23<4:11:24,  2.81s/it] 11%|â–ˆ         | 636/6000 [29:26<4:07:03,  2.76s/it]                                                    {'loss': 2.791, 'grad_norm': 2.7087278366088867, 'learning_rate': 4.545762711864407e-05, 'epoch': 0.11}
 11%|â–ˆ         | 636/6000 [29:26<4:07:03,  2.76s/it] 11%|â–ˆ         | 637/6000 [29:29<4:05:35,  2.75s/it]                                                    {'loss': 2.7693, 'grad_norm': 2.836358070373535, 'learning_rate': 4.544915254237288e-05, 'epoch': 0.11}
 11%|â–ˆ         | 637/6000 [29:29<4:05:35,  2.75s/it] 11%|â–ˆ         | 638/6000 [29:32<4:13:27,  2.84s/it]                                                    {'loss': 2.7243, 'grad_norm': 2.935671091079712, 'learning_rate': 4.5440677966101694e-05, 'epoch': 0.11}
 11%|â–ˆ         | 638/6000 [29:32<4:13:27,  2.84s/it] 11%|â–ˆ         | 639/6000 [29:34<4:08:51,  2.79s/it]                                                    {'loss': 2.8115, 'grad_norm': 4.650075912475586, 'learning_rate': 4.543220338983051e-05, 'epoch': 0.11}
 11%|â–ˆ         | 639/6000 [29:34<4:08:51,  2.79s/it] 11%|â–ˆ         | 640/6000 [29:37<4:16:34,  2.87s/it]                                                    {'loss': 2.8314, 'grad_norm': 3.3544373512268066, 'learning_rate': 4.542372881355932e-05, 'epoch': 0.11}
 11%|â–ˆ         | 640/6000 [29:37<4:16:34,  2.87s/it] 11%|â–ˆ         | 641/6000 [29:40<4:23:02,  2.95s/it]                                                    {'loss': 2.7907, 'grad_norm': 4.383864879608154, 'learning_rate': 4.5415254237288135e-05, 'epoch': 0.11}
 11%|â–ˆ         | 641/6000 [29:40<4:23:02,  2.95s/it] 11%|â–ˆ         | 642/6000 [29:43<4:13:27,  2.84s/it]                                                    {'loss': 2.753, 'grad_norm': 2.5315799713134766, 'learning_rate': 4.540677966101695e-05, 'epoch': 0.11}
 11%|â–ˆ         | 642/6000 [29:43<4:13:27,  2.84s/it] 11%|â–ˆ         | 643/6000 [29:46<4:10:41,  2.81s/it]                                                    {'loss': 2.791, 'grad_norm': 2.156953811645508, 'learning_rate': 4.5398305084745764e-05, 'epoch': 0.11}
 11%|â–ˆ         | 643/6000 [29:46<4:10:41,  2.81s/it] 11%|â–ˆ         | 644/6000 [29:48<4:07:10,  2.77s/it]                                                    {'loss': 2.8002, 'grad_norm': 2.590024471282959, 'learning_rate': 4.538983050847458e-05, 'epoch': 0.11}
 11%|â–ˆ         | 644/6000 [29:48<4:07:10,  2.77s/it] 11%|â–ˆ         | 645/6000 [29:51<4:01:52,  2.71s/it]                                                    {'loss': 2.7326, 'grad_norm': 3.0678250789642334, 'learning_rate': 4.5381355932203387e-05, 'epoch': 0.11}
 11%|â–ˆ         | 645/6000 [29:51<4:01:52,  2.71s/it] 11%|â–ˆ         | 646/6000 [29:54<4:10:11,  2.80s/it]                                                    {'loss': 2.7675, 'grad_norm': 2.2285656929016113, 'learning_rate': 4.5372881355932205e-05, 'epoch': 0.11}
 11%|â–ˆ         | 646/6000 [29:54<4:10:11,  2.80s/it] 11%|â–ˆ         | 647/6000 [29:57<4:06:47,  2.77s/it]                                                    {'loss': 2.866, 'grad_norm': 1.907895803451538, 'learning_rate': 4.5364406779661016e-05, 'epoch': 0.11}
 11%|â–ˆ         | 647/6000 [29:57<4:06:47,  2.77s/it] 11%|â–ˆ         | 648/6000 [30:00<4:13:04,  2.84s/it]                                                    {'loss': 2.7473, 'grad_norm': 2.3615217208862305, 'learning_rate': 4.5355932203389834e-05, 'epoch': 0.11}
 11%|â–ˆ         | 648/6000 [30:00<4:13:04,  2.84s/it] 11%|â–ˆ         | 649/6000 [30:02<4:07:16,  2.77s/it]                                                    {'loss': 2.776, 'grad_norm': 2.4301555156707764, 'learning_rate': 4.5347457627118645e-05, 'epoch': 0.11}
 11%|â–ˆ         | 649/6000 [30:02<4:07:16,  2.77s/it] 11%|â–ˆ         | 650/6000 [30:05<4:02:17,  2.72s/it]                                                    {'loss': 2.8184, 'grad_norm': 1.9270436763763428, 'learning_rate': 4.533898305084746e-05, 'epoch': 0.11}
 11%|â–ˆ         | 650/6000 [30:05<4:02:17,  2.72s/it][2025-10-21 01:19:00,030] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 11%|â–ˆ         | 651/6000 [30:09<4:51:30,  3.27s/it]                                                    {'loss': 2.783, 'grad_norm': 2.3519911766052246, 'learning_rate': 4.5330508474576275e-05, 'epoch': 0.11}
 11%|â–ˆ         | 651/6000 [30:09<4:51:30,  3.27s/it] 11%|â–ˆ         | 652/6000 [30:12<4:43:52,  3.18s/it]                                                    {'loss': 2.7803, 'grad_norm': 2.676589012145996, 'learning_rate': 4.5322033898305086e-05, 'epoch': 0.11}
 11%|â–ˆ         | 652/6000 [30:12<4:43:52,  3.18s/it] 11%|â–ˆ         | 653/6000 [30:15<4:33:00,  3.06s/it]                                                    {'loss': 2.7784, 'grad_norm': 2.4155519008636475, 'learning_rate': 4.53135593220339e-05, 'epoch': 0.11}
 11%|â–ˆ         | 653/6000 [30:15<4:33:00,  3.06s/it] 11%|â–ˆ         | 654/6000 [30:18<4:20:43,  2.93s/it]                                                    {'loss': 2.7384, 'grad_norm': 2.046835422515869, 'learning_rate': 4.5305084745762715e-05, 'epoch': 0.11}
 11%|â–ˆ         | 654/6000 [30:18<4:20:43,  2.93s/it] 11%|â–ˆ         | 655/6000 [30:21<4:14:54,  2.86s/it]                                                    {'loss': 2.77, 'grad_norm': 2.1494576930999756, 'learning_rate': 4.5296610169491527e-05, 'epoch': 0.11}
 11%|â–ˆ         | 655/6000 [30:21<4:14:54,  2.86s/it] 11%|â–ˆ         | 656/6000 [30:23<4:09:48,  2.80s/it]                                                    {'loss': 2.7565, 'grad_norm': 2.1145808696746826, 'learning_rate': 4.5288135593220345e-05, 'epoch': 0.11}
 11%|â–ˆ         | 656/6000 [30:23<4:09:48,  2.80s/it] 11%|â–ˆ         | 657/6000 [30:26<4:10:57,  2.82s/it]                                                    {'loss': 2.7701, 'grad_norm': 2.157566785812378, 'learning_rate': 4.5279661016949156e-05, 'epoch': 0.11}
 11%|â–ˆ         | 657/6000 [30:26<4:10:57,  2.82s/it] 11%|â–ˆ         | 658/6000 [30:29<4:17:21,  2.89s/it]                                                    {'loss': 2.8047, 'grad_norm': 3.2610301971435547, 'learning_rate': 4.5271186440677974e-05, 'epoch': 0.11}
 11%|â–ˆ         | 658/6000 [30:29<4:17:21,  2.89s/it] 11%|â–ˆ         | 659/6000 [30:32<4:10:33,  2.81s/it]                                                    {'loss': 2.7025, 'grad_norm': 2.8679049015045166, 'learning_rate': 4.526271186440678e-05, 'epoch': 0.11}
 11%|â–ˆ         | 659/6000 [30:32<4:10:33,  2.81s/it] 11%|â–ˆ         | 660/6000 [30:34<4:06:24,  2.77s/it]                                                    {'loss': 2.7251, 'grad_norm': 3.8093173503875732, 'learning_rate': 4.5254237288135596e-05, 'epoch': 0.11}
 11%|â–ˆ         | 660/6000 [30:34<4:06:24,  2.77s/it] 11%|â–ˆ         | 661/6000 [30:37<4:02:16,  2.72s/it]                                                    {'loss': 2.8267, 'grad_norm': 4.49846076965332, 'learning_rate': 4.524576271186441e-05, 'epoch': 0.11}
 11%|â–ˆ         | 661/6000 [30:37<4:02:16,  2.72s/it] 11%|â–ˆ         | 662/6000 [30:40<4:01:48,  2.72s/it]                                                    {'loss': 2.8289, 'grad_norm': 4.267376899719238, 'learning_rate': 4.523728813559322e-05, 'epoch': 0.11}
 11%|â–ˆ         | 662/6000 [30:40<4:01:48,  2.72s/it] 11%|â–ˆ         | 663/6000 [30:42<4:00:41,  2.71s/it]                                                    {'loss': 2.7086, 'grad_norm': 2.9973502159118652, 'learning_rate': 4.522881355932204e-05, 'epoch': 0.11}
 11%|â–ˆ         | 663/6000 [30:42<4:00:41,  2.71s/it] 11%|â–ˆ         | 664/6000 [30:45<4:01:23,  2.71s/it]                                                    {'loss': 2.8953, 'grad_norm': 4.9013590812683105, 'learning_rate': 4.522033898305085e-05, 'epoch': 0.11}
 11%|â–ˆ         | 664/6000 [30:45<4:01:23,  2.71s/it] 11%|â–ˆ         | 665/6000 [30:48<4:00:25,  2.70s/it]                                                    {'loss': 2.755, 'grad_norm': 3.110887050628662, 'learning_rate': 4.5211864406779666e-05, 'epoch': 0.11}
 11%|â–ˆ         | 665/6000 [30:48<4:00:25,  2.70s/it] 11%|â–ˆ         | 666/6000 [30:51<3:59:39,  2.70s/it]                                                    {'loss': 2.7708, 'grad_norm': 2.4573423862457275, 'learning_rate': 4.520338983050848e-05, 'epoch': 0.11}
 11%|â–ˆ         | 666/6000 [30:51<3:59:39,  2.70s/it] 11%|â–ˆ         | 667/6000 [30:53<3:59:31,  2.69s/it]                                                    {'loss': 2.7739, 'grad_norm': 2.941384792327881, 'learning_rate': 4.519491525423729e-05, 'epoch': 0.11}
 11%|â–ˆ         | 667/6000 [30:53<3:59:31,  2.69s/it] 11%|â–ˆ         | 668/6000 [30:56<3:58:24,  2.68s/it]                                                    {'loss': 2.7967, 'grad_norm': 2.5525357723236084, 'learning_rate': 4.51864406779661e-05, 'epoch': 0.11}
 11%|â–ˆ         | 668/6000 [30:56<3:58:24,  2.68s/it] 11%|â–ˆ         | 669/6000 [30:59<3:57:42,  2.68s/it]                                                    {'loss': 2.8119, 'grad_norm': 3.7173867225646973, 'learning_rate': 4.517796610169492e-05, 'epoch': 0.11}
 11%|â–ˆ         | 669/6000 [30:59<3:57:42,  2.68s/it] 11%|â–ˆ         | 670/6000 [31:01<4:02:02,  2.72s/it]                                                    {'loss': 2.7551, 'grad_norm': 3.4276182651519775, 'learning_rate': 4.516949152542373e-05, 'epoch': 0.11}
 11%|â–ˆ         | 670/6000 [31:01<4:02:02,  2.72s/it] 11%|â–ˆ         | 671/6000 [31:04<4:01:51,  2.72s/it]                                                    {'loss': 2.787, 'grad_norm': 2.78104829788208, 'learning_rate': 4.516101694915255e-05, 'epoch': 0.11}
 11%|â–ˆ         | 671/6000 [31:04<4:01:51,  2.72s/it] 11%|â–ˆ         | 672/6000 [31:07<3:57:50,  2.68s/it]                                                    {'loss': 2.8077, 'grad_norm': 2.9684154987335205, 'learning_rate': 4.515254237288136e-05, 'epoch': 0.11}
 11%|â–ˆ         | 672/6000 [31:07<3:57:50,  2.68s/it] 11%|â–ˆ         | 673/6000 [31:10<4:02:13,  2.73s/it]                                                    {'loss': 2.8248, 'grad_norm': 3.314842700958252, 'learning_rate': 4.514406779661017e-05, 'epoch': 0.11}
 11%|â–ˆ         | 673/6000 [31:10<4:02:13,  2.73s/it] 11%|â–ˆ         | 674/6000 [31:12<4:00:51,  2.71s/it]                                                    {'loss': 2.736, 'grad_norm': 4.465291500091553, 'learning_rate': 4.513559322033898e-05, 'epoch': 0.11}
 11%|â–ˆ         | 674/6000 [31:12<4:00:51,  2.71s/it] 11%|â–ˆâ–        | 675/6000 [31:15<3:58:52,  2.69s/it]                                                    {'loss': 2.8239, 'grad_norm': 6.196664333343506, 'learning_rate': 4.51271186440678e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 675/6000 [31:15<3:58:52,  2.69s/it] 11%|â–ˆâ–        | 676/6000 [31:18<3:58:30,  2.69s/it]                                                    {'loss': 2.8066, 'grad_norm': 2.7620697021484375, 'learning_rate': 4.511864406779661e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 676/6000 [31:18<3:58:30,  2.69s/it] 11%|â–ˆâ–        | 677/6000 [31:20<3:56:36,  2.67s/it]                                                    {'loss': 2.7372, 'grad_norm': 2.59306001663208, 'learning_rate': 4.511016949152543e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 677/6000 [31:20<3:56:36,  2.67s/it] 11%|â–ˆâ–        | 678/6000 [31:23<3:57:28,  2.68s/it]                                                    {'loss': 2.7656, 'grad_norm': 2.91054105758667, 'learning_rate': 4.510169491525424e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 678/6000 [31:23<3:57:28,  2.68s/it] 11%|â–ˆâ–        | 679/6000 [31:25<3:55:04,  2.65s/it]                                                    {'loss': 2.7734, 'grad_norm': 2.252305746078491, 'learning_rate': 4.509322033898306e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 679/6000 [31:25<3:55:04,  2.65s/it] 11%|â–ˆâ–        | 680/6000 [31:28<3:57:07,  2.67s/it]                                                    {'loss': 2.7794, 'grad_norm': 2.0601565837860107, 'learning_rate': 4.508474576271187e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 680/6000 [31:28<3:57:07,  2.67s/it] 11%|â–ˆâ–        | 681/6000 [31:31<4:06:01,  2.78s/it]                                                    {'loss': 2.8188, 'grad_norm': 22.22109031677246, 'learning_rate': 4.507627118644068e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 681/6000 [31:31<4:06:01,  2.78s/it] 11%|â–ˆâ–        | 682/6000 [31:34<4:04:54,  2.76s/it]                                                    {'loss': 2.7752, 'grad_norm': 7.055570602416992, 'learning_rate': 4.506779661016949e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 682/6000 [31:34<4:04:54,  2.76s/it] 11%|â–ˆâ–        | 683/6000 [31:37<4:02:20,  2.73s/it]                                                    {'loss': 2.7185, 'grad_norm': 2.8344457149505615, 'learning_rate': 4.5059322033898304e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 683/6000 [31:37<4:02:20,  2.73s/it] 11%|â–ˆâ–        | 684/6000 [31:39<3:59:02,  2.70s/it]                                                    {'loss': 2.8124, 'grad_norm': 2.197237968444824, 'learning_rate': 4.505084745762712e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 684/6000 [31:39<3:59:02,  2.70s/it] 11%|â–ˆâ–        | 685/6000 [31:42<3:57:08,  2.68s/it]                                                    {'loss': 2.7933, 'grad_norm': 1.844387412071228, 'learning_rate': 4.504237288135593e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 685/6000 [31:42<3:57:08,  2.68s/it] 11%|â–ˆâ–        | 686/6000 [31:45<4:11:17,  2.84s/it]                                                    {'loss': 2.781, 'grad_norm': 1.870312213897705, 'learning_rate': 4.503389830508475e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 686/6000 [31:45<4:11:17,  2.84s/it] 11%|â–ˆâ–        | 687/6000 [31:48<4:05:44,  2.78s/it]                                                    {'loss': 2.7802, 'grad_norm': 2.187096357345581, 'learning_rate': 4.502542372881356e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 687/6000 [31:48<4:05:44,  2.78s/it] 11%|â–ˆâ–        | 688/6000 [31:50<4:03:39,  2.75s/it]                                                    {'loss': 2.835, 'grad_norm': 1.5572896003723145, 'learning_rate': 4.5016949152542373e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 688/6000 [31:50<4:03:39,  2.75s/it] 11%|â–ˆâ–        | 689/6000 [31:53<4:02:04,  2.73s/it]                                                    {'loss': 2.7712, 'grad_norm': 1.7836101055145264, 'learning_rate': 4.5008474576271185e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 689/6000 [31:53<4:02:04,  2.73s/it] 12%|â–ˆâ–        | 690/6000 [31:56<4:01:08,  2.72s/it]                                                    {'loss': 2.7504, 'grad_norm': 1.9113526344299316, 'learning_rate': 4.5e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 690/6000 [31:56<4:01:08,  2.72s/it] 12%|â–ˆâ–        | 691/6000 [31:59<4:02:00,  2.74s/it]                                                    {'loss': 2.7756, 'grad_norm': 1.9662935733795166, 'learning_rate': 4.4991525423728814e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 691/6000 [31:59<4:02:00,  2.74s/it] 12%|â–ˆâ–        | 692/6000 [32:01<4:00:56,  2.72s/it]                                                    {'loss': 2.7216, 'grad_norm': 2.1094133853912354, 'learning_rate': 4.498305084745763e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 692/6000 [32:01<4:00:56,  2.72s/it] 12%|â–ˆâ–        | 693/6000 [32:04<3:58:18,  2.69s/it]                                                    {'loss': 2.7723, 'grad_norm': 2.025580644607544, 'learning_rate': 4.4974576271186443e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 693/6000 [32:04<3:58:18,  2.69s/it] 12%|â–ˆâ–        | 694/6000 [32:07<3:59:33,  2.71s/it]                                                    {'loss': 2.763, 'grad_norm': 2.615635395050049, 'learning_rate': 4.4966101694915255e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 694/6000 [32:07<3:59:33,  2.71s/it] 12%|â–ˆâ–        | 695/6000 [32:09<3:59:21,  2.71s/it]                                                    {'loss': 2.8127, 'grad_norm': 3.5892231464385986, 'learning_rate': 4.4957627118644066e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 695/6000 [32:09<3:59:21,  2.71s/it] 12%|â–ˆâ–        | 696/6000 [32:12<4:08:22,  2.81s/it]                                                    {'loss': 2.7886, 'grad_norm': 7.863860130310059, 'learning_rate': 4.4949152542372884e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 696/6000 [32:12<4:08:22,  2.81s/it] 12%|â–ˆâ–        | 697/6000 [32:15<4:03:32,  2.76s/it]                                                    {'loss': 2.766, 'grad_norm': 2.935746192932129, 'learning_rate': 4.4940677966101695e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 697/6000 [32:15<4:03:32,  2.76s/it] 12%|â–ˆâ–        | 698/6000 [32:18<4:00:27,  2.72s/it]                                                    {'loss': 2.8237, 'grad_norm': 2.420680284500122, 'learning_rate': 4.4932203389830513e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 698/6000 [32:18<4:00:27,  2.72s/it] 12%|â–ˆâ–        | 699/6000 [32:20<4:00:34,  2.72s/it]                                                    {'loss': 2.7634, 'grad_norm': 2.592881679534912, 'learning_rate': 4.4923728813559325e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 699/6000 [32:20<4:00:34,  2.72s/it] 12%|â–ˆâ–        | 700/6000 [32:23<4:02:27,  2.74s/it]                                                    {'loss': 2.7947, 'grad_norm': 2.8797590732574463, 'learning_rate': 4.491525423728814e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 700/6000 [32:23<4:02:27,  2.74s/it][2025-10-21 01:21:18,214] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 12%|â–ˆâ–        | 701/6000 [32:28<4:55:14,  3.34s/it]                                                    {'loss': 2.8649, 'grad_norm': 5.3892693519592285, 'learning_rate': 4.4906779661016954e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 701/6000 [32:28<4:55:14,  3.34s/it] 12%|â–ˆâ–        | 702/6000 [32:31<4:48:12,  3.26s/it]                                                    {'loss': 2.7972, 'grad_norm': 2.6387298107147217, 'learning_rate': 4.4898305084745765e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 702/6000 [32:31<4:48:12,  3.26s/it] 12%|â–ˆâ–        | 703/6000 [32:34<4:33:26,  3.10s/it]                                                    {'loss': 2.7031, 'grad_norm': 2.5346519947052, 'learning_rate': 4.488983050847458e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 703/6000 [32:34<4:33:26,  3.10s/it] 12%|â–ˆâ–        | 704/6000 [32:37<4:30:11,  3.06s/it]                                                    {'loss': 2.7604, 'grad_norm': 2.839271306991577, 'learning_rate': 4.488135593220339e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 704/6000 [32:37<4:30:11,  3.06s/it] 12%|â–ˆâ–        | 705/6000 [32:39<4:21:26,  2.96s/it]                                                    {'loss': 2.8151, 'grad_norm': 2.4733965396881104, 'learning_rate': 4.4872881355932206e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 705/6000 [32:39<4:21:26,  2.96s/it] 12%|â–ˆâ–        | 706/6000 [32:42<4:13:05,  2.87s/it]                                                    {'loss': 2.7331, 'grad_norm': 2.885496139526367, 'learning_rate': 4.486440677966102e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 706/6000 [32:42<4:13:05,  2.87s/it] 12%|â–ˆâ–        | 707/6000 [32:45<4:08:12,  2.81s/it]                                                    {'loss': 2.775, 'grad_norm': 2.507997751235962, 'learning_rate': 4.4855932203389835e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 707/6000 [32:45<4:08:12,  2.81s/it] 12%|â–ˆâ–        | 708/6000 [32:47<4:05:47,  2.79s/it]                                                    {'loss': 2.7673, 'grad_norm': 2.741342067718506, 'learning_rate': 4.484745762711865e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 708/6000 [32:47<4:05:47,  2.79s/it] 12%|â–ˆâ–        | 709/6000 [32:50<4:02:12,  2.75s/it]                                                    {'loss': 2.7566, 'grad_norm': 1.7319689989089966, 'learning_rate': 4.483898305084746e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 709/6000 [32:50<4:02:12,  2.75s/it] 12%|â–ˆâ–        | 710/6000 [32:53<4:08:26,  2.82s/it]                                                    {'loss': 2.7571, 'grad_norm': 2.0811381340026855, 'learning_rate': 4.483050847457627e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 710/6000 [32:53<4:08:26,  2.82s/it] 12%|â–ˆâ–        | 711/6000 [32:56<4:02:38,  2.75s/it]                                                    {'loss': 2.7676, 'grad_norm': 2.172330379486084, 'learning_rate': 4.482203389830509e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 711/6000 [32:56<4:02:38,  2.75s/it] 12%|â–ˆâ–        | 712/6000 [32:58<4:02:19,  2.75s/it]                                                    {'loss': 2.8108, 'grad_norm': 4.519002437591553, 'learning_rate': 4.48135593220339e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 712/6000 [32:58<4:02:19,  2.75s/it] 12%|â–ˆâ–        | 713/6000 [33:01<4:00:03,  2.72s/it]                                                    {'loss': 2.759, 'grad_norm': 2.812373161315918, 'learning_rate': 4.480508474576272e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 713/6000 [33:01<4:00:03,  2.72s/it] 12%|â–ˆâ–        | 714/6000 [33:04<3:59:56,  2.72s/it]                                                    {'loss': 2.7748, 'grad_norm': 2.1326704025268555, 'learning_rate': 4.479661016949153e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 714/6000 [33:04<3:59:56,  2.72s/it] 12%|â–ˆâ–        | 715/6000 [33:06<3:58:09,  2.70s/it]                                                    {'loss': 2.7925, 'grad_norm': 3.558483123779297, 'learning_rate': 4.4788135593220346e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 715/6000 [33:06<3:58:09,  2.70s/it] 12%|â–ˆâ–        | 716/6000 [33:09<3:58:34,  2.71s/it]                                                    {'loss': 2.82, 'grad_norm': 1.6674683094024658, 'learning_rate': 4.477966101694915e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 716/6000 [33:09<3:58:34,  2.71s/it] 12%|â–ˆâ–        | 717/6000 [33:12<4:07:45,  2.81s/it]                                                    {'loss': 2.801, 'grad_norm': 2.9408063888549805, 'learning_rate': 4.477118644067797e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 717/6000 [33:12<4:07:45,  2.81s/it] 12%|â–ˆâ–        | 718/6000 [33:15<4:03:23,  2.76s/it]                                                    {'loss': 2.7917, 'grad_norm': 2.457756280899048, 'learning_rate': 4.476271186440678e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 718/6000 [33:15<4:03:23,  2.76s/it] 12%|â–ˆâ–        | 719/6000 [33:18<4:03:39,  2.77s/it]                                                    {'loss': 2.7317, 'grad_norm': 2.130859136581421, 'learning_rate': 4.47542372881356e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 719/6000 [33:18<4:03:39,  2.77s/it] 12%|â–ˆâ–        | 720/6000 [33:20<3:58:46,  2.71s/it]                                                    {'loss': 2.765, 'grad_norm': 1.2881615161895752, 'learning_rate': 4.474576271186441e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 720/6000 [33:20<3:58:46,  2.71s/it] 12%|â–ˆâ–        | 721/6000 [33:23<3:58:58,  2.72s/it]                                                    {'loss': 2.781, 'grad_norm': 4.019320487976074, 'learning_rate': 4.473728813559323e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 721/6000 [33:23<3:58:58,  2.72s/it] 12%|â–ˆâ–        | 722/6000 [33:26<3:59:56,  2.73s/it]                                                    {'loss': 2.7684, 'grad_norm': 2.4549148082733154, 'learning_rate': 4.472881355932204e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 722/6000 [33:26<3:59:56,  2.73s/it] 12%|â–ˆâ–        | 723/6000 [33:29<4:10:03,  2.84s/it]                                                    {'loss': 2.7974, 'grad_norm': 2.235034704208374, 'learning_rate': 4.472033898305085e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 723/6000 [33:29<4:10:03,  2.84s/it] 12%|â–ˆâ–        | 724/6000 [33:31<4:03:48,  2.77s/it]                                                    {'loss': 2.7858, 'grad_norm': 1.5911900997161865, 'learning_rate': 4.471186440677966e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 724/6000 [33:31<4:03:48,  2.77s/it] 12%|â–ˆâ–        | 725/6000 [33:34<3:59:55,  2.73s/it]                                                    {'loss': 2.7456, 'grad_norm': 2.0966413021087646, 'learning_rate': 4.470338983050847e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 725/6000 [33:34<3:59:55,  2.73s/it] 12%|â–ˆâ–        | 726/6000 [33:37<3:57:16,  2.70s/it]                                                    {'loss': 2.7495, 'grad_norm': 1.9288115501403809, 'learning_rate': 4.469491525423729e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 726/6000 [33:37<3:57:16,  2.70s/it] 12%|â–ˆâ–        | 727/6000 [33:39<3:56:38,  2.69s/it]                                                    {'loss': 2.7462, 'grad_norm': 2.480681896209717, 'learning_rate': 4.46864406779661e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 727/6000 [33:39<3:56:38,  2.69s/it] 12%|â–ˆâ–        | 728/6000 [33:42<3:55:40,  2.68s/it]                                                    {'loss': 2.7758, 'grad_norm': 2.229229688644409, 'learning_rate': 4.467796610169492e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 728/6000 [33:42<3:55:40,  2.68s/it] 12%|â–ˆâ–        | 729/6000 [33:45<3:55:37,  2.68s/it]                                                    {'loss': 2.7648, 'grad_norm': 2.21816349029541, 'learning_rate': 4.466949152542373e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 729/6000 [33:45<3:55:37,  2.68s/it] 12%|â–ˆâ–        | 730/6000 [33:47<3:54:33,  2.67s/it]                                                    {'loss': 2.7916, 'grad_norm': 3.0737338066101074, 'learning_rate': 4.466101694915254e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 730/6000 [33:47<3:54:33,  2.67s/it] 12%|â–ˆâ–        | 731/6000 [33:50<3:52:44,  2.65s/it]                                                    {'loss': 2.7438, 'grad_norm': 2.303605318069458, 'learning_rate': 4.4652542372881354e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 731/6000 [33:50<3:52:44,  2.65s/it] 12%|â–ˆâ–        | 732/6000 [33:53<3:57:00,  2.70s/it]                                                    {'loss': 2.7548, 'grad_norm': 3.005711793899536, 'learning_rate': 4.464406779661017e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 732/6000 [33:53<3:57:00,  2.70s/it] 12%|â–ˆâ–        | 733/6000 [33:56<3:59:23,  2.73s/it]                                                    {'loss': 2.8953, 'grad_norm': 5.082886695861816, 'learning_rate': 4.463559322033898e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 733/6000 [33:56<3:59:23,  2.73s/it] 12%|â–ˆâ–        | 734/6000 [33:59<4:09:25,  2.84s/it]                                                    {'loss': 2.7437, 'grad_norm': 3.3073620796203613, 'learning_rate': 4.46271186440678e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 734/6000 [33:59<4:09:25,  2.84s/it] 12%|â–ˆâ–        | 735/6000 [34:01<4:04:05,  2.78s/it]                                                    {'loss': 2.7575, 'grad_norm': 3.064126968383789, 'learning_rate': 4.461864406779661e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 735/6000 [34:01<4:04:05,  2.78s/it] 12%|â–ˆâ–        | 736/6000 [34:04<3:59:52,  2.73s/it]                                                    {'loss': 2.7462, 'grad_norm': 2.564899444580078, 'learning_rate': 4.461016949152543e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 736/6000 [34:04<3:59:52,  2.73s/it] 12%|â–ˆâ–        | 737/6000 [34:07<3:57:08,  2.70s/it]                                                    {'loss': 2.7858, 'grad_norm': 2.2305691242218018, 'learning_rate': 4.460169491525424e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 737/6000 [34:07<3:57:08,  2.70s/it] 12%|â–ˆâ–        | 738/6000 [34:09<3:56:55,  2.70s/it]                                                    {'loss': 2.8267, 'grad_norm': 4.038323879241943, 'learning_rate': 4.459322033898305e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 738/6000 [34:09<3:56:55,  2.70s/it] 12%|â–ˆâ–        | 739/6000 [34:12<3:55:37,  2.69s/it]                                                    {'loss': 2.7247, 'grad_norm': 6.147045612335205, 'learning_rate': 4.4584745762711864e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 739/6000 [34:12<3:55:37,  2.69s/it] 12%|â–ˆâ–        | 740/6000 [34:15<3:56:22,  2.70s/it]                                                    {'loss': 2.7075, 'grad_norm': 5.678760528564453, 'learning_rate': 4.457627118644068e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 740/6000 [34:15<3:56:22,  2.70s/it] 12%|â–ˆâ–        | 741/6000 [34:17<3:55:23,  2.69s/it]                                                    {'loss': 2.8691, 'grad_norm': 5.711162090301514, 'learning_rate': 4.4567796610169494e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 741/6000 [34:17<3:55:23,  2.69s/it] 12%|â–ˆâ–        | 742/6000 [34:20<3:53:46,  2.67s/it]                                                    {'loss': 2.8699, 'grad_norm': 4.6007843017578125, 'learning_rate': 4.455932203389831e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 742/6000 [34:20<3:53:46,  2.67s/it] 12%|â–ˆâ–        | 743/6000 [34:22<3:51:34,  2.64s/it]                                                    {'loss': 2.7792, 'grad_norm': 2.7767622470855713, 'learning_rate': 4.455084745762712e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 743/6000 [34:23<3:51:34,  2.64s/it] 12%|â–ˆâ–        | 744/6000 [34:25<3:49:44,  2.62s/it]                                                    {'loss': 2.7187, 'grad_norm': 4.527004718780518, 'learning_rate': 4.4542372881355934e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 744/6000 [34:25<3:49:44,  2.62s/it] 12%|â–ˆâ–        | 745/6000 [34:28<3:48:24,  2.61s/it]                                                    {'loss': 2.7724, 'grad_norm': 2.01005220413208, 'learning_rate': 4.4533898305084746e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 745/6000 [34:28<3:48:24,  2.61s/it] 12%|â–ˆâ–        | 746/6000 [34:30<3:49:27,  2.62s/it]                                                    {'loss': 2.758, 'grad_norm': 2.922659158706665, 'learning_rate': 4.452542372881356e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 746/6000 [34:30<3:49:27,  2.62s/it] 12%|â–ˆâ–        | 747/6000 [34:33<3:50:32,  2.63s/it]                                                    {'loss': 2.7955, 'grad_norm': 2.2062363624572754, 'learning_rate': 4.4516949152542375e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 747/6000 [34:33<3:50:32,  2.63s/it] 12%|â–ˆâ–        | 748/6000 [34:36<3:55:16,  2.69s/it]                                                    {'loss': 2.751, 'grad_norm': 1.9820305109024048, 'learning_rate': 4.4508474576271186e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 748/6000 [34:36<3:55:16,  2.69s/it] 12%|â–ˆâ–        | 749/6000 [34:39<4:14:22,  2.91s/it]                                                    {'loss': 2.7466, 'grad_norm': 1.9711767435073853, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 749/6000 [34:39<4:14:22,  2.91s/it] 12%|â–ˆâ–Ž        | 750/6000 [34:42<4:16:31,  2.93s/it]                                                    {'loss': 2.7796, 'grad_norm': 2.1340596675872803, 'learning_rate': 4.4491525423728816e-05, 'epoch': 0.12}
 12%|â–ˆâ–Ž        | 750/6000 [34:42<4:16:31,  2.93s/it][2025-10-21 01:23:37,281] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 751/6000 [34:47<5:02:53,  3.46s/it]                                                    {'loss': 2.7805, 'grad_norm': 1.8239394426345825, 'learning_rate': 4.448305084745763e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 751/6000 [34:47<5:02:53,  3.46s/it] 13%|â–ˆâ–Ž        | 752/6000 [34:50<4:52:09,  3.34s/it]                                                    {'loss': 2.8282, 'grad_norm': 2.0721566677093506, 'learning_rate': 4.447457627118644e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 752/6000 [34:50<4:52:09,  3.34s/it] 13%|â–ˆâ–Ž        | 753/6000 [34:53<4:35:29,  3.15s/it]                                                    {'loss': 2.8054, 'grad_norm': 2.0309529304504395, 'learning_rate': 4.4466101694915256e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 753/6000 [34:53<4:35:29,  3.15s/it]