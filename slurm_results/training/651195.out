==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Alibaba-NLP/gme-Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-6 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token False --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/train.log
W1103 14:28:03.861000 125270482888512 torch/distributed/run.py:779] 
W1103 14:28:03.861000 125270482888512 torch/distributed/run.py:779] *****************************************
W1103 14:28:03.861000 125270482888512 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 14:28:03.861000 125270482888512 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 14:28:14,342] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]wandb: setting up run 48oqzcok
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/wandb/run-20251103_142814-48oqzcok
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/48oqzcok
[2025-11-03 14:28:17,331] INFO [src.utils:19] Loading backbone [qwen2_vl] from Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:39<01:18, 39.47s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:37<01:14, 37.13s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:10<00:35, 35.07s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [01:13<00:36, 36.07s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 21.80s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:16<00:00, 25.59s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:19<00:00, 22.33s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [01:19<00:00, 26.38s/it]
[2025-11-03 14:29:34,506] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 14:29:44,712] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 14:29:44,713] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 14:29:44,714] INFO [src.utils:19] Loading processor from: Alibaba-NLP/gme-Qwen2-VL-2B-Instruct
[2025-11-03 14:29:49,457] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 14:29:49,458] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 14:29:50,313] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 14:29:50,314] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 14:29:50,315] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 14:29:50,316] INFO [src.utils:19] ==================================================
[2025-11-03 14:29:50,316] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 14:29:50,318] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 14:29:50,319] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 14:29:50,319] INFO [src.utils:19] ==================================================
[2025-11-03 14:29:52,501] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 14:29:52,501] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 14:29:52,501] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 14:29:52,501] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 14:29:52,501] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 14:29:52,501] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 14:29:52,501] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 14:29:52,501] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 14:29:52,502] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 14:29:52,502] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 14:29:52,503] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 14:29:52,503] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 14:29:52,503] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 14:29:52,503] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 14:29:52,512] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-11-03 14:29:52,515] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-11-03 14:29:52,522] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 14:29:52,526] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1103 14:29:56.070150614 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1103 14:29:56.094542772 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:05<8:23:35,  5.04s/it]                                                  {'loss': 20.4889, 'grad_norm': 1391.4508056640625, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}
  0%|          | 1/6000 [00:05<8:23:35,  5.04s/it]  0%|          | 2/6000 [00:08<6:38:03,  3.98s/it]                                                  {'loss': 15.561, 'grad_norm': 924.259765625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:08<6:38:03,  3.98s/it]  0%|          | 3/6000 [00:11<6:10:37,  3.71s/it]                                                  {'loss': 17.3615, 'grad_norm': 863.131103515625, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:11<6:10:37,  3.71s/it]  0%|          | 4/6000 [00:15<6:00:00,  3.60s/it]                                                  {'loss': 16.0536, 'grad_norm': 949.576904296875, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:15<6:00:00,  3.60s/it]  0%|          | 5/6000 [00:18<5:47:44,  3.48s/it]                                                  {'loss': 17.8809, 'grad_norm': 964.5952758789062, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:18<5:47:44,  3.48s/it]  0%|          | 6/6000 [00:21<5:42:26,  3.43s/it]                                                  {'loss': 17.7655, 'grad_norm': 1159.9365234375, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:21<5:42:26,  3.43s/it]  0%|          | 7/6000 [00:24<5:37:43,  3.38s/it]                                                  {'loss': 17.1863, 'grad_norm': 1223.8299560546875, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:24<5:37:43,  3.38s/it]  0%|          | 8/6000 [00:28<5:33:16,  3.34s/it]                                                  {'loss': 16.7915, 'grad_norm': 1127.166015625, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:28<5:33:16,  3.34s/it]  0%|          | 9/6000 [00:31<5:33:11,  3.34s/it]                                                  {'loss': 13.3038, 'grad_norm': 803.5757446289062, 'learning_rate': 4.5000000000000003e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:31<5:33:11,  3.34s/it]  0%|          | 10/6000 [00:34<5:29:34,  3.30s/it]                                                   {'loss': 15.1993, 'grad_norm': 1077.3341064453125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 10/6000 [00:34<5:29:34,  3.30s/it]  0%|          | 11/6000 [00:38<5:39:12,  3.40s/it]                                                   {'loss': 19.2965, 'grad_norm': 1111.8826904296875, 'learning_rate': 5.5e-07, 'epoch': 0.0}
  0%|          | 11/6000 [00:38<5:39:12,  3.40s/it]  0%|          | 12/6000 [00:41<5:41:13,  3.42s/it]                                                   {'loss': 17.4028, 'grad_norm': 1180.7442626953125, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 12/6000 [00:41<5:41:13,  3.42s/it]  0%|          | 13/6000 [00:45<5:37:57,  3.39s/it]                                                   {'loss': 17.9007, 'grad_norm': 1263.9854736328125, 'learning_rate': 6.5e-07, 'epoch': 0.0}
  0%|          | 13/6000 [00:45<5:37:57,  3.39s/it]  0%|          | 14/6000 [00:48<5:39:25,  3.40s/it]                                                   {'loss': 18.784, 'grad_norm': 1166.3980712890625, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 14/6000 [00:48<5:39:25,  3.40s/it]  0%|          | 15/6000 [00:51<5:36:23,  3.37s/it]                                                   {'loss': 15.42, 'grad_norm': 1380.0654296875, 'learning_rate': 7.5e-07, 'epoch': 0.0}
  0%|          | 15/6000 [00:51<5:36:23,  3.37s/it]  0%|          | 16/6000 [00:55<5:34:30,  3.35s/it]                                                   {'loss': 19.2324, 'grad_norm': 1106.4249267578125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 16/6000 [00:55<5:34:30,  3.35s/it]  0%|          | 17/6000 [00:58<5:32:38,  3.34s/it]                                                   {'loss': 15.0824, 'grad_norm': 1200.8687744140625, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.0}
  0%|          | 17/6000 [00:58<5:32:38,  3.34s/it]  0%|          | 18/6000 [01:01<5:34:03,  3.35s/it]                                                   {'loss': 14.539, 'grad_norm': 929.23876953125, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 18/6000 [01:01<5:34:03,  3.35s/it]  0%|          | 19/6000 [01:05<5:31:50,  3.33s/it]                                                   {'loss': 18.2317, 'grad_norm': 1307.9725341796875, 'learning_rate': 9.500000000000001e-07, 'epoch': 0.0}
  0%|          | 19/6000 [01:05<5:31:50,  3.33s/it]  0%|          | 20/6000 [01:08<5:32:44,  3.34s/it]                                                   {'loss': 16.5651, 'grad_norm': 1662.80078125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 20/6000 [01:08<5:32:44,  3.34s/it]  0%|          | 21/6000 [01:12<5:36:15,  3.37s/it]                                                   {'loss': 15.6762, 'grad_norm': 966.1131591796875, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:12<5:36:15,  3.37s/it]  0%|          | 22/6000 [01:15<5:35:27,  3.37s/it]                                                   {'loss': 12.7401, 'grad_norm': 1212.9373779296875, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:15<5:35:27,  3.37s/it]  0%|          | 23/6000 [01:18<5:33:01,  3.34s/it]                                                   {'loss': 14.0075, 'grad_norm': 1122.2318115234375, 'learning_rate': 1.1500000000000002e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:18<5:33:01,  3.34s/it]  0%|          | 24/6000 [01:22<5:36:34,  3.38s/it]                                                   {'loss': 9.5879, 'grad_norm': 766.8324584960938, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:22<5:36:34,  3.38s/it]  0%|          | 25/6000 [01:25<5:36:50,  3.38s/it]                                                   {'loss': 12.8672, 'grad_norm': 1238.96044921875, 'learning_rate': 1.25e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:25<5:36:50,  3.38s/it]  0%|          | 26/6000 [01:28<5:37:52,  3.39s/it]                                                   {'loss': 13.4918, 'grad_norm': 1346.320068359375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:28<5:37:52,  3.39s/it]  0%|          | 27/6000 [01:32<5:38:08,  3.40s/it]                                                   {'loss': 14.0976, 'grad_norm': 1372.6923828125, 'learning_rate': 1.3500000000000002e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:32<5:38:08,  3.40s/it]  0%|          | 28/6000 [01:36<6:04:24,  3.66s/it]                                                   {'loss': 11.3366, 'grad_norm': 1148.5804443359375, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:36<6:04:24,  3.66s/it]  0%|          | 29/6000 [01:39<5:54:35,  3.56s/it]                                                   {'loss': 10.5473, 'grad_norm': 1066.7020263671875, 'learning_rate': 1.45e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:39<5:54:35,  3.56s/it]  0%|          | 30/6000 [01:43<5:48:58,  3.51s/it]                                                   {'loss': 10.9055, 'grad_norm': 1206.837158203125, 'learning_rate': 1.5e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:43<5:48:58,  3.51s/it]  1%|          | 31/6000 [01:46<5:42:36,  3.44s/it]                                                   {'loss': 9.7629, 'grad_norm': 1182.3602294921875, 'learning_rate': 1.5500000000000002e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:46<5:42:36,  3.44s/it]  1%|          | 32/6000 [01:49<5:40:51,  3.43s/it]                                                   {'loss': 10.9085, 'grad_norm': 1189.7779541015625, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:50<5:40:51,  3.43s/it]  1%|          | 33/6000 [01:53<5:39:58,  3.42s/it]                                                   {'loss': 10.5805, 'grad_norm': 1295.1363525390625, 'learning_rate': 1.6500000000000003e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:53<5:39:58,  3.42s/it]  1%|          | 34/6000 [01:56<5:36:09,  3.38s/it]                                                   {'loss': 9.8628, 'grad_norm': 1357.13818359375, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:56<5:36:09,  3.38s/it]  1%|          | 35/6000 [02:00<5:37:27,  3.39s/it]                                                   {'loss': 8.9942, 'grad_norm': 1164.8463134765625, 'learning_rate': 1.75e-06, 'epoch': 0.01}
  1%|          | 35/6000 [02:00<5:37:27,  3.39s/it]  1%|          | 36/6000 [02:03<5:35:00,  3.37s/it]                                                   {'loss': 9.3667, 'grad_norm': 1030.58203125, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.01}
  1%|          | 36/6000 [02:03<5:35:00,  3.37s/it]  1%|          | 37/6000 [02:06<5:34:09,  3.36s/it]                                                   {'loss': 7.9336, 'grad_norm': 1267.9310302734375, 'learning_rate': 1.85e-06, 'epoch': 0.01}
  1%|          | 37/6000 [02:06<5:34:09,  3.36s/it]  1%|          | 38/6000 [02:10<5:31:50,  3.34s/it]                                                   {'loss': 7.4495, 'grad_norm': 1376.7890625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.01}
  1%|          | 38/6000 [02:10<5:31:50,  3.34s/it]  1%|          | 39/6000 [02:13<5:31:06,  3.33s/it]                                                   {'loss': 6.9524, 'grad_norm': 1119.8052978515625, 'learning_rate': 1.9500000000000004e-06, 'epoch': 0.01}
  1%|          | 39/6000 [02:13<5:31:06,  3.33s/it]  1%|          | 40/6000 [02:16<5:32:03,  3.34s/it]                                                   {'loss': 8.298, 'grad_norm': 1056.2332763671875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
  1%|          | 40/6000 [02:16<5:32:03,  3.34s/it]  1%|          | 41/6000 [02:20<5:31:38,  3.34s/it]                                                   {'loss': 7.2056, 'grad_norm': 1202.590576171875, 'learning_rate': 2.05e-06, 'epoch': 0.01}
  1%|          | 41/6000 [02:20<5:31:38,  3.34s/it]  1%|          | 42/6000 [02:23<5:30:18,  3.33s/it]                                                   {'loss': 5.701, 'grad_norm': 865.3931884765625, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:23<5:30:18,  3.33s/it]  1%|          | 43/6000 [02:27<6:02:51,  3.65s/it]                                                   {'loss': 5.1682, 'grad_norm': 820.8139038085938, 'learning_rate': 2.15e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:27<6:02:51,  3.65s/it]  1%|          | 44/6000 [02:31<6:08:04,  3.71s/it]                                                   {'loss': 5.1255, 'grad_norm': 1421.4185791015625, 'learning_rate': 2.2e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:31<6:08:04,  3.71s/it]  1%|          | 45/6000 [02:35<5:59:46,  3.62s/it]                                                   {'loss': 5.9947, 'grad_norm': 1329.96875, 'learning_rate': 2.25e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:35<5:59:46,  3.62s/it]  1%|          | 46/6000 [02:38<5:54:20,  3.57s/it]                                                   {'loss': 5.7061, 'grad_norm': 2376.856689453125, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:38<5:54:20,  3.57s/it]  1%|          | 47/6000 [02:41<5:46:50,  3.50s/it]                                                   {'loss': 5.918, 'grad_norm': 1232.51708984375, 'learning_rate': 2.35e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:41<5:46:50,  3.50s/it]  1%|          | 48/6000 [02:45<5:42:26,  3.45s/it]                                                   {'loss': 6.4908, 'grad_norm': 1330.5762939453125, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:45<5:42:26,  3.45s/it]  1%|          | 49/6000 [02:48<5:36:48,  3.40s/it]                                                   {'loss': 6.2314, 'grad_norm': 1371.4637451171875, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:48<5:36:48,  3.40s/it]  1%|          | 50/6000 [02:51<5:40:17,  3.43s/it]                                                   {'loss': 5.1978, 'grad_norm': 1402.85791015625, 'learning_rate': 2.5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:51<5:40:17,  3.43s/it][2025-11-03 14:32:44,630] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:57<6:42:40,  4.06s/it]                                                   {'loss': 5.7606, 'grad_norm': 1049.1556396484375, 'learning_rate': 2.55e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:57<6:42:40,  4.06s/it]  1%|          | 52/6000 [03:00<6:20:19,  3.84s/it]                                                   {'loss': 6.2325, 'grad_norm': 787.6621704101562, 'learning_rate': 2.6e-06, 'epoch': 0.01}
  1%|          | 52/6000 [03:00<6:20:19,  3.84s/it]  1%|          | 53/6000 [03:04<6:06:28,  3.70s/it]                                                   {'loss': 6.487, 'grad_norm': 894.0127563476562, 'learning_rate': 2.6500000000000005e-06, 'epoch': 0.01}
  1%|          | 53/6000 [03:04<6:06:28,  3.70s/it]  1%|          | 54/6000 [03:07<5:55:29,  3.59s/it]                                                   {'loss': 4.8111, 'grad_norm': 1233.5191650390625, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.01}
  1%|          | 54/6000 [03:07<5:55:29,  3.59s/it]  1%|          | 55/6000 [03:10<5:49:05,  3.52s/it]                                                   {'loss': 4.9361, 'grad_norm': 757.3134155273438, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.01}
  1%|          | 55/6000 [03:10<5:49:05,  3.52s/it]  1%|          | 56/6000 [03:14<5:44:18,  3.48s/it]                                                   {'loss': 5.1562, 'grad_norm': 775.13134765625, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.01}
  1%|          | 56/6000 [03:14<5:44:18,  3.48s/it]  1%|          | 57/6000 [03:17<5:41:07,  3.44s/it]                                                   {'loss': 4.8287, 'grad_norm': 616.7665405273438, 'learning_rate': 2.85e-06, 'epoch': 0.01}
  1%|          | 57/6000 [03:17<5:41:07,  3.44s/it]  1%|          | 58/6000 [03:20<5:38:21,  3.42s/it]                                                   {'loss': 4.446, 'grad_norm': 632.9931640625, 'learning_rate': 2.9e-06, 'epoch': 0.01}
  1%|          | 58/6000 [03:20<5:38:21,  3.42s/it]  1%|          | 59/6000 [03:24<5:33:05,  3.36s/it]                                                   {'loss': 5.0378, 'grad_norm': 1010.257080078125, 'learning_rate': 2.95e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:24<5:33:05,  3.36s/it]  1%|          | 60/6000 [03:27<5:34:11,  3.38s/it]                                                   {'loss': 3.68, 'grad_norm': 750.7659301757812, 'learning_rate': 3e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:27<5:34:11,  3.38s/it]  1%|          | 61/6000 [03:30<5:32:17,  3.36s/it]                                                   {'loss': 3.3678, 'grad_norm': 748.7987670898438, 'learning_rate': 3.05e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:30<5:32:17,  3.36s/it]  1%|          | 62/6000 [03:34<5:30:55,  3.34s/it]                                                   {'loss': 3.8999, 'grad_norm': 515.8223266601562, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:34<5:30:55,  3.34s/it]  1%|          | 63/6000 [03:37<5:32:11,  3.36s/it]                                                   {'loss': 4.0015, 'grad_norm': 605.01123046875, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:37<5:32:11,  3.36s/it]  1%|          | 64/6000 [03:40<5:30:47,  3.34s/it]                                                   {'loss': 3.2046, 'grad_norm': 479.5261535644531, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:40<5:30:47,  3.34s/it]  1%|          | 65/6000 [03:44<5:29:45,  3.33s/it]                                                   {'loss': 3.3088, 'grad_norm': 591.6235961914062, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:44<5:29:45,  3.33s/it]  1%|          | 66/6000 [03:48<5:44:21,  3.48s/it]                                                   {'loss': 4.7616, 'grad_norm': 892.17138671875, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:48<5:44:21,  3.48s/it]  1%|          | 67/6000 [03:51<5:39:45,  3.44s/it]                                                   {'loss': 3.4345, 'grad_norm': 836.6680297851562, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:51<5:39:45,  3.44s/it]  1%|          | 68/6000 [03:54<5:34:31,  3.38s/it]                                                   {'loss': 3.0935, 'grad_norm': 625.9556884765625, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:54<5:34:31,  3.38s/it]  1%|          | 69/6000 [03:57<5:32:12,  3.36s/it]                                                   {'loss': 2.9292, 'grad_norm': 663.4773559570312, 'learning_rate': 3.45e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:57<5:32:12,  3.36s/it]  1%|          | 70/6000 [04:01<5:42:14,  3.46s/it]                                                   {'loss': 2.7659, 'grad_norm': 478.3918762207031, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 70/6000 [04:01<5:42:14,  3.46s/it]  1%|          | 71/6000 [04:04<5:37:23,  3.41s/it]                                                   {'loss': 3.0636, 'grad_norm': 486.6505126953125, 'learning_rate': 3.5500000000000003e-06, 'epoch': 0.01}
  1%|          | 71/6000 [04:04<5:37:23,  3.41s/it]  1%|          | 72/6000 [04:08<5:46:57,  3.51s/it]                                                   {'loss': 2.979, 'grad_norm': 748.768798828125, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 72/6000 [04:08<5:46:57,  3.51s/it]  1%|          | 73/6000 [04:12<5:45:15,  3.50s/it]                                                   {'loss': 2.3206, 'grad_norm': 455.3871154785156, 'learning_rate': 3.65e-06, 'epoch': 0.01}
  1%|          | 73/6000 [04:12<5:45:15,  3.50s/it]  1%|          | 74/6000 [04:15<5:38:39,  3.43s/it]                                                   {'loss': 2.8533, 'grad_norm': 869.8031616210938, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 74/6000 [04:15<5:38:39,  3.43s/it]  1%|â–         | 75/6000 [04:18<5:35:00,  3.39s/it]                                                   {'loss': 2.0013, 'grad_norm': 774.8616943359375, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [04:18<5:35:00,  3.39s/it]  1%|â–         | 76/6000 [04:22<5:37:17,  3.42s/it]                                                   {'loss': 2.5937, 'grad_norm': 648.8207397460938, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [04:22<5:37:17,  3.42s/it]  1%|â–         | 77/6000 [04:25<5:36:06,  3.40s/it]                                                   {'loss': 3.4404, 'grad_norm': 601.9776000976562, 'learning_rate': 3.85e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [04:25<5:36:06,  3.40s/it]  1%|â–         | 78/6000 [04:29<5:46:47,  3.51s/it]                                                   {'loss': 2.5104, 'grad_norm': 800.2304077148438, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [04:29<5:46:47,  3.51s/it]  1%|â–         | 79/6000 [04:32<5:41:44,  3.46s/it]                                                   {'loss': 2.6388, 'grad_norm': 671.0933227539062, 'learning_rate': 3.95e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [04:32<5:41:44,  3.46s/it]  1%|â–         | 80/6000 [04:36<5:38:06,  3.43s/it]                                                   {'loss': 2.4875, 'grad_norm': 531.7750854492188, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:36<5:38:06,  3.43s/it]  1%|â–         | 81/6000 [04:39<5:33:53,  3.38s/it]                                                   {'loss': 2.4759, 'grad_norm': 755.342041015625, 'learning_rate': 4.05e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:39<5:33:53,  3.38s/it]  1%|â–         | 82/6000 [04:42<5:33:49,  3.38s/it]                                                   {'loss': 2.4637, 'grad_norm': 1106.0294189453125, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:42<5:33:49,  3.38s/it]  1%|â–         | 83/6000 [04:46<5:35:46,  3.40s/it]                                                   {'loss': 2.943, 'grad_norm': 1163.1357421875, 'learning_rate': 4.15e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:46<5:35:46,  3.40s/it]  1%|â–         | 84/6000 [04:49<5:36:40,  3.41s/it]                                                   {'loss': 2.1029, 'grad_norm': 530.1162719726562, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:49<5:36:40,  3.41s/it]  1%|â–         | 85/6000 [04:53<5:47:02,  3.52s/it]                                                   {'loss': 2.1849, 'grad_norm': 514.1270141601562, 'learning_rate': 4.25e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:53<5:47:02,  3.52s/it]  1%|â–         | 86/6000 [04:56<5:43:47,  3.49s/it]                                                   {'loss': 2.2819, 'grad_norm': 661.9259033203125, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:56<5:43:47,  3.49s/it]  1%|â–         | 87/6000 [05:00<5:37:25,  3.42s/it]                                                   {'loss': 2.28, 'grad_norm': 700.7855224609375, 'learning_rate': 4.350000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [05:00<5:37:25,  3.42s/it]  1%|â–         | 88/6000 [05:03<5:32:54,  3.38s/it]                                                   {'loss': 2.0082, 'grad_norm': 595.9474487304688, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [05:03<5:32:54,  3.38s/it]  1%|â–         | 89/6000 [05:06<5:32:09,  3.37s/it]                                                   {'loss': 1.6217, 'grad_norm': 622.8410034179688, 'learning_rate': 4.450000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [05:06<5:32:09,  3.37s/it]  2%|â–         | 90/6000 [05:10<5:36:05,  3.41s/it]                                                   {'loss': 1.1599, 'grad_norm': 653.6063842773438, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [05:10<5:36:05,  3.41s/it]  2%|â–         | 91/6000 [05:13<5:33:22,  3.39s/it]                                                   {'loss': 2.5501, 'grad_norm': 825.2324829101562, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [05:13<5:33:22,  3.39s/it]  2%|â–         | 92/6000 [05:17<5:43:57,  3.49s/it]                                                   {'loss': 2.1354, 'grad_norm': 784.231689453125, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [05:17<5:43:57,  3.49s/it]  2%|â–         | 93/6000 [05:20<5:45:37,  3.51s/it]                                                   {'loss': 1.7564, 'grad_norm': 586.2081298828125, 'learning_rate': 4.65e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [05:20<5:45:37,  3.51s/it]  2%|â–         | 94/6000 [05:24<5:40:27,  3.46s/it]                                                   {'loss': 1.7213, 'grad_norm': 1068.6781005859375, 'learning_rate': 4.7e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [05:24<5:40:27,  3.46s/it]  2%|â–         | 95/6000 [05:27<5:37:34,  3.43s/it]                                                   {'loss': 1.5056, 'grad_norm': 595.7913818359375, 'learning_rate': 4.75e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [05:27<5:37:34,  3.43s/it]  2%|â–         | 96/6000 [05:30<5:35:09,  3.41s/it]                                                   {'loss': 1.5282, 'grad_norm': 928.710693359375, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [05:30<5:35:09,  3.41s/it]  2%|â–         | 97/6000 [05:34<5:32:56,  3.38s/it]                                                   {'loss': 1.5098, 'grad_norm': 364.9593811035156, 'learning_rate': 4.85e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [05:34<5:32:56,  3.38s/it]  2%|â–         | 98/6000 [05:37<5:32:38,  3.38s/it]                                                   {'loss': 1.165, 'grad_norm': 344.5555114746094, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [05:37<5:32:38,  3.38s/it]  2%|â–         | 99/6000 [05:40<5:30:54,  3.36s/it]                                                   {'loss': 1.4714, 'grad_norm': 312.4606018066406, 'learning_rate': 4.95e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [05:40<5:30:54,  3.36s/it]  2%|â–         | 100/6000 [05:44<5:27:11,  3.33s/it]                                                    {'loss': 1.5929, 'grad_norm': 665.15576171875, 'learning_rate': 5e-06, 'epoch': 0.02}
  2%|â–         | 100/6000 [05:44<5:27:11,  3.33s/it][2025-11-03 14:35:36,827] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [05:50<7:04:09,  4.31s/it]                                                    {'loss': 1.433, 'grad_norm': 502.8454895019531, 'learning_rate': 4.999152542372881e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:50<7:04:09,  4.31s/it]  2%|â–         | 102/6000 [05:54<6:36:55,  4.04s/it]                                                    {'loss': 1.0448, 'grad_norm': 364.45361328125, 'learning_rate': 4.998305084745763e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:54<6:36:55,  4.04s/it]  2%|â–         | 103/6000 [05:57<6:18:50,  3.85s/it]                                                    {'loss': 0.8363, 'grad_norm': 412.8004455566406, 'learning_rate': 4.9974576271186445e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:57<6:18:50,  3.85s/it]  2%|â–         | 104/6000 [06:01<6:10:37,  3.77s/it]                                                    {'loss': 1.047, 'grad_norm': 372.3655700683594, 'learning_rate': 4.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [06:01<6:10:37,  3.77s/it]  2%|â–         | 105/6000 [06:04<6:00:12,  3.67s/it]                                                    {'loss': 0.8735, 'grad_norm': 558.3289184570312, 'learning_rate': 4.995762711864407e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [06:04<6:00:12,  3.67s/it]  2%|â–         | 106/6000 [06:08<5:55:47,  3.62s/it]                                                    {'loss': 0.9939, 'grad_norm': 975.4159545898438, 'learning_rate': 4.9949152542372885e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [06:08<5:55:47,  3.62s/it]  2%|â–         | 107/6000 [06:11<5:46:30,  3.53s/it]                                                    {'loss': 0.9882, 'grad_norm': 556.678955078125, 'learning_rate': 4.994067796610169e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [06:11<5:46:30,  3.53s/it]  2%|â–         | 108/6000 [06:14<5:46:09,  3.53s/it]                                                    {'loss': 0.8445, 'grad_norm': 354.15771484375, 'learning_rate': 4.993220338983051e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [06:14<5:46:09,  3.53s/it]  2%|â–         | 109/6000 [06:18<5:54:07,  3.61s/it]                                                    {'loss': 0.9683, 'grad_norm': 377.6543884277344, 'learning_rate': 4.992372881355933e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [06:18<5:54:07,  3.61s/it]  2%|â–         | 110/6000 [06:22<5:46:44,  3.53s/it]                                                    {'loss': 0.3613, 'grad_norm': 271.5955810546875, 'learning_rate': 4.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [06:22<5:46:44,  3.53s/it]  2%|â–         | 111/6000 [06:25<5:45:24,  3.52s/it]                                                    {'loss': 0.3988, 'grad_norm': 199.3945770263672, 'learning_rate': 4.990677966101695e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [06:25<5:45:24,  3.52s/it]  2%|â–         | 112/6000 [06:29<5:57:20,  3.64s/it]                                                    {'loss': 0.9362, 'grad_norm': 381.5223083496094, 'learning_rate': 4.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [06:29<5:57:20,  3.64s/it]  2%|â–         | 113/6000 [06:32<5:51:53,  3.59s/it]                                                    {'loss': 0.4304, 'grad_norm': 431.7914733886719, 'learning_rate': 4.988983050847458e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [06:32<5:51:53,  3.59s/it]  2%|â–         | 114/6000 [06:36<5:45:30,  3.52s/it]                                                    {'loss': 0.8454, 'grad_norm': 344.70892333984375, 'learning_rate': 4.98813559322034e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [06:36<5:45:30,  3.52s/it]  2%|â–         | 115/6000 [06:39<5:41:16,  3.48s/it]                                                    {'loss': 0.6764, 'grad_norm': 153.2080078125, 'learning_rate': 4.987288135593221e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [06:39<5:41:16,  3.48s/it]  2%|â–         | 116/6000 [06:43<5:36:57,  3.44s/it]                                                    {'loss': 0.2943, 'grad_norm': 425.0556640625, 'learning_rate': 4.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [06:43<5:36:57,  3.44s/it]  2%|â–         | 117/6000 [06:46<5:35:17,  3.42s/it]                                                    {'loss': 1.1023, 'grad_norm': 363.3475646972656, 'learning_rate': 4.985593220338983e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [06:46<5:35:17,  3.42s/it]  2%|â–         | 118/6000 [06:50<5:52:41,  3.60s/it]                                                    {'loss': 0.7317, 'grad_norm': 238.17599487304688, 'learning_rate': 4.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:50<5:52:41,  3.60s/it]  2%|â–         | 119/6000 [06:53<5:43:40,  3.51s/it]                                                    {'loss': 0.7446, 'grad_norm': 414.0502624511719, 'learning_rate': 4.9838983050847464e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:53<5:43:40,  3.51s/it]  2%|â–         | 120/6000 [06:57<5:38:22,  3.45s/it]                                                    {'loss': 0.3723, 'grad_norm': 313.2390441894531, 'learning_rate': 4.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:57<5:38:22,  3.45s/it]  2%|â–         | 121/6000 [07:00<5:35:29,  3.42s/it]                                                    {'loss': 0.2859, 'grad_norm': 265.9617004394531, 'learning_rate': 4.982203389830509e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [07:00<5:35:29,  3.42s/it]  2%|â–         | 122/6000 [07:03<5:38:08,  3.45s/it]                                                    {'loss': 0.5539, 'grad_norm': 277.8022766113281, 'learning_rate': 4.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [07:03<5:38:08,  3.45s/it]  2%|â–         | 123/6000 [07:07<5:33:27,  3.40s/it]                                                    {'loss': 0.3005, 'grad_norm': 354.8773193359375, 'learning_rate': 4.980508474576271e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [07:07<5:33:27,  3.40s/it]  2%|â–         | 124/6000 [07:10<5:32:30,  3.40s/it]                                                    {'loss': 0.2418, 'grad_norm': 123.9896240234375, 'learning_rate': 4.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [07:10<5:32:30,  3.40s/it]  2%|â–         | 125/6000 [07:14<5:38:43,  3.46s/it]                                                    {'loss': 0.0713, 'grad_norm': 38.493648529052734, 'learning_rate': 4.9788135593220346e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [07:14<5:38:43,  3.46s/it]  2%|â–         | 126/6000 [07:17<5:38:15,  3.46s/it]                                                    {'loss': 0.4885, 'grad_norm': 441.4180603027344, 'learning_rate': 4.977966101694915e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [07:17<5:38:15,  3.46s/it]  2%|â–         | 127/6000 [07:21<5:40:43,  3.48s/it]                                                    {'loss': 0.5759, 'grad_norm': 88.13349914550781, 'learning_rate': 4.977118644067797e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [07:21<5:40:43,  3.48s/it]  2%|â–         | 128/6000 [07:24<5:35:49,  3.43s/it]                                                    {'loss': 0.5612, 'grad_norm': 256.0809631347656, 'learning_rate': 4.976271186440678e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [07:24<5:35:49,  3.43s/it]  2%|â–         | 129/6000 [07:27<5:33:41,  3.41s/it]                                                    {'loss': 0.2957, 'grad_norm': 153.12200927734375, 'learning_rate': 4.97542372881356e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [07:27<5:33:41,  3.41s/it]  2%|â–         | 130/6000 [07:31<5:32:32,  3.40s/it]                                                    {'loss': 0.2101, 'grad_norm': 134.22840881347656, 'learning_rate': 4.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [07:31<5:32:32,  3.40s/it]  2%|â–         | 131/6000 [07:34<5:30:47,  3.38s/it]                                                    {'loss': 0.3763, 'grad_norm': 405.35430908203125, 'learning_rate': 4.973728813559323e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [07:34<5:30:47,  3.38s/it]  2%|â–         | 132/6000 [07:37<5:28:24,  3.36s/it]                                                    {'loss': 0.1582, 'grad_norm': 156.36134338378906, 'learning_rate': 4.9728813559322035e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [07:38<5:28:24,  3.36s/it]  2%|â–         | 133/6000 [07:41<5:33:50,  3.41s/it]                                                    {'loss': 0.3644, 'grad_norm': 136.42918395996094, 'learning_rate': 4.972033898305085e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [07:41<5:33:50,  3.41s/it]  2%|â–         | 134/6000 [07:45<5:39:24,  3.47s/it]                                                    {'loss': 0.1863, 'grad_norm': 103.68827819824219, 'learning_rate': 4.971186440677967e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [07:45<5:39:24,  3.47s/it]  2%|â–         | 135/6000 [07:48<5:34:51,  3.43s/it]                                                    {'loss': 1.0009, 'grad_norm': 263.8162841796875, 'learning_rate': 4.970338983050848e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [07:48<5:34:51,  3.43s/it]  2%|â–         | 136/6000 [07:51<5:31:21,  3.39s/it]                                                    {'loss': 0.2357, 'grad_norm': 160.0003662109375, 'learning_rate': 4.969491525423729e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [07:51<5:31:21,  3.39s/it]  2%|â–         | 137/6000 [07:55<5:45:14,  3.53s/it]                                                    {'loss': 0.4933, 'grad_norm': 118.53752136230469, 'learning_rate': 4.968644067796611e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [07:55<5:45:14,  3.53s/it]  2%|â–         | 138/6000 [07:58<5:38:09,  3.46s/it]                                                    {'loss': 0.2743, 'grad_norm': 143.8336639404297, 'learning_rate': 4.967796610169492e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [07:58<5:38:09,  3.46s/it]  2%|â–         | 139/6000 [08:02<5:40:32,  3.49s/it]                                                    {'loss': 0.4869, 'grad_norm': 180.76898193359375, 'learning_rate': 4.966949152542373e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [08:02<5:40:32,  3.49s/it]  2%|â–         | 140/6000 [08:06<5:50:02,  3.58s/it]                                                    {'loss': 0.3069, 'grad_norm': 91.35289764404297, 'learning_rate': 4.966101694915255e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [08:06<5:50:02,  3.58s/it]  2%|â–         | 141/6000 [08:09<5:43:49,  3.52s/it]                                                    {'loss': 0.321, 'grad_norm': 90.55115509033203, 'learning_rate': 4.9652542372881365e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [08:09<5:43:49,  3.52s/it]  2%|â–         | 142/6000 [08:12<5:38:02,  3.46s/it]                                                    {'loss': 0.4538, 'grad_norm': 230.78167724609375, 'learning_rate': 4.964406779661017e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [08:12<5:38:02,  3.46s/it]  2%|â–         | 143/6000 [08:16<5:33:11,  3.41s/it]                                                    {'loss': 0.3962, 'grad_norm': 180.7334747314453, 'learning_rate': 4.963559322033898e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [08:16<5:33:11,  3.41s/it]  2%|â–         | 144/6000 [08:19<5:32:27,  3.41s/it]                                                    {'loss': 0.3213, 'grad_norm': 261.6407470703125, 'learning_rate': 4.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [08:19<5:32:27,  3.41s/it]  2%|â–         | 145/6000 [08:23<5:34:06,  3.42s/it]                                                    {'loss': 0.1243, 'grad_norm': 75.65681457519531, 'learning_rate': 4.961864406779661e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [08:23<5:34:06,  3.42s/it]  2%|â–         | 146/6000 [08:26<5:34:25,  3.43s/it]                                                    {'loss': 0.284, 'grad_norm': 268.6338195800781, 'learning_rate': 4.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [08:26<5:34:25,  3.43s/it]  2%|â–         | 147/6000 [08:29<5:30:05,  3.38s/it]                                                    {'loss': 0.1783, 'grad_norm': 55.43607711791992, 'learning_rate': 4.960169491525424e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [08:29<5:30:05,  3.38s/it]  2%|â–         | 148/6000 [08:33<5:30:39,  3.39s/it]                                                    {'loss': 0.2177, 'grad_norm': 117.81158447265625, 'learning_rate': 4.9593220338983054e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [08:33<5:30:39,  3.39s/it]  2%|â–         | 149/6000 [08:36<5:27:22,  3.36s/it]                                                    {'loss': 0.3944, 'grad_norm': 271.6236572265625, 'learning_rate': 4.958474576271187e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [08:36<5:27:22,  3.36s/it]  2%|â–Ž         | 150/6000 [08:39<5:28:47,  3.37s/it]                                                    {'loss': 0.041, 'grad_norm': 20.602148056030273, 'learning_rate': 4.957627118644069e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [08:39<5:28:47,  3.37s/it][2025-11-03 14:38:32,519] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Alibaba-NLP/gme-Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [08:45<6:32:50,  4.03s/it]                                                    {'loss': 0.1786, 'grad_norm': 86.41290283203125, 'learning_rate': 4.9567796610169495e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [08:45<6:32:50,  4.03s/it]  3%|â–Ž         | 152/6000 [08:48<6:14:27,  3.84s/it]                                                    {'loss': 0.0813, 'grad_norm': 66.48502349853516, 'learning_rate': 4.955932203389831e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [08:48<6:14:27,  3.84s/it]  3%|â–Ž         | 153/6000 [08:52<5:57:53,  3.67s/it]                                                    {'loss': 0.0294, 'grad_norm': 35.8929557800293, 'learning_rate': 4.955084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [08:52<5:57:53,  3.67s/it]  3%|â–Ž         | 154/6000 [08:55<5:50:35,  3.60s/it]                                                    {'loss': 0.2744, 'grad_norm': 83.2088851928711, 'learning_rate': 4.9542372881355936e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [08:55<5:50:35,  3.60s/it]  3%|â–Ž         | 155/6000 [08:58<5:43:14,  3.52s/it]                                                    {'loss': 0.0676, 'grad_norm': 37.19756317138672, 'learning_rate': 4.953389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [08:58<5:43:14,  3.52s/it]  3%|â–Ž         | 156/6000 [09:02<5:40:50,  3.50s/it]                                                    {'loss': 0.1179, 'grad_norm': 75.15348052978516, 'learning_rate': 4.952542372881357e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [09:02<5:40:50,  3.50s/it]  3%|â–Ž         | 157/6000 [09:06<5:52:26,  3.62s/it]                                                    {'loss': 0.4484, 'grad_norm': 406.5030517578125, 'learning_rate': 4.951694915254238e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [09:06<5:52:26,  3.62s/it]  3%|â–Ž         | 158/6000 [09:09<5:42:14,  3.51s/it]                                                    {'loss': 0.0305, 'grad_norm': 30.25627326965332, 'learning_rate': 4.950847457627119e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [09:09<5:42:14,  3.51s/it]  3%|â–Ž         | 159/6000 [09:12<5:36:36,  3.46s/it]                                                    {'loss': 0.2973, 'grad_norm': 64.2966079711914, 'learning_rate': 4.95e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [09:12<5:36:36,  3.46s/it]  3%|â–Ž         | 160/6000 [09:16<5:52:52,  3.63s/it]                                                    {'loss': 0.2579, 'grad_norm': 97.46253967285156, 'learning_rate': 4.949152542372882e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [09:16<5:52:52,  3.63s/it]  3%|â–Ž         | 161/6000 [09:20<5:48:08,  3.58s/it]                                                    {'loss': 0.1802, 'grad_norm': 77.35051727294922, 'learning_rate': 4.948305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [09:20<5:48:08,  3.58s/it]  3%|â–Ž         | 162/6000 [09:23<5:44:11,  3.54s/it]                                                    {'loss': 0.0468, 'grad_norm': 36.82339859008789, 'learning_rate': 4.947457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [09:23<5:44:11,  3.54s/it]  3%|â–Ž         | 163/6000 [09:27<5:51:36,  3.61s/it]                                                    {'loss': 0.0715, 'grad_norm': 93.25054168701172, 'learning_rate': 4.946610169491526e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [09:27<5:51:36,  3.61s/it]  3%|â–Ž         | 164/6000 [09:30<5:43:01,  3.53s/it]                                                    {'loss': 0.2055, 'grad_norm': 101.63197326660156, 'learning_rate': 4.9457627118644065e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [09:30<5:43:01,  3.53s/it]  3%|â–Ž         | 165/6000 [09:34<5:42:56,  3.53s/it]                                                    {'loss': 0.0665, 'grad_norm': 74.24702453613281, 'learning_rate': 4.944915254237288e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [09:34<5:42:56,  3.53s/it]  3%|â–Ž         | 166/6000 [09:37<5:35:45,  3.45s/it]                                                    {'loss': 0.3575, 'grad_norm': 195.19570922851562, 'learning_rate': 4.94406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [09:37<5:35:45,  3.45s/it]  3%|â–Ž         | 167/6000 [09:40<5:32:18,  3.42s/it]                                                    {'loss': 0.1212, 'grad_norm': 74.59613037109375, 'learning_rate': 4.9432203389830514e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [09:40<5:32:18,  3.42s/it]  3%|â–Ž         | 168/6000 [09:44<5:30:54,  3.40s/it]                                                    {'loss': 0.1524, 'grad_norm': 319.0818786621094, 'learning_rate': 4.942372881355932e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [09:44<5:30:54,  3.40s/it]  3%|â–Ž         | 169/6000 [09:48<5:42:19,  3.52s/it]                                                    {'loss': 0.3116, 'grad_norm': 176.8993377685547, 'learning_rate': 4.941525423728814e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [09:48<5:42:19,  3.52s/it]  3%|â–Ž         | 170/6000 [09:51<5:35:56,  3.46s/it]                                                    {'loss': 0.2646, 'grad_norm': 93.36994934082031, 'learning_rate': 4.9406779661016955e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [09:51<5:35:56,  3.46s/it]  3%|â–Ž         | 171/6000 [09:54<5:30:11,  3.40s/it]                                                    {'loss': 0.1462, 'grad_norm': 83.65017700195312, 'learning_rate': 4.939830508474577e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [09:54<5:30:11,  3.40s/it]  3%|â–Ž         | 172/6000 [09:58<5:33:14,  3.43s/it]                                                    {'loss': 0.2763, 'grad_norm': 103.56834411621094, 'learning_rate': 4.938983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [09:58<5:33:14,  3.43s/it]  3%|â–Ž         | 173/6000 [10:01<5:32:32,  3.42s/it]                                                    {'loss': 0.1892, 'grad_norm': 66.31851959228516, 'learning_rate': 4.9381355932203396e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [10:01<5:32:32,  3.42s/it]  3%|â–Ž         | 174/6000 [10:05<5:53:36,  3.64s/it]                                                    {'loss': 0.1425, 'grad_norm': 113.08151245117188, 'learning_rate': 4.93728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [10:05<5:53:36,  3.64s/it]  3%|â–Ž         | 175/6000 [10:09<5:46:39,  3.57s/it]                                                    {'loss': 0.2264, 'grad_norm': 57.44486999511719, 'learning_rate': 4.936440677966102e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [10:09<5:46:39,  3.57s/it]  3%|â–Ž         | 176/6000 [10:12<5:50:40,  3.61s/it]                                                    {'loss': 0.1716, 'grad_norm': 68.5137939453125, 'learning_rate': 4.935593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [10:12<5:50:40,  3.61s/it]  3%|â–Ž         | 177/6000 [10:16<5:42:30,  3.53s/it]                                                    {'loss': 0.0382, 'grad_norm': 22.41349983215332, 'learning_rate': 4.934745762711865e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [10:16<5:42:30,  3.53s/it]  3%|â–Ž         | 178/6000 [10:19<5:38:26,  3.49s/it]                                                    {'loss': 0.0564, 'grad_norm': 52.86204147338867, 'learning_rate': 4.933898305084746e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [10:19<5:38:26,  3.49s/it]  3%|â–Ž         | 179/6000 [10:22<5:34:33,  3.45s/it]                                                    {'loss': 0.1714, 'grad_norm': 69.95741271972656, 'learning_rate': 4.933050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [10:22<5:34:33,  3.45s/it]  3%|â–Ž         | 180/6000 [10:26<5:35:01,  3.45s/it]                                                    {'loss': 0.3918, 'grad_norm': 95.86277770996094, 'learning_rate': 4.9322033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [10:26<5:35:01,  3.45s/it]  3%|â–Ž         | 181/6000 [10:29<5:32:09,  3.42s/it]                                                    {'loss': 0.502, 'grad_norm': 71.67239379882812, 'learning_rate': 4.93135593220339e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [10:29<5:32:09,  3.42s/it]  3%|â–Ž         | 182/6000 [10:33<5:30:09,  3.40s/it]                                                    {'loss': 0.1289, 'grad_norm': 68.84673309326172, 'learning_rate': 4.930508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [10:33<5:30:09,  3.40s/it]