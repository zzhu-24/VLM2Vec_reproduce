==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailToken-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 8 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 5000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/train.log
W1019 17:24:19.111000 126329717585728 torch/distributed/run.py:779] 
W1019 17:24:19.111000 126329717585728 torch/distributed/run.py:779] *****************************************
W1019 17:24:19.111000 126329717585728 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1019 17:24:19.111000 126329717585728 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-19 17:24:28,701] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.09it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.89it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_172429-wpc6mond
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailToken-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/wpc6mond
[2025-10-19 17:24:30,266] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.22it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.09it/s]
[2025-10-19 17:24:30,882] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-19 17:24:30,887] INFO [src.utils:19] Loading lora adapter from TailTokenWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-19 17:24:39,851] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-19 17:24:41,088] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-19 17:24:41,088] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-19 17:24:45,820] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-19 17:24:45,821] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-19 17:24:46,639] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-19 17:24:46,640] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-19 17:24:46,641] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=16
		estimated num step per epoch=13330.4375
		interleave_batch_size=0.0
[2025-10-19 17:24:46,642] INFO [src.utils:19] ==================================================
[2025-10-19 17:24:46,643] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-19 17:24:46,644] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:24:46,645] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 17:24:46,645] INFO [src.utils:19] ==================================================
[2025-10-19 17:24:47,922] INFO [src.utils:19] âœ… Custom optimizer (gme.learnable_token only) enabled
[2025-10-19 17:24:48,376] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:24:48,376] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 17:24:48,376] INFO [src.trainer:343]   Num examples = 80,000
[2025-10-19 17:24:48,376] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:24:48,376] INFO [src.trainer:345]   Instantaneous batch size per device = 8
[2025-10-19 17:24:48,376] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 16
[2025-10-19 17:24:48,376] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:24:48,376] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:24:48,376] INFO [src.trainer:343]   Num examples = 80,000
[2025-10-19 17:24:48,377] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 17:24:48,377] INFO [src.trainer:345]   Instantaneous batch size per device = 8
[2025-10-19 17:24:48,377] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 16
[2025-10-19 17:24:48,378] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 17:24:48,378] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 17:24:48,387] INFO [src.trainer:351]   Number of trainable parameters = 1,536
[2025-10-19 17:24:48,390] INFO [src.trainer:351]   Number of trainable parameters = 1,536
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1019 17:24:50.182277452 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1019 17:24:50.199179658 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/5000 [00:02<3:18:03,  2.38s/it]                                                  {'loss': 19.6585, 'grad_norm': 24.411909103393555, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 1/5000 [00:02<3:18:03,  2.38s/it]  0%|          | 2/5000 [00:03<2:20:04,  1.68s/it]                                                  {'loss': 18.4614, 'grad_norm': 25.63677406311035, 'learning_rate': 2e-05, 'epoch': 0.0}
  0%|          | 2/5000 [00:03<2:20:04,  1.68s/it]  0%|          | 3/5000 [00:04<2:01:29,  1.46s/it]                                                  {'loss': 14.7357, 'grad_norm': 24.69359016418457, 'learning_rate': 3e-05, 'epoch': 0.0}
  0%|          | 3/5000 [00:04<2:01:29,  1.46s/it]  0%|          | 4/5000 [00:05<1:53:26,  1.36s/it]                                                  {'loss': 17.2893, 'grad_norm': 24.21335792541504, 'learning_rate': 4e-05, 'epoch': 0.0}
  0%|          | 4/5000 [00:05<1:53:26,  1.36s/it]  0%|          | 5/5000 [00:07<1:51:22,  1.34s/it]                                                  {'loss': 16.5666, 'grad_norm': 25.636960983276367, 'learning_rate': 5e-05, 'epoch': 0.0}
  0%|          | 5/5000 [00:07<1:51:22,  1.34s/it]  0%|          | 6/5000 [00:08<1:47:18,  1.29s/it]                                                  {'loss': 15.7364, 'grad_norm': 21.559959411621094, 'learning_rate': 6e-05, 'epoch': 0.0}
  0%|          | 6/5000 [00:08<1:47:18,  1.29s/it]  0%|          | 7/5000 [00:09<1:44:19,  1.25s/it]                                                  {'loss': 17.8119, 'grad_norm': 23.57635498046875, 'learning_rate': 7.000000000000001e-05, 'epoch': 0.0}
  0%|          | 7/5000 [00:09<1:44:19,  1.25s/it]  0%|          | 8/5000 [00:10<1:45:15,  1.27s/it]                                                  {'loss': 14.9774, 'grad_norm': 24.84943962097168, 'learning_rate': 8e-05, 'epoch': 0.0}
  0%|          | 8/5000 [00:10<1:45:15,  1.27s/it]  0%|          | 9/5000 [00:12<1:44:15,  1.25s/it]                                                  {'loss': 15.1135, 'grad_norm': 22.914766311645508, 'learning_rate': 8.999999999999999e-05, 'epoch': 0.0}
  0%|          | 9/5000 [00:12<1:44:15,  1.25s/it]  0%|          | 10/5000 [00:13<1:43:55,  1.25s/it]                                                   {'loss': 18.2394, 'grad_norm': 22.983787536621094, 'learning_rate': 0.0001, 'epoch': 0.0}
  0%|          | 10/5000 [00:13<1:43:55,  1.25s/it]  0%|          | 11/5000 [00:14<1:44:02,  1.25s/it]                                                   {'loss': 16.553, 'grad_norm': 23.003915786743164, 'learning_rate': 0.00011, 'epoch': 0.0}
  0%|          | 11/5000 [00:14<1:44:02,  1.25s/it]  0%|          | 12/5000 [00:15<1:42:27,  1.23s/it]                                                   {'loss': 18.8148, 'grad_norm': 24.536602020263672, 'learning_rate': 0.00012, 'epoch': 0.0}
  0%|          | 12/5000 [00:15<1:42:27,  1.23s/it]  0%|          | 13/5000 [00:17<1:43:21,  1.24s/it]                                                   {'loss': 17.7265, 'grad_norm': 23.13918685913086, 'learning_rate': 0.00013000000000000002, 'epoch': 0.0}
  0%|          | 13/5000 [00:17<1:43:21,  1.24s/it]  0%|          | 14/5000 [00:18<1:43:15,  1.24s/it]                                                   {'loss': 17.0795, 'grad_norm': 22.69755744934082, 'learning_rate': 0.00014000000000000001, 'epoch': 0.0}
  0%|          | 14/5000 [00:18<1:43:15,  1.24s/it]  0%|          | 15/5000 [00:19<1:41:33,  1.22s/it]                                                   {'loss': 15.5849, 'grad_norm': 20.04547882080078, 'learning_rate': 0.00015, 'epoch': 0.0}
  0%|          | 15/5000 [00:19<1:41:33,  1.22s/it]  0%|          | 16/5000 [00:20<1:40:40,  1.21s/it]                                                   {'loss': 17.6408, 'grad_norm': 23.45403289794922, 'learning_rate': 0.00016, 'epoch': 0.0}
  0%|          | 16/5000 [00:20<1:40:40,  1.21s/it]  0%|          | 17/5000 [00:21<1:41:17,  1.22s/it]                                                   {'loss': 13.1543, 'grad_norm': 24.702720642089844, 'learning_rate': 0.00017, 'epoch': 0.0}
  0%|          | 17/5000 [00:21<1:41:17,  1.22s/it]  0%|          | 18/5000 [00:23<1:41:43,  1.23s/it]                                                   {'loss': 15.1997, 'grad_norm': 22.30278205871582, 'learning_rate': 0.00017999999999999998, 'epoch': 0.0}
  0%|          | 18/5000 [00:23<1:41:43,  1.23s/it]  0%|          | 19/5000 [00:24<1:40:06,  1.21s/it]                                                   {'loss': 18.7958, 'grad_norm': 23.782318115234375, 'learning_rate': 0.00019, 'epoch': 0.0}
  0%|          | 19/5000 [00:24<1:40:06,  1.21s/it]  0%|          | 20/5000 [00:25<1:39:53,  1.20s/it]                                                   {'loss': 10.9724, 'grad_norm': 17.278284072875977, 'learning_rate': 0.0002, 'epoch': 0.0}
  0%|          | 20/5000 [00:25<1:39:53,  1.20s/it]  0%|          | 21/5000 [00:26<1:44:06,  1.25s/it]                                                   {'loss': 18.7592, 'grad_norm': 23.32860565185547, 'learning_rate': 0.00021, 'epoch': 0.0}
  0%|          | 21/5000 [00:26<1:44:06,  1.25s/it]  0%|          | 22/5000 [00:28<1:43:30,  1.25s/it]                                                   {'loss': 18.7889, 'grad_norm': 22.67354393005371, 'learning_rate': 0.00022, 'epoch': 0.0}
  0%|          | 22/5000 [00:28<1:43:30,  1.25s/it]  0%|          | 23/5000 [00:29<1:45:09,  1.27s/it]                                                   {'loss': 18.1308, 'grad_norm': 22.07151222229004, 'learning_rate': 0.00023, 'epoch': 0.0}
  0%|          | 23/5000 [00:29<1:45:09,  1.27s/it]  0%|          | 24/5000 [00:30<1:45:24,  1.27s/it]                                                   {'loss': 16.2007, 'grad_norm': 21.576860427856445, 'learning_rate': 0.00024, 'epoch': 0.0}
  0%|          | 24/5000 [00:30<1:45:24,  1.27s/it]  0%|          | 25/5000 [00:32<1:45:53,  1.28s/it]                                                   {'loss': 15.5898, 'grad_norm': 18.871780395507812, 'learning_rate': 0.00025, 'epoch': 0.01}
  0%|          | 25/5000 [00:32<1:45:53,  1.28s/it]  1%|          | 26/5000 [00:33<1:44:30,  1.26s/it]                                                   {'loss': 16.6902, 'grad_norm': 20.35700225830078, 'learning_rate': 0.00026000000000000003, 'epoch': 0.01}
  1%|          | 26/5000 [00:33<1:44:30,  1.26s/it]  1%|          | 27/5000 [00:34<1:45:14,  1.27s/it]                                                   {'loss': 15.6401, 'grad_norm': 20.977041244506836, 'learning_rate': 0.00027, 'epoch': 0.01}
  1%|          | 27/5000 [00:34<1:45:14,  1.27s/it]  1%|          | 28/5000 [00:35<1:44:21,  1.26s/it]                                                   {'loss': 16.686, 'grad_norm': 20.620365142822266, 'learning_rate': 0.00028000000000000003, 'epoch': 0.01}
  1%|          | 28/5000 [00:35<1:44:21,  1.26s/it]  1%|          | 29/5000 [00:37<1:43:47,  1.25s/it]                                                   {'loss': 14.5675, 'grad_norm': 19.52995491027832, 'learning_rate': 0.00029, 'epoch': 0.01}
  1%|          | 29/5000 [00:37<1:43:47,  1.25s/it]  1%|          | 30/5000 [00:38<1:43:05,  1.24s/it]                                                   {'loss': 15.3956, 'grad_norm': 19.05043601989746, 'learning_rate': 0.0003, 'epoch': 0.01}
  1%|          | 30/5000 [00:38<1:43:05,  1.24s/it]  1%|          | 31/5000 [00:39<1:42:33,  1.24s/it]                                                   {'loss': 14.8685, 'grad_norm': 17.2545166015625, 'learning_rate': 0.00031, 'epoch': 0.01}
  1%|          | 31/5000 [00:39<1:42:33,  1.24s/it]  1%|          | 32/5000 [00:40<1:41:30,  1.23s/it]                                                   {'loss': 15.7077, 'grad_norm': 19.39854621887207, 'learning_rate': 0.00032, 'epoch': 0.01}
  1%|          | 32/5000 [00:40<1:41:30,  1.23s/it]  1%|          | 33/5000 [00:41<1:42:13,  1.23s/it]                                                   {'loss': 12.3837, 'grad_norm': 18.03058433532715, 'learning_rate': 0.00033, 'epoch': 0.01}
  1%|          | 33/5000 [00:41<1:42:13,  1.23s/it]  1%|          | 34/5000 [00:43<1:41:24,  1.23s/it]                                                   {'loss': 15.7646, 'grad_norm': 18.00196647644043, 'learning_rate': 0.00034, 'epoch': 0.01}
  1%|          | 34/5000 [00:43<1:41:24,  1.23s/it]  1%|          | 35/5000 [00:44<1:41:23,  1.23s/it]                                                   {'loss': 9.5184, 'grad_norm': 16.266971588134766, 'learning_rate': 0.00035, 'epoch': 0.01}
  1%|          | 35/5000 [00:44<1:41:23,  1.23s/it]  1%|          | 36/5000 [00:45<1:41:50,  1.23s/it]                                                   {'loss': 14.5581, 'grad_norm': 16.768104553222656, 'learning_rate': 0.00035999999999999997, 'epoch': 0.01}
  1%|          | 36/5000 [00:45<1:41:50,  1.23s/it]  1%|          | 37/5000 [00:46<1:40:46,  1.22s/it]                                                   {'loss': 13.3735, 'grad_norm': 17.201995849609375, 'learning_rate': 0.00037, 'epoch': 0.01}
  1%|          | 37/5000 [00:46<1:40:46,  1.22s/it]  1%|          | 38/5000 [00:48<1:40:45,  1.22s/it]                                                   {'loss': 15.512, 'grad_norm': 17.86381721496582, 'learning_rate': 0.00038, 'epoch': 0.01}
  1%|          | 38/5000 [00:48<1:40:45,  1.22s/it]  1%|          | 39/5000 [00:49<1:40:47,  1.22s/it]                                                   {'loss': 15.0688, 'grad_norm': 17.011232376098633, 'learning_rate': 0.00039000000000000005, 'epoch': 0.01}
  1%|          | 39/5000 [00:49<1:40:47,  1.22s/it]  1%|          | 40/5000 [00:50<1:40:36,  1.22s/it]                                                   {'loss': 14.5839, 'grad_norm': 17.519346237182617, 'learning_rate': 0.0004, 'epoch': 0.01}
  1%|          | 40/5000 [00:50<1:40:36,  1.22s/it]  1%|          | 41/5000 [00:51<1:43:04,  1.25s/it]                                                   {'loss': 12.5827, 'grad_norm': 15.145492553710938, 'learning_rate': 0.00041, 'epoch': 0.01}
  1%|          | 41/5000 [00:51<1:43:04,  1.25s/it]  1%|          | 42/5000 [00:53<1:44:12,  1.26s/it]                                                   {'loss': 11.2282, 'grad_norm': 13.782793045043945, 'learning_rate': 0.00042, 'epoch': 0.01}
  1%|          | 42/5000 [00:53<1:44:12,  1.26s/it]  1%|          | 43/5000 [00:54<1:42:14,  1.24s/it]                                                   {'loss': 11.2297, 'grad_norm': 13.116660118103027, 'learning_rate': 0.00043, 'epoch': 0.01}
  1%|          | 43/5000 [00:54<1:42:14,  1.24s/it]  1%|          | 44/5000 [00:55<1:43:19,  1.25s/it]                                                   {'loss': 13.3039, 'grad_norm': 12.992040634155273, 'learning_rate': 0.00044, 'epoch': 0.01}
  1%|          | 44/5000 [00:55<1:43:19,  1.25s/it]  1%|          | 45/5000 [00:56<1:42:53,  1.25s/it]                                                   {'loss': 14.6445, 'grad_norm': 15.058252334594727, 'learning_rate': 0.00045000000000000004, 'epoch': 0.01}
  1%|          | 45/5000 [00:56<1:42:53,  1.25s/it]  1%|          | 46/5000 [00:57<1:41:23,  1.23s/it]                                                   {'loss': 8.6934, 'grad_norm': 12.80859661102295, 'learning_rate': 0.00046, 'epoch': 0.01}
  1%|          | 46/5000 [00:57<1:41:23,  1.23s/it]  1%|          | 47/5000 [00:59<1:42:34,  1.24s/it]                                                   {'loss': 9.2305, 'grad_norm': 13.250036239624023, 'learning_rate': 0.00047, 'epoch': 0.01}
  1%|          | 47/5000 [00:59<1:42:34,  1.24s/it]  1%|          | 48/5000 [01:00<1:41:18,  1.23s/it]                                                   {'loss': 9.5107, 'grad_norm': 11.829731941223145, 'learning_rate': 0.00048, 'epoch': 0.01}
  1%|          | 48/5000 [01:00<1:41:18,  1.23s/it]  1%|          | 49/5000 [01:01<1:40:16,  1.22s/it]                                                   {'loss': 12.5469, 'grad_norm': 11.30003833770752, 'learning_rate': 0.00049, 'epoch': 0.01}
  1%|          | 49/5000 [01:01<1:40:16,  1.22s/it]  1%|          | 50/5000 [01:02<1:40:50,  1.22s/it]                                                   {'loss': 11.2402, 'grad_norm': 11.318585395812988, 'learning_rate': 0.0005, 'epoch': 0.01}
  1%|          | 50/5000 [01:02<1:40:50,  1.22s/it][2025-10-19 17:25:51,430] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/5000 [01:05<2:13:26,  1.62s/it]                                                   {'loss': 12.5674, 'grad_norm': 12.176187515258789, 'learning_rate': 0.00051, 'epoch': 0.01}
  1%|          | 51/5000 [01:05<2:13:26,  1.62s/it]  1%|          | 52/5000 [01:06<2:02:43,  1.49s/it]                                                   {'loss': 11.9301, 'grad_norm': 10.977036476135254, 'learning_rate': 0.0005200000000000001, 'epoch': 0.01}
  1%|          | 52/5000 [01:06<2:02:43,  1.49s/it]  1%|          | 53/5000 [01:07<1:56:11,  1.41s/it]                                                   {'loss': 11.9406, 'grad_norm': 9.97645092010498, 'learning_rate': 0.0005300000000000001, 'epoch': 0.01}
  1%|          | 53/5000 [01:07<1:56:11,  1.41s/it]  1%|          | 54/5000 [01:09<1:52:35,  1.37s/it]                                                   {'loss': 12.6265, 'grad_norm': 9.424574851989746, 'learning_rate': 0.00054, 'epoch': 0.01}
  1%|          | 54/5000 [01:09<1:52:35,  1.37s/it]  1%|          | 55/5000 [01:10<2:00:45,  1.47s/it]                                                   {'loss': 10.9717, 'grad_norm': 9.623127937316895, 'learning_rate': 0.00055, 'epoch': 0.01}
  1%|          | 55/5000 [01:10<2:00:45,  1.47s/it]  1%|          | 56/5000 [01:12<1:55:33,  1.40s/it]                                                   {'loss': 9.8556, 'grad_norm': 11.003586769104004, 'learning_rate': 0.0005600000000000001, 'epoch': 0.01}
  1%|          | 56/5000 [01:12<1:55:33,  1.40s/it]  1%|          | 57/5000 [01:13<1:50:18,  1.34s/it]                                                   {'loss': 7.1883, 'grad_norm': 6.866147994995117, 'learning_rate': 0.00057, 'epoch': 0.01}
  1%|          | 57/5000 [01:13<1:50:18,  1.34s/it]  1%|          | 58/5000 [01:14<1:46:36,  1.29s/it]                                                   {'loss': 10.2869, 'grad_norm': 7.544398784637451, 'learning_rate': 0.00058, 'epoch': 0.01}
  1%|          | 58/5000 [01:14<1:46:36,  1.29s/it]  1%|          | 59/5000 [01:15<1:45:19,  1.28s/it]                                                   {'loss': 9.5387, 'grad_norm': 5.5403828620910645, 'learning_rate': 0.00059, 'epoch': 0.01}
  1%|          | 59/5000 [01:15<1:45:19,  1.28s/it]  1%|          | 60/5000 [01:16<1:43:29,  1.26s/it]                                                   {'loss': 11.0915, 'grad_norm': 5.496756553649902, 'learning_rate': 0.0006, 'epoch': 0.01}
  1%|          | 60/5000 [01:16<1:43:29,  1.26s/it]  1%|          | 61/5000 [01:18<1:42:01,  1.24s/it]                                                   {'loss': 7.8761, 'grad_norm': 7.68766975402832, 'learning_rate': 0.00061, 'epoch': 0.01}
  1%|          | 61/5000 [01:18<1:42:01,  1.24s/it]  1%|          | 62/5000 [01:19<1:41:45,  1.24s/it]                                                   {'loss': 10.5141, 'grad_norm': 8.241302490234375, 'learning_rate': 0.00062, 'epoch': 0.01}
  1%|          | 62/5000 [01:19<1:41:45,  1.24s/it]  1%|â–         | 63/5000 [01:20<1:41:41,  1.24s/it]                                                   {'loss': 8.7592, 'grad_norm': 6.761886119842529, 'learning_rate': 0.00063, 'epoch': 0.01}
  1%|â–         | 63/5000 [01:20<1:41:41,  1.24s/it]  1%|â–         | 64/5000 [01:21<1:40:33,  1.22s/it]                                                   {'loss': 10.7074, 'grad_norm': 5.258038520812988, 'learning_rate': 0.00064, 'epoch': 0.01}
  1%|â–         | 64/5000 [01:21<1:40:33,  1.22s/it]  1%|â–         | 65/5000 [01:22<1:41:30,  1.23s/it]                                                   {'loss': 10.3422, 'grad_norm': 5.620903015136719, 'learning_rate': 0.0006500000000000001, 'epoch': 0.01}
  1%|â–         | 65/5000 [01:22<1:41:30,  1.23s/it]  1%|â–         | 66/5000 [01:24<1:41:04,  1.23s/it]                                                   {'loss': 9.8821, 'grad_norm': 4.453099727630615, 'learning_rate': 0.00066, 'epoch': 0.01}
  1%|â–         | 66/5000 [01:24<1:41:04,  1.23s/it]  1%|â–         | 67/5000 [01:25<1:40:45,  1.23s/it]                                                   {'loss': 12.177, 'grad_norm': 5.533409118652344, 'learning_rate': 0.00067, 'epoch': 0.01}
  1%|â–         | 67/5000 [01:25<1:40:45,  1.23s/it]  1%|â–         | 68/5000 [01:26<1:41:05,  1.23s/it]                                                   {'loss': 9.9481, 'grad_norm': 5.751228332519531, 'learning_rate': 0.00068, 'epoch': 0.01}
  1%|â–         | 68/5000 [01:26<1:41:05,  1.23s/it]  1%|â–         | 69/5000 [01:27<1:42:34,  1.25s/it]                                                   {'loss': 7.0723, 'grad_norm': 4.338633060455322, 'learning_rate': 0.00069, 'epoch': 0.01}
  1%|â–         | 69/5000 [01:27<1:42:34,  1.25s/it]  1%|â–         | 70/5000 [01:29<1:42:26,  1.25s/it]                                                   {'loss': 8.8751, 'grad_norm': 4.744473934173584, 'learning_rate': 0.0007, 'epoch': 0.01}
  1%|â–         | 70/5000 [01:29<1:42:26,  1.25s/it]  1%|â–         | 71/5000 [01:30<1:41:11,  1.23s/it]                                                   {'loss': 10.6159, 'grad_norm': 5.877329349517822, 'learning_rate': 0.00071, 'epoch': 0.01}
  1%|â–         | 71/5000 [01:30<1:41:11,  1.23s/it]  1%|â–         | 72/5000 [01:31<1:40:42,  1.23s/it]                                                   {'loss': 10.1711, 'grad_norm': 4.34423303604126, 'learning_rate': 0.0007199999999999999, 'epoch': 0.01}
  1%|â–         | 72/5000 [01:31<1:40:42,  1.23s/it]  1%|â–         | 73/5000 [01:32<1:41:00,  1.23s/it]                                                   {'loss': 8.6423, 'grad_norm': 5.068480014801025, 'learning_rate': 0.00073, 'epoch': 0.01}
  1%|â–         | 73/5000 [01:32<1:41:00,  1.23s/it]  1%|â–         | 74/5000 [01:33<1:39:55,  1.22s/it]                                                   {'loss': 8.553, 'grad_norm': 4.872986316680908, 'learning_rate': 0.00074, 'epoch': 0.01}
  1%|â–         | 74/5000 [01:34<1:39:55,  1.22s/it]  2%|â–         | 75/5000 [01:35<1:39:21,  1.21s/it]                                                   {'loss': 7.6128, 'grad_norm': 4.76850700378418, 'learning_rate': 0.00075, 'epoch': 0.01}
  2%|â–         | 75/5000 [01:35<1:39:21,  1.21s/it]  2%|â–         | 76/5000 [01:36<1:38:55,  1.21s/it]                                                   {'loss': 9.3675, 'grad_norm': 4.652299880981445, 'learning_rate': 0.00076, 'epoch': 0.02}
  2%|â–         | 76/5000 [01:36<1:38:55,  1.21s/it]  2%|â–         | 77/5000 [01:37<1:38:38,  1.20s/it]                                                   {'loss': 7.9483, 'grad_norm': 3.5009639263153076, 'learning_rate': 0.0007700000000000001, 'epoch': 0.02}
  2%|â–         | 77/5000 [01:37<1:38:38,  1.20s/it]  2%|â–         | 78/5000 [01:38<1:40:03,  1.22s/it]                                                   {'loss': 5.9214, 'grad_norm': 6.817309379577637, 'learning_rate': 0.0007800000000000001, 'epoch': 0.02}
  2%|â–         | 78/5000 [01:38<1:40:03,  1.22s/it]  2%|â–         | 79/5000 [01:40<1:39:51,  1.22s/it]                                                   {'loss': 9.8411, 'grad_norm': 4.784084320068359, 'learning_rate': 0.00079, 'epoch': 0.02}
  2%|â–         | 79/5000 [01:40<1:39:51,  1.22s/it]  2%|â–         | 80/5000 [01:41<1:40:30,  1.23s/it]                                                   {'loss': 6.8371, 'grad_norm': 4.026737213134766, 'learning_rate': 0.0008, 'epoch': 0.02}
  2%|â–         | 80/5000 [01:41<1:40:30,  1.23s/it]  2%|â–         | 81/5000 [01:42<1:40:09,  1.22s/it]                                                   {'loss': 7.6644, 'grad_norm': 4.774351119995117, 'learning_rate': 0.0008100000000000001, 'epoch': 0.02}
  2%|â–         | 81/5000 [01:42<1:40:09,  1.22s/it]  2%|â–         | 82/5000 [01:43<1:39:42,  1.22s/it]                                                   {'loss': 10.5215, 'grad_norm': 5.146538257598877, 'learning_rate': 0.00082, 'epoch': 0.02}
  2%|â–         | 82/5000 [01:43<1:39:42,  1.22s/it]  2%|â–         | 83/5000 [01:44<1:39:08,  1.21s/it]                                                   {'loss': 7.8159, 'grad_norm': 4.7991719245910645, 'learning_rate': 0.00083, 'epoch': 0.02}
  2%|â–         | 83/5000 [01:44<1:39:08,  1.21s/it]  2%|â–         | 84/5000 [01:46<1:39:36,  1.22s/it]                                                   {'loss': 8.956, 'grad_norm': 5.3584489822387695, 'learning_rate': 0.00084, 'epoch': 0.02}
  2%|â–         | 84/5000 [01:46<1:39:36,  1.22s/it]  2%|â–         | 85/5000 [01:47<1:39:07,  1.21s/it]                                                   {'loss': 6.9916, 'grad_norm': 4.493368148803711, 'learning_rate': 0.00085, 'epoch': 0.02}
  2%|â–         | 85/5000 [01:47<1:39:07,  1.21s/it]  2%|â–         | 86/5000 [01:49<1:52:43,  1.38s/it]                                                   {'loss': 6.4525, 'grad_norm': 3.308898687362671, 'learning_rate': 0.00086, 'epoch': 0.02}
  2%|â–         | 86/5000 [01:49<1:52:43,  1.38s/it]  2%|â–         | 87/5000 [01:50<1:54:58,  1.40s/it]                                                   {'loss': 8.0504, 'grad_norm': 6.07150936126709, 'learning_rate': 0.00087, 'epoch': 0.02}
  2%|â–         | 87/5000 [01:50<1:54:58,  1.40s/it]  2%|â–         | 88/5000 [01:51<1:50:53,  1.35s/it]                                                   {'loss': 5.9522, 'grad_norm': 3.2430548667907715, 'learning_rate': 0.00088, 'epoch': 0.02}
  2%|â–         | 88/5000 [01:51<1:50:53,  1.35s/it]  2%|â–         | 89/5000 [01:53<1:48:55,  1.33s/it]                                                   {'loss': 8.2833, 'grad_norm': 7.271790504455566, 'learning_rate': 0.0008900000000000001, 'epoch': 0.02}
  2%|â–         | 89/5000 [01:53<1:48:55,  1.33s/it]  2%|â–         | 90/5000 [01:54<1:46:30,  1.30s/it]                                                   {'loss': 6.9896, 'grad_norm': 3.867314338684082, 'learning_rate': 0.0009000000000000001, 'epoch': 0.02}
  2%|â–         | 90/5000 [01:54<1:46:30,  1.30s/it]  2%|â–         | 91/5000 [01:55<1:47:07,  1.31s/it]                                                   {'loss': 6.3958, 'grad_norm': 4.414178848266602, 'learning_rate': 0.00091, 'epoch': 0.02}
  2%|â–         | 91/5000 [01:55<1:47:07,  1.31s/it]  2%|â–         | 92/5000 [01:56<1:45:04,  1.28s/it]                                                   {'loss': 6.3855, 'grad_norm': 4.31947660446167, 'learning_rate': 0.00092, 'epoch': 0.02}
  2%|â–         | 92/5000 [01:56<1:45:04,  1.28s/it]  2%|â–         | 93/5000 [01:58<1:42:58,  1.26s/it]                                                   {'loss': 5.7617, 'grad_norm': 3.510200262069702, 'learning_rate': 0.00093, 'epoch': 0.02}
  2%|â–         | 93/5000 [01:58<1:42:58,  1.26s/it]  2%|â–         | 94/5000 [01:59<1:42:45,  1.26s/it]                                                   {'loss': 7.1897, 'grad_norm': 6.84418249130249, 'learning_rate': 0.00094, 'epoch': 0.02}
  2%|â–         | 94/5000 [01:59<1:42:45,  1.26s/it]  2%|â–         | 95/5000 [02:00<1:42:38,  1.26s/it]                                                   {'loss': 5.8064, 'grad_norm': 4.842447280883789, 'learning_rate': 0.00095, 'epoch': 0.02}
  2%|â–         | 95/5000 [02:00<1:42:38,  1.26s/it]  2%|â–         | 96/5000 [02:01<1:43:47,  1.27s/it]                                                   {'loss': 5.9048, 'grad_norm': 4.936947345733643, 'learning_rate': 0.00096, 'epoch': 0.02}
  2%|â–         | 96/5000 [02:01<1:43:47,  1.27s/it]  2%|â–         | 97/5000 [02:03<1:42:03,  1.25s/it]                                                   {'loss': 5.3474, 'grad_norm': 3.0845415592193604, 'learning_rate': 0.0009699999999999999, 'epoch': 0.02}
  2%|â–         | 97/5000 [02:03<1:42:03,  1.25s/it]  2%|â–         | 98/5000 [02:04<1:41:21,  1.24s/it]                                                   {'loss': 7.1774, 'grad_norm': 6.099386692047119, 'learning_rate': 0.00098, 'epoch': 0.02}
  2%|â–         | 98/5000 [02:04<1:41:21,  1.24s/it]  2%|â–         | 99/5000 [02:05<1:41:13,  1.24s/it]                                                   {'loss': 6.1564, 'grad_norm': 5.268840789794922, 'learning_rate': 0.00099, 'epoch': 0.02}
  2%|â–         | 99/5000 [02:05<1:41:13,  1.24s/it]  2%|â–         | 100/5000 [02:06<1:45:05,  1.29s/it]                                                    {'loss': 7.0384, 'grad_norm': 5.833930969238281, 'learning_rate': 0.001, 'epoch': 0.02}
  2%|â–         | 100/5000 [02:06<1:45:05,  1.29s/it][2025-10-19 17:26:55,524] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/5000 [02:09<2:13:00,  1.63s/it]                                                    {'loss': 6.7211, 'grad_norm': 4.467911720275879, 'learning_rate': 0.000999795918367347, 'epoch': 0.02}
  2%|â–         | 101/5000 [02:09<2:13:00,  1.63s/it]  2%|â–         | 102/5000 [02:10<2:03:24,  1.51s/it]                                                    {'loss': 5.5064, 'grad_norm': 4.572006702423096, 'learning_rate': 0.0009995918367346939, 'epoch': 0.02}
  2%|â–         | 102/5000 [02:10<2:03:24,  1.51s/it]  2%|â–         | 103/5000 [02:11<1:56:07,  1.42s/it]                                                    {'loss': 5.3179, 'grad_norm': 4.453691482543945, 'learning_rate': 0.0009993877551020408, 'epoch': 0.02}
  2%|â–         | 103/5000 [02:11<1:56:07,  1.42s/it]  2%|â–         | 104/5000 [02:13<1:50:59,  1.36s/it]                                                    {'loss': 4.9409, 'grad_norm': 4.246560096740723, 'learning_rate': 0.0009991836734693877, 'epoch': 0.02}
  2%|â–         | 104/5000 [02:13<1:50:59,  1.36s/it]  2%|â–         | 105/5000 [02:14<1:49:12,  1.34s/it]                                                    {'loss': 6.1648, 'grad_norm': 9.267337799072266, 'learning_rate': 0.0009989795918367347, 'epoch': 0.02}
  2%|â–         | 105/5000 [02:14<1:49:12,  1.34s/it]  2%|â–         | 106/5000 [02:15<1:46:30,  1.31s/it]                                                    {'loss': 6.482, 'grad_norm': 8.82401180267334, 'learning_rate': 0.0009987755102040816, 'epoch': 0.02}
  2%|â–         | 106/5000 [02:15<1:46:30,  1.31s/it]  2%|â–         | 107/5000 [02:16<1:44:01,  1.28s/it]                                                    {'loss': 5.3279, 'grad_norm': 5.777060031890869, 'learning_rate': 0.0009985714285714285, 'epoch': 0.02}
  2%|â–         | 107/5000 [02:16<1:44:01,  1.28s/it]  2%|â–         | 108/5000 [02:17<1:42:05,  1.25s/it]                                                    {'loss': 4.9507, 'grad_norm': 3.982823371887207, 'learning_rate': 0.0009983673469387755, 'epoch': 0.02}
  2%|â–         | 108/5000 [02:17<1:42:05,  1.25s/it]  2%|â–         | 109/5000 [02:19<1:41:51,  1.25s/it]                                                    {'loss': 3.9848, 'grad_norm': 5.518250942230225, 'learning_rate': 0.0009981632653061224, 'epoch': 0.02}
  2%|â–         | 109/5000 [02:19<1:41:51,  1.25s/it]  2%|â–         | 110/5000 [02:20<1:41:01,  1.24s/it]                                                    {'loss': 6.1791, 'grad_norm': 4.843945026397705, 'learning_rate': 0.0009979591836734693, 'epoch': 0.02}
  2%|â–         | 110/5000 [02:20<1:41:01,  1.24s/it]  2%|â–         | 111/5000 [02:21<1:41:23,  1.24s/it]                                                    {'loss': 5.9487, 'grad_norm': 5.013949394226074, 'learning_rate': 0.0009977551020408162, 'epoch': 0.02}
  2%|â–         | 111/5000 [02:21<1:41:23,  1.24s/it]  2%|â–         | 112/5000 [02:22<1:40:05,  1.23s/it]                                                    {'loss': 4.2461, 'grad_norm': 5.586040496826172, 'learning_rate': 0.0009975510204081632, 'epoch': 0.02}
  2%|â–         | 112/5000 [02:22<1:40:05,  1.23s/it]  2%|â–         | 113/5000 [02:24<1:40:41,  1.24s/it]                                                    {'loss': 5.3112, 'grad_norm': 4.41458797454834, 'learning_rate': 0.00099734693877551, 'epoch': 0.02}
  2%|â–         | 113/5000 [02:24<1:40:41,  1.24s/it]  2%|â–         | 114/5000 [02:25<1:39:46,  1.23s/it]                                                    {'loss': 5.22, 'grad_norm': 10.446401596069336, 'learning_rate': 0.000997142857142857, 'epoch': 0.02}
  2%|â–         | 114/5000 [02:25<1:39:46,  1.23s/it]  2%|â–         | 115/5000 [02:26<1:41:04,  1.24s/it]                                                    {'loss': 5.3575, 'grad_norm': 8.815459251403809, 'learning_rate': 0.000996938775510204, 'epoch': 0.02}
  2%|â–         | 115/5000 [02:26<1:41:04,  1.24s/it]  2%|â–         | 116/5000 [02:27<1:40:28,  1.23s/it]                                                    {'loss': 4.3473, 'grad_norm': 4.96218204498291, 'learning_rate': 0.000996734693877551, 'epoch': 0.02}
  2%|â–         | 116/5000 [02:27<1:40:28,  1.23s/it]  2%|â–         | 117/5000 [02:29<1:40:28,  1.23s/it]                                                    {'loss': 3.4383, 'grad_norm': 4.31491756439209, 'learning_rate': 0.000996530612244898, 'epoch': 0.02}
  2%|â–         | 117/5000 [02:29<1:40:28,  1.23s/it]  2%|â–         | 118/5000 [02:30<1:39:30,  1.22s/it]                                                    {'loss': 5.0118, 'grad_norm': 6.465320110321045, 'learning_rate': 0.000996326530612245, 'epoch': 0.02}
  2%|â–         | 118/5000 [02:30<1:39:30,  1.22s/it]  2%|â–         | 119/5000 [02:31<1:38:52,  1.22s/it]                                                    {'loss': 4.2246, 'grad_norm': 7.390726566314697, 'learning_rate': 0.000996122448979592, 'epoch': 0.02}
  2%|â–         | 119/5000 [02:31<1:38:52,  1.22s/it]  2%|â–         | 120/5000 [02:32<1:39:06,  1.22s/it]                                                    {'loss': 4.6485, 'grad_norm': 4.211230754852295, 'learning_rate': 0.0009959183673469388, 'epoch': 0.02}
  2%|â–         | 120/5000 [02:32<1:39:06,  1.22s/it]  2%|â–         | 121/5000 [02:33<1:38:56,  1.22s/it]                                                    {'loss': 3.1442, 'grad_norm': 2.7808046340942383, 'learning_rate': 0.0009957142857142858, 'epoch': 0.02}
  2%|â–         | 121/5000 [02:33<1:38:56,  1.22s/it]  2%|â–         | 122/5000 [02:35<1:39:29,  1.22s/it]                                                    {'loss': 3.8631, 'grad_norm': 4.911402225494385, 'learning_rate': 0.0009955102040816327, 'epoch': 0.02}
  2%|â–         | 122/5000 [02:35<1:39:29,  1.22s/it]  2%|â–         | 123/5000 [02:36<1:40:01,  1.23s/it]                                                    {'loss': 4.7713, 'grad_norm': 3.4436519145965576, 'learning_rate': 0.0009953061224489796, 'epoch': 0.02}
  2%|â–         | 123/5000 [02:36<1:40:01,  1.23s/it]  2%|â–         | 124/5000 [02:37<1:40:31,  1.24s/it]                                                    {'loss': 2.9589, 'grad_norm': 2.046165704727173, 'learning_rate': 0.0009951020408163265, 'epoch': 0.02}
  2%|â–         | 124/5000 [02:37<1:40:31,  1.24s/it]  2%|â–Ž         | 125/5000 [02:38<1:40:36,  1.24s/it]                                                    {'loss': 4.2107, 'grad_norm': 6.481864929199219, 'learning_rate': 0.0009948979591836735, 'epoch': 0.03}
  2%|â–Ž         | 125/5000 [02:38<1:40:36,  1.24s/it]  3%|â–Ž         | 126/5000 [02:40<1:39:25,  1.22s/it]                                                    {'loss': 3.5138, 'grad_norm': 3.980008125305176, 'learning_rate': 0.0009946938775510204, 'epoch': 0.03}
  3%|â–Ž         | 126/5000 [02:40<1:39:25,  1.22s/it]  3%|â–Ž         | 127/5000 [02:41<1:39:13,  1.22s/it]                                                    {'loss': 3.0082, 'grad_norm': 1.9417047500610352, 'learning_rate': 0.0009944897959183673, 'epoch': 0.03}
  3%|â–Ž         | 127/5000 [02:41<1:39:13,  1.22s/it]  3%|â–Ž         | 128/5000 [02:42<1:38:25,  1.21s/it]                                                    {'loss': 3.1126, 'grad_norm': 3.483726978302002, 'learning_rate': 0.0009942857142857143, 'epoch': 0.03}
  3%|â–Ž         | 128/5000 [02:42<1:38:25,  1.21s/it]  3%|â–Ž         | 129/5000 [02:43<1:38:46,  1.22s/it]                                                    {'loss': 3.2647, 'grad_norm': 2.4598987102508545, 'learning_rate': 0.0009940816326530612, 'epoch': 0.03}
  3%|â–Ž         | 129/5000 [02:43<1:38:46,  1.22s/it]  3%|â–Ž         | 130/5000 [02:44<1:38:21,  1.21s/it]                                                    {'loss': 4.4907, 'grad_norm': 4.460352897644043, 'learning_rate': 0.0009938775510204081, 'epoch': 0.03}
  3%|â–Ž         | 130/5000 [02:44<1:38:21,  1.21s/it]  3%|â–Ž         | 131/5000 [02:46<1:44:14,  1.28s/it]                                                    {'loss': 3.1092, 'grad_norm': 2.1549072265625, 'learning_rate': 0.0009936734693877553, 'epoch': 0.03}
  3%|â–Ž         | 131/5000 [02:46<1:44:14,  1.28s/it]  3%|â–Ž         | 132/5000 [02:47<1:44:38,  1.29s/it]                                                    {'loss': 3.0933, 'grad_norm': 4.814151763916016, 'learning_rate': 0.0009934693877551022, 'epoch': 0.03}
  3%|â–Ž         | 132/5000 [02:47<1:44:38,  1.29s/it]  3%|â–Ž         | 133/5000 [02:48<1:43:27,  1.28s/it]                                                    {'loss': 2.6221, 'grad_norm': 1.9527573585510254, 'learning_rate': 0.0009932653061224491, 'epoch': 0.03}
  3%|â–Ž         | 133/5000 [02:48<1:43:27,  1.28s/it]  3%|â–Ž         | 134/5000 [02:50<1:42:02,  1.26s/it]                                                    {'loss': 4.153, 'grad_norm': 3.7337913513183594, 'learning_rate': 0.000993061224489796, 'epoch': 0.03}
  3%|â–Ž         | 134/5000 [02:50<1:42:02,  1.26s/it]  3%|â–Ž         | 135/5000 [02:51<1:40:46,  1.24s/it]                                                    {'loss': 3.0586, 'grad_norm': 2.1827282905578613, 'learning_rate': 0.000992857142857143, 'epoch': 0.03}
  3%|â–Ž         | 135/5000 [02:51<1:40:46,  1.24s/it]  3%|â–Ž         | 136/5000 [02:52<1:40:05,  1.23s/it]                                                    {'loss': 3.5487, 'grad_norm': 2.783863067626953, 'learning_rate': 0.00099265306122449, 'epoch': 0.03}
  3%|â–Ž         | 136/5000 [02:52<1:40:05,  1.23s/it]  3%|â–Ž         | 137/5000 [02:53<1:39:18,  1.23s/it]                                                    {'loss': 3.6637, 'grad_norm': 2.130396604537964, 'learning_rate': 0.0009924489795918368, 'epoch': 0.03}
  3%|â–Ž         | 137/5000 [02:53<1:39:18,  1.23s/it]  3%|â–Ž         | 138/5000 [02:54<1:40:21,  1.24s/it]                                                    {'loss': 3.1991, 'grad_norm': 4.643302917480469, 'learning_rate': 0.0009922448979591838, 'epoch': 0.03}
  3%|â–Ž         | 138/5000 [02:54<1:40:21,  1.24s/it]  3%|â–Ž         | 139/5000 [02:56<1:42:24,  1.26s/it]                                                    {'loss': 2.9257, 'grad_norm': 2.622863531112671, 'learning_rate': 0.0009920408163265307, 'epoch': 0.03}
  3%|â–Ž         | 139/5000 [02:56<1:42:24,  1.26s/it]  3%|â–Ž         | 140/5000 [02:57<1:42:08,  1.26s/it]                                                    {'loss': 3.4066, 'grad_norm': 2.119312047958374, 'learning_rate': 0.0009918367346938776, 'epoch': 0.03}
  3%|â–Ž         | 140/5000 [02:57<1:42:08,  1.26s/it]  3%|â–Ž         | 141/5000 [02:58<1:41:00,  1.25s/it]                                                    {'loss': 3.2564, 'grad_norm': 2.951880693435669, 'learning_rate': 0.0009916326530612246, 'epoch': 0.03}
  3%|â–Ž         | 141/5000 [02:58<1:41:00,  1.25s/it]  3%|â–Ž         | 142/5000 [02:59<1:40:11,  1.24s/it]                                                    {'loss': 4.5243, 'grad_norm': 4.654910087585449, 'learning_rate': 0.0009914285714285715, 'epoch': 0.03}
  3%|â–Ž         | 142/5000 [03:00<1:40:11,  1.24s/it]  3%|â–Ž         | 143/5000 [03:01<1:41:01,  1.25s/it]                                                    {'loss': 4.7551, 'grad_norm': 3.9740982055664062, 'learning_rate': 0.0009912244897959184, 'epoch': 0.03}
  3%|â–Ž         | 143/5000 [03:01<1:41:01,  1.25s/it]  3%|â–Ž         | 144/5000 [03:02<1:44:58,  1.30s/it]                                                    {'loss': 3.3335, 'grad_norm': 2.9501397609710693, 'learning_rate': 0.0009910204081632653, 'epoch': 0.03}
  3%|â–Ž         | 144/5000 [03:02<1:44:58,  1.30s/it]  3%|â–Ž         | 145/5000 [03:03<1:44:24,  1.29s/it]                                                    {'loss': 5.0823, 'grad_norm': 2.6738083362579346, 'learning_rate': 0.0009908163265306123, 'epoch': 0.03}
  3%|â–Ž         | 145/5000 [03:03<1:44:24,  1.29s/it]  3%|â–Ž         | 146/5000 [03:05<1:42:47,  1.27s/it]                                                    {'loss': 2.6445, 'grad_norm': 1.801511287689209, 'learning_rate': 0.0009906122448979592, 'epoch': 0.03}
  3%|â–Ž         | 146/5000 [03:05<1:42:47,  1.27s/it]  3%|â–Ž         | 147/5000 [03:06<1:40:57,  1.25s/it]                                                    {'loss': 3.8621, 'grad_norm': 2.4160773754119873, 'learning_rate': 0.0009904081632653061, 'epoch': 0.03}
  3%|â–Ž         | 147/5000 [03:06<1:40:57,  1.25s/it]  3%|â–Ž         | 148/5000 [03:07<1:39:59,  1.24s/it]                                                    {'loss': 3.9394, 'grad_norm': 3.286679267883301, 'learning_rate': 0.000990204081632653, 'epoch': 0.03}
  3%|â–Ž         | 148/5000 [03:07<1:39:59,  1.24s/it]  3%|â–Ž         | 149/5000 [03:08<1:39:25,  1.23s/it]                                                    {'loss': 2.7879, 'grad_norm': 5.736386775970459, 'learning_rate': 0.00099, 'epoch': 0.03}
  3%|â–Ž         | 149/5000 [03:08<1:39:25,  1.23s/it]  3%|â–Ž         | 150/5000 [03:09<1:38:18,  1.22s/it]                                                    {'loss': 2.6819, 'grad_norm': 2.537566900253296, 'learning_rate': 0.000989795918367347, 'epoch': 0.03}
  3%|â–Ž         | 150/5000 [03:09<1:38:18,  1.22s/it][2025-10-19 17:27:58,574] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/5000 [03:12<2:10:34,  1.62s/it]                                                    {'loss': 4.1487, 'grad_norm': 2.8665084838867188, 'learning_rate': 0.0009895918367346939, 'epoch': 0.03}
  3%|â–Ž         | 151/5000 [03:12<2:10:34,  1.62s/it]  3%|â–Ž         | 152/5000 [03:13<2:00:31,  1.49s/it]                                                    {'loss': 2.5293, 'grad_norm': 1.0256563425064087, 'learning_rate': 0.0009893877551020408, 'epoch': 0.03}
  3%|â–Ž         | 152/5000 [03:13<2:00:31,  1.49s/it]  3%|â–Ž         | 153/5000 [03:14<1:54:14,  1.41s/it]                                                    {'loss': 4.0756, 'grad_norm': 3.9153316020965576, 'learning_rate': 0.0009891836734693877, 'epoch': 0.03}
  3%|â–Ž         | 153/5000 [03:14<1:54:14,  1.41s/it]  3%|â–Ž         | 154/5000 [03:16<1:50:45,  1.37s/it]                                                    {'loss': 4.3342, 'grad_norm': 3.362901449203491, 'learning_rate': 0.0009889795918367346, 'epoch': 0.03}
  3%|â–Ž         | 154/5000 [03:16<1:50:45,  1.37s/it]  3%|â–Ž         | 155/5000 [03:17<1:52:26,  1.39s/it]                                                    {'loss': 3.3949, 'grad_norm': 3.1851465702056885, 'learning_rate': 0.0009887755102040816, 'epoch': 0.03}
  3%|â–Ž         | 155/5000 [03:17<1:52:26,  1.39s/it]  3%|â–Ž         | 156/5000 [03:18<1:47:35,  1.33s/it]                                                    {'loss': 3.2811, 'grad_norm': 2.4321517944335938, 'learning_rate': 0.0009885714285714285, 'epoch': 0.03}
  3%|â–Ž         | 156/5000 [03:18<1:47:35,  1.33s/it]  3%|â–Ž         | 157/5000 [03:20<1:45:01,  1.30s/it]                                                    {'loss': 2.7685, 'grad_norm': 1.2370070219039917, 'learning_rate': 0.0009883673469387754, 'epoch': 0.03}
  3%|â–Ž         | 157/5000 [03:20<1:45:01,  1.30s/it]  3%|â–Ž         | 158/5000 [03:21<1:43:44,  1.29s/it]                                                    {'loss': 4.2791, 'grad_norm': 3.973006010055542, 'learning_rate': 0.0009881632653061224, 'epoch': 0.03}
  3%|â–Ž         | 158/5000 [03:21<1:43:44,  1.29s/it]  3%|â–Ž         | 159/5000 [03:22<1:41:50,  1.26s/it]                                                    {'loss': 3.1839, 'grad_norm': 3.704012393951416, 'learning_rate': 0.0009879591836734693, 'epoch': 0.03}
  3%|â–Ž         | 159/5000 [03:22<1:41:50,  1.26s/it]  3%|â–Ž         | 160/5000 [03:23<1:40:53,  1.25s/it]                                                    {'loss': 2.9753, 'grad_norm': 1.6954915523529053, 'learning_rate': 0.0009877551020408162, 'epoch': 0.03}
  3%|â–Ž         | 160/5000 [03:23<1:40:53,  1.25s/it]  3%|â–Ž         | 161/5000 [03:25<1:40:23,  1.24s/it]                                                    {'loss': 3.9765, 'grad_norm': 3.3305792808532715, 'learning_rate': 0.0009875510204081631, 'epoch': 0.03}
  3%|â–Ž         | 161/5000 [03:25<1:40:23,  1.24s/it]  3%|â–Ž         | 162/5000 [03:26<1:39:10,  1.23s/it]                                                    {'loss': 2.6244, 'grad_norm': 1.4697033166885376, 'learning_rate': 0.00098734693877551, 'epoch': 0.03}
  3%|â–Ž         | 162/5000 [03:26<1:39:10,  1.23s/it]  3%|â–Ž         | 163/5000 [03:27<1:38:36,  1.22s/it]                                                    {'loss': 3.6911, 'grad_norm': 1.9327685832977295, 'learning_rate': 0.0009871428571428572, 'epoch': 0.03}
  3%|â–Ž         | 163/5000 [03:27<1:38:36,  1.22s/it]  3%|â–Ž         | 164/5000 [03:28<1:39:22,  1.23s/it]                                                    {'loss': 3.3632, 'grad_norm': 3.1701605319976807, 'learning_rate': 0.0009869387755102042, 'epoch': 0.03}
  3%|â–Ž         | 164/5000 [03:28<1:39:22,  1.23s/it]  3%|â–Ž         | 165/5000 [03:29<1:38:38,  1.22s/it]                                                    {'loss': 2.4007, 'grad_norm': 1.9451159238815308, 'learning_rate': 0.000986734693877551, 'epoch': 0.03}
  3%|â–Ž         | 165/5000 [03:29<1:38:38,  1.22s/it]  3%|â–Ž         | 166/5000 [03:31<1:38:48,  1.23s/it]                                                    {'loss': 5.5066, 'grad_norm': 3.65493106842041, 'learning_rate': 0.000986530612244898, 'epoch': 0.03}
  3%|â–Ž         | 166/5000 [03:31<1:38:48,  1.23s/it]  3%|â–Ž         | 167/5000 [03:32<1:38:44,  1.23s/it]                                                    {'loss': 3.8175, 'grad_norm': 7.223940849304199, 'learning_rate': 0.000986326530612245, 'epoch': 0.03}
  3%|â–Ž         | 167/5000 [03:32<1:38:44,  1.23s/it]  3%|â–Ž         | 168/5000 [03:33<1:41:49,  1.26s/it]                                                    {'loss': 3.2146, 'grad_norm': 3.2666800022125244, 'learning_rate': 0.0009861224489795919, 'epoch': 0.03}
  3%|â–Ž         | 168/5000 [03:33<1:41:49,  1.26s/it]  3%|â–Ž         | 169/5000 [03:35<1:46:02,  1.32s/it]                                                    {'loss': 3.8, 'grad_norm': 7.9407501220703125, 'learning_rate': 0.0009859183673469388, 'epoch': 0.03}
  3%|â–Ž         | 169/5000 [03:35<1:46:02,  1.32s/it]  3%|â–Ž         | 170/5000 [03:36<1:43:17,  1.28s/it]                                                    {'loss': 3.2811, 'grad_norm': 2.0234076976776123, 'learning_rate': 0.0009857142857142857, 'epoch': 0.03}
  3%|â–Ž         | 170/5000 [03:36<1:43:17,  1.28s/it]  3%|â–Ž         | 171/5000 [03:37<1:42:21,  1.27s/it]                                                    {'loss': 3.3018, 'grad_norm': 2.1223509311676025, 'learning_rate': 0.0009855102040816327, 'epoch': 0.03}
  3%|â–Ž         | 171/5000 [03:37<1:42:21,  1.27s/it]  3%|â–Ž         | 172/5000 [03:38<1:41:51,  1.27s/it]                                                    {'loss': 2.4691, 'grad_norm': 3.131551504135132, 'learning_rate': 0.0009853061224489796, 'epoch': 0.03}
  3%|â–Ž         | 172/5000 [03:38<1:41:51,  1.27s/it]  3%|â–Ž         | 173/5000 [03:40<1:41:27,  1.26s/it]                                                    {'loss': 3.3848, 'grad_norm': 3.694291114807129, 'learning_rate': 0.0009851020408163265, 'epoch': 0.03}
  3%|â–Ž         | 173/5000 [03:40<1:41:27,  1.26s/it]  3%|â–Ž         | 174/5000 [03:41<1:39:56,  1.24s/it]                                                    {'loss': 3.969, 'grad_norm': 2.3719465732574463, 'learning_rate': 0.0009848979591836734, 'epoch': 0.03}
  3%|â–Ž         | 174/5000 [03:41<1:39:56,  1.24s/it]  4%|â–Ž         | 175/5000 [03:42<1:39:12,  1.23s/it]                                                    {'loss': 6.1478, 'grad_norm': 18.798423767089844, 'learning_rate': 0.0009846938775510204, 'epoch': 0.04}
  4%|â–Ž         | 175/5000 [03:42<1:39:12,  1.23s/it]  4%|â–Ž         | 176/5000 [03:43<1:38:57,  1.23s/it]                                                    {'loss': 2.6681, 'grad_norm': 2.380692720413208, 'learning_rate': 0.0009844897959183673, 'epoch': 0.04}
  4%|â–Ž         | 176/5000 [03:43<1:38:57,  1.23s/it]  4%|â–Ž         | 177/5000 [03:44<1:38:55,  1.23s/it]                                                    {'loss': 2.9819, 'grad_norm': 2.45745587348938, 'learning_rate': 0.0009842857142857142, 'epoch': 0.04}
  4%|â–Ž         | 177/5000 [03:44<1:38:55,  1.23s/it]  4%|â–Ž         | 178/5000 [03:46<1:38:25,  1.22s/it]                                                    {'loss': 2.885, 'grad_norm': 4.849757194519043, 'learning_rate': 0.0009840816326530614, 'epoch': 0.04}
  4%|â–Ž         | 178/5000 [03:46<1:38:25,  1.22s/it]  4%|â–Ž         | 179/5000 [03:47<1:39:39,  1.24s/it]                                                    {'loss': 4.3748, 'grad_norm': 4.819950103759766, 'learning_rate': 0.0009838775510204083, 'epoch': 0.04}
  4%|â–Ž         | 179/5000 [03:47<1:39:39,  1.24s/it]  4%|â–Ž         | 180/5000 [03:48<1:39:36,  1.24s/it]                                                    {'loss': 2.6213, 'grad_norm': 3.682539939880371, 'learning_rate': 0.0009836734693877552, 'epoch': 0.04}
  4%|â–Ž         | 180/5000 [03:48<1:39:36,  1.24s/it]  4%|â–Ž         | 181/5000 [03:49<1:38:55,  1.23s/it]                                                    {'loss': 3.0087, 'grad_norm': 1.8551579713821411, 'learning_rate': 0.0009834693877551022, 'epoch': 0.04}
  4%|â–Ž         | 181/5000 [03:49<1:38:55,  1.23s/it]  4%|â–Ž         | 182/5000 [03:51<1:40:41,  1.25s/it]                                                    {'loss': 2.9832, 'grad_norm': 1.8853143453598022, 'learning_rate': 0.000983265306122449, 'epoch': 0.04}
  4%|â–Ž         | 182/5000 [03:51<1:40:41,  1.25s/it]  4%|â–Ž         | 183/5000 [03:52<1:45:36,  1.32s/it]                                                    {'loss': 3.6465, 'grad_norm': 3.852850914001465, 'learning_rate': 0.000983061224489796, 'epoch': 0.04}
  4%|â–Ž         | 183/5000 [03:52<1:45:36,  1.32s/it]  4%|â–Ž         | 184/5000 [03:53<1:42:55,  1.28s/it]                                                    {'loss': 2.4868, 'grad_norm': 1.0769597291946411, 'learning_rate': 0.000982857142857143, 'epoch': 0.04}
  4%|â–Ž         | 184/5000 [03:53<1:42:55,  1.28s/it]  4%|â–Ž         | 185/5000 [03:55<1:40:51,  1.26s/it]                                                    {'loss': 3.6148, 'grad_norm': 3.561262369155884, 'learning_rate': 0.0009826530612244899, 'epoch': 0.04}
  4%|â–Ž         | 185/5000 [03:55<1:40:51,  1.26s/it]  4%|â–Ž         | 186/5000 [03:56<1:43:14,  1.29s/it]                                                    {'loss': 3.2337, 'grad_norm': 2.276089668273926, 'learning_rate': 0.0009824489795918368, 'epoch': 0.04}
  4%|â–Ž         | 186/5000 [03:56<1:43:14,  1.29s/it]  4%|â–Ž         | 187/5000 [03:57<1:41:31,  1.27s/it]                                                    {'loss': 5.7013, 'grad_norm': 21.21769905090332, 'learning_rate': 0.0009822448979591837, 'epoch': 0.04}
  4%|â–Ž         | 187/5000 [03:57<1:41:31,  1.27s/it]  4%|â–         | 188/5000 [03:58<1:41:17,  1.26s/it]                                                    {'loss': 4.4945, 'grad_norm': 4.416645050048828, 'learning_rate': 0.0009820408163265307, 'epoch': 0.04}
  4%|â–         | 188/5000 [03:58<1:41:17,  1.26s/it]  4%|â–         | 189/5000 [04:00<1:41:21,  1.26s/it]                                                    {'loss': 3.6484, 'grad_norm': 4.51686429977417, 'learning_rate': 0.0009818367346938776, 'epoch': 0.04}
  4%|â–         | 189/5000 [04:00<1:41:21,  1.26s/it]