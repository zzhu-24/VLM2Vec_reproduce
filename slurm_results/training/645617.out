==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test6-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-6 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/train.log
W1023 00:25:58.963000 134470232262464 torch/distributed/run.py:779] 
W1023 00:25:58.963000 134470232262464 torch/distributed/run.py:779] *****************************************
W1023 00:25:58.963000 134470232262464 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1023 00:25:58.963000 134470232262464 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-23 00:26:09,229] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.12it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.94it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251023_002609-lr2lqmd2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test6-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/lr2lqmd2
[2025-10-23 00:26:10,728] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.12it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.92it/s]
[2025-10-23 00:26:11,357] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-10-23 00:26:20,308] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-23 00:26:21,494] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-23 00:26:21,495] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-23 00:26:25,970] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-23 00:26:25,971] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-23 00:26:26,822] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-23 00:26:26,822] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-23 00:26:26,822] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-23 00:26:26,824] INFO [src.utils:19] ==================================================
[2025-10-23 00:26:26,824] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-23 00:26:26,824] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-23 00:26:26,825] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-23 00:26:26,825] INFO [src.utils:19] ==================================================
[2025-10-23 00:26:28,583] INFO [src.trainer:342] ***** Running training *****
[2025-10-23 00:26:28,583] INFO [src.trainer:342] ***** Running training *****
[2025-10-23 00:26:28,583] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-23 00:26:28,583] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-23 00:26:28,583] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-23 00:26:28,583] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-23 00:26:28,583] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-23 00:26:28,583] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-23 00:26:28,584] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-23 00:26:28,584] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-23 00:26:28,584] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-23 00:26:28,584] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-23 00:26:28,585] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-23 00:26:28,585] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-23 00:26:28,591] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-23 00:26:28,593] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-23 00:26:28,597] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-10-23 00:26:28,600] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1023 00:26:31.360130465 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1023 00:26:31.401555108 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:50:37,  4.11s/it]                                                  {'loss': 11.1988, 'grad_norm': 788.8121337890625, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:50:37,  4.11s/it]  0%|          | 2/6000 [00:06<5:24:08,  3.24s/it]                                                  {'loss': 8.8866, 'grad_norm': 782.5199584960938, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:24:08,  3.24s/it]  0%|          | 3/6000 [00:09<5:01:06,  3.01s/it]                                                  {'loss': 9.4419, 'grad_norm': 777.7600708007812, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:01:06,  3.01s/it]  0%|          | 4/6000 [00:12<4:49:41,  2.90s/it]                                                  {'loss': 8.2737, 'grad_norm': 661.85888671875, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:49:41,  2.90s/it]  0%|          | 5/6000 [00:14<4:43:26,  2.84s/it]                                                  {'loss': 9.8534, 'grad_norm': 870.6966552734375, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:43:26,  2.84s/it]  0%|          | 6/6000 [00:17<4:39:18,  2.80s/it]                                                  {'loss': 10.7177, 'grad_norm': 769.7997436523438, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:39:18,  2.80s/it]  0%|          | 7/6000 [00:20<4:36:37,  2.77s/it]                                                  {'loss': 11.3603, 'grad_norm': 849.6456909179688, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:36:37,  2.77s/it]  0%|          | 8/6000 [00:23<4:32:22,  2.73s/it]                                                  {'loss': 11.0577, 'grad_norm': 1005.2344360351562, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:23<4:32:22,  2.73s/it]  0%|          | 9/6000 [00:25<4:33:07,  2.74s/it]                                                  {'loss': 8.0292, 'grad_norm': 688.7311401367188, 'learning_rate': 4.5000000000000003e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:33:07,  2.74s/it]  0%|          | 10/6000 [00:28<4:30:16,  2.71s/it]                                                   {'loss': 10.4387, 'grad_norm': 755.1838989257812, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:30:16,  2.71s/it]  0%|          | 11/6000 [00:31<4:41:26,  2.82s/it]                                                   {'loss': 10.8635, 'grad_norm': 712.912353515625, 'learning_rate': 5.5e-07, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:41:26,  2.82s/it]  0%|          | 12/6000 [00:34<4:44:37,  2.85s/it]                                                   {'loss': 10.1293, 'grad_norm': 877.01611328125, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:44:37,  2.85s/it]  0%|          | 13/6000 [00:37<4:41:29,  2.82s/it]                                                   {'loss': 10.0225, 'grad_norm': 774.7396850585938, 'learning_rate': 6.5e-07, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:41:29,  2.82s/it]  0%|          | 14/6000 [00:39<4:41:02,  2.82s/it]                                                   {'loss': 8.6927, 'grad_norm': 606.9588623046875, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 14/6000 [00:39<4:41:02,  2.82s/it]  0%|          | 15/6000 [00:42<4:38:24,  2.79s/it]                                                   {'loss': 8.6757, 'grad_norm': 678.3477172851562, 'learning_rate': 7.5e-07, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:38:24,  2.79s/it]  0%|          | 16/6000 [00:45<4:34:26,  2.75s/it]                                                   {'loss': 8.2823, 'grad_norm': 586.6107177734375, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:34:26,  2.75s/it]  0%|          | 17/6000 [00:48<4:33:20,  2.74s/it]                                                   {'loss': 9.043, 'grad_norm': 617.3910522460938, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.0}
  0%|          | 17/6000 [00:48<4:33:20,  2.74s/it]  0%|          | 18/6000 [00:50<4:34:33,  2.75s/it]                                                   {'loss': 7.2252, 'grad_norm': 471.9698791503906, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:34:33,  2.75s/it]  0%|          | 19/6000 [00:53<4:32:53,  2.74s/it]                                                   {'loss': 8.3673, 'grad_norm': 542.51513671875, 'learning_rate': 9.500000000000001e-07, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:32:53,  2.74s/it]  0%|          | 20/6000 [00:56<4:33:12,  2.74s/it]                                                   {'loss': 8.0511, 'grad_norm': 516.0450439453125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:56<4:33:12,  2.74s/it]  0%|          | 21/6000 [00:59<4:37:11,  2.78s/it]                                                   {'loss': 6.836, 'grad_norm': 439.2812805175781, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:59<4:37:11,  2.78s/it]  0%|          | 22/6000 [01:02<4:40:46,  2.82s/it]                                                   {'loss': 8.8996, 'grad_norm': 619.2706298828125, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:02<4:40:46,  2.82s/it]  0%|          | 23/6000 [01:04<4:37:31,  2.79s/it]                                                   {'loss': 7.9455, 'grad_norm': 623.841552734375, 'learning_rate': 1.1500000000000002e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:37:31,  2.79s/it]  0%|          | 24/6000 [01:07<4:37:20,  2.78s/it]                                                   {'loss': 5.4589, 'grad_norm': 302.64703369140625, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:37:20,  2.78s/it]  0%|          | 25/6000 [01:10<4:35:57,  2.77s/it]                                                   {'loss': 7.4468, 'grad_norm': 442.3136901855469, 'learning_rate': 1.25e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:10<4:35:57,  2.77s/it]  0%|          | 26/6000 [01:13<4:37:09,  2.78s/it]                                                   {'loss': 7.7333, 'grad_norm': 455.723876953125, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:13<4:37:09,  2.78s/it]  0%|          | 27/6000 [01:15<4:36:23,  2.78s/it]                                                   {'loss': 8.4931, 'grad_norm': 505.0144958496094, 'learning_rate': 1.3500000000000002e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:36:23,  2.78s/it]  0%|          | 28/6000 [01:19<5:01:24,  3.03s/it]                                                   {'loss': 6.916, 'grad_norm': 374.7027893066406, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:19<5:01:24,  3.03s/it]  0%|          | 29/6000 [01:22<4:51:15,  2.93s/it]                                                   {'loss': 6.3535, 'grad_norm': 339.1739196777344, 'learning_rate': 1.45e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:22<4:51:15,  2.93s/it]  0%|          | 30/6000 [01:24<4:45:04,  2.87s/it]                                                   {'loss': 7.2626, 'grad_norm': 534.02001953125, 'learning_rate': 1.5e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:45:04,  2.87s/it]  1%|          | 31/6000 [01:27<4:40:53,  2.82s/it]                                                   {'loss': 5.7389, 'grad_norm': 293.63995361328125, 'learning_rate': 1.5500000000000002e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:27<4:40:53,  2.82s/it]  1%|          | 32/6000 [01:30<4:38:38,  2.80s/it]                                                   {'loss': 6.3607, 'grad_norm': 555.3851318359375, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:30<4:38:38,  2.80s/it]  1%|          | 33/6000 [01:33<4:37:08,  2.79s/it]                                                   {'loss': 6.4895, 'grad_norm': 411.18548583984375, 'learning_rate': 1.6500000000000003e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:33<4:37:08,  2.79s/it]  1%|          | 34/6000 [01:35<4:36:46,  2.78s/it]                                                   {'loss': 6.9221, 'grad_norm': 569.9043579101562, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:36:46,  2.78s/it]  1%|          | 35/6000 [01:38<4:36:35,  2.78s/it]                                                   {'loss': 5.758, 'grad_norm': 274.3092346191406, 'learning_rate': 1.75e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:38<4:36:35,  2.78s/it]  1%|          | 36/6000 [01:41<4:34:12,  2.76s/it]                                                   {'loss': 6.4367, 'grad_norm': 414.904541015625, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:41<4:34:12,  2.76s/it]  1%|          | 37/6000 [01:44<4:33:05,  2.75s/it]                                                   {'loss': 5.4092, 'grad_norm': 394.5413818359375, 'learning_rate': 1.85e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:44<4:33:05,  2.75s/it]  1%|          | 38/6000 [01:46<4:31:46,  2.74s/it]                                                   {'loss': 5.3906, 'grad_norm': 399.4877624511719, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:46<4:31:46,  2.74s/it]  1%|          | 39/6000 [01:49<4:31:54,  2.74s/it]                                                   {'loss': 4.237, 'grad_norm': 270.9654846191406, 'learning_rate': 1.9500000000000004e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:49<4:31:54,  2.74s/it]  1%|          | 40/6000 [01:52<4:32:13,  2.74s/it]                                                   {'loss': 5.0802, 'grad_norm': 353.7177734375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:52<4:32:13,  2.74s/it]  1%|          | 41/6000 [01:55<4:30:39,  2.73s/it]                                                   {'loss': 4.7445, 'grad_norm': 360.7178955078125, 'learning_rate': 2.05e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:55<4:30:39,  2.73s/it]  1%|          | 42/6000 [01:57<4:30:22,  2.72s/it]                                                   {'loss': 4.546, 'grad_norm': 421.66522216796875, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:57<4:30:22,  2.72s/it]  1%|          | 43/6000 [02:01<5:01:40,  3.04s/it]                                                   {'loss': 4.334, 'grad_norm': 306.6794128417969, 'learning_rate': 2.15e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:01<5:01:40,  3.04s/it]  1%|          | 44/6000 [02:04<5:06:55,  3.09s/it]                                                   {'loss': 3.7961, 'grad_norm': 318.8073425292969, 'learning_rate': 2.2e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:04<5:06:55,  3.09s/it]  1%|          | 45/6000 [02:07<4:56:54,  2.99s/it]                                                   {'loss': 4.0275, 'grad_norm': 420.9824523925781, 'learning_rate': 2.25e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:07<4:56:54,  2.99s/it]  1%|          | 46/6000 [02:10<4:53:18,  2.96s/it]                                                   {'loss': 3.3524, 'grad_norm': 195.80140686035156, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:10<4:53:18,  2.96s/it]  1%|          | 47/6000 [02:13<4:47:56,  2.90s/it]                                                   {'loss': 3.3703, 'grad_norm': 194.2559051513672, 'learning_rate': 2.35e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:13<4:47:56,  2.90s/it]  1%|          | 48/6000 [02:15<4:43:54,  2.86s/it]                                                   {'loss': 3.2785, 'grad_norm': 139.90077209472656, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:15<4:43:54,  2.86s/it]  1%|          | 49/6000 [02:18<4:38:35,  2.81s/it]                                                   {'loss': 3.2478, 'grad_norm': 193.01365661621094, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:18<4:38:35,  2.81s/it]  1%|          | 50/6000 [02:21<4:40:49,  2.83s/it]                                                   {'loss': 3.2385, 'grad_norm': 248.78878784179688, 'learning_rate': 2.5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:21<4:40:49,  2.83s/it][2025-10-23 00:28:50,258] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-10-23 00:28:50,270] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:28:50,891] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:26<5:41:12,  3.44s/it]                                                   {'loss': 3.3323, 'grad_norm': 99.49266052246094, 'learning_rate': 2.55e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:26<5:41:12,  3.44s/it]  1%|          | 52/6000 [02:29<5:19:16,  3.22s/it]                                                   {'loss': 3.2551, 'grad_norm': 114.44964599609375, 'learning_rate': 2.6e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:29<5:19:16,  3.22s/it]  1%|          | 53/6000 [02:31<5:06:16,  3.09s/it]                                                   {'loss': 3.7828, 'grad_norm': 185.3607940673828, 'learning_rate': 2.6500000000000005e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:31<5:06:16,  3.09s/it]  1%|          | 54/6000 [02:34<4:53:45,  2.96s/it]                                                   {'loss': 3.1772, 'grad_norm': 191.7880401611328, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:34<4:53:45,  2.96s/it]  1%|          | 55/6000 [02:37<4:46:21,  2.89s/it]                                                   {'loss': 3.4222, 'grad_norm': 209.0522003173828, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:37<4:46:21,  2.89s/it]  1%|          | 56/6000 [02:39<4:42:49,  2.85s/it]                                                   {'loss': 3.6827, 'grad_norm': 212.00790405273438, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:39<4:42:49,  2.85s/it]  1%|          | 57/6000 [02:42<4:38:04,  2.81s/it]                                                   {'loss': 3.199, 'grad_norm': 151.6741485595703, 'learning_rate': 2.85e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:42<4:38:04,  2.81s/it]  1%|          | 58/6000 [02:45<4:35:33,  2.78s/it]                                                   {'loss': 3.2264, 'grad_norm': 279.10516357421875, 'learning_rate': 2.9e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:45<4:35:33,  2.78s/it]  1%|          | 59/6000 [02:48<4:31:49,  2.75s/it]                                                   {'loss': 3.2212, 'grad_norm': 167.80177307128906, 'learning_rate': 2.95e-06, 'epoch': 0.01}
  1%|          | 59/6000 [02:48<4:31:49,  2.75s/it]  1%|          | 60/6000 [02:50<4:30:08,  2.73s/it]                                                   {'loss': 3.0321, 'grad_norm': 141.45828247070312, 'learning_rate': 3e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:50<4:30:08,  2.73s/it]  1%|          | 61/6000 [02:53<4:29:09,  2.72s/it]                                                   {'loss': 3.0041, 'grad_norm': 94.04246520996094, 'learning_rate': 3.05e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:53<4:29:09,  2.72s/it]  1%|          | 62/6000 [02:56<4:30:22,  2.73s/it]                                                   {'loss': 3.0064, 'grad_norm': 71.75276947021484, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:56<4:30:22,  2.73s/it]  1%|          | 63/6000 [02:58<4:29:22,  2.72s/it]                                                   {'loss': 3.0385, 'grad_norm': 123.53790283203125, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:58<4:29:22,  2.72s/it]  1%|          | 64/6000 [03:01<4:27:59,  2.71s/it]                                                   {'loss': 2.8631, 'grad_norm': 45.78572082519531, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:01<4:27:59,  2.71s/it]  1%|          | 65/6000 [03:04<4:27:02,  2.70s/it]                                                   {'loss': 3.0171, 'grad_norm': 132.6643524169922, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:04<4:27:02,  2.70s/it]  1%|          | 66/6000 [03:07<4:40:00,  2.83s/it]                                                   {'loss': 3.0684, 'grad_norm': 162.59616088867188, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:07<4:40:00,  2.83s/it]  1%|          | 67/6000 [03:10<4:37:53,  2.81s/it]                                                   {'loss': 3.0914, 'grad_norm': 195.05426025390625, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:10<4:37:53,  2.81s/it]  1%|          | 68/6000 [03:12<4:34:29,  2.78s/it]                                                   {'loss': 3.0719, 'grad_norm': 66.31611633300781, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:12<4:34:29,  2.78s/it]  1%|          | 69/6000 [03:15<4:32:09,  2.75s/it]                                                   {'loss': 3.0616, 'grad_norm': 85.21212768554688, 'learning_rate': 3.45e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:15<4:32:09,  2.75s/it]  1%|          | 70/6000 [03:18<4:32:59,  2.76s/it]                                                   {'loss': 2.9053, 'grad_norm': 182.000244140625, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:18<4:32:59,  2.76s/it]  1%|          | 71/6000 [03:21<4:30:32,  2.74s/it]                                                   {'loss': 3.0238, 'grad_norm': 108.76115417480469, 'learning_rate': 3.5500000000000003e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:21<4:30:32,  2.74s/it]  1%|          | 72/6000 [03:24<4:41:23,  2.85s/it]                                                   {'loss': 3.0, 'grad_norm': 126.94174194335938, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:24<4:41:23,  2.85s/it]  1%|          | 73/6000 [03:27<4:46:04,  2.90s/it]                                                   {'loss': 2.9497, 'grad_norm': 87.8965072631836, 'learning_rate': 3.65e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:27<4:46:04,  2.90s/it]  1%|          | 74/6000 [03:29<4:39:24,  2.83s/it]                                                   {'loss': 3.0543, 'grad_norm': 83.4417724609375, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:29<4:39:24,  2.83s/it]  1%|â–         | 75/6000 [03:32<4:34:36,  2.78s/it]                                                   {'loss': 2.8874, 'grad_norm': 60.70589065551758, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:32<4:34:36,  2.78s/it]  1%|â–         | 76/6000 [03:35<4:36:23,  2.80s/it]                                                   {'loss': 2.8332, 'grad_norm': 56.04691696166992, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:35<4:36:23,  2.80s/it]  1%|â–         | 77/6000 [03:38<4:35:47,  2.79s/it]                                                   {'loss': 3.0075, 'grad_norm': 65.11962127685547, 'learning_rate': 3.85e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:38<4:35:47,  2.79s/it]  1%|â–         | 78/6000 [03:41<4:47:24,  2.91s/it]                                                   {'loss': 3.248, 'grad_norm': 133.56820678710938, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:41<4:47:24,  2.91s/it]  1%|â–         | 79/6000 [03:44<4:42:59,  2.87s/it]                                                   {'loss': 2.8334, 'grad_norm': 41.52505111694336, 'learning_rate': 3.95e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:44<4:42:59,  2.87s/it]  1%|â–         | 80/6000 [03:47<4:49:28,  2.93s/it]                                                   {'loss': 2.9585, 'grad_norm': 91.1393051147461, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:47<4:49:28,  2.93s/it]  1%|â–         | 81/6000 [03:49<4:46:45,  2.91s/it]                                                   {'loss': 2.8225, 'grad_norm': 41.948829650878906, 'learning_rate': 4.05e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:49<4:46:45,  2.91s/it]  1%|â–         | 82/6000 [03:52<4:45:14,  2.89s/it]                                                   {'loss': 2.8625, 'grad_norm': 38.17771530151367, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:52<4:45:14,  2.89s/it]  1%|â–         | 83/6000 [03:55<4:42:41,  2.87s/it]                                                   {'loss': 2.8943, 'grad_norm': 71.6963119506836, 'learning_rate': 4.15e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:55<4:42:41,  2.87s/it]  1%|â–         | 84/6000 [03:58<4:42:46,  2.87s/it]                                                   {'loss': 2.8457, 'grad_norm': 51.324913024902344, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:58<4:42:46,  2.87s/it]  1%|â–         | 85/6000 [04:01<4:51:09,  2.95s/it]                                                   {'loss': 2.8632, 'grad_norm': 110.4267349243164, 'learning_rate': 4.25e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:01<4:51:09,  2.95s/it]  1%|â–         | 86/6000 [04:04<4:45:55,  2.90s/it]                                                   {'loss': 2.8136, 'grad_norm': 73.13422393798828, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:04<4:45:55,  2.90s/it]  1%|â–         | 87/6000 [04:07<4:42:10,  2.86s/it]                                                   {'loss': 2.864, 'grad_norm': 61.41824722290039, 'learning_rate': 4.350000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:07<4:42:10,  2.86s/it]  1%|â–         | 88/6000 [04:10<4:40:34,  2.85s/it]                                                   {'loss': 2.9319, 'grad_norm': 96.93684387207031, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:10<4:40:34,  2.85s/it]  1%|â–         | 89/6000 [04:12<4:38:51,  2.83s/it]                                                   {'loss': 2.7743, 'grad_norm': 28.373424530029297, 'learning_rate': 4.450000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:12<4:38:51,  2.83s/it]  2%|â–         | 90/6000 [04:15<4:38:59,  2.83s/it]                                                   {'loss': 2.9167, 'grad_norm': 77.71045684814453, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:15<4:38:59,  2.83s/it]  2%|â–         | 91/6000 [04:18<4:38:31,  2.83s/it]                                                   {'loss': 2.8858, 'grad_norm': 65.68425750732422, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:18<4:38:31,  2.83s/it]  2%|â–         | 92/6000 [04:21<4:49:08,  2.94s/it]                                                   {'loss': 2.8686, 'grad_norm': 37.57170867919922, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:21<4:49:08,  2.94s/it]  2%|â–         | 93/6000 [04:24<4:48:12,  2.93s/it]                                                   {'loss': 2.817, 'grad_norm': 56.645973205566406, 'learning_rate': 4.65e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:24<4:48:12,  2.93s/it]  2%|â–         | 94/6000 [04:27<4:44:50,  2.89s/it]                                                   {'loss': 2.8364, 'grad_norm': 23.845252990722656, 'learning_rate': 4.7e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:27<4:44:50,  2.89s/it]  2%|â–         | 95/6000 [04:30<4:42:55,  2.87s/it]                                                   {'loss': 2.8016, 'grad_norm': 39.6398811340332, 'learning_rate': 4.75e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:30<4:42:55,  2.87s/it]  2%|â–         | 96/6000 [04:33<4:43:01,  2.88s/it]                                                   {'loss': 2.8147, 'grad_norm': 20.781951904296875, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:33<4:43:01,  2.88s/it]  2%|â–         | 97/6000 [04:35<4:42:26,  2.87s/it]                                                   {'loss': 2.8641, 'grad_norm': 40.404090881347656, 'learning_rate': 4.85e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:35<4:42:26,  2.87s/it]  2%|â–         | 98/6000 [04:38<4:36:51,  2.81s/it]                                                   {'loss': 2.7803, 'grad_norm': 31.42292594909668, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:38<4:36:51,  2.81s/it]  2%|â–         | 99/6000 [04:41<4:34:02,  2.79s/it]                                                   {'loss': 2.8208, 'grad_norm': 24.104581832885742, 'learning_rate': 4.95e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:41<4:34:02,  2.79s/it]  2%|â–         | 100/6000 [04:44<4:30:48,  2.75s/it]                                                    {'loss': 2.7836, 'grad_norm': 41.35264205932617, 'learning_rate': 5e-06, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:44<4:30:48,  2.75s/it][2025-10-23 00:31:12,845] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-10-23 00:31:12,856] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:31:13,461] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:49<6:02:10,  3.68s/it]                                                    {'loss': 2.8308, 'grad_norm': 46.770381927490234, 'learning_rate': 4.999152542372881e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:49<6:02:10,  3.68s/it]  2%|â–         | 102/6000 [04:52<5:35:27,  3.41s/it]                                                    {'loss': 2.8196, 'grad_norm': 16.913463592529297, 'learning_rate': 4.998305084745763e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:52<5:35:27,  3.41s/it]  2%|â–         | 103/6000 [04:55<5:19:35,  3.25s/it]                                                    {'loss': 2.842, 'grad_norm': 65.94464874267578, 'learning_rate': 4.9974576271186445e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:55<5:19:35,  3.25s/it]  2%|â–         | 104/6000 [04:58<5:11:01,  3.17s/it]                                                    {'loss': 2.7779, 'grad_norm': 23.638378143310547, 'learning_rate': 4.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:58<5:11:01,  3.17s/it]  2%|â–         | 105/6000 [05:01<5:01:49,  3.07s/it]                                                    {'loss': 2.8191, 'grad_norm': 60.367652893066406, 'learning_rate': 4.995762711864407e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [05:01<5:01:49,  3.07s/it]  2%|â–         | 106/6000 [05:04<4:56:15,  3.02s/it]                                                    {'loss': 2.784, 'grad_norm': 15.18035888671875, 'learning_rate': 4.9949152542372885e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:04<4:56:15,  3.02s/it]  2%|â–         | 107/6000 [05:06<4:46:42,  2.92s/it]                                                    {'loss': 2.8196, 'grad_norm': 37.4593505859375, 'learning_rate': 4.994067796610169e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:06<4:46:42,  2.92s/it]  2%|â–         | 108/6000 [05:09<4:44:56,  2.90s/it]                                                    {'loss': 2.8151, 'grad_norm': 61.40212631225586, 'learning_rate': 4.993220338983051e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:09<4:44:56,  2.90s/it]  2%|â–         | 109/6000 [05:12<4:52:19,  2.98s/it]                                                    {'loss': 2.8223, 'grad_norm': 33.59894561767578, 'learning_rate': 4.992372881355933e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:12<4:52:19,  2.98s/it]  2%|â–         | 110/6000 [05:15<4:46:53,  2.92s/it]                                                    {'loss': 2.8749, 'grad_norm': 33.0490608215332, 'learning_rate': 4.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:15<4:46:53,  2.92s/it]  2%|â–         | 111/6000 [05:18<4:44:28,  2.90s/it]                                                    {'loss': 2.7959, 'grad_norm': 38.31587219238281, 'learning_rate': 4.990677966101695e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:18<4:44:28,  2.90s/it]  2%|â–         | 112/6000 [05:21<4:56:41,  3.02s/it]                                                    {'loss': 2.8373, 'grad_norm': 68.60453796386719, 'learning_rate': 4.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:21<4:56:41,  3.02s/it]  2%|â–         | 113/6000 [05:24<4:48:07,  2.94s/it]                                                    {'loss': 2.7886, 'grad_norm': 15.943643569946289, 'learning_rate': 4.988983050847458e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:24<4:48:07,  2.94s/it]  2%|â–         | 114/6000 [05:27<4:43:44,  2.89s/it]                                                    {'loss': 2.8021, 'grad_norm': 58.14665985107422, 'learning_rate': 4.98813559322034e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:27<4:43:44,  2.89s/it]  2%|â–         | 115/6000 [05:30<4:39:25,  2.85s/it]                                                    {'loss': 2.856, 'grad_norm': 12.829139709472656, 'learning_rate': 4.987288135593221e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:30<4:39:25,  2.85s/it]  2%|â–         | 116/6000 [05:32<4:35:07,  2.81s/it]                                                    {'loss': 2.8031, 'grad_norm': 25.653217315673828, 'learning_rate': 4.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:32<4:35:07,  2.81s/it]  2%|â–         | 117/6000 [05:35<4:33:31,  2.79s/it]                                                    {'loss': 2.9253, 'grad_norm': 36.49930953979492, 'learning_rate': 4.985593220338983e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:35<4:33:31,  2.79s/it]  2%|â–         | 118/6000 [05:39<4:51:12,  2.97s/it]                                                    {'loss': 2.8501, 'grad_norm': 32.67579650878906, 'learning_rate': 4.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:39<4:51:12,  2.97s/it]  2%|â–         | 119/6000 [05:41<4:43:24,  2.89s/it]                                                    {'loss': 2.7818, 'grad_norm': 14.036734580993652, 'learning_rate': 4.9838983050847464e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:41<4:43:24,  2.89s/it]  2%|â–         | 120/6000 [05:44<4:41:03,  2.87s/it]                                                    {'loss': 2.7907, 'grad_norm': 53.26231002807617, 'learning_rate': 4.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:44<4:41:03,  2.87s/it]  2%|â–         | 121/6000 [05:47<4:39:36,  2.85s/it]                                                    {'loss': 2.7846, 'grad_norm': 35.29514694213867, 'learning_rate': 4.982203389830509e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:47<4:39:36,  2.85s/it]  2%|â–         | 122/6000 [05:50<4:42:23,  2.88s/it]                                                    {'loss': 2.8238, 'grad_norm': 72.18779754638672, 'learning_rate': 4.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:50<4:42:23,  2.88s/it]  2%|â–         | 123/6000 [05:53<4:38:25,  2.84s/it]                                                    {'loss': 2.8044, 'grad_norm': 12.302240371704102, 'learning_rate': 4.980508474576271e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:53<4:38:25,  2.84s/it]  2%|â–         | 124/6000 [05:55<4:33:29,  2.79s/it]                                                    {'loss': 2.822, 'grad_norm': 8.885993003845215, 'learning_rate': 4.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:55<4:33:29,  2.79s/it]  2%|â–         | 125/6000 [05:58<4:38:21,  2.84s/it]                                                    {'loss': 2.9475, 'grad_norm': 20.823808670043945, 'learning_rate': 4.9788135593220346e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:58<4:38:21,  2.84s/it]  2%|â–         | 126/6000 [06:01<4:38:15,  2.84s/it]                                                    {'loss': 2.7983, 'grad_norm': 20.754621505737305, 'learning_rate': 4.977966101694915e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [06:01<4:38:15,  2.84s/it]  2%|â–         | 127/6000 [06:04<4:40:37,  2.87s/it]                                                    {'loss': 2.7813, 'grad_norm': 24.56211280822754, 'learning_rate': 4.977118644067797e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:04<4:40:37,  2.87s/it]  2%|â–         | 128/6000 [06:07<4:35:39,  2.82s/it]                                                    {'loss': 2.786, 'grad_norm': 29.06827163696289, 'learning_rate': 4.976271186440678e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:07<4:35:39,  2.82s/it]  2%|â–         | 129/6000 [06:09<4:34:24,  2.80s/it]                                                    {'loss': 2.8681, 'grad_norm': 18.813655853271484, 'learning_rate': 4.97542372881356e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:09<4:34:24,  2.80s/it]  2%|â–         | 130/6000 [06:12<4:32:45,  2.79s/it]                                                    {'loss': 2.7622, 'grad_norm': 19.447265625, 'learning_rate': 4.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:12<4:32:45,  2.79s/it]  2%|â–         | 131/6000 [06:15<4:32:33,  2.79s/it]                                                    {'loss': 2.7861, 'grad_norm': 34.25400924682617, 'learning_rate': 4.973728813559323e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:15<4:32:33,  2.79s/it]  2%|â–         | 132/6000 [06:18<4:30:11,  2.76s/it]                                                    {'loss': 2.8115, 'grad_norm': 13.182577133178711, 'learning_rate': 4.9728813559322035e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:18<4:30:11,  2.76s/it]  2%|â–         | 133/6000 [06:20<4:28:46,  2.75s/it]                                                    {'loss': 2.7671, 'grad_norm': 15.28743839263916, 'learning_rate': 4.972033898305085e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:20<4:28:46,  2.75s/it]  2%|â–         | 134/6000 [06:23<4:34:13,  2.80s/it]                                                    {'loss': 2.7849, 'grad_norm': 22.873329162597656, 'learning_rate': 4.971186440677967e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:23<4:34:13,  2.80s/it]  2%|â–         | 135/6000 [06:26<4:32:41,  2.79s/it]                                                    {'loss': 2.7892, 'grad_norm': 13.858415603637695, 'learning_rate': 4.970338983050848e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:26<4:32:41,  2.79s/it]  2%|â–         | 136/6000 [06:29<4:31:45,  2.78s/it]                                                    {'loss': 2.7821, 'grad_norm': 22.524866104125977, 'learning_rate': 4.969491525423729e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:29<4:31:45,  2.78s/it]  2%|â–         | 137/6000 [06:32<4:48:18,  2.95s/it]                                                    {'loss': 2.7755, 'grad_norm': 16.208415985107422, 'learning_rate': 4.968644067796611e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:32<4:48:18,  2.95s/it]  2%|â–         | 138/6000 [06:35<4:40:40,  2.87s/it]                                                    {'loss': 2.7811, 'grad_norm': 10.779875755310059, 'learning_rate': 4.967796610169492e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:35<4:40:40,  2.87s/it]  2%|â–         | 139/6000 [06:38<4:40:09,  2.87s/it]                                                    {'loss': 2.8335, 'grad_norm': 35.806480407714844, 'learning_rate': 4.966949152542373e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:38<4:40:09,  2.87s/it]  2%|â–         | 140/6000 [06:41<4:47:20,  2.94s/it]                                                    {'loss': 2.7736, 'grad_norm': 10.618979454040527, 'learning_rate': 4.966101694915255e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:41<4:47:20,  2.94s/it]  2%|â–         | 141/6000 [06:44<4:41:28,  2.88s/it]                                                    {'loss': 2.7712, 'grad_norm': 19.494117736816406, 'learning_rate': 4.9652542372881365e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:44<4:41:28,  2.88s/it]  2%|â–         | 142/6000 [06:46<4:34:19,  2.81s/it]                                                    {'loss': 2.7832, 'grad_norm': 35.7634162902832, 'learning_rate': 4.964406779661017e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:46<4:34:19,  2.81s/it]  2%|â–         | 143/6000 [06:49<4:32:48,  2.79s/it]                                                    {'loss': 2.7604, 'grad_norm': 35.47395706176758, 'learning_rate': 4.963559322033898e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:49<4:32:48,  2.79s/it]  2%|â–         | 144/6000 [06:52<4:31:59,  2.79s/it]                                                    {'loss': 2.8017, 'grad_norm': 15.701753616333008, 'learning_rate': 4.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:52<4:31:59,  2.79s/it]  2%|â–         | 145/6000 [06:54<4:28:45,  2.75s/it]                                                    {'loss': 2.8197, 'grad_norm': 21.213226318359375, 'learning_rate': 4.961864406779661e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:54<4:28:45,  2.75s/it]  2%|â–         | 146/6000 [06:57<4:29:49,  2.77s/it]                                                    {'loss': 2.8195, 'grad_norm': 24.012832641601562, 'learning_rate': 4.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:57<4:29:49,  2.77s/it]  2%|â–         | 147/6000 [07:00<4:27:56,  2.75s/it]                                                    {'loss': 2.7919, 'grad_norm': 19.798721313476562, 'learning_rate': 4.960169491525424e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [07:00<4:27:56,  2.75s/it]  2%|â–         | 148/6000 [07:03<4:27:32,  2.74s/it]                                                    {'loss': 2.7971, 'grad_norm': 25.369720458984375, 'learning_rate': 4.9593220338983054e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [07:03<4:27:32,  2.74s/it]  2%|â–         | 149/6000 [07:06<4:31:55,  2.79s/it]                                                    {'loss': 2.7678, 'grad_norm': 21.495758056640625, 'learning_rate': 4.958474576271187e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:06<4:31:55,  2.79s/it]  2%|â–Ž         | 150/6000 [07:08<4:29:51,  2.77s/it]                                                    {'loss': 2.8856, 'grad_norm': 54.501949310302734, 'learning_rate': 4.957627118644069e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:08<4:29:51,  2.77s/it][2025-10-23 00:33:37,596] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-10-23 00:33:37,609] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:33:38,261] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:13<5:36:08,  3.45s/it]                                                    {'loss': 2.7848, 'grad_norm': 48.19898223876953, 'learning_rate': 4.9567796610169495e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:13<5:36:08,  3.45s/it]  3%|â–Ž         | 152/6000 [07:16<5:14:59,  3.23s/it]                                                    {'loss': 2.7898, 'grad_norm': 14.552070617675781, 'learning_rate': 4.955932203389831e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:16<5:14:59,  3.23s/it]  3%|â–Ž         | 153/6000 [07:19<4:58:23,  3.06s/it]                                                    {'loss': 2.798, 'grad_norm': 42.236080169677734, 'learning_rate': 4.955084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:19<4:58:23,  3.06s/it]  3%|â–Ž         | 154/6000 [07:22<4:50:29,  2.98s/it]                                                    {'loss': 2.7853, 'grad_norm': 46.694271087646484, 'learning_rate': 4.9542372881355936e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:22<4:50:29,  2.98s/it]  3%|â–Ž         | 155/6000 [07:24<4:42:56,  2.90s/it]                                                    {'loss': 2.8184, 'grad_norm': 21.733705520629883, 'learning_rate': 4.953389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:24<4:42:56,  2.90s/it]  3%|â–Ž         | 156/6000 [07:27<4:39:15,  2.87s/it]                                                    {'loss': 2.8462, 'grad_norm': 29.18792724609375, 'learning_rate': 4.952542372881357e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:27<4:39:15,  2.87s/it]  3%|â–Ž         | 157/6000 [07:30<4:49:12,  2.97s/it]                                                    {'loss': 2.7848, 'grad_norm': 20.070552825927734, 'learning_rate': 4.951694915254238e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:30<4:49:12,  2.97s/it]  3%|â–Ž         | 158/6000 [07:33<4:42:04,  2.90s/it]                                                    {'loss': 2.8048, 'grad_norm': 13.73369026184082, 'learning_rate': 4.950847457627119e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:33<4:42:04,  2.90s/it]  3%|â–Ž         | 159/6000 [07:36<4:36:48,  2.84s/it]                                                    {'loss': 2.7877, 'grad_norm': 7.983635425567627, 'learning_rate': 4.95e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:36<4:36:48,  2.84s/it]  3%|â–Ž         | 160/6000 [07:39<4:50:46,  2.99s/it]                                                    {'loss': 2.8305, 'grad_norm': 12.819462776184082, 'learning_rate': 4.949152542372882e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:39<4:50:46,  2.99s/it]  3%|â–Ž         | 161/6000 [07:42<4:48:09,  2.96s/it]                                                    {'loss': 2.7822, 'grad_norm': 8.614446640014648, 'learning_rate': 4.948305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:42<4:48:09,  2.96s/it]  3%|â–Ž         | 162/6000 [07:45<4:40:16,  2.88s/it]                                                    {'loss': 2.7912, 'grad_norm': 10.551363945007324, 'learning_rate': 4.947457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:45<4:40:16,  2.88s/it]  3%|â–Ž         | 163/6000 [07:48<4:53:46,  3.02s/it]                                                    {'loss': 2.7556, 'grad_norm': 50.26981735229492, 'learning_rate': 4.946610169491526e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:48<4:53:46,  3.02s/it]  3%|â–Ž         | 164/6000 [07:51<4:44:36,  2.93s/it]                                                    {'loss': 2.9072, 'grad_norm': 9.076498031616211, 'learning_rate': 4.9457627118644065e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:51<4:44:36,  2.93s/it]  3%|â–Ž         | 165/6000 [07:53<4:41:40,  2.90s/it]                                                    {'loss': 2.7775, 'grad_norm': 22.769094467163086, 'learning_rate': 4.944915254237288e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:53<4:41:40,  2.90s/it]  3%|â–Ž         | 166/6000 [07:56<4:37:19,  2.85s/it]                                                    {'loss': 2.7744, 'grad_norm': 25.36570167541504, 'learning_rate': 4.94406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:56<4:37:19,  2.85s/it]  3%|â–Ž         | 167/6000 [07:59<4:35:35,  2.83s/it]                                                    {'loss': 2.7841, 'grad_norm': 11.04550838470459, 'learning_rate': 4.9432203389830514e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:59<4:35:35,  2.83s/it]  3%|â–Ž         | 168/6000 [08:02<4:34:00,  2.82s/it]                                                    {'loss': 2.8335, 'grad_norm': 33.53510284423828, 'learning_rate': 4.942372881355932e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [08:02<4:34:00,  2.82s/it]  3%|â–Ž         | 169/6000 [08:05<4:44:54,  2.93s/it]                                                    {'loss': 2.8228, 'grad_norm': 12.16828441619873, 'learning_rate': 4.941525423728814e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:05<4:44:54,  2.93s/it]  3%|â–Ž         | 170/6000 [08:08<4:36:36,  2.85s/it]                                                    {'loss': 2.7757, 'grad_norm': 8.4061279296875, 'learning_rate': 4.9406779661016955e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:08<4:36:36,  2.85s/it]  3%|â–Ž         | 171/6000 [08:10<4:32:04,  2.80s/it]                                                    {'loss': 2.7693, 'grad_norm': 25.060184478759766, 'learning_rate': 4.939830508474577e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:10<4:32:04,  2.80s/it]  3%|â–Ž         | 172/6000 [08:13<4:33:42,  2.82s/it]                                                    {'loss': 2.8159, 'grad_norm': 14.406643867492676, 'learning_rate': 4.938983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:13<4:33:42,  2.82s/it]  3%|â–Ž         | 173/6000 [08:16<4:31:57,  2.80s/it]                                                    {'loss': 2.7776, 'grad_norm': 17.3490047454834, 'learning_rate': 4.9381355932203396e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:16<4:31:57,  2.80s/it]  3%|â–Ž         | 174/6000 [08:19<4:49:25,  2.98s/it]                                                    {'loss': 2.7638, 'grad_norm': 9.828994750976562, 'learning_rate': 4.93728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:19<4:49:25,  2.98s/it]  3%|â–Ž         | 175/6000 [08:22<4:44:54,  2.93s/it]                                                    {'loss': 2.7905, 'grad_norm': 20.94748306274414, 'learning_rate': 4.936440677966102e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:22<4:44:54,  2.93s/it]  3%|â–Ž         | 176/6000 [08:25<4:48:33,  2.97s/it]                                                    {'loss': 2.7798, 'grad_norm': 14.824119567871094, 'learning_rate': 4.935593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:25<4:48:33,  2.97s/it]  3%|â–Ž         | 177/6000 [08:28<4:40:30,  2.89s/it]                                                    {'loss': 2.7824, 'grad_norm': 12.295769691467285, 'learning_rate': 4.934745762711865e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:28<4:40:30,  2.89s/it]  3%|â–Ž         | 178/6000 [08:31<4:39:44,  2.88s/it]                                                    {'loss': 2.7941, 'grad_norm': 20.798601150512695, 'learning_rate': 4.933898305084746e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:31<4:39:44,  2.88s/it]  3%|â–Ž         | 179/6000 [08:34<4:35:11,  2.84s/it]                                                    {'loss': 2.7922, 'grad_norm': 13.816494941711426, 'learning_rate': 4.933050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:34<4:35:11,  2.84s/it]  3%|â–Ž         | 180/6000 [08:36<4:35:22,  2.84s/it]                                                    {'loss': 2.7842, 'grad_norm': 18.458131790161133, 'learning_rate': 4.9322033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:36<4:35:22,  2.84s/it]  3%|â–Ž         | 181/6000 [08:39<4:33:37,  2.82s/it]                                                    {'loss': 2.8051, 'grad_norm': 21.692062377929688, 'learning_rate': 4.93135593220339e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:39<4:33:37,  2.82s/it]  3%|â–Ž         | 182/6000 [08:42<4:31:25,  2.80s/it]                                                    {'loss': 2.8263, 'grad_norm': 11.504865646362305, 'learning_rate': 4.930508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:42<4:31:25,  2.80s/it]  3%|â–Ž         | 183/6000 [08:45<4:28:44,  2.77s/it]                                                    {'loss': 2.7717, 'grad_norm': 11.057597160339355, 'learning_rate': 4.929661016949153e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:45<4:28:44,  2.77s/it]  3%|â–Ž         | 184/6000 [08:47<4:28:44,  2.77s/it]                                                    {'loss': 2.7697, 'grad_norm': 17.162294387817383, 'learning_rate': 4.928813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:47<4:28:44,  2.77s/it]  3%|â–Ž         | 185/6000 [08:50<4:27:43,  2.76s/it]                                                    {'loss': 2.7891, 'grad_norm': 28.757450103759766, 'learning_rate': 4.927966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:50<4:27:43,  2.76s/it]  3%|â–Ž         | 186/6000 [08:53<4:27:11,  2.76s/it]                                                    {'loss': 2.7693, 'grad_norm': 10.108061790466309, 'learning_rate': 4.9271186440677975e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:53<4:27:11,  2.76s/it]  3%|â–Ž         | 187/6000 [08:56<4:26:01,  2.75s/it]                                                    {'loss': 2.7956, 'grad_norm': 12.5277099609375, 'learning_rate': 4.926271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:56<4:26:01,  2.75s/it]  3%|â–Ž         | 188/6000 [08:58<4:25:33,  2.74s/it]                                                    {'loss': 2.7757, 'grad_norm': 9.21733283996582, 'learning_rate': 4.92542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:58<4:25:33,  2.74s/it]  3%|â–Ž         | 189/6000 [09:01<4:26:01,  2.75s/it]                                                    {'loss': 2.7717, 'grad_norm': 11.930604934692383, 'learning_rate': 4.924576271186441e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [09:01<4:26:01,  2.75s/it]  3%|â–Ž         | 190/6000 [09:04<4:24:22,  2.73s/it]                                                    {'loss': 2.797, 'grad_norm': 12.917581558227539, 'learning_rate': 4.923728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [09:04<4:24:22,  2.73s/it]  3%|â–Ž         | 191/6000 [09:07<4:25:16,  2.74s/it]                                                    {'loss': 2.7674, 'grad_norm': 7.925787448883057, 'learning_rate': 4.922881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:07<4:25:16,  2.74s/it]  3%|â–Ž         | 192/6000 [09:10<4:42:41,  2.92s/it]                                                    {'loss': 2.8707, 'grad_norm': 8.690713882446289, 'learning_rate': 4.922033898305086e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:10<4:42:41,  2.92s/it]  3%|â–Ž         | 193/6000 [09:13<4:47:05,  2.97s/it]                                                    {'loss': 2.772, 'grad_norm': 15.352931022644043, 'learning_rate': 4.921186440677966e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:13<4:47:05,  2.97s/it]  3%|â–Ž         | 194/6000 [09:16<4:37:37,  2.87s/it]                                                    {'loss': 2.7867, 'grad_norm': 8.129373550415039, 'learning_rate': 4.920338983050848e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:16<4:37:37,  2.87s/it]  3%|â–Ž         | 195/6000 [09:18<4:36:03,  2.85s/it]                                                    {'loss': 2.8335, 'grad_norm': 27.219745635986328, 'learning_rate': 4.919491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:18<4:36:03,  2.85s/it]  3%|â–Ž         | 196/6000 [09:21<4:33:01,  2.82s/it]                                                    {'loss': 2.7839, 'grad_norm': 13.264339447021484, 'learning_rate': 4.9186440677966104e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:21<4:33:01,  2.82s/it]  3%|â–Ž         | 197/6000 [09:24<4:34:36,  2.84s/it]                                                    {'loss': 2.8, 'grad_norm': 12.673644065856934, 'learning_rate': 4.917796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:24<4:34:36,  2.84s/it]  3%|â–Ž         | 198/6000 [09:27<4:34:16,  2.84s/it]                                                    {'loss': 2.7832, 'grad_norm': 11.055702209472656, 'learning_rate': 4.916949152542374e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:27<4:34:16,  2.84s/it]  3%|â–Ž         | 199/6000 [09:30<4:31:18,  2.81s/it]                                                    {'loss': 2.7793, 'grad_norm': 11.056346893310547, 'learning_rate': 4.9161016949152545e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:30<4:31:18,  2.81s/it]  3%|â–Ž         | 200/6000 [09:33<4:44:29,  2.94s/it]                                                    {'loss': 2.7833, 'grad_norm': 13.651615142822266, 'learning_rate': 4.915254237288136e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:33<4:44:29,  2.94s/it][2025-10-23 00:36:02,174] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-10-23 00:36:02,187] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:36:02,792] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:38<5:42:55,  3.55s/it]                                                    {'loss': 2.7869, 'grad_norm': 8.58098316192627, 'learning_rate': 4.914406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:38<5:42:55,  3.55s/it]  3%|â–Ž         | 202/6000 [09:41<5:19:03,  3.30s/it]                                                    {'loss': 2.7761, 'grad_norm': 9.774658203125, 'learning_rate': 4.9135593220338986e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:41<5:19:03,  3.30s/it]  3%|â–Ž         | 203/6000 [09:43<5:03:37,  3.14s/it]                                                    {'loss': 2.7799, 'grad_norm': 10.76868724822998, 'learning_rate': 4.91271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:43<5:03:37,  3.14s/it]  3%|â–Ž         | 204/6000 [09:46<4:54:19,  3.05s/it]                                                    {'loss': 2.7854, 'grad_norm': 18.892919540405273, 'learning_rate': 4.911864406779661e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:46<4:54:19,  3.05s/it]  3%|â–Ž         | 205/6000 [09:49<4:43:51,  2.94s/it]                                                    {'loss': 2.7586, 'grad_norm': 21.242401123046875, 'learning_rate': 4.911016949152543e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:49<4:43:51,  2.94s/it]  3%|â–Ž         | 206/6000 [09:52<4:36:44,  2.87s/it]                                                    {'loss': 2.8355, 'grad_norm': 7.991968631744385, 'learning_rate': 4.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:52<4:36:44,  2.87s/it]  3%|â–Ž         | 207/6000 [09:54<4:31:52,  2.82s/it]                                                    {'loss': 2.8766, 'grad_norm': 9.85266399383545, 'learning_rate': 4.909322033898306e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:54<4:31:52,  2.82s/it]  3%|â–Ž         | 208/6000 [09:57<4:28:53,  2.79s/it]                                                    {'loss': 2.8469, 'grad_norm': 14.301871299743652, 'learning_rate': 4.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:57<4:28:53,  2.79s/it]  3%|â–Ž         | 209/6000 [10:00<4:29:45,  2.79s/it]                                                    {'loss': 2.7805, 'grad_norm': 8.704198837280273, 'learning_rate': 4.907627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [10:00<4:29:45,  2.79s/it]  4%|â–Ž         | 210/6000 [10:03<4:40:31,  2.91s/it]                                                    {'loss': 2.7763, 'grad_norm': 8.576189041137695, 'learning_rate': 4.906779661016949e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [10:03<4:40:31,  2.91s/it]  4%|â–Ž         | 211/6000 [10:06<4:41:34,  2.92s/it]                                                    {'loss': 2.7944, 'grad_norm': 21.69361114501953, 'learning_rate': 4.905932203389831e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:06<4:41:34,  2.92s/it]  4%|â–Ž         | 212/6000 [10:09<4:47:42,  2.98s/it]                                                    {'loss': 2.7682, 'grad_norm': 11.603376388549805, 'learning_rate': 4.905084745762712e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:09<4:47:42,  2.98s/it]  4%|â–Ž         | 213/6000 [10:12<4:42:42,  2.93s/it]                                                    {'loss': 2.7915, 'grad_norm': 18.270811080932617, 'learning_rate': 4.904237288135594e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:12<4:42:42,  2.93s/it]  4%|â–Ž         | 214/6000 [10:15<4:35:46,  2.86s/it]                                                    {'loss': 2.7795, 'grad_norm': 9.162145614624023, 'learning_rate': 4.903389830508475e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:15<4:35:46,  2.86s/it]  4%|â–Ž         | 215/6000 [10:17<4:31:53,  2.82s/it]                                                    {'loss': 2.8024, 'grad_norm': 9.141225814819336, 'learning_rate': 4.9025423728813565e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:17<4:31:53,  2.82s/it]  4%|â–Ž         | 216/6000 [10:20<4:31:53,  2.82s/it]                                                    {'loss': 2.7813, 'grad_norm': 12.760625839233398, 'learning_rate': 4.901694915254237e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:20<4:31:53,  2.82s/it]  4%|â–Ž         | 217/6000 [10:23<4:33:16,  2.84s/it]                                                    {'loss': 2.8058, 'grad_norm': 16.506662368774414, 'learning_rate': 4.900847457627119e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:23<4:33:16,  2.84s/it]  4%|â–Ž         | 218/6000 [10:26<4:33:25,  2.84s/it]                                                    {'loss': 2.8135, 'grad_norm': 18.857088088989258, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:26<4:33:25,  2.84s/it]  4%|â–Ž         | 219/6000 [10:28<4:29:54,  2.80s/it]                                                    {'loss': 2.8233, 'grad_norm': 13.231144905090332, 'learning_rate': 4.899152542372882e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:29<4:29:54,  2.80s/it]  4%|â–Ž         | 220/6000 [10:31<4:28:24,  2.79s/it]                                                    {'loss': 2.7823, 'grad_norm': 13.356584548950195, 'learning_rate': 4.898305084745763e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:31<4:28:24,  2.79s/it]  4%|â–Ž         | 221/6000 [10:34<4:29:27,  2.80s/it]                                                    {'loss': 2.7871, 'grad_norm': 7.245900630950928, 'learning_rate': 4.897457627118645e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:34<4:29:27,  2.80s/it]  4%|â–Ž         | 222/6000 [10:37<4:28:07,  2.78s/it]                                                    {'loss': 2.7859, 'grad_norm': 7.0835723876953125, 'learning_rate': 4.896610169491525e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:37<4:28:07,  2.78s/it]  4%|â–Ž         | 223/6000 [10:40<4:31:55,  2.82s/it]                                                    {'loss': 2.7622, 'grad_norm': 9.207929611206055, 'learning_rate': 4.895762711864408e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:40<4:31:55,  2.82s/it]  4%|â–Ž         | 224/6000 [10:42<4:28:00,  2.78s/it]                                                    {'loss': 2.8147, 'grad_norm': 8.379280090332031, 'learning_rate': 4.894915254237289e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:42<4:28:00,  2.78s/it]  4%|â–         | 225/6000 [10:45<4:28:54,  2.79s/it]                                                    {'loss': 2.7791, 'grad_norm': 8.279467582702637, 'learning_rate': 4.8940677966101694e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:45<4:28:54,  2.79s/it]  4%|â–         | 226/6000 [10:48<4:40:55,  2.92s/it]                                                    {'loss': 2.7624, 'grad_norm': 23.308551788330078, 'learning_rate': 4.893220338983051e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:48<4:40:55,  2.92s/it]  4%|â–         | 227/6000 [10:51<4:34:03,  2.85s/it]                                                    {'loss': 2.8207, 'grad_norm': 14.847111701965332, 'learning_rate': 4.892372881355933e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:51<4:34:03,  2.85s/it]  4%|â–         | 228/6000 [10:54<4:32:16,  2.83s/it]                                                    {'loss': 2.767, 'grad_norm': 12.012645721435547, 'learning_rate': 4.891525423728814e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:54<4:32:16,  2.83s/it]  4%|â–         | 229/6000 [10:57<4:28:15,  2.79s/it]                                                    {'loss': 2.793, 'grad_norm': 10.505533218383789, 'learning_rate': 4.890677966101695e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:57<4:28:15,  2.79s/it]  4%|â–         | 230/6000 [10:59<4:28:14,  2.79s/it]                                                    {'loss': 2.7889, 'grad_norm': 8.65564250946045, 'learning_rate': 4.889830508474577e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:59<4:28:14,  2.79s/it]  4%|â–         | 231/6000 [11:02<4:26:02,  2.77s/it]                                                    {'loss': 2.7885, 'grad_norm': 9.096731185913086, 'learning_rate': 4.8889830508474576e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [11:02<4:26:02,  2.77s/it]  4%|â–         | 232/6000 [11:05<4:25:07,  2.76s/it]                                                    {'loss': 2.7747, 'grad_norm': 7.862581253051758, 'learning_rate': 4.888135593220339e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [11:05<4:25:07,  2.76s/it]  4%|â–         | 233/6000 [11:08<4:34:42,  2.86s/it]                                                    {'loss': 2.7894, 'grad_norm': 22.06032943725586, 'learning_rate': 4.887288135593221e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:08<4:34:42,  2.86s/it]  4%|â–         | 234/6000 [11:11<4:39:23,  2.91s/it]                                                    {'loss': 2.7606, 'grad_norm': 8.914477348327637, 'learning_rate': 4.8864406779661025e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:11<4:39:23,  2.91s/it]  4%|â–         | 235/6000 [11:14<4:31:58,  2.83s/it]                                                    {'loss': 2.8496, 'grad_norm': 11.103084564208984, 'learning_rate': 4.885593220338983e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:14<4:31:58,  2.83s/it]  4%|â–         | 236/6000 [11:16<4:29:49,  2.81s/it]                                                    {'loss': 2.7994, 'grad_norm': 15.574858665466309, 'learning_rate': 4.884745762711865e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:16<4:29:49,  2.81s/it]  4%|â–         | 237/6000 [11:19<4:29:06,  2.80s/it]                                                    {'loss': 2.8183, 'grad_norm': 28.695375442504883, 'learning_rate': 4.883898305084746e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:19<4:29:06,  2.80s/it]  4%|â–         | 238/6000 [11:22<4:42:00,  2.94s/it]                                                    {'loss': 2.8536, 'grad_norm': 16.93340301513672, 'learning_rate': 4.883050847457627e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:22<4:42:00,  2.94s/it]  4%|â–         | 239/6000 [11:25<4:38:11,  2.90s/it]                                                    {'loss': 2.769, 'grad_norm': 12.369025230407715, 'learning_rate': 4.882203389830509e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:25<4:38:11,  2.90s/it]  4%|â–         | 240/6000 [11:28<4:34:02,  2.85s/it]                                                    {'loss': 2.7854, 'grad_norm': 10.79375171661377, 'learning_rate': 4.881355932203391e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:28<4:34:02,  2.85s/it]  4%|â–         | 241/6000 [11:31<4:31:36,  2.83s/it]                                                    {'loss': 2.7831, 'grad_norm': 18.166200637817383, 'learning_rate': 4.880508474576271e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:31<4:31:36,  2.83s/it]  4%|â–         | 242/6000 [11:33<4:26:45,  2.78s/it]                                                    {'loss': 2.8005, 'grad_norm': 8.941896438598633, 'learning_rate': 4.879661016949153e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:33<4:26:45,  2.78s/it]  4%|â–         | 243/6000 [11:36<4:34:45,  2.86s/it]                                                    {'loss': 2.7934, 'grad_norm': 11.273163795471191, 'learning_rate': 4.878813559322035e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:36<4:34:45,  2.86s/it]  4%|â–         | 244/6000 [11:39<4:30:10,  2.82s/it]                                                    {'loss': 2.7696, 'grad_norm': 10.467207908630371, 'learning_rate': 4.877966101694916e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:39<4:30:10,  2.82s/it]  4%|â–         | 245/6000 [11:42<4:30:09,  2.82s/it]                                                    {'loss': 2.7846, 'grad_norm': 12.491877555847168, 'learning_rate': 4.877118644067797e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:42<4:30:09,  2.82s/it]  4%|â–         | 246/6000 [11:45<4:31:03,  2.83s/it]                                                    {'loss': 2.7961, 'grad_norm': 11.417987823486328, 'learning_rate': 4.876271186440678e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:45<4:31:03,  2.83s/it]  4%|â–         | 247/6000 [11:48<4:26:40,  2.78s/it]                                                    {'loss': 2.7682, 'grad_norm': 6.595901966094971, 'learning_rate': 4.8754237288135595e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:48<4:26:40,  2.78s/it]  4%|â–         | 248/6000 [11:51<4:33:30,  2.85s/it]                                                    {'loss': 2.777, 'grad_norm': 15.690900802612305, 'learning_rate': 4.874576271186441e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:51<4:33:30,  2.85s/it]  4%|â–         | 249/6000 [11:53<4:28:57,  2.81s/it]                                                    {'loss': 2.784, 'grad_norm': 24.070199966430664, 'learning_rate': 4.873728813559323e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:53<4:28:57,  2.81s/it]  4%|â–         | 250/6000 [11:56<4:25:40,  2.77s/it]                                                    {'loss': 2.7973, 'grad_norm': 7.435427665710449, 'learning_rate': 4.872881355932204e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:56<4:25:40,  2.77s/it][2025-10-23 00:38:25,233] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
[2025-10-23 00:38:25,243] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:38:25,840] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [12:01<5:24:53,  3.39s/it]                                                    {'loss': 2.7909, 'grad_norm': 13.006241798400879, 'learning_rate': 4.872033898305085e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [12:01<5:24:53,  3.39s/it]  4%|â–         | 252/6000 [12:04<5:13:19,  3.27s/it]                                                    {'loss': 2.8344, 'grad_norm': 13.130356788635254, 'learning_rate': 4.871186440677966e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [12:04<5:13:19,  3.27s/it]  4%|â–         | 253/6000 [12:06<4:57:42,  3.11s/it]                                                    {'loss': 2.8045, 'grad_norm': 14.594277381896973, 'learning_rate': 4.870338983050848e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [12:06<4:57:42,  3.11s/it]  4%|â–         | 254/6000 [12:09<4:49:23,  3.02s/it]                                                    {'loss': 2.7598, 'grad_norm': 24.338417053222656, 'learning_rate': 4.869491525423729e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:09<4:49:23,  3.02s/it]  4%|â–         | 255/6000 [12:12<4:39:46,  2.92s/it]                                                    {'loss': 2.7711, 'grad_norm': 8.75771713256836, 'learning_rate': 4.868644067796611e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:12<4:39:46,  2.92s/it]  4%|â–         | 256/6000 [12:15<4:47:01,  3.00s/it]                                                    {'loss': 2.7741, 'grad_norm': 28.825061798095703, 'learning_rate': 4.867796610169492e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:15<4:47:01,  3.00s/it]  4%|â–         | 257/6000 [12:18<4:39:26,  2.92s/it]                                                    {'loss': 2.7704, 'grad_norm': 9.69129467010498, 'learning_rate': 4.866949152542373e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:18<4:39:26,  2.92s/it]  4%|â–         | 258/6000 [12:21<4:33:29,  2.86s/it]                                                    {'loss': 2.7823, 'grad_norm': 8.839325904846191, 'learning_rate': 4.866101694915254e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:21<4:33:29,  2.86s/it]  4%|â–         | 259/6000 [12:23<4:28:02,  2.80s/it]                                                    {'loss': 2.7804, 'grad_norm': 14.611761093139648, 'learning_rate': 4.865254237288136e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:23<4:28:02,  2.80s/it]  4%|â–         | 260/6000 [12:26<4:26:03,  2.78s/it]                                                    {'loss': 2.7964, 'grad_norm': 11.42992877960205, 'learning_rate': 4.864406779661017e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:26<4:26:03,  2.78s/it]  4%|â–         | 261/6000 [12:29<4:29:55,  2.82s/it]                                                    {'loss': 2.7995, 'grad_norm': 7.656152725219727, 'learning_rate': 4.863559322033899e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:29<4:29:55,  2.82s/it]  4%|â–         | 262/6000 [12:32<4:28:09,  2.80s/it]                                                    {'loss': 2.772, 'grad_norm': 11.62927532196045, 'learning_rate': 4.86271186440678e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:32<4:28:09,  2.80s/it]  4%|â–         | 263/6000 [12:35<4:36:43,  2.89s/it]                                                    {'loss': 2.8257, 'grad_norm': 9.639254570007324, 'learning_rate': 4.8618644067796615e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:35<4:36:43,  2.89s/it]  4%|â–         | 264/6000 [12:38<4:31:07,  2.84s/it]                                                    {'loss': 2.7928, 'grad_norm': 10.783960342407227, 'learning_rate': 4.861016949152543e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:38<4:31:07,  2.84s/it]  4%|â–         | 265/6000 [12:40<4:33:44,  2.86s/it]                                                    {'loss': 2.7719, 'grad_norm': 7.797488689422607, 'learning_rate': 4.860169491525425e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:40<4:33:44,  2.86s/it]  4%|â–         | 266/6000 [12:43<4:34:48,  2.88s/it]                                                    {'loss': 2.807, 'grad_norm': 16.333709716796875, 'learning_rate': 4.8593220338983055e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:43<4:34:48,  2.88s/it]  4%|â–         | 267/6000 [12:46<4:30:37,  2.83s/it]                                                    {'loss': 2.7733, 'grad_norm': 6.989773273468018, 'learning_rate': 4.858474576271186e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:46<4:30:37,  2.83s/it]  4%|â–         | 268/6000 [12:49<4:27:58,  2.80s/it]                                                    {'loss': 2.8437, 'grad_norm': 27.50626564025879, 'learning_rate': 4.857627118644068e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:49<4:27:58,  2.80s/it]  4%|â–         | 269/6000 [12:52<4:29:56,  2.83s/it]                                                    {'loss': 2.7602, 'grad_norm': 14.353041648864746, 'learning_rate': 4.85677966101695e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:52<4:29:56,  2.83s/it]  4%|â–         | 270/6000 [12:54<4:28:23,  2.81s/it]                                                    {'loss': 2.7694, 'grad_norm': 17.484453201293945, 'learning_rate': 4.855932203389831e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:54<4:28:23,  2.81s/it]  5%|â–         | 271/6000 [12:57<4:25:19,  2.78s/it]                                                    {'loss': 2.7934, 'grad_norm': 21.039560317993164, 'learning_rate': 4.855084745762712e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:57<4:25:19,  2.78s/it]  5%|â–         | 272/6000 [13:00<4:23:55,  2.76s/it]                                                    {'loss': 2.7805, 'grad_norm': 10.291504859924316, 'learning_rate': 4.854237288135594e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [13:00<4:23:55,  2.76s/it]  5%|â–         | 273/6000 [13:03<4:23:53,  2.76s/it]                                                    {'loss': 2.8185, 'grad_norm': 18.99921226501465, 'learning_rate': 4.8533898305084745e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [13:03<4:23:53,  2.76s/it]  5%|â–         | 274/6000 [13:06<4:34:42,  2.88s/it]                                                    {'loss': 2.7618, 'grad_norm': 12.830296516418457, 'learning_rate': 4.852542372881356e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [13:06<4:34:42,  2.88s/it]  5%|â–         | 275/6000 [13:09<4:32:09,  2.85s/it]                                                    {'loss': 2.7936, 'grad_norm': 17.401939392089844, 'learning_rate': 4.851694915254238e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:09<4:32:09,  2.85s/it]  5%|â–         | 276/6000 [13:11<4:31:09,  2.84s/it]                                                    {'loss': 2.8457, 'grad_norm': 10.957002639770508, 'learning_rate': 4.850847457627119e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:11<4:31:09,  2.84s/it]  5%|â–         | 277/6000 [13:14<4:26:31,  2.79s/it]                                                    {'loss': 2.771, 'grad_norm': 13.605552673339844, 'learning_rate': 4.85e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:14<4:26:31,  2.79s/it]  5%|â–         | 278/6000 [13:17<4:26:18,  2.79s/it]                                                    {'loss': 2.7831, 'grad_norm': 8.484208106994629, 'learning_rate': 4.849152542372882e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:17<4:26:18,  2.79s/it]  5%|â–         | 279/6000 [13:20<4:25:52,  2.79s/it]                                                    {'loss': 2.8027, 'grad_norm': 17.930747985839844, 'learning_rate': 4.8483050847457634e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:20<4:25:52,  2.79s/it]  5%|â–         | 280/6000 [13:22<4:23:52,  2.77s/it]                                                    {'loss': 2.7764, 'grad_norm': 10.143795013427734, 'learning_rate': 4.847457627118645e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:22<4:23:52,  2.77s/it]  5%|â–         | 281/6000 [13:25<4:23:44,  2.77s/it]                                                    {'loss': 2.7688, 'grad_norm': 37.6278190612793, 'learning_rate': 4.846610169491526e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:25<4:23:44,  2.77s/it]  5%|â–         | 282/6000 [13:28<4:34:15,  2.88s/it]                                                    {'loss': 2.7748, 'grad_norm': 16.112552642822266, 'learning_rate': 4.8457627118644075e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:28<4:34:15,  2.88s/it]  5%|â–         | 283/6000 [13:31<4:38:38,  2.92s/it]                                                    {'loss': 2.7731, 'grad_norm': 27.556243896484375, 'learning_rate': 4.844915254237288e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:31<4:38:38,  2.92s/it]  5%|â–         | 284/6000 [13:35<4:52:20,  3.07s/it]                                                    {'loss': 2.8016, 'grad_norm': 11.16849422454834, 'learning_rate': 4.84406779661017e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:35<4:52:20,  3.07s/it]  5%|â–         | 285/6000 [13:38<4:46:36,  3.01s/it]                                                    {'loss': 2.776, 'grad_norm': 18.68035125732422, 'learning_rate': 4.8432203389830516e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:38<4:46:36,  3.01s/it]  5%|â–         | 286/6000 [13:40<4:36:19,  2.90s/it]                                                    {'loss': 2.7722, 'grad_norm': 20.213747024536133, 'learning_rate': 4.842372881355933e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:40<4:36:19,  2.90s/it]  5%|â–         | 287/6000 [13:43<4:34:13,  2.88s/it]                                                    {'loss': 2.7991, 'grad_norm': 22.948701858520508, 'learning_rate': 4.841525423728814e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:43<4:34:13,  2.88s/it]  5%|â–         | 288/6000 [13:46<4:30:02,  2.84s/it]                                                    {'loss': 2.776, 'grad_norm': 14.1120023727417, 'learning_rate': 4.840677966101695e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:46<4:30:02,  2.84s/it]  5%|â–         | 289/6000 [13:49<4:31:19,  2.85s/it]                                                    {'loss': 2.7832, 'grad_norm': 9.422952651977539, 'learning_rate': 4.839830508474576e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:49<4:31:19,  2.85s/it]  5%|â–         | 290/6000 [13:51<4:28:04,  2.82s/it]                                                    {'loss': 2.8215, 'grad_norm': 15.876049041748047, 'learning_rate': 4.838983050847458e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:51<4:28:04,  2.82s/it]  5%|â–         | 291/6000 [13:54<4:23:15,  2.77s/it]                                                    {'loss': 2.7732, 'grad_norm': 32.75008010864258, 'learning_rate': 4.83813559322034e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:54<4:23:15,  2.77s/it]  5%|â–         | 292/6000 [13:57<4:21:15,  2.75s/it]                                                    {'loss': 2.757, 'grad_norm': 23.82897186279297, 'learning_rate': 4.8372881355932205e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:57<4:21:15,  2.75s/it]  5%|â–         | 293/6000 [13:59<4:19:35,  2.73s/it]                                                    {'loss': 2.8569, 'grad_norm': 49.15443801879883, 'learning_rate': 4.836440677966102e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:59<4:19:35,  2.73s/it]  5%|â–         | 294/6000 [14:02<4:18:50,  2.72s/it]                                                    {'loss': 2.795, 'grad_norm': 14.702116966247559, 'learning_rate': 4.835593220338983e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [14:02<4:18:50,  2.72s/it]  5%|â–         | 295/6000 [14:05<4:18:41,  2.72s/it]                                                    {'loss': 2.7844, 'grad_norm': 11.664867401123047, 'learning_rate': 4.8347457627118645e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [14:05<4:18:41,  2.72s/it]  5%|â–         | 296/6000 [14:08<4:23:38,  2.77s/it]                                                    {'loss': 2.7763, 'grad_norm': 15.924742698669434, 'learning_rate': 4.833898305084746e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:08<4:23:38,  2.77s/it]  5%|â–         | 297/6000 [14:11<4:23:44,  2.77s/it]                                                    {'loss': 2.7994, 'grad_norm': 24.228252410888672, 'learning_rate': 4.833050847457628e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:11<4:23:44,  2.77s/it]  5%|â–         | 298/6000 [14:14<4:28:49,  2.83s/it]                                                    {'loss': 2.763, 'grad_norm': 14.43307876586914, 'learning_rate': 4.832203389830509e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:14<4:28:49,  2.83s/it]  5%|â–         | 299/6000 [14:17<4:50:47,  3.06s/it]                                                    {'loss': 2.8015, 'grad_norm': 8.55922794342041, 'learning_rate': 4.83135593220339e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:17<4:50:47,  3.06s/it]  5%|â–Œ         | 300/6000 [14:20<4:40:45,  2.96s/it]                                                    {'loss': 2.7795, 'grad_norm': 10.63149356842041, 'learning_rate': 4.830508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:20<4:40:45,  2.96s/it][2025-10-23 00:40:49,134] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
[2025-10-23 00:40:49,149] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:40:49,775] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [14:25<5:45:08,  3.63s/it]                                                    {'loss': 2.798, 'grad_norm': 32.22608184814453, 'learning_rate': 4.8296610169491535e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:25<5:45:08,  3.63s/it]  5%|â–Œ         | 302/6000 [14:28<5:21:34,  3.39s/it]                                                    {'loss': 2.783, 'grad_norm': 6.994548797607422, 'learning_rate': 4.828813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:28<5:21:34,  3.39s/it]  5%|â–Œ         | 303/6000 [14:31<5:04:53,  3.21s/it]                                                    {'loss': 2.7678, 'grad_norm': 12.395203590393066, 'learning_rate': 4.827966101694916e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:31<5:04:53,  3.21s/it]  5%|â–Œ         | 304/6000 [14:34<4:56:15,  3.12s/it]                                                    {'loss': 2.7857, 'grad_norm': 18.096500396728516, 'learning_rate': 4.827118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:34<4:56:15,  3.12s/it]  5%|â–Œ         | 305/6000 [14:36<4:42:33,  2.98s/it]                                                    {'loss': 2.7856, 'grad_norm': 28.526350021362305, 'learning_rate': 4.826271186440678e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:36<4:42:33,  2.98s/it]  5%|â–Œ         | 306/6000 [14:39<4:36:37,  2.91s/it]                                                    {'loss': 2.7787, 'grad_norm': 10.212275505065918, 'learning_rate': 4.82542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:39<4:36:37,  2.91s/it]  5%|â–Œ         | 307/6000 [14:42<4:31:57,  2.87s/it]                                                    {'loss': 2.8027, 'grad_norm': 31.236059188842773, 'learning_rate': 4.824576271186442e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:42<4:31:57,  2.87s/it]  5%|â–Œ         | 308/6000 [14:44<4:26:57,  2.81s/it]                                                    {'loss': 2.7737, 'grad_norm': 25.4998779296875, 'learning_rate': 4.823728813559322e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:44<4:26:57,  2.81s/it]  5%|â–Œ         | 309/6000 [14:47<4:28:37,  2.83s/it]                                                    {'loss': 2.7817, 'grad_norm': 14.3936185836792, 'learning_rate': 4.822881355932203e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:47<4:28:37,  2.83s/it]  5%|â–Œ         | 310/6000 [14:50<4:25:21,  2.80s/it]                                                    {'loss': 2.824, 'grad_norm': 10.976863861083984, 'learning_rate': 4.822033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:50<4:25:21,  2.80s/it]  5%|â–Œ         | 311/6000 [14:53<4:24:30,  2.79s/it]                                                    {'loss': 2.7891, 'grad_norm': 9.325031280517578, 'learning_rate': 4.8211864406779665e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:53<4:24:30,  2.79s/it]  5%|â–Œ         | 312/6000 [14:56<4:22:50,  2.77s/it]                                                    {'loss': 2.7825, 'grad_norm': 6.915848731994629, 'learning_rate': 4.820338983050848e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:56<4:22:50,  2.77s/it]  5%|â–Œ         | 313/6000 [14:58<4:24:02,  2.79s/it]                                                    {'loss': 2.8464, 'grad_norm': 15.150528907775879, 'learning_rate': 4.819491525423729e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:58<4:24:02,  2.79s/it]  5%|â–Œ         | 314/6000 [15:01<4:22:23,  2.77s/it]                                                    {'loss': 2.7922, 'grad_norm': 20.86367416381836, 'learning_rate': 4.8186440677966105e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [15:01<4:22:23,  2.77s/it]  5%|â–Œ         | 315/6000 [15:04<4:20:39,  2.75s/it]                                                    {'loss': 2.7787, 'grad_norm': 7.060734748840332, 'learning_rate': 4.817796610169491e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [15:04<4:20:39,  2.75s/it]  5%|â–Œ         | 316/6000 [15:06<4:19:27,  2.74s/it]                                                    {'loss': 2.7759, 'grad_norm': 12.11135196685791, 'learning_rate': 4.816949152542373e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [15:07<4:19:27,  2.74s/it]  5%|â–Œ         | 317/6000 [15:09<4:23:04,  2.78s/it]                                                    {'loss': 2.8423, 'grad_norm': 13.632817268371582, 'learning_rate': 4.816101694915255e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [15:09<4:23:04,  2.78s/it]  5%|â–Œ         | 318/6000 [15:12<4:31:54,  2.87s/it]                                                    {'loss': 2.7894, 'grad_norm': 12.641120910644531, 'learning_rate': 4.815254237288136e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [15:12<4:31:54,  2.87s/it]  5%|â–Œ         | 319/6000 [15:15<4:29:15,  2.84s/it]                                                    {'loss': 2.7681, 'grad_norm': 11.231876373291016, 'learning_rate': 4.814406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [15:15<4:29:15,  2.84s/it]  5%|â–Œ         | 320/6000 [15:18<4:28:48,  2.84s/it]                                                    {'loss': 2.771, 'grad_norm': 11.44093132019043, 'learning_rate': 4.813559322033899e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:18<4:28:48,  2.84s/it]  5%|â–Œ         | 321/6000 [15:21<4:37:13,  2.93s/it]                                                    {'loss': 2.7953, 'grad_norm': 21.954998016357422, 'learning_rate': 4.81271186440678e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:21<4:37:13,  2.93s/it]  5%|â–Œ         | 322/6000 [15:24<4:32:21,  2.88s/it]                                                    {'loss': 2.7909, 'grad_norm': 9.681015014648438, 'learning_rate': 4.811864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:24<4:32:21,  2.88s/it]  5%|â–Œ         | 323/6000 [15:27<4:45:09,  3.01s/it]                                                    {'loss': 2.7795, 'grad_norm': 9.994943618774414, 'learning_rate': 4.811016949152543e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:27<4:45:09,  3.01s/it]  5%|â–Œ         | 324/6000 [15:30<4:35:29,  2.91s/it]                                                    {'loss': 2.8013, 'grad_norm': 11.834028244018555, 'learning_rate': 4.810169491525424e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:30<4:35:29,  2.91s/it]  5%|â–Œ         | 325/6000 [15:33<4:29:44,  2.85s/it]                                                    {'loss': 2.7679, 'grad_norm': 20.16252326965332, 'learning_rate': 4.809322033898305e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:33<4:29:44,  2.85s/it]  5%|â–Œ         | 326/6000 [15:35<4:25:35,  2.81s/it]                                                    {'loss': 2.7954, 'grad_norm': 14.45102596282959, 'learning_rate': 4.808474576271187e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:35<4:25:35,  2.81s/it]  5%|â–Œ         | 327/6000 [15:38<4:21:54,  2.77s/it]                                                    {'loss': 2.7626, 'grad_norm': 14.380637168884277, 'learning_rate': 4.8076271186440684e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:38<4:21:54,  2.77s/it]  5%|â–Œ         | 328/6000 [15:41<4:22:20,  2.78s/it]                                                    {'loss': 2.7641, 'grad_norm': 25.509050369262695, 'learning_rate': 4.80677966101695e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:41<4:22:20,  2.78s/it]  5%|â–Œ         | 329/6000 [15:44<4:22:52,  2.78s/it]                                                    {'loss': 2.7855, 'grad_norm': 14.348586082458496, 'learning_rate': 4.805932203389831e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:44<4:22:52,  2.78s/it]  6%|â–Œ         | 330/6000 [15:46<4:21:55,  2.77s/it]                                                    {'loss': 2.7902, 'grad_norm': 16.041980743408203, 'learning_rate': 4.805084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:46<4:21:55,  2.77s/it]  6%|â–Œ         | 331/6000 [15:49<4:21:22,  2.77s/it]                                                    {'loss': 2.7834, 'grad_norm': 9.562405586242676, 'learning_rate': 4.804237288135593e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:49<4:21:22,  2.77s/it]  6%|â–Œ         | 332/6000 [15:52<4:19:05,  2.74s/it]                                                    {'loss': 2.7758, 'grad_norm': 22.332746505737305, 'learning_rate': 4.803389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:52<4:19:05,  2.74s/it]  6%|â–Œ         | 333/6000 [15:55<4:18:51,  2.74s/it]                                                    {'loss': 2.777, 'grad_norm': 24.131914138793945, 'learning_rate': 4.8025423728813566e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:55<4:18:51,  2.74s/it]  6%|â–Œ         | 334/6000 [15:57<4:23:15,  2.79s/it]                                                    {'loss': 2.7937, 'grad_norm': 8.699893951416016, 'learning_rate': 4.801694915254237e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:57<4:23:15,  2.79s/it]  6%|â–Œ         | 335/6000 [16:00<4:23:47,  2.79s/it]                                                    {'loss': 2.8225, 'grad_norm': 25.002471923828125, 'learning_rate': 4.800847457627119e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [16:00<4:23:47,  2.79s/it]  6%|â–Œ         | 336/6000 [16:03<4:21:04,  2.77s/it]                                                    {'loss': 2.7658, 'grad_norm': 22.00924301147461, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [16:03<4:21:04,  2.77s/it]  6%|â–Œ         | 337/6000 [16:06<4:22:09,  2.78s/it]                                                    {'loss': 2.7788, 'grad_norm': 8.846098899841309, 'learning_rate': 4.799152542372882e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [16:06<4:22:09,  2.78s/it]  6%|â–Œ         | 338/6000 [16:08<4:20:01,  2.76s/it]                                                    {'loss': 2.7762, 'grad_norm': 8.077539443969727, 'learning_rate': 4.798305084745763e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [16:08<4:20:01,  2.76s/it]  6%|â–Œ         | 339/6000 [16:11<4:19:54,  2.75s/it]                                                    {'loss': 2.7724, 'grad_norm': 9.69314956665039, 'learning_rate': 4.797457627118645e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [16:11<4:19:54,  2.75s/it]  6%|â–Œ         | 340/6000 [16:14<4:27:01,  2.83s/it]                                                    {'loss': 2.769, 'grad_norm': 7.261702060699463, 'learning_rate': 4.7966101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [16:14<4:27:01,  2.83s/it]  6%|â–Œ         | 341/6000 [16:17<4:23:19,  2.79s/it]                                                    {'loss': 2.761, 'grad_norm': 19.167926788330078, 'learning_rate': 4.795762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [16:17<4:23:19,  2.79s/it]  6%|â–Œ         | 342/6000 [16:20<4:21:04,  2.77s/it]                                                    {'loss': 2.8128, 'grad_norm': 11.596817016601562, 'learning_rate': 4.794915254237289e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [16:20<4:21:04,  2.77s/it]  6%|â–Œ         | 343/6000 [16:23<4:37:57,  2.95s/it]                                                    {'loss': 2.8061, 'grad_norm': 15.684625625610352, 'learning_rate': 4.79406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [16:23<4:37:57,  2.95s/it]  6%|â–Œ         | 344/6000 [16:26<4:32:00,  2.89s/it]                                                    {'loss': 2.7777, 'grad_norm': 7.153811931610107, 'learning_rate': 4.793220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [16:26<4:32:00,  2.89s/it]  6%|â–Œ         | 345/6000 [16:28<4:26:34,  2.83s/it]                                                    {'loss': 2.8106, 'grad_norm': 6.951721668243408, 'learning_rate': 4.792372881355933e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [16:28<4:26:34,  2.83s/it]  6%|â–Œ         | 346/6000 [16:31<4:25:01,  2.81s/it]                                                    {'loss': 2.7957, 'grad_norm': 14.382664680480957, 'learning_rate': 4.791525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:31<4:25:01,  2.81s/it]  6%|â–Œ         | 347/6000 [16:34<4:21:37,  2.78s/it]                                                    {'loss': 2.7946, 'grad_norm': 21.099872589111328, 'learning_rate': 4.790677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:34<4:21:37,  2.78s/it]  6%|â–Œ         | 348/6000 [16:37<4:23:29,  2.80s/it]                                                    {'loss': 2.7872, 'grad_norm': 7.661309242248535, 'learning_rate': 4.789830508474577e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:37<4:23:29,  2.80s/it]  6%|â–Œ         | 349/6000 [16:40<4:22:26,  2.79s/it]                                                    {'loss': 2.7932, 'grad_norm': 6.355313777923584, 'learning_rate': 4.7889830508474585e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:40<4:22:26,  2.79s/it]  6%|â–Œ         | 350/6000 [16:42<4:21:10,  2.77s/it]                                                    {'loss': 2.7948, 'grad_norm': 13.157464027404785, 'learning_rate': 4.788135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:42<4:21:10,  2.77s/it][2025-10-23 00:43:11,585] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
[2025-10-23 00:43:11,596] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:43:12,208] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [16:47<5:29:20,  3.50s/it]                                                    {'loss': 2.7646, 'grad_norm': 15.047931671142578, 'learning_rate': 4.78728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:47<5:29:20,  3.50s/it]  6%|â–Œ         | 352/6000 [16:50<5:06:46,  3.26s/it]                                                    {'loss': 2.7788, 'grad_norm': 10.21825885772705, 'learning_rate': 4.786440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:50<5:06:46,  3.26s/it]  6%|â–Œ         | 353/6000 [16:53<4:50:11,  3.08s/it]                                                    {'loss': 2.8343, 'grad_norm': 11.311318397521973, 'learning_rate': 4.785593220338983e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:53<4:50:11,  3.08s/it]  6%|â–Œ         | 354/6000 [16:56<4:40:43,  2.98s/it]                                                    {'loss': 2.7763, 'grad_norm': 19.419322967529297, 'learning_rate': 4.784745762711865e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:56<4:40:43,  2.98s/it]  6%|â–Œ         | 355/6000 [16:58<4:32:42,  2.90s/it]                                                    {'loss': 2.8215, 'grad_norm': 11.782645225524902, 'learning_rate': 4.783898305084746e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:58<4:32:42,  2.90s/it]  6%|â–Œ         | 356/6000 [17:01<4:35:06,  2.92s/it]                                                    {'loss': 2.7835, 'grad_norm': 15.695640563964844, 'learning_rate': 4.7830508474576274e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [17:01<4:35:06,  2.92s/it]  6%|â–Œ         | 357/6000 [17:04<4:30:35,  2.88s/it]                                                    {'loss': 2.8024, 'grad_norm': 30.224056243896484, 'learning_rate': 4.782203389830509e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [17:04<4:30:35,  2.88s/it]  6%|â–Œ         | 358/6000 [17:07<4:24:13,  2.81s/it]                                                    {'loss': 2.7751, 'grad_norm': 14.958518028259277, 'learning_rate': 4.781355932203391e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [17:07<4:24:13,  2.81s/it]  6%|â–Œ         | 359/6000 [17:09<4:23:11,  2.80s/it]                                                    {'loss': 2.7731, 'grad_norm': 8.502477645874023, 'learning_rate': 4.7805084745762715e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [17:09<4:23:11,  2.80s/it]  6%|â–Œ         | 360/6000 [17:12<4:21:49,  2.79s/it]                                                    {'loss': 2.8067, 'grad_norm': 7.34273099899292, 'learning_rate': 4.779661016949153e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [17:12<4:21:49,  2.79s/it]  6%|â–Œ         | 361/6000 [17:15<4:20:45,  2.77s/it]                                                    {'loss': 2.7886, 'grad_norm': 19.467329025268555, 'learning_rate': 4.778813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [17:15<4:20:45,  2.77s/it]  6%|â–Œ         | 362/6000 [17:18<4:18:50,  2.75s/it]                                                    {'loss': 2.7613, 'grad_norm': 13.215448379516602, 'learning_rate': 4.7779661016949156e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [17:18<4:18:50,  2.75s/it]  6%|â–Œ         | 363/6000 [17:20<4:18:11,  2.75s/it]                                                    {'loss': 2.8124, 'grad_norm': 20.069072723388672, 'learning_rate': 4.777118644067797e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [17:20<4:18:11,  2.75s/it]  6%|â–Œ         | 364/6000 [17:23<4:21:50,  2.79s/it]                                                    {'loss': 2.7852, 'grad_norm': 10.643144607543945, 'learning_rate': 4.776271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [17:23<4:21:50,  2.79s/it]  6%|â–Œ         | 365/6000 [17:26<4:19:16,  2.76s/it]                                                    {'loss': 2.7875, 'grad_norm': 23.03070640563965, 'learning_rate': 4.77542372881356e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [17:26<4:19:16,  2.76s/it]  6%|â–Œ         | 366/6000 [17:29<4:18:19,  2.75s/it]                                                    {'loss': 2.7713, 'grad_norm': 8.423112869262695, 'learning_rate': 4.774576271186441e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [17:29<4:18:19,  2.75s/it]  6%|â–Œ         | 367/6000 [17:31<4:17:46,  2.75s/it]                                                    {'loss': 2.7685, 'grad_norm': 5.556795120239258, 'learning_rate': 4.773728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [17:31<4:17:46,  2.75s/it]  6%|â–Œ         | 368/6000 [17:35<4:30:57,  2.89s/it]                                                    {'loss': 2.7728, 'grad_norm': 10.739373207092285, 'learning_rate': 4.772881355932204e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [17:35<4:30:57,  2.89s/it]  6%|â–Œ         | 369/6000 [17:37<4:26:01,  2.83s/it]                                                    {'loss': 2.807, 'grad_norm': 12.062491416931152, 'learning_rate': 4.772033898305085e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [17:37<4:26:01,  2.83s/it]  6%|â–Œ         | 370/6000 [17:40<4:23:37,  2.81s/it]                                                    {'loss': 2.7782, 'grad_norm': 6.880507946014404, 'learning_rate': 4.771186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [17:40<4:23:37,  2.81s/it]  6%|â–Œ         | 371/6000 [17:43<4:20:04,  2.77s/it]                                                    {'loss': 2.7715, 'grad_norm': 5.103255748748779, 'learning_rate': 4.770338983050848e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [17:43<4:20:04,  2.77s/it]  6%|â–Œ         | 372/6000 [17:46<4:19:46,  2.77s/it]                                                    {'loss': 2.796, 'grad_norm': 21.00635528564453, 'learning_rate': 4.7694915254237285e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [17:46<4:19:46,  2.77s/it]  6%|â–Œ         | 373/6000 [17:48<4:18:00,  2.75s/it]                                                    {'loss': 2.8995, 'grad_norm': 9.485919952392578, 'learning_rate': 4.76864406779661e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [17:48<4:18:00,  2.75s/it]  6%|â–Œ         | 374/6000 [17:51<4:17:14,  2.74s/it]                                                    {'loss': 2.7779, 'grad_norm': 6.554408550262451, 'learning_rate': 4.767796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [17:51<4:17:14,  2.74s/it]  6%|â–‹         | 375/6000 [17:54<4:15:28,  2.73s/it]                                                    {'loss': 2.7976, 'grad_norm': 5.732100009918213, 'learning_rate': 4.7669491525423735e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [17:54<4:15:28,  2.73s/it]  6%|â–‹         | 376/6000 [17:56<4:13:21,  2.70s/it]                                                    {'loss': 2.7782, 'grad_norm': 12.95075798034668, 'learning_rate': 4.766101694915254e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [17:56<4:13:21,  2.70s/it]  6%|â–‹         | 377/6000 [17:59<4:14:06,  2.71s/it]                                                    {'loss': 2.8094, 'grad_norm': 18.41009521484375, 'learning_rate': 4.765254237288136e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [17:59<4:14:06,  2.71s/it]  6%|â–‹         | 378/6000 [18:02<4:21:44,  2.79s/it]                                                    {'loss': 2.7711, 'grad_norm': 7.583724498748779, 'learning_rate': 4.7644067796610175e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [18:02<4:21:44,  2.79s/it]  6%|â–‹         | 379/6000 [18:05<4:21:15,  2.79s/it]                                                    {'loss': 2.7678, 'grad_norm': 20.74089241027832, 'learning_rate': 4.763559322033899e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [18:05<4:21:15,  2.79s/it]  6%|â–‹         | 380/6000 [18:08<4:28:30,  2.87s/it]                                                    {'loss': 2.7782, 'grad_norm': 16.656816482543945, 'learning_rate': 4.76271186440678e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [18:08<4:28:30,  2.87s/it]  6%|â–‹         | 381/6000 [18:11<4:24:23,  2.82s/it]                                                    {'loss': 2.7935, 'grad_norm': 8.703664779663086, 'learning_rate': 4.761864406779662e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [18:11<4:24:23,  2.82s/it]  6%|â–‹         | 382/6000 [18:13<4:22:09,  2.80s/it]                                                    {'loss': 2.741, 'grad_norm': 8.958131790161133, 'learning_rate': 4.761016949152542e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [18:13<4:22:09,  2.80s/it]  6%|â–‹         | 383/6000 [18:16<4:19:57,  2.78s/it]                                                    {'loss': 2.8218, 'grad_norm': 9.052217483520508, 'learning_rate': 4.760169491525424e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [18:16<4:19:57,  2.78s/it]  6%|â–‹         | 384/6000 [18:19<4:17:52,  2.76s/it]                                                    {'loss': 2.7705, 'grad_norm': 23.70524024963379, 'learning_rate': 4.759322033898306e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [18:19<4:17:52,  2.76s/it]  6%|â–‹         | 385/6000 [18:22<4:27:18,  2.86s/it]                                                    {'loss': 2.782, 'grad_norm': 12.18419361114502, 'learning_rate': 4.758474576271187e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [18:22<4:27:18,  2.86s/it]  6%|â–‹         | 386/6000 [18:25<4:25:06,  2.83s/it]                                                    {'loss': 2.7724, 'grad_norm': 23.036209106445312, 'learning_rate': 4.757627118644068e-06, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [18:25<4:25:06,  2.83s/it]  6%|â–‹         | 387/6000 [18:28<4:35:33,  2.95s/it]                                                    {'loss': 2.7712, 'grad_norm': 11.623820304870605, 'learning_rate': 4.75677966101695e-06, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [18:28<4:35:33,  2.95s/it]  6%|â–‹         | 388/6000 [18:31<4:33:52,  2.93s/it]                                                    {'loss': 2.817, 'grad_norm': 13.576556205749512, 'learning_rate': 4.7559322033898305e-06, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [18:31<4:33:52,  2.93s/it]  6%|â–‹         | 389/6000 [18:33<4:27:42,  2.86s/it]                                                    {'loss': 2.7907, 'grad_norm': 6.663678169250488, 'learning_rate': 4.755084745762712e-06, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [18:33<4:27:42,  2.86s/it]  6%|â–‹         | 390/6000 [18:36<4:23:26,  2.82s/it]                                                    {'loss': 2.7787, 'grad_norm': 13.083340644836426, 'learning_rate': 4.754237288135594e-06, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [18:36<4:23:26,  2.82s/it]  7%|â–‹         | 391/6000 [18:39<4:19:08,  2.77s/it]                                                    {'loss': 2.7666, 'grad_norm': 7.569731712341309, 'learning_rate': 4.7533898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [18:39<4:19:08,  2.77s/it]  7%|â–‹         | 392/6000 [18:42<4:21:09,  2.79s/it]                                                    {'loss': 2.7744, 'grad_norm': 9.129639625549316, 'learning_rate': 4.752542372881356e-06, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [18:42<4:21:09,  2.79s/it]  7%|â–‹         | 393/6000 [18:44<4:19:13,  2.77s/it]                                                    {'loss': 2.7755, 'grad_norm': 8.715798377990723, 'learning_rate': 4.751694915254238e-06, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [18:44<4:19:13,  2.77s/it]  7%|â–‹         | 394/6000 [18:47<4:19:01,  2.77s/it]                                                    {'loss': 2.778, 'grad_norm': 8.398935317993164, 'learning_rate': 4.7508474576271195e-06, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [18:47<4:19:01,  2.77s/it]  7%|â–‹         | 395/6000 [18:50<4:19:45,  2.78s/it]                                                    {'loss': 2.7777, 'grad_norm': 7.306328296661377, 'learning_rate': 4.75e-06, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [18:50<4:19:45,  2.78s/it]  7%|â–‹         | 396/6000 [18:53<4:18:05,  2.76s/it]                                                    {'loss': 2.7812, 'grad_norm': 13.746875762939453, 'learning_rate': 4.749152542372882e-06, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [18:53<4:18:05,  2.76s/it]  7%|â–‹         | 397/6000 [18:56<4:28:31,  2.88s/it]                                                    {'loss': 2.7759, 'grad_norm': 35.05221176147461, 'learning_rate': 4.748305084745763e-06, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [18:56<4:28:31,  2.88s/it]  7%|â–‹         | 398/6000 [18:59<4:24:07,  2.83s/it]                                                    {'loss': 2.7745, 'grad_norm': 7.679290771484375, 'learning_rate': 4.747457627118644e-06, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [18:59<4:24:07,  2.83s/it]  7%|â–‹         | 399/6000 [19:01<4:22:16,  2.81s/it]                                                    {'loss': 2.7778, 'grad_norm': 27.79691505432129, 'learning_rate': 4.746610169491526e-06, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [19:01<4:22:16,  2.81s/it]  7%|â–‹         | 400/6000 [19:04<4:19:40,  2.78s/it]                                                    {'loss': 2.7788, 'grad_norm': 8.523673057556152, 'learning_rate': 4.745762711864408e-06, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [19:04<4:19:40,  2.78s/it][2025-10-23 00:45:33,346] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
[2025-10-23 00:45:33,361] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:45:33,970] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [19:09<5:16:29,  3.39s/it]                                                    {'loss': 2.775, 'grad_norm': 12.730791091918945, 'learning_rate': 4.744915254237288e-06, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [19:09<5:16:29,  3.39s/it]  7%|â–‹         | 402/6000 [19:12<4:57:11,  3.19s/it]                                                    {'loss': 2.7673, 'grad_norm': 26.106550216674805, 'learning_rate': 4.74406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [19:12<4:57:11,  3.19s/it]  7%|â–‹         | 403/6000 [19:14<4:45:56,  3.07s/it]                                                    {'loss': 2.7774, 'grad_norm': 11.752829551696777, 'learning_rate': 4.743220338983051e-06, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [19:14<4:45:56,  3.07s/it]  7%|â–‹         | 404/6000 [19:17<4:41:13,  3.02s/it]                                                    {'loss': 2.7412, 'grad_norm': 26.023536682128906, 'learning_rate': 4.7423728813559325e-06, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [19:17<4:41:13,  3.02s/it]  7%|â–‹         | 405/6000 [19:20<4:32:42,  2.92s/it]                                                    {'loss': 2.7718, 'grad_norm': 7.370977401733398, 'learning_rate': 4.741525423728814e-06, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [19:20<4:32:42,  2.92s/it]  7%|â–‹         | 406/6000 [19:23<4:27:53,  2.87s/it]                                                    {'loss': 2.7819, 'grad_norm': 10.118720054626465, 'learning_rate': 4.740677966101696e-06, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [19:23<4:27:53,  2.87s/it]  7%|â–‹         | 407/6000 [19:26<4:26:31,  2.86s/it]                                                    {'loss': 2.7306, 'grad_norm': 29.026466369628906, 'learning_rate': 4.7398305084745765e-06, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [19:26<4:26:31,  2.86s/it]  7%|â–‹         | 408/6000 [19:29<4:43:01,  3.04s/it]                                                    {'loss': 2.8243, 'grad_norm': 18.28220558166504, 'learning_rate': 4.738983050847458e-06, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [19:29<4:43:01,  3.04s/it]  7%|â–‹         | 409/6000 [19:32<4:35:32,  2.96s/it]                                                    {'loss': 2.7856, 'grad_norm': 19.185962677001953, 'learning_rate': 4.738135593220339e-06, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [19:32<4:35:32,  2.96s/it]  7%|â–‹         | 410/6000 [19:35<4:29:39,  2.89s/it]                                                    {'loss': 2.7836, 'grad_norm': 12.925013542175293, 'learning_rate': 4.737288135593221e-06, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [19:35<4:29:39,  2.89s/it]  7%|â–‹         | 411/6000 [19:38<4:45:16,  3.06s/it]                                                    {'loss': 2.8159, 'grad_norm': 21.10361671447754, 'learning_rate': 4.736440677966102e-06, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [19:38<4:45:16,  3.06s/it]  7%|â–‹         | 412/6000 [19:41<4:34:42,  2.95s/it]                                                    {'loss': 2.8002, 'grad_norm': 11.755244255065918, 'learning_rate': 4.735593220338983e-06, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [19:41<4:34:42,  2.95s/it]  7%|â–‹         | 413/6000 [19:44<4:33:54,  2.94s/it]                                                    {'loss': 2.7743, 'grad_norm': 9.336588859558105, 'learning_rate': 4.734745762711865e-06, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [19:44<4:33:54,  2.94s/it]  7%|â–‹         | 414/6000 [19:46<4:26:27,  2.86s/it]                                                    {'loss': 2.7906, 'grad_norm': 7.403476238250732, 'learning_rate': 4.733898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [19:46<4:26:27,  2.86s/it]  7%|â–‹         | 415/6000 [19:49<4:26:39,  2.86s/it]                                                    {'loss': 2.7701, 'grad_norm': 7.8443450927734375, 'learning_rate': 4.733050847457628e-06, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [19:49<4:26:39,  2.86s/it]  7%|â–‹         | 416/6000 [19:52<4:23:26,  2.83s/it]                                                    {'loss': 2.8175, 'grad_norm': 14.583707809448242, 'learning_rate': 4.732203389830509e-06, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [19:52<4:23:26,  2.83s/it]  7%|â–‹         | 417/6000 [19:55<4:19:53,  2.79s/it]                                                    {'loss': 2.7999, 'grad_norm': 22.112125396728516, 'learning_rate': 4.73135593220339e-06, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [19:55<4:19:53,  2.79s/it]  7%|â–‹         | 418/6000 [19:57<4:17:46,  2.77s/it]                                                    {'loss': 2.7841, 'grad_norm': 7.306840419769287, 'learning_rate': 4.730508474576271e-06, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [19:57<4:17:46,  2.77s/it]  7%|â–‹         | 419/6000 [20:00<4:29:20,  2.90s/it]                                                    {'loss': 2.7971, 'grad_norm': 12.47939682006836, 'learning_rate': 4.729661016949153e-06, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [20:00<4:29:20,  2.90s/it]  7%|â–‹         | 420/6000 [20:03<4:31:15,  2.92s/it]                                                    {'loss': 2.798, 'grad_norm': 9.278995513916016, 'learning_rate': 4.728813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [20:03<4:31:15,  2.92s/it]  7%|â–‹         | 421/6000 [20:06<4:28:04,  2.88s/it]                                                    {'loss': 2.769, 'grad_norm': 26.216726303100586, 'learning_rate': 4.727966101694916e-06, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [20:06<4:28:04,  2.88s/it]  7%|â–‹         | 422/6000 [20:09<4:23:20,  2.83s/it]                                                    {'loss': 2.7713, 'grad_norm': 22.742538452148438, 'learning_rate': 4.727118644067797e-06, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [20:09<4:23:20,  2.83s/it]  7%|â–‹         | 423/6000 [20:12<4:20:25,  2.80s/it]                                                    {'loss': 2.7687, 'grad_norm': 7.243199825286865, 'learning_rate': 4.7262711864406785e-06, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [20:12<4:20:25,  2.80s/it]  7%|â–‹         | 424/6000 [20:14<4:18:07,  2.78s/it]                                                    {'loss': 2.757, 'grad_norm': 16.469379425048828, 'learning_rate': 4.725423728813559e-06, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [20:14<4:18:07,  2.78s/it]  7%|â–‹         | 425/6000 [20:17<4:16:18,  2.76s/it]                                                    {'loss': 2.7862, 'grad_norm': 9.822205543518066, 'learning_rate': 4.724576271186441e-06, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [20:17<4:16:18,  2.76s/it]  7%|â–‹         | 426/6000 [20:20<4:14:56,  2.74s/it]                                                    {'loss': 2.7781, 'grad_norm': 18.217763900756836, 'learning_rate': 4.7237288135593225e-06, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [20:20<4:14:56,  2.74s/it]  7%|â–‹         | 427/6000 [20:23<4:15:26,  2.75s/it]                                                    {'loss': 2.7902, 'grad_norm': 13.338489532470703, 'learning_rate': 4.722881355932204e-06, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [20:23<4:15:26,  2.75s/it]  7%|â–‹         | 428/6000 [20:25<4:14:23,  2.74s/it]                                                    {'loss': 2.8111, 'grad_norm': 18.50798225402832, 'learning_rate': 4.722033898305085e-06, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [20:25<4:14:23,  2.74s/it]  7%|â–‹         | 429/6000 [20:28<4:12:25,  2.72s/it]                                                    {'loss': 2.7702, 'grad_norm': 14.621472358703613, 'learning_rate': 4.721186440677967e-06, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [20:28<4:12:25,  2.72s/it]  7%|â–‹         | 430/6000 [20:31<4:13:33,  2.73s/it]                                                    {'loss': 2.8461, 'grad_norm': 9.325200080871582, 'learning_rate': 4.720338983050848e-06, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [20:31<4:13:33,  2.73s/it]  7%|â–‹         | 431/6000 [20:33<4:13:06,  2.73s/it]                                                    {'loss': 2.7638, 'grad_norm': 12.809221267700195, 'learning_rate': 4.71949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [20:33<4:13:06,  2.73s/it]  7%|â–‹         | 432/6000 [20:36<4:10:52,  2.70s/it]                                                    {'loss': 2.8124, 'grad_norm': 9.289926528930664, 'learning_rate': 4.718644067796611e-06, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [20:36<4:10:52,  2.70s/it]  7%|â–‹         | 433/6000 [20:39<4:09:33,  2.69s/it]                                                    {'loss': 2.7886, 'grad_norm': 11.063140869140625, 'learning_rate': 4.7177966101694914e-06, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [20:39<4:09:33,  2.69s/it]  7%|â–‹         | 434/6000 [20:42<4:10:28,  2.70s/it]                                                    {'loss': 2.7693, 'grad_norm': 8.302790641784668, 'learning_rate': 4.716949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [20:42<4:10:28,  2.70s/it]  7%|â–‹         | 435/6000 [20:44<4:12:29,  2.72s/it]                                                    {'loss': 2.7936, 'grad_norm': 17.97270965576172, 'learning_rate': 4.716101694915255e-06, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [20:44<4:12:29,  2.72s/it]  7%|â–‹         | 436/6000 [20:47<4:12:55,  2.73s/it]                                                    {'loss': 2.8089, 'grad_norm': 14.719334602355957, 'learning_rate': 4.715254237288136e-06, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [20:47<4:12:55,  2.73s/it]  7%|â–‹         | 437/6000 [20:50<4:15:43,  2.76s/it]                                                    {'loss': 2.7923, 'grad_norm': 13.738131523132324, 'learning_rate': 4.714406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [20:50<4:15:43,  2.76s/it]  7%|â–‹         | 438/6000 [20:53<4:19:21,  2.80s/it]                                                    {'loss': 2.8004, 'grad_norm': 20.175107955932617, 'learning_rate': 4.713559322033899e-06, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [20:53<4:19:21,  2.80s/it]  7%|â–‹         | 439/6000 [20:55<4:17:25,  2.78s/it]                                                    {'loss': 2.7959, 'grad_norm': 10.11091423034668, 'learning_rate': 4.7127118644067796e-06, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [20:55<4:17:25,  2.78s/it]  7%|â–‹         | 440/6000 [20:58<4:17:05,  2.77s/it]                                                    {'loss': 2.762, 'grad_norm': 8.078333854675293, 'learning_rate': 4.711864406779661e-06, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [20:58<4:17:05,  2.77s/it]  7%|â–‹         | 441/6000 [21:01<4:24:13,  2.85s/it]                                                    {'loss': 2.8041, 'grad_norm': 9.781267166137695, 'learning_rate': 4.711016949152543e-06, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [21:01<4:24:13,  2.85s/it]  7%|â–‹         | 442/6000 [21:04<4:29:05,  2.90s/it]                                                    {'loss': 2.7814, 'grad_norm': 7.395763397216797, 'learning_rate': 4.7101694915254245e-06, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [21:04<4:29:05,  2.90s/it]  7%|â–‹         | 443/6000 [21:07<4:24:16,  2.85s/it]                                                    {'loss': 2.8069, 'grad_norm': 12.070772171020508, 'learning_rate': 4.709322033898305e-06, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [21:07<4:24:16,  2.85s/it]  7%|â–‹         | 444/6000 [21:10<4:20:09,  2.81s/it]                                                    {'loss': 2.8209, 'grad_norm': 10.269449234008789, 'learning_rate': 4.708474576271187e-06, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [21:10<4:20:09,  2.81s/it]  7%|â–‹         | 445/6000 [21:13<4:20:10,  2.81s/it]                                                    {'loss': 2.7873, 'grad_norm': 13.601938247680664, 'learning_rate': 4.707627118644068e-06, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [21:13<4:20:10,  2.81s/it]  7%|â–‹         | 446/6000 [21:15<4:18:11,  2.79s/it]                                                    {'loss': 2.7735, 'grad_norm': 9.200596809387207, 'learning_rate': 4.706779661016949e-06, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [21:15<4:18:11,  2.79s/it]  7%|â–‹         | 447/6000 [21:18<4:17:08,  2.78s/it]                                                    {'loss': 2.771, 'grad_norm': 5.781081199645996, 'learning_rate': 4.705932203389831e-06, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [21:18<4:17:08,  2.78s/it]  7%|â–‹         | 448/6000 [21:21<4:16:43,  2.77s/it]                                                    {'loss': 2.7852, 'grad_norm': 5.908667087554932, 'learning_rate': 4.705084745762713e-06, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [21:21<4:16:43,  2.77s/it]  7%|â–‹         | 449/6000 [21:24<4:14:54,  2.76s/it]                                                    {'loss': 2.7825, 'grad_norm': 6.179904937744141, 'learning_rate': 4.704237288135593e-06, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [21:24<4:14:54,  2.76s/it]  8%|â–Š         | 450/6000 [21:26<4:14:46,  2.75s/it]                                                    {'loss': 2.7609, 'grad_norm': 8.974591255187988, 'learning_rate': 4.703389830508475e-06, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [21:26<4:14:46,  2.75s/it][2025-10-23 00:47:55,572] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
[2025-10-23 00:47:55,583] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:47:56,198] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 451/6000 [21:31<5:13:04,  3.39s/it]                                                    {'loss': 2.8001, 'grad_norm': 13.060894012451172, 'learning_rate': 4.702542372881357e-06, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [21:31<5:13:04,  3.39s/it]  8%|â–Š         | 452/6000 [21:34<5:02:45,  3.27s/it]                                                    {'loss': 2.7936, 'grad_norm': 9.988669395446777, 'learning_rate': 4.701694915254238e-06, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [21:34<5:02:45,  3.27s/it]  8%|â–Š         | 453/6000 [21:37<4:46:32,  3.10s/it]                                                    {'loss': 2.7605, 'grad_norm': 9.748177528381348, 'learning_rate': 4.700847457627119e-06, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [21:37<4:46:32,  3.10s/it]  8%|â–Š         | 454/6000 [21:40<4:36:32,  2.99s/it]                                                    {'loss': 2.8508, 'grad_norm': 4.971807479858398, 'learning_rate': 4.7e-06, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [21:40<4:36:32,  2.99s/it]  8%|â–Š         | 455/6000 [21:42<4:26:56,  2.89s/it]                                                    {'loss': 2.7689, 'grad_norm': 9.842488288879395, 'learning_rate': 4.6991525423728815e-06, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [21:42<4:26:56,  2.89s/it]  8%|â–Š         | 456/6000 [21:45<4:22:14,  2.84s/it]                                                    {'loss': 2.7743, 'grad_norm': 5.32858943939209, 'learning_rate': 4.698305084745763e-06, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [21:45<4:22:14,  2.84s/it]  8%|â–Š         | 457/6000 [21:48<4:22:51,  2.85s/it]                                                    {'loss': 2.8025, 'grad_norm': 10.301786422729492, 'learning_rate': 4.697457627118645e-06, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [21:48<4:22:51,  2.85s/it]  8%|â–Š         | 458/6000 [21:50<4:18:25,  2.80s/it]                                                    {'loss': 2.8191, 'grad_norm': 8.344407081604004, 'learning_rate': 4.696610169491526e-06, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [21:50<4:18:25,  2.80s/it]  8%|â–Š         | 459/6000 [21:53<4:16:39,  2.78s/it]                                                    {'loss': 2.7785, 'grad_norm': 8.796813011169434, 'learning_rate': 4.695762711864407e-06, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [21:53<4:16:39,  2.78s/it]  8%|â–Š         | 460/6000 [21:56<4:14:53,  2.76s/it]                                                    {'loss': 2.7787, 'grad_norm': 14.28681468963623, 'learning_rate': 4.694915254237288e-06, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [21:56<4:14:53,  2.76s/it]  8%|â–Š         | 461/6000 [21:59<4:13:23,  2.74s/it]                                                    {'loss': 2.8084, 'grad_norm': 14.085991859436035, 'learning_rate': 4.69406779661017e-06, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [21:59<4:13:23,  2.74s/it]  8%|â–Š         | 462/6000 [22:01<4:10:42,  2.72s/it]                                                    {'loss': 2.7732, 'grad_norm': 12.428317070007324, 'learning_rate': 4.693220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [22:01<4:10:42,  2.72s/it]  8%|â–Š         | 463/6000 [22:04<4:13:07,  2.74s/it]                                                    {'loss': 2.7855, 'grad_norm': 14.411952018737793, 'learning_rate': 4.692372881355933e-06, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [22:04<4:13:07,  2.74s/it]  8%|â–Š         | 464/6000 [22:07<4:12:08,  2.73s/it]                                                    {'loss': 2.7794, 'grad_norm': 7.76271915435791, 'learning_rate': 4.691525423728814e-06, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [22:07<4:12:08,  2.73s/it]  8%|â–Š         | 465/6000 [22:10<4:16:22,  2.78s/it]                                                    {'loss': 2.7634, 'grad_norm': 7.580301284790039, 'learning_rate': 4.690677966101695e-06, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [22:10<4:16:22,  2.78s/it]  8%|â–Š         | 466/6000 [22:12<4:15:09,  2.77s/it]                                                    {'loss': 2.768, 'grad_norm': 6.903004169464111, 'learning_rate': 4.689830508474576e-06, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [22:12<4:15:09,  2.77s/it]  8%|â–Š         | 467/6000 [22:15<4:14:51,  2.76s/it]                                                    {'loss': 2.8045, 'grad_norm': 6.0384016036987305, 'learning_rate': 4.688983050847458e-06, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [22:15<4:14:51,  2.76s/it]  8%|â–Š         | 468/6000 [22:18<4:15:20,  2.77s/it]                                                    {'loss': 2.7602, 'grad_norm': 6.218225955963135, 'learning_rate': 4.688135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [22:18<4:15:20,  2.77s/it]  8%|â–Š         | 469/6000 [22:21<4:13:10,  2.75s/it]                                                    {'loss': 2.7796, 'grad_norm': 7.07310152053833, 'learning_rate': 4.687288135593221e-06, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [22:21<4:13:10,  2.75s/it]  8%|â–Š         | 470/6000 [22:23<4:12:24,  2.74s/it]                                                    {'loss': 2.775, 'grad_norm': 6.8175530433654785, 'learning_rate': 4.686440677966102e-06, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [22:23<4:12:24,  2.74s/it]  8%|â–Š         | 471/6000 [22:26<4:11:37,  2.73s/it]                                                    {'loss': 2.8506, 'grad_norm': 6.890472888946533, 'learning_rate': 4.6855932203389835e-06, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [22:26<4:11:37,  2.73s/it]  8%|â–Š         | 472/6000 [22:29<4:12:17,  2.74s/it]                                                    {'loss': 2.8082, 'grad_norm': 5.733752727508545, 'learning_rate': 4.684745762711865e-06, 'epoch': 0.08}
  8%|â–Š         | 472/6000 [22:29<4:12:17,  2.74s/it]  8%|â–Š         | 473/6000 [22:32<4:11:12,  2.73s/it]                                                    {'loss': 2.7639, 'grad_norm': 8.136795997619629, 'learning_rate': 4.683898305084747e-06, 'epoch': 0.08}
  8%|â–Š         | 473/6000 [22:32<4:11:12,  2.73s/it]  8%|â–Š         | 474/6000 [22:34<4:09:35,  2.71s/it]                                                    {'loss': 2.7729, 'grad_norm': 12.996996879577637, 'learning_rate': 4.6830508474576275e-06, 'epoch': 0.08}
  8%|â–Š         | 474/6000 [22:34<4:09:35,  2.71s/it]  8%|â–Š         | 475/6000 [22:37<4:10:46,  2.72s/it]                                                    {'loss': 2.7711, 'grad_norm': 10.571589469909668, 'learning_rate': 4.682203389830508e-06, 'epoch': 0.08}
  8%|â–Š         | 475/6000 [22:37<4:10:46,  2.72s/it]  8%|â–Š         | 476/6000 [22:40<4:09:44,  2.71s/it]                                                    {'loss': 2.7925, 'grad_norm': 15.931282997131348, 'learning_rate': 4.68135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 476/6000 [22:40<4:09:44,  2.71s/it]  8%|â–Š         | 477/6000 [22:42<4:10:02,  2.72s/it]                                                    {'loss': 2.7727, 'grad_norm': 5.211528301239014, 'learning_rate': 4.680508474576272e-06, 'epoch': 0.08}
  8%|â–Š         | 477/6000 [22:42<4:10:02,  2.72s/it]  8%|â–Š         | 478/6000 [22:45<4:09:31,  2.71s/it]                                                    {'loss': 2.7812, 'grad_norm': 11.943361282348633, 'learning_rate': 4.679661016949153e-06, 'epoch': 0.08}
  8%|â–Š         | 478/6000 [22:45<4:09:31,  2.71s/it]  8%|â–Š         | 479/6000 [22:48<4:09:05,  2.71s/it]                                                    {'loss': 2.7968, 'grad_norm': 15.559222221374512, 'learning_rate': 4.678813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 479/6000 [22:48<4:09:05,  2.71s/it]  8%|â–Š         | 480/6000 [22:50<4:07:24,  2.69s/it]                                                    {'loss': 2.789, 'grad_norm': 10.371848106384277, 'learning_rate': 4.677966101694916e-06, 'epoch': 0.08}
  8%|â–Š         | 480/6000 [22:50<4:07:24,  2.69s/it]  8%|â–Š         | 481/6000 [22:53<4:11:20,  2.73s/it]                                                    {'loss': 2.7844, 'grad_norm': 5.4847331047058105, 'learning_rate': 4.6771186440677965e-06, 'epoch': 0.08}
  8%|â–Š         | 481/6000 [22:53<4:11:20,  2.73s/it]  8%|â–Š         | 482/6000 [22:56<4:12:17,  2.74s/it]                                                    {'loss': 2.7622, 'grad_norm': 9.828492164611816, 'learning_rate': 4.676271186440678e-06, 'epoch': 0.08}
  8%|â–Š         | 482/6000 [22:56<4:12:17,  2.74s/it]  8%|â–Š         | 483/6000 [22:59<4:11:20,  2.73s/it]                                                    {'loss': 2.7983, 'grad_norm': 5.672049045562744, 'learning_rate': 4.67542372881356e-06, 'epoch': 0.08}
  8%|â–Š         | 483/6000 [22:59<4:11:20,  2.73s/it]  8%|â–Š         | 484/6000 [23:02<4:14:37,  2.77s/it]                                                    {'loss': 2.7765, 'grad_norm': 6.754022121429443, 'learning_rate': 4.674576271186441e-06, 'epoch': 0.08}
  8%|â–Š         | 484/6000 [23:02<4:14:37,  2.77s/it]  8%|â–Š         | 485/6000 [23:05<4:27:22,  2.91s/it]                                                    {'loss': 2.7692, 'grad_norm': 7.5143609046936035, 'learning_rate': 4.673728813559322e-06, 'epoch': 0.08}
  8%|â–Š         | 485/6000 [23:05<4:27:22,  2.91s/it]  8%|â–Š         | 486/6000 [23:08<4:21:18,  2.84s/it]                                                    {'loss': 2.7929, 'grad_norm': 5.022519588470459, 'learning_rate': 4.672881355932204e-06, 'epoch': 0.08}
  8%|â–Š         | 486/6000 [23:08<4:21:18,  2.84s/it]  8%|â–Š         | 487/6000 [23:10<4:21:59,  2.85s/it]                                                    {'loss': 2.787, 'grad_norm': 5.337488651275635, 'learning_rate': 4.6720338983050854e-06, 'epoch': 0.08}
  8%|â–Š         | 487/6000 [23:10<4:21:59,  2.85s/it]  8%|â–Š         | 488/6000 [23:13<4:22:51,  2.86s/it]                                                    {'loss': 2.7865, 'grad_norm': 5.236697196960449, 'learning_rate': 4.671186440677967e-06, 'epoch': 0.08}
  8%|â–Š         | 488/6000 [23:13<4:22:51,  2.86s/it]  8%|â–Š         | 489/6000 [23:16<4:22:25,  2.86s/it]                                                    {'loss': 2.7719, 'grad_norm': 7.683692932128906, 'learning_rate': 4.670338983050848e-06, 'epoch': 0.08}
  8%|â–Š         | 489/6000 [23:16<4:22:25,  2.86s/it]  8%|â–Š         | 490/6000 [23:19<4:20:31,  2.84s/it]                                                    {'loss': 2.8293, 'grad_norm': 6.994695663452148, 'learning_rate': 4.6694915254237295e-06, 'epoch': 0.08}
  8%|â–Š         | 490/6000 [23:19<4:20:31,  2.84s/it]  8%|â–Š         | 491/6000 [23:22<4:19:55,  2.83s/it]                                                    {'loss': 2.7658, 'grad_norm': 6.25003719329834, 'learning_rate': 4.66864406779661e-06, 'epoch': 0.08}
  8%|â–Š         | 491/6000 [23:22<4:19:55,  2.83s/it]  8%|â–Š         | 492/6000 [23:24<4:17:09,  2.80s/it]                                                    {'loss': 2.8034, 'grad_norm': 13.654976844787598, 'learning_rate': 4.667796610169492e-06, 'epoch': 0.08}
  8%|â–Š         | 492/6000 [23:24<4:17:09,  2.80s/it]  8%|â–Š         | 493/6000 [23:27<4:19:02,  2.82s/it]                                                    {'loss': 2.7795, 'grad_norm': 6.721336364746094, 'learning_rate': 4.6669491525423736e-06, 'epoch': 0.08}
  8%|â–Š         | 493/6000 [23:27<4:19:02,  2.82s/it]  8%|â–Š         | 494/6000 [23:30<4:14:42,  2.78s/it]                                                    {'loss': 2.7748, 'grad_norm': 5.091583728790283, 'learning_rate': 4.666101694915255e-06, 'epoch': 0.08}
  8%|â–Š         | 494/6000 [23:30<4:14:42,  2.78s/it]  8%|â–Š         | 495/6000 [23:33<4:16:05,  2.79s/it]                                                    {'loss': 2.777, 'grad_norm': 7.173426151275635, 'learning_rate': 4.665254237288136e-06, 'epoch': 0.08}
  8%|â–Š         | 495/6000 [23:33<4:16:05,  2.79s/it]  8%|â–Š         | 496/6000 [23:36<4:13:00,  2.76s/it]                                                    {'loss': 2.7762, 'grad_norm': 4.7206902503967285, 'learning_rate': 4.664406779661017e-06, 'epoch': 0.08}
  8%|â–Š         | 496/6000 [23:36<4:13:00,  2.76s/it]  8%|â–Š         | 497/6000 [23:38<4:10:51,  2.74s/it]                                                    {'loss': 2.7753, 'grad_norm': 5.330358505249023, 'learning_rate': 4.663559322033898e-06, 'epoch': 0.08}
  8%|â–Š         | 497/6000 [23:38<4:10:51,  2.74s/it]  8%|â–Š         | 498/6000 [23:41<4:11:29,  2.74s/it]                                                    {'loss': 2.7714, 'grad_norm': 6.34869384765625, 'learning_rate': 4.66271186440678e-06, 'epoch': 0.08}
  8%|â–Š         | 498/6000 [23:41<4:11:29,  2.74s/it]  8%|â–Š         | 499/6000 [23:44<4:12:39,  2.76s/it]                                                    {'loss': 2.7598, 'grad_norm': 7.728686332702637, 'learning_rate': 4.661864406779662e-06, 'epoch': 0.08}
  8%|â–Š         | 499/6000 [23:44<4:12:39,  2.76s/it]  8%|â–Š         | 500/6000 [23:47<4:12:46,  2.76s/it]                                                    {'loss': 2.7749, 'grad_norm': 7.067577838897705, 'learning_rate': 4.6610169491525425e-06, 'epoch': 0.08}
  8%|â–Š         | 500/6000 [23:47<4:12:46,  2.76s/it][2025-10-23 00:50:15,810] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
[2025-10-23 00:50:15,821] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:50:16,425] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 501/6000 [23:51<5:11:48,  3.40s/it]                                                    {'loss': 2.8005, 'grad_norm': 11.174636840820312, 'learning_rate': 4.660169491525424e-06, 'epoch': 0.08}
  8%|â–Š         | 501/6000 [23:51<5:11:48,  3.40s/it]  8%|â–Š         | 502/6000 [23:54<4:57:40,  3.25s/it]                                                    {'loss': 2.7542, 'grad_norm': 4.985822677612305, 'learning_rate': 4.659322033898305e-06, 'epoch': 0.08}
  8%|â–Š         | 502/6000 [23:54<4:57:40,  3.25s/it]  8%|â–Š         | 503/6000 [23:57<4:42:36,  3.08s/it]                                                    {'loss': 2.8284, 'grad_norm': 11.251516342163086, 'learning_rate': 4.6584745762711865e-06, 'epoch': 0.08}
  8%|â–Š         | 503/6000 [23:57<4:42:36,  3.08s/it]  8%|â–Š         | 504/6000 [24:00<4:34:02,  2.99s/it]                                                    {'loss': 2.78, 'grad_norm': 6.196283340454102, 'learning_rate': 4.657627118644068e-06, 'epoch': 0.08}
  8%|â–Š         | 504/6000 [24:00<4:34:02,  2.99s/it]  8%|â–Š         | 505/6000 [24:03<4:28:48,  2.94s/it]                                                    {'loss': 2.8289, 'grad_norm': 11.639533042907715, 'learning_rate': 4.65677966101695e-06, 'epoch': 0.08}
  8%|â–Š         | 505/6000 [24:03<4:28:48,  2.94s/it]  8%|â–Š         | 506/6000 [24:06<4:38:21,  3.04s/it]                                                    {'loss': 2.7572, 'grad_norm': 12.226346969604492, 'learning_rate': 4.655932203389831e-06, 'epoch': 0.08}
  8%|â–Š         | 506/6000 [24:06<4:38:21,  3.04s/it]  8%|â–Š         | 507/6000 [24:09<4:31:46,  2.97s/it]                                                    {'loss': 2.789, 'grad_norm': 5.588422775268555, 'learning_rate': 4.655084745762712e-06, 'epoch': 0.08}
  8%|â–Š         | 507/6000 [24:09<4:31:46,  2.97s/it]  8%|â–Š         | 508/6000 [24:11<4:25:22,  2.90s/it]                                                    {'loss': 2.792, 'grad_norm': 4.527457237243652, 'learning_rate': 4.654237288135594e-06, 'epoch': 0.08}
  8%|â–Š         | 508/6000 [24:11<4:25:22,  2.90s/it]  8%|â–Š         | 509/6000 [24:14<4:18:54,  2.83s/it]                                                    {'loss': 2.7998, 'grad_norm': 13.558154106140137, 'learning_rate': 4.6533898305084755e-06, 'epoch': 0.08}
  8%|â–Š         | 509/6000 [24:14<4:18:54,  2.83s/it]  8%|â–Š         | 510/6000 [24:17<4:17:47,  2.82s/it]                                                    {'loss': 2.757, 'grad_norm': 12.392672538757324, 'learning_rate': 4.652542372881356e-06, 'epoch': 0.09}
  8%|â–Š         | 510/6000 [24:17<4:17:47,  2.82s/it]  9%|â–Š         | 511/6000 [24:20<4:18:59,  2.83s/it]                                                    {'loss': 2.7312, 'grad_norm': 30.53751564025879, 'learning_rate': 4.651694915254238e-06, 'epoch': 0.09}
  9%|â–Š         | 511/6000 [24:20<4:18:59,  2.83s/it]  9%|â–Š         | 512/6000 [24:22<4:16:14,  2.80s/it]                                                    {'loss': 2.7843, 'grad_norm': 5.968908309936523, 'learning_rate': 4.650847457627119e-06, 'epoch': 0.09}
  9%|â–Š         | 512/6000 [24:22<4:16:14,  2.80s/it]  9%|â–Š         | 513/6000 [24:25<4:20:14,  2.85s/it]                                                    {'loss': 2.7555, 'grad_norm': 11.900686264038086, 'learning_rate': 4.65e-06, 'epoch': 0.09}
  9%|â–Š         | 513/6000 [24:25<4:20:14,  2.85s/it]  9%|â–Š         | 514/6000 [24:28<4:18:18,  2.83s/it]                                                    {'loss': 2.7925, 'grad_norm': 12.102285385131836, 'learning_rate': 4.649152542372882e-06, 'epoch': 0.09}
  9%|â–Š         | 514/6000 [24:28<4:18:18,  2.83s/it]  9%|â–Š         | 515/6000 [24:31<4:14:51,  2.79s/it]                                                    {'loss': 2.7791, 'grad_norm': 5.324114799499512, 'learning_rate': 4.648305084745764e-06, 'epoch': 0.09}
  9%|â–Š         | 515/6000 [24:31<4:14:51,  2.79s/it]  9%|â–Š         | 516/6000 [24:34<4:21:11,  2.86s/it]                                                    {'loss': 2.7489, 'grad_norm': 14.086812973022461, 'learning_rate': 4.6474576271186444e-06, 'epoch': 0.09}
  9%|â–Š         | 516/6000 [24:34<4:21:11,  2.86s/it]  9%|â–Š         | 517/6000 [24:37<4:18:39,  2.83s/it]                                                    {'loss': 2.7765, 'grad_norm': 8.139983177185059, 'learning_rate': 4.646610169491525e-06, 'epoch': 0.09}
  9%|â–Š         | 517/6000 [24:37<4:18:39,  2.83s/it]  9%|â–Š         | 518/6000 [24:39<4:16:03,  2.80s/it]                                                    {'loss': 2.7747, 'grad_norm': 5.5549726486206055, 'learning_rate': 4.645762711864407e-06, 'epoch': 0.09}
  9%|â–Š         | 518/6000 [24:39<4:16:03,  2.80s/it]  9%|â–Š         | 519/6000 [24:42<4:13:51,  2.78s/it]                                                    {'loss': 2.7726, 'grad_norm': 9.470422744750977, 'learning_rate': 4.6449152542372885e-06, 'epoch': 0.09}
  9%|â–Š         | 519/6000 [24:42<4:13:51,  2.78s/it]  9%|â–Š         | 520/6000 [24:45<4:19:03,  2.84s/it]                                                    {'loss': 2.7685, 'grad_norm': 9.930594444274902, 'learning_rate': 4.64406779661017e-06, 'epoch': 0.09}
  9%|â–Š         | 520/6000 [24:45<4:19:03,  2.84s/it]  9%|â–Š         | 521/6000 [24:48<4:14:28,  2.79s/it]                                                    {'loss': 2.7794, 'grad_norm': 12.841113090515137, 'learning_rate': 4.643220338983051e-06, 'epoch': 0.09}
  9%|â–Š         | 521/6000 [24:48<4:14:28,  2.79s/it]  9%|â–Š         | 522/6000 [24:51<4:16:25,  2.81s/it]                                                    {'loss': 2.7626, 'grad_norm': 6.7813334465026855, 'learning_rate': 4.6423728813559326e-06, 'epoch': 0.09}
  9%|â–Š         | 522/6000 [24:51<4:16:25,  2.81s/it]  9%|â–Š         | 523/6000 [24:54<4:19:51,  2.85s/it]                                                    {'loss': 2.7791, 'grad_norm': 7.646177768707275, 'learning_rate': 4.641525423728813e-06, 'epoch': 0.09}
  9%|â–Š         | 523/6000 [24:54<4:19:51,  2.85s/it]  9%|â–Š         | 524/6000 [24:56<4:16:38,  2.81s/it]                                                    {'loss': 2.8358, 'grad_norm': 6.596290588378906, 'learning_rate': 4.640677966101695e-06, 'epoch': 0.09}
  9%|â–Š         | 524/6000 [24:56<4:16:38,  2.81s/it]  9%|â–‰         | 525/6000 [24:59<4:16:12,  2.81s/it]                                                    {'loss': 2.7694, 'grad_norm': 11.658770561218262, 'learning_rate': 4.639830508474577e-06, 'epoch': 0.09}
  9%|â–‰         | 525/6000 [24:59<4:16:12,  2.81s/it]  9%|â–‰         | 526/6000 [25:02<4:14:56,  2.79s/it]                                                    {'loss': 2.8348, 'grad_norm': 13.051589965820312, 'learning_rate': 4.638983050847458e-06, 'epoch': 0.09}
  9%|â–‰         | 526/6000 [25:02<4:14:56,  2.79s/it]  9%|â–‰         | 527/6000 [25:05<4:14:18,  2.79s/it]                                                    {'loss': 2.7652, 'grad_norm': 17.47945785522461, 'learning_rate': 4.638135593220339e-06, 'epoch': 0.09}
  9%|â–‰         | 527/6000 [25:05<4:14:18,  2.79s/it]  9%|â–‰         | 528/6000 [25:08<4:16:22,  2.81s/it]                                                    {'loss': 2.7514, 'grad_norm': 26.21778678894043, 'learning_rate': 4.637288135593221e-06, 'epoch': 0.09}
  9%|â–‰         | 528/6000 [25:08<4:16:22,  2.81s/it]  9%|â–‰         | 529/6000 [25:10<4:13:59,  2.79s/it]                                                    {'loss': 2.7581, 'grad_norm': 15.032692909240723, 'learning_rate': 4.636440677966102e-06, 'epoch': 0.09}
  9%|â–‰         | 529/6000 [25:10<4:13:59,  2.79s/it]  9%|â–‰         | 530/6000 [25:13<4:15:46,  2.81s/it]                                                    {'loss': 2.7793, 'grad_norm': 19.70487403869629, 'learning_rate': 4.635593220338984e-06, 'epoch': 0.09}
  9%|â–‰         | 530/6000 [25:13<4:15:46,  2.81s/it]  9%|â–‰         | 531/6000 [25:16<4:13:33,  2.78s/it]                                                    {'loss': 2.7631, 'grad_norm': 7.965463638305664, 'learning_rate': 4.634745762711865e-06, 'epoch': 0.09}
  9%|â–‰         | 531/6000 [25:16<4:13:33,  2.78s/it]  9%|â–‰         | 532/6000 [25:19<4:13:17,  2.78s/it]                                                    {'loss': 2.7552, 'grad_norm': 14.231023788452148, 'learning_rate': 4.633898305084746e-06, 'epoch': 0.09}
  9%|â–‰         | 532/6000 [25:19<4:13:17,  2.78s/it]  9%|â–‰         | 533/6000 [25:21<4:10:42,  2.75s/it]                                                    {'loss': 2.7631, 'grad_norm': 9.66207218170166, 'learning_rate': 4.633050847457627e-06, 'epoch': 0.09}
  9%|â–‰         | 533/6000 [25:21<4:10:42,  2.75s/it]  9%|â–‰         | 534/6000 [25:24<4:10:22,  2.75s/it]                                                    {'loss': 2.8064, 'grad_norm': 9.18441104888916, 'learning_rate': 4.632203389830509e-06, 'epoch': 0.09}
  9%|â–‰         | 534/6000 [25:24<4:10:22,  2.75s/it]  9%|â–‰         | 535/6000 [25:27<4:09:32,  2.74s/it]                                                    {'loss': 2.801, 'grad_norm': 7.561723709106445, 'learning_rate': 4.6313559322033905e-06, 'epoch': 0.09}
  9%|â–‰         | 535/6000 [25:27<4:09:32,  2.74s/it]  9%|â–‰         | 536/6000 [25:29<4:08:21,  2.73s/it]                                                    {'loss': 2.7263, 'grad_norm': 34.93367004394531, 'learning_rate': 4.630508474576272e-06, 'epoch': 0.09}
  9%|â–‰         | 536/6000 [25:29<4:08:21,  2.73s/it]  9%|â–‰         | 537/6000 [25:32<4:07:55,  2.72s/it]                                                    {'loss': 2.7739, 'grad_norm': 20.463712692260742, 'learning_rate': 4.629661016949153e-06, 'epoch': 0.09}
  9%|â–‰         | 537/6000 [25:32<4:07:55,  2.72s/it]  9%|â–‰         | 538/6000 [25:35<4:12:33,  2.77s/it]                                                    {'loss': 2.8002, 'grad_norm': 15.41845989227295, 'learning_rate': 4.628813559322034e-06, 'epoch': 0.09}
  9%|â–‰         | 538/6000 [25:35<4:12:33,  2.77s/it]  9%|â–‰         | 539/6000 [25:38<4:09:56,  2.75s/it]                                                    {'loss': 2.7625, 'grad_norm': 12.983407020568848, 'learning_rate': 4.627966101694915e-06, 'epoch': 0.09}
  9%|â–‰         | 539/6000 [25:38<4:09:56,  2.75s/it]  9%|â–‰         | 540/6000 [25:40<4:08:17,  2.73s/it]                                                    {'loss': 2.7922, 'grad_norm': 21.66196632385254, 'learning_rate': 4.627118644067797e-06, 'epoch': 0.09}
  9%|â–‰         | 540/6000 [25:40<4:08:17,  2.73s/it]  9%|â–‰         | 541/6000 [25:43<4:12:07,  2.77s/it]                                                    {'loss': 2.7936, 'grad_norm': 12.266736030578613, 'learning_rate': 4.626271186440679e-06, 'epoch': 0.09}
  9%|â–‰         | 541/6000 [25:43<4:12:07,  2.77s/it]  9%|â–‰         | 542/6000 [25:46<4:10:31,  2.75s/it]                                                    {'loss': 2.7476, 'grad_norm': 14.350971221923828, 'learning_rate': 4.625423728813559e-06, 'epoch': 0.09}
  9%|â–‰         | 542/6000 [25:46<4:10:31,  2.75s/it]  9%|â–‰         | 543/6000 [25:49<4:09:07,  2.74s/it]                                                    {'loss': 2.7507, 'grad_norm': 17.832626342773438, 'learning_rate': 4.624576271186441e-06, 'epoch': 0.09}
  9%|â–‰         | 543/6000 [25:49<4:09:07,  2.74s/it]  9%|â–‰         | 544/6000 [25:51<4:08:59,  2.74s/it]                                                    {'loss': 2.7795, 'grad_norm': 15.49839973449707, 'learning_rate': 4.623728813559323e-06, 'epoch': 0.09}
  9%|â–‰         | 544/6000 [25:51<4:08:59,  2.74s/it]  9%|â–‰         | 545/6000 [25:54<4:11:39,  2.77s/it]                                                    {'loss': 2.7648, 'grad_norm': 8.984380722045898, 'learning_rate': 4.622881355932204e-06, 'epoch': 0.09}
  9%|â–‰         | 545/6000 [25:54<4:11:39,  2.77s/it]  9%|â–‰         | 546/6000 [25:57<4:10:13,  2.75s/it]                                                    {'loss': 2.7726, 'grad_norm': 9.290392875671387, 'learning_rate': 4.622033898305085e-06, 'epoch': 0.09}
  9%|â–‰         | 546/6000 [25:57<4:10:13,  2.75s/it]  9%|â–‰         | 547/6000 [26:00<4:13:17,  2.79s/it]                                                    {'loss': 2.8126, 'grad_norm': 31.75341033935547, 'learning_rate': 4.621186440677967e-06, 'epoch': 0.09}
  9%|â–‰         | 547/6000 [26:00<4:13:17,  2.79s/it]  9%|â–‰         | 548/6000 [26:03<4:13:27,  2.79s/it]                                                    {'loss': 2.7443, 'grad_norm': 25.38689422607422, 'learning_rate': 4.6203389830508475e-06, 'epoch': 0.09}
  9%|â–‰         | 548/6000 [26:03<4:13:27,  2.79s/it]  9%|â–‰         | 549/6000 [26:05<4:10:54,  2.76s/it]                                                    {'loss': 2.7914, 'grad_norm': 8.126958847045898, 'learning_rate': 4.619491525423729e-06, 'epoch': 0.09}
  9%|â–‰         | 549/6000 [26:05<4:10:54,  2.76s/it]  9%|â–‰         | 550/6000 [26:08<4:09:22,  2.75s/it]                                                    {'loss': 2.7807, 'grad_norm': 8.078387260437012, 'learning_rate': 4.618644067796611e-06, 'epoch': 0.09}
  9%|â–‰         | 550/6000 [26:08<4:09:22,  2.75s/it][2025-10-23 00:52:37,349] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550
[2025-10-23 00:52:37,360] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:52:37,981] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 551/6000 [26:14<5:36:23,  3.70s/it]                                                    {'loss': 2.7556, 'grad_norm': 21.235088348388672, 'learning_rate': 4.617796610169492e-06, 'epoch': 0.09}
  9%|â–‰         | 551/6000 [26:14<5:36:23,  3.70s/it]  9%|â–‰         | 552/6000 [26:17<5:09:35,  3.41s/it]                                                    {'loss': 2.7981, 'grad_norm': 23.145339965820312, 'learning_rate': 4.616949152542373e-06, 'epoch': 0.09}
  9%|â–‰         | 552/6000 [26:17<5:09:35,  3.41s/it]  9%|â–‰         | 553/6000 [26:20<4:54:19,  3.24s/it]                                                    {'loss': 2.8008, 'grad_norm': 13.515155792236328, 'learning_rate': 4.616101694915255e-06, 'epoch': 0.09}
  9%|â–‰         | 553/6000 [26:20<4:54:19,  3.24s/it]  9%|â–‰         | 554/6000 [26:22<4:40:40,  3.09s/it]                                                    {'loss': 2.7919, 'grad_norm': 15.451899528503418, 'learning_rate': 4.615254237288136e-06, 'epoch': 0.09}
  9%|â–‰         | 554/6000 [26:22<4:40:40,  3.09s/it]  9%|â–‰         | 555/6000 [26:25<4:30:31,  2.98s/it]                                                    {'loss': 2.7621, 'grad_norm': 20.03084373474121, 'learning_rate': 4.614406779661017e-06, 'epoch': 0.09}
  9%|â–‰         | 555/6000 [26:25<4:30:31,  2.98s/it]  9%|â–‰         | 556/6000 [26:28<4:23:35,  2.91s/it]                                                    {'loss': 2.7638, 'grad_norm': 20.569305419921875, 'learning_rate': 4.613559322033899e-06, 'epoch': 0.09}
  9%|â–‰         | 556/6000 [26:28<4:23:35,  2.91s/it]  9%|â–‰         | 557/6000 [26:31<4:20:40,  2.87s/it]                                                    {'loss': 2.7491, 'grad_norm': 14.378568649291992, 'learning_rate': 4.6127118644067805e-06, 'epoch': 0.09}
  9%|â–‰         | 557/6000 [26:31<4:20:40,  2.87s/it]  9%|â–‰         | 558/6000 [26:33<4:21:49,  2.89s/it]                                                    {'loss': 2.7696, 'grad_norm': 14.544312477111816, 'learning_rate': 4.611864406779661e-06, 'epoch': 0.09}
  9%|â–‰         | 558/6000 [26:33<4:21:49,  2.89s/it]  9%|â–‰         | 559/6000 [26:36<4:20:54,  2.88s/it]                                                    {'loss': 2.7824, 'grad_norm': 8.727004051208496, 'learning_rate': 4.611016949152542e-06, 'epoch': 0.09}
  9%|â–‰         | 559/6000 [26:36<4:20:54,  2.88s/it]  9%|â–‰         | 560/6000 [26:39<4:15:39,  2.82s/it]                                                    {'loss': 2.7682, 'grad_norm': 9.03664493560791, 'learning_rate': 4.610169491525424e-06, 'epoch': 0.09}
  9%|â–‰         | 560/6000 [26:39<4:15:39,  2.82s/it]  9%|â–‰         | 561/6000 [26:42<4:25:52,  2.93s/it]                                                    {'loss': 2.7105, 'grad_norm': 35.44624328613281, 'learning_rate': 4.609322033898305e-06, 'epoch': 0.09}
  9%|â–‰         | 561/6000 [26:42<4:25:52,  2.93s/it]  9%|â–‰         | 562/6000 [26:45<4:18:46,  2.86s/it]                                                    {'loss': 2.8711, 'grad_norm': 7.701780319213867, 'learning_rate': 4.608474576271187e-06, 'epoch': 0.09}
  9%|â–‰         | 562/6000 [26:45<4:18:46,  2.86s/it]  9%|â–‰         | 563/6000 [26:48<4:16:32,  2.83s/it]                                                    {'loss': 2.7548, 'grad_norm': 13.251182556152344, 'learning_rate': 4.607627118644068e-06, 'epoch': 0.09}
  9%|â–‰         | 563/6000 [26:48<4:16:32,  2.83s/it]  9%|â–‰         | 564/6000 [26:50<4:14:36,  2.81s/it]                                                    {'loss': 2.8031, 'grad_norm': 20.59082794189453, 'learning_rate': 4.6067796610169495e-06, 'epoch': 0.09}
  9%|â–‰         | 564/6000 [26:50<4:14:36,  2.81s/it]  9%|â–‰         | 565/6000 [26:53<4:19:05,  2.86s/it]                                                    {'loss': 2.7521, 'grad_norm': 24.93160629272461, 'learning_rate': 4.605932203389831e-06, 'epoch': 0.09}
  9%|â–‰         | 565/6000 [26:53<4:19:05,  2.86s/it]  9%|â–‰         | 566/6000 [26:56<4:15:53,  2.83s/it]                                                    {'loss': 2.7747, 'grad_norm': 20.93191146850586, 'learning_rate': 4.605084745762713e-06, 'epoch': 0.09}
  9%|â–‰         | 566/6000 [26:56<4:15:53,  2.83s/it]  9%|â–‰         | 567/6000 [26:59<4:24:10,  2.92s/it]                                                    {'loss': 2.7691, 'grad_norm': 10.565908432006836, 'learning_rate': 4.6042372881355935e-06, 'epoch': 0.09}
  9%|â–‰         | 567/6000 [26:59<4:24:10,  2.92s/it]  9%|â–‰         | 568/6000 [27:02<4:21:08,  2.88s/it]                                                    {'loss': 2.8113, 'grad_norm': 18.55461883544922, 'learning_rate': 4.603389830508475e-06, 'epoch': 0.09}
  9%|â–‰         | 568/6000 [27:02<4:21:08,  2.88s/it]  9%|â–‰         | 569/6000 [27:05<4:19:25,  2.87s/it]                                                    {'loss': 2.7515, 'grad_norm': 28.635801315307617, 'learning_rate': 4.602542372881356e-06, 'epoch': 0.09}
  9%|â–‰         | 569/6000 [27:05<4:19:25,  2.87s/it] 10%|â–‰         | 570/6000 [27:08<4:15:44,  2.83s/it]                                                    {'loss': 2.7618, 'grad_norm': 16.976484298706055, 'learning_rate': 4.601694915254238e-06, 'epoch': 0.1}
 10%|â–‰         | 570/6000 [27:08<4:15:44,  2.83s/it] 10%|â–‰         | 571/6000 [27:12<4:48:25,  3.19s/it]                                                    {'loss': 2.7744, 'grad_norm': 17.22336196899414, 'learning_rate': 4.600847457627119e-06, 'epoch': 0.1}
 10%|â–‰         | 571/6000 [27:12<4:48:25,  3.19s/it] 10%|â–‰         | 572/6000 [27:14<4:37:46,  3.07s/it]                                                    {'loss': 2.8363, 'grad_norm': 24.47640609741211, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.1}
 10%|â–‰         | 572/6000 [27:14<4:37:46,  3.07s/it] 10%|â–‰         | 573/6000 [27:17<4:28:01,  2.96s/it]                                                    {'loss': 2.8009, 'grad_norm': 15.954193115234375, 'learning_rate': 4.599152542372882e-06, 'epoch': 0.1}
 10%|â–‰         | 573/6000 [27:17<4:28:01,  2.96s/it] 10%|â–‰         | 574/6000 [27:20<4:23:58,  2.92s/it]                                                    {'loss': 2.8101, 'grad_norm': 13.313516616821289, 'learning_rate': 4.598305084745763e-06, 'epoch': 0.1}
 10%|â–‰         | 574/6000 [27:20<4:23:58,  2.92s/it] 10%|â–‰         | 575/6000 [27:23<4:18:25,  2.86s/it]                                                    {'loss': 2.8058, 'grad_norm': 19.594032287597656, 'learning_rate': 4.597457627118644e-06, 'epoch': 0.1}
 10%|â–‰         | 575/6000 [27:23<4:18:25,  2.86s/it] 10%|â–‰         | 576/6000 [27:26<4:19:47,  2.87s/it]                                                    {'loss': 2.7999, 'grad_norm': 17.62721824645996, 'learning_rate': 4.596610169491526e-06, 'epoch': 0.1}
 10%|â–‰         | 576/6000 [27:26<4:19:47,  2.87s/it] 10%|â–‰         | 577/6000 [27:29<4:23:26,  2.91s/it]                                                    {'loss': 2.735, 'grad_norm': 18.29096221923828, 'learning_rate': 4.595762711864407e-06, 'epoch': 0.1}
 10%|â–‰         | 577/6000 [27:29<4:23:26,  2.91s/it] 10%|â–‰         | 578/6000 [27:32<4:35:47,  3.05s/it]                                                    {'loss': 2.7529, 'grad_norm': 17.39533042907715, 'learning_rate': 4.594915254237288e-06, 'epoch': 0.1}
 10%|â–‰         | 578/6000 [27:32<4:35:47,  3.05s/it] 10%|â–‰         | 579/6000 [27:35<4:40:53,  3.11s/it]                                                    {'loss': 2.7712, 'grad_norm': 11.644842147827148, 'learning_rate': 4.59406779661017e-06, 'epoch': 0.1}
 10%|â–‰         | 579/6000 [27:35<4:40:53,  3.11s/it] 10%|â–‰         | 580/6000 [27:38<4:32:20,  3.01s/it]                                                    {'loss': 2.7831, 'grad_norm': 15.176987648010254, 'learning_rate': 4.5932203389830506e-06, 'epoch': 0.1}
 10%|â–‰         | 580/6000 [27:38<4:32:20,  3.01s/it] 10%|â–‰         | 581/6000 [27:42<4:47:30,  3.18s/it]                                                    {'loss': 2.8312, 'grad_norm': 39.91130447387695, 'learning_rate': 4.592372881355933e-06, 'epoch': 0.1}
 10%|â–‰         | 581/6000 [27:42<4:47:30,  3.18s/it] 10%|â–‰         | 582/6000 [27:45<5:00:54,  3.33s/it]                                                    {'loss': 2.7469, 'grad_norm': 19.59641456604004, 'learning_rate': 4.591525423728814e-06, 'epoch': 0.1}
 10%|â–‰         | 582/6000 [27:45<5:00:54,  3.33s/it] 10%|â–‰         | 583/6000 [27:48<4:49:25,  3.21s/it]                                                    {'loss': 2.7647, 'grad_norm': 18.08234214782715, 'learning_rate': 4.5906779661016955e-06, 'epoch': 0.1}
 10%|â–‰         | 583/6000 [27:48<4:49:25,  3.21s/it] 10%|â–‰         | 584/6000 [27:51<4:37:55,  3.08s/it]                                                    {'loss': 2.8268, 'grad_norm': 18.923303604125977, 'learning_rate': 4.589830508474576e-06, 'epoch': 0.1}
 10%|â–‰         | 584/6000 [27:51<4:37:55,  3.08s/it] 10%|â–‰         | 585/6000 [27:54<4:41:57,  3.12s/it]                                                    {'loss': 2.7889, 'grad_norm': 15.744562149047852, 'learning_rate': 4.588983050847458e-06, 'epoch': 0.1}
 10%|â–‰         | 585/6000 [27:54<4:41:57,  3.12s/it] 10%|â–‰         | 586/6000 [27:57<4:30:22,  3.00s/it]                                                    {'loss': 2.7732, 'grad_norm': 10.98457145690918, 'learning_rate': 4.5881355932203395e-06, 'epoch': 0.1}
 10%|â–‰         | 586/6000 [27:57<4:30:22,  3.00s/it] 10%|â–‰         | 587/6000 [28:00<4:23:01,  2.92s/it]                                                    {'loss': 2.8145, 'grad_norm': 28.19114875793457, 'learning_rate': 4.587288135593221e-06, 'epoch': 0.1}
 10%|â–‰         | 587/6000 [28:00<4:23:01,  2.92s/it] 10%|â–‰         | 588/6000 [28:02<4:18:15,  2.86s/it]                                                    {'loss': 2.798, 'grad_norm': 15.16242790222168, 'learning_rate': 4.586440677966102e-06, 'epoch': 0.1}
 10%|â–‰         | 588/6000 [28:02<4:18:15,  2.86s/it] 10%|â–‰         | 589/6000 [28:05<4:16:57,  2.85s/it]                                                    {'loss': 2.8061, 'grad_norm': 7.949665546417236, 'learning_rate': 4.585593220338984e-06, 'epoch': 0.1}
 10%|â–‰         | 589/6000 [28:05<4:16:57,  2.85s/it] 10%|â–‰         | 590/6000 [28:08<4:18:22,  2.87s/it]                                                    {'loss': 2.797, 'grad_norm': 14.92880916595459, 'learning_rate': 4.584745762711864e-06, 'epoch': 0.1}
 10%|â–‰         | 590/6000 [28:08<4:18:22,  2.87s/it] 10%|â–‰         | 591/6000 [28:11<4:20:39,  2.89s/it]                                                    {'loss': 2.7784, 'grad_norm': 9.952180862426758, 'learning_rate': 4.583898305084746e-06, 'epoch': 0.1}
 10%|â–‰         | 591/6000 [28:11<4:20:39,  2.89s/it] 10%|â–‰         | 592/6000 [28:14<4:16:16,  2.84s/it]                                                    {'loss': 2.7798, 'grad_norm': 8.822712898254395, 'learning_rate': 4.583050847457628e-06, 'epoch': 0.1}
 10%|â–‰         | 592/6000 [28:14<4:16:16,  2.84s/it] 10%|â–‰         | 593/6000 [28:17<4:13:21,  2.81s/it]                                                    {'loss': 2.7747, 'grad_norm': 12.916194915771484, 'learning_rate': 4.582203389830509e-06, 'epoch': 0.1}
 10%|â–‰         | 593/6000 [28:17<4:13:21,  2.81s/it] 10%|â–‰         | 594/6000 [28:19<4:14:45,  2.83s/it]                                                    {'loss': 2.7705, 'grad_norm': 10.491591453552246, 'learning_rate': 4.58135593220339e-06, 'epoch': 0.1}
 10%|â–‰         | 594/6000 [28:19<4:14:45,  2.83s/it] 10%|â–‰         | 595/6000 [28:23<4:22:32,  2.91s/it]                                                    {'loss': 2.7449, 'grad_norm': 22.333829879760742, 'learning_rate': 4.580508474576272e-06, 'epoch': 0.1}
 10%|â–‰         | 595/6000 [28:23<4:22:32,  2.91s/it] 10%|â–‰         | 596/6000 [28:26<4:42:50,  3.14s/it]                                                    {'loss': 2.7336, 'grad_norm': 12.041609764099121, 'learning_rate': 4.5796610169491525e-06, 'epoch': 0.1}
 10%|â–‰         | 596/6000 [28:26<4:42:50,  3.14s/it] 10%|â–‰         | 597/6000 [28:29<4:30:35,  3.00s/it]                                                    {'loss': 2.7738, 'grad_norm': 14.506400108337402, 'learning_rate': 4.578813559322034e-06, 'epoch': 0.1}
 10%|â–‰         | 597/6000 [28:29<4:30:35,  3.00s/it] 10%|â–‰         | 598/6000 [28:32<4:24:16,  2.94s/it]                                                    {'loss': 2.8101, 'grad_norm': 10.701716423034668, 'learning_rate': 4.577966101694916e-06, 'epoch': 0.1}
 10%|â–‰         | 598/6000 [28:32<4:24:16,  2.94s/it] 10%|â–‰         | 599/6000 [28:34<4:17:09,  2.86s/it]                                                    {'loss': 2.8015, 'grad_norm': 23.943098068237305, 'learning_rate': 4.5771186440677966e-06, 'epoch': 0.1}
 10%|â–‰         | 599/6000 [28:34<4:17:09,  2.86s/it] 10%|â–ˆ         | 600/6000 [28:37<4:13:21,  2.82s/it]                                                    {'loss': 2.8326, 'grad_norm': 6.28135871887207, 'learning_rate': 4.576271186440678e-06, 'epoch': 0.1}
 10%|â–ˆ         | 600/6000 [28:37<4:13:21,  2.82s/it][2025-10-23 00:55:06,333] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600
[2025-10-23 00:55:06,343] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:55:06,968] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 601/6000 [28:42<5:09:50,  3.44s/it]                                                    {'loss': 2.7923, 'grad_norm': 7.2929816246032715, 'learning_rate': 4.57542372881356e-06, 'epoch': 0.1}
 10%|â–ˆ         | 601/6000 [28:42<5:09:50,  3.44s/it] 10%|â–ˆ         | 602/6000 [28:45<4:50:55,  3.23s/it]                                                    {'loss': 2.7576, 'grad_norm': 22.550142288208008, 'learning_rate': 4.5745762711864415e-06, 'epoch': 0.1}
 10%|â–ˆ         | 602/6000 [28:45<4:50:55,  3.23s/it] 10%|â–ˆ         | 603/6000 [28:47<4:36:08,  3.07s/it]                                                    {'loss': 2.8438, 'grad_norm': 8.199872970581055, 'learning_rate': 4.573728813559322e-06, 'epoch': 0.1}
 10%|â–ˆ         | 603/6000 [28:47<4:36:08,  3.07s/it] 10%|â–ˆ         | 604/6000 [28:51<4:43:36,  3.15s/it]                                                    {'loss': 2.7788, 'grad_norm': 5.760775089263916, 'learning_rate': 4.572881355932204e-06, 'epoch': 0.1}
 10%|â–ˆ         | 604/6000 [28:51<4:43:36,  3.15s/it] 10%|â–ˆ         | 605/6000 [28:53<4:32:09,  3.03s/it]                                                    {'loss': 2.7891, 'grad_norm': 10.615952491760254, 'learning_rate': 4.572033898305085e-06, 'epoch': 0.1}
 10%|â–ˆ         | 605/6000 [28:53<4:32:09,  3.03s/it] 10%|â–ˆ         | 606/6000 [28:56<4:30:44,  3.01s/it]                                                    {'loss': 2.7705, 'grad_norm': 19.15322494506836, 'learning_rate': 4.571186440677966e-06, 'epoch': 0.1}
 10%|â–ˆ         | 606/6000 [28:56<4:30:44,  3.01s/it] 10%|â–ˆ         | 607/6000 [28:59<4:22:39,  2.92s/it]                                                    {'loss': 2.8089, 'grad_norm': 11.194295883178711, 'learning_rate': 4.570338983050848e-06, 'epoch': 0.1}
 10%|â–ˆ         | 607/6000 [28:59<4:22:39,  2.92s/it] 10%|â–ˆ         | 608/6000 [29:02<4:20:51,  2.90s/it]                                                    {'loss': 2.759, 'grad_norm': 20.904638290405273, 'learning_rate': 4.56949152542373e-06, 'epoch': 0.1}
 10%|â–ˆ         | 608/6000 [29:02<4:20:51,  2.90s/it] 10%|â–ˆ         | 609/6000 [29:05<4:15:33,  2.84s/it]                                                    {'loss': 2.7956, 'grad_norm': 19.350990295410156, 'learning_rate': 4.56864406779661e-06, 'epoch': 0.1}
 10%|â–ˆ         | 609/6000 [29:05<4:15:33,  2.84s/it] 10%|â–ˆ         | 610/6000 [29:07<4:11:24,  2.80s/it]                                                    {'loss': 2.7588, 'grad_norm': 11.460561752319336, 'learning_rate': 4.567796610169492e-06, 'epoch': 0.1}
 10%|â–ˆ         | 610/6000 [29:07<4:11:24,  2.80s/it] 10%|â–ˆ         | 611/6000 [29:10<4:10:01,  2.78s/it]                                                    {'loss': 2.817, 'grad_norm': 8.372108459472656, 'learning_rate': 4.566949152542373e-06, 'epoch': 0.1}
 10%|â–ˆ         | 611/6000 [29:10<4:10:01,  2.78s/it] 10%|â–ˆ         | 612/6000 [29:13<4:12:50,  2.82s/it]                                                    {'loss': 2.7754, 'grad_norm': 10.552589416503906, 'learning_rate': 4.5661016949152545e-06, 'epoch': 0.1}
 10%|â–ˆ         | 612/6000 [29:13<4:12:50,  2.82s/it] 10%|â–ˆ         | 613/6000 [29:16<4:16:38,  2.86s/it]                                                    {'loss': 2.8109, 'grad_norm': 6.178886890411377, 'learning_rate': 4.565254237288136e-06, 'epoch': 0.1}
 10%|â–ˆ         | 613/6000 [29:16<4:16:38,  2.86s/it] 10%|â–ˆ         | 614/6000 [29:19<4:12:36,  2.81s/it]                                                    {'loss': 2.7549, 'grad_norm': 13.281410217285156, 'learning_rate': 4.564406779661018e-06, 'epoch': 0.1}
 10%|â–ˆ         | 614/6000 [29:19<4:12:36,  2.81s/it] 10%|â–ˆ         | 615/6000 [29:21<4:10:01,  2.79s/it]                                                    {'loss': 2.7845, 'grad_norm': 6.368099689483643, 'learning_rate': 4.5635593220338985e-06, 'epoch': 0.1}
 10%|â–ˆ         | 615/6000 [29:21<4:10:01,  2.79s/it] 10%|â–ˆ         | 616/6000 [29:24<4:08:27,  2.77s/it]                                                    {'loss': 2.7872, 'grad_norm': 6.456555366516113, 'learning_rate': 4.56271186440678e-06, 'epoch': 0.1}
 10%|â–ˆ         | 616/6000 [29:24<4:08:27,  2.77s/it] 10%|â–ˆ         | 617/6000 [29:27<4:09:46,  2.78s/it]                                                    {'loss': 2.7933, 'grad_norm': 14.45806884765625, 'learning_rate': 4.561864406779661e-06, 'epoch': 0.1}
 10%|â–ˆ         | 617/6000 [29:27<4:09:46,  2.78s/it] 10%|â–ˆ         | 618/6000 [29:30<4:08:14,  2.77s/it]                                                    {'loss': 2.7834, 'grad_norm': 9.304981231689453, 'learning_rate': 4.561016949152543e-06, 'epoch': 0.1}
 10%|â–ˆ         | 618/6000 [29:30<4:08:14,  2.77s/it] 10%|â–ˆ         | 619/6000 [29:32<4:07:50,  2.76s/it]                                                    {'loss': 2.7712, 'grad_norm': 6.2880120277404785, 'learning_rate': 4.560169491525424e-06, 'epoch': 0.1}
 10%|â–ˆ         | 619/6000 [29:32<4:07:50,  2.76s/it] 10%|â–ˆ         | 620/6000 [29:35<4:05:53,  2.74s/it]                                                    {'loss': 2.7738, 'grad_norm': 7.511111736297607, 'learning_rate': 4.559322033898305e-06, 'epoch': 0.1}
 10%|â–ˆ         | 620/6000 [29:35<4:05:53,  2.74s/it] 10%|â–ˆ         | 621/6000 [29:38<4:05:17,  2.74s/it]                                                    {'loss': 2.8392, 'grad_norm': 6.3311004638671875, 'learning_rate': 4.558474576271187e-06, 'epoch': 0.1}
 10%|â–ˆ         | 621/6000 [29:38<4:05:17,  2.74s/it] 10%|â–ˆ         | 622/6000 [29:41<4:04:10,  2.72s/it]                                                    {'loss': 2.7766, 'grad_norm': 11.140645027160645, 'learning_rate': 4.557627118644068e-06, 'epoch': 0.1}
 10%|â–ˆ         | 622/6000 [29:41<4:04:10,  2.72s/it] 10%|â–ˆ         | 623/6000 [29:43<4:03:39,  2.72s/it]                                                    {'loss': 2.7908, 'grad_norm': 24.218406677246094, 'learning_rate': 4.55677966101695e-06, 'epoch': 0.1}
 10%|â–ˆ         | 623/6000 [29:43<4:03:39,  2.72s/it] 10%|â–ˆ         | 624/6000 [29:46<4:02:56,  2.71s/it]                                                    {'loss': 2.7684, 'grad_norm': 5.512537956237793, 'learning_rate': 4.555932203389831e-06, 'epoch': 0.1}
 10%|â–ˆ         | 624/6000 [29:46<4:02:56,  2.71s/it] 10%|â–ˆ         | 625/6000 [29:49<4:07:53,  2.77s/it]                                                    {'loss': 2.8127, 'grad_norm': 5.123323440551758, 'learning_rate': 4.555084745762712e-06, 'epoch': 0.1}
 10%|â–ˆ         | 625/6000 [29:49<4:07:53,  2.77s/it] 10%|â–ˆ         | 626/6000 [29:52<4:06:35,  2.75s/it]                                                    {'loss': 2.7767, 'grad_norm': 7.935954570770264, 'learning_rate': 4.554237288135593e-06, 'epoch': 0.1}
 10%|â–ˆ         | 626/6000 [29:52<4:06:35,  2.75s/it] 10%|â–ˆ         | 627/6000 [29:55<4:27:08,  2.98s/it]                                                    {'loss': 2.751, 'grad_norm': 6.9331278800964355, 'learning_rate': 4.553389830508475e-06, 'epoch': 0.1}
 10%|â–ˆ         | 627/6000 [29:55<4:27:08,  2.98s/it] 10%|â–ˆ         | 628/6000 [29:58<4:20:40,  2.91s/it]                                                    {'loss': 2.7764, 'grad_norm': 19.24639892578125, 'learning_rate': 4.552542372881356e-06, 'epoch': 0.1}
 10%|â–ˆ         | 628/6000 [29:58<4:20:40,  2.91s/it] 10%|â–ˆ         | 629/6000 [30:01<4:14:09,  2.84s/it]                                                    {'loss': 2.7666, 'grad_norm': 13.730998992919922, 'learning_rate': 4.551694915254238e-06, 'epoch': 0.1}
 10%|â–ˆ         | 629/6000 [30:01<4:14:09,  2.84s/it] 10%|â–ˆ         | 630/6000 [30:03<4:10:12,  2.80s/it]                                                    {'loss': 2.7837, 'grad_norm': 6.1990461349487305, 'learning_rate': 4.550847457627119e-06, 'epoch': 0.1}
 10%|â–ˆ         | 630/6000 [30:03<4:10:12,  2.80s/it] 11%|â–ˆ         | 631/6000 [30:06<4:11:08,  2.81s/it]                                                    {'loss': 2.7365, 'grad_norm': 27.621545791625977, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.11}
 11%|â–ˆ         | 631/6000 [30:06<4:11:08,  2.81s/it] 11%|â–ˆ         | 632/6000 [30:09<4:11:36,  2.81s/it]                                                    {'loss': 2.8685, 'grad_norm': 11.078629493713379, 'learning_rate': 4.549152542372881e-06, 'epoch': 0.11}
 11%|â–ˆ         | 632/6000 [30:09<4:11:36,  2.81s/it] 11%|â–ˆ         | 633/6000 [30:12<4:10:29,  2.80s/it]                                                    {'loss': 2.7713, 'grad_norm': 7.9658966064453125, 'learning_rate': 4.548305084745763e-06, 'epoch': 0.11}
 11%|â–ˆ         | 633/6000 [30:12<4:10:29,  2.80s/it] 11%|â–ˆ         | 634/6000 [30:15<4:18:31,  2.89s/it]                                                    {'loss': 2.759, 'grad_norm': 17.545066833496094, 'learning_rate': 4.5474576271186445e-06, 'epoch': 0.11}
 11%|â–ˆ         | 634/6000 [30:15<4:18:31,  2.89s/it] 11%|â–ˆ         | 635/6000 [30:18<4:17:32,  2.88s/it]                                                    {'loss': 2.7659, 'grad_norm': 8.722585678100586, 'learning_rate': 4.546610169491526e-06, 'epoch': 0.11}
 11%|â–ˆ         | 635/6000 [30:18<4:17:32,  2.88s/it] 11%|â–ˆ         | 636/6000 [30:20<4:13:40,  2.84s/it]                                                    {'loss': 2.7784, 'grad_norm': 10.488479614257812, 'learning_rate': 4.545762711864407e-06, 'epoch': 0.11}
 11%|â–ˆ         | 636/6000 [30:20<4:13:40,  2.84s/it] 11%|â–ˆ         | 637/6000 [30:23<4:11:44,  2.82s/it]                                                    {'loss': 2.7764, 'grad_norm': 14.726287841796875, 'learning_rate': 4.544915254237289e-06, 'epoch': 0.11}
 11%|â–ˆ         | 637/6000 [30:23<4:11:44,  2.82s/it] 11%|â–ˆ         | 638/6000 [30:26<4:20:13,  2.91s/it]                                                    {'loss': 2.7554, 'grad_norm': 14.8944730758667, 'learning_rate': 4.54406779661017e-06, 'epoch': 0.11}
 11%|â–ˆ         | 638/6000 [30:26<4:20:13,  2.91s/it] 11%|â–ˆ         | 639/6000 [30:29<4:15:17,  2.86s/it]                                                    {'loss': 2.7971, 'grad_norm': 6.963656902313232, 'learning_rate': 4.543220338983052e-06, 'epoch': 0.11}
 11%|â–ˆ         | 639/6000 [30:29<4:15:17,  2.86s/it] 11%|â–ˆ         | 640/6000 [30:32<4:22:57,  2.94s/it]                                                    {'loss': 2.8085, 'grad_norm': 8.442292213439941, 'learning_rate': 4.542372881355933e-06, 'epoch': 0.11}
 11%|â–ˆ         | 640/6000 [30:32<4:22:57,  2.94s/it] 11%|â–ˆ         | 641/6000 [30:35<4:31:32,  3.04s/it]                                                    {'loss': 2.7621, 'grad_norm': 10.562826156616211, 'learning_rate': 4.5415254237288135e-06, 'epoch': 0.11}
 11%|â–ˆ         | 641/6000 [30:35<4:31:32,  3.04s/it] 11%|â–ˆ         | 642/6000 [30:38<4:22:00,  2.93s/it]                                                    {'loss': 2.771, 'grad_norm': 13.425252914428711, 'learning_rate': 4.540677966101695e-06, 'epoch': 0.11}
 11%|â–ˆ         | 642/6000 [30:38<4:22:00,  2.93s/it] 11%|â–ˆ         | 643/6000 [30:41<4:17:18,  2.88s/it]                                                    {'loss': 2.8119, 'grad_norm': 8.961785316467285, 'learning_rate': 4.539830508474577e-06, 'epoch': 0.11}
 11%|â–ˆ         | 643/6000 [30:41<4:17:18,  2.88s/it] 11%|â–ˆ         | 644/6000 [30:44<4:14:14,  2.85s/it]                                                    {'loss': 2.7836, 'grad_norm': 8.664240837097168, 'learning_rate': 4.538983050847458e-06, 'epoch': 0.11}
 11%|â–ˆ         | 644/6000 [30:44<4:14:14,  2.85s/it] 11%|â–ˆ         | 645/6000 [30:46<4:09:34,  2.80s/it]                                                    {'loss': 2.7405, 'grad_norm': 12.325626373291016, 'learning_rate': 4.538135593220339e-06, 'epoch': 0.11}
 11%|â–ˆ         | 645/6000 [30:46<4:09:34,  2.80s/it] 11%|â–ˆ         | 646/6000 [30:49<4:18:49,  2.90s/it]                                                    {'loss': 2.7636, 'grad_norm': 10.278870582580566, 'learning_rate': 4.537288135593221e-06, 'epoch': 0.11}
 11%|â–ˆ         | 646/6000 [30:49<4:18:49,  2.90s/it] 11%|â–ˆ         | 647/6000 [30:52<4:13:19,  2.84s/it]                                                    {'loss': 2.8489, 'grad_norm': 9.70252513885498, 'learning_rate': 4.536440677966102e-06, 'epoch': 0.11}
 11%|â–ˆ         | 647/6000 [30:52<4:13:19,  2.84s/it] 11%|â–ˆ         | 648/6000 [30:55<4:20:22,  2.92s/it]                                                    {'loss': 2.7592, 'grad_norm': 10.440430641174316, 'learning_rate': 4.535593220338983e-06, 'epoch': 0.11}
 11%|â–ˆ         | 648/6000 [30:55<4:20:22,  2.92s/it] 11%|â–ˆ         | 649/6000 [30:58<4:14:51,  2.86s/it]                                                    {'loss': 2.7586, 'grad_norm': 8.343619346618652, 'learning_rate': 4.534745762711865e-06, 'epoch': 0.11}
 11%|â–ˆ         | 649/6000 [30:58<4:14:51,  2.86s/it] 11%|â–ˆ         | 650/6000 [31:01<4:10:39,  2.81s/it]                                                    {'loss': 2.7926, 'grad_norm': 15.746139526367188, 'learning_rate': 4.5338983050847465e-06, 'epoch': 0.11}
 11%|â–ˆ         | 650/6000 [31:01<4:10:39,  2.81s/it][2025-10-23 00:57:29,944] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650
[2025-10-23 00:57:29,956] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:57:30,564] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 11%|â–ˆ         | 651/6000 [31:06<5:07:48,  3.45s/it]                                                    {'loss': 2.817, 'grad_norm': 10.508238792419434, 'learning_rate': 4.533050847457627e-06, 'epoch': 0.11}
 11%|â–ˆ         | 651/6000 [31:06<5:07:48,  3.45s/it] 11%|â–ˆ         | 652/6000 [31:09<4:58:43,  3.35s/it]                                                    {'loss': 2.7873, 'grad_norm': 13.05823040008545, 'learning_rate': 4.532203389830509e-06, 'epoch': 0.11}
 11%|â–ˆ         | 652/6000 [31:09<4:58:43,  3.35s/it] 11%|â–ˆ         | 653/6000 [31:12<4:45:53,  3.21s/it]                                                    {'loss': 2.7664, 'grad_norm': 22.83526039123535, 'learning_rate': 4.53135593220339e-06, 'epoch': 0.11}
 11%|â–ˆ         | 653/6000 [31:12<4:45:53,  3.21s/it] 11%|â–ˆ         | 654/6000 [31:14<4:33:28,  3.07s/it]                                                    {'loss': 2.782, 'grad_norm': 9.453132629394531, 'learning_rate': 4.530508474576271e-06, 'epoch': 0.11}
 11%|â–ˆ         | 654/6000 [31:14<4:33:28,  3.07s/it] 11%|â–ˆ         | 655/6000 [31:17<4:23:59,  2.96s/it]                                                    {'loss': 2.7718, 'grad_norm': 14.548675537109375, 'learning_rate': 4.529661016949153e-06, 'epoch': 0.11}
 11%|â–ˆ         | 655/6000 [31:17<4:23:59,  2.96s/it] 11%|â–ˆ         | 656/6000 [31:20<4:18:23,  2.90s/it]                                                    {'loss': 2.7336, 'grad_norm': 21.401105880737305, 'learning_rate': 4.528813559322035e-06, 'epoch': 0.11}
 11%|â–ˆ         | 656/6000 [31:20<4:18:23,  2.90s/it] 11%|â–ˆ         | 657/6000 [31:23<4:18:59,  2.91s/it]                                                    {'loss': 2.7864, 'grad_norm': 15.87287425994873, 'learning_rate': 4.527966101694915e-06, 'epoch': 0.11}
 11%|â–ˆ         | 657/6000 [31:23<4:18:59,  2.91s/it] 11%|â–ˆ         | 658/6000 [31:26<4:25:46,  2.99s/it]                                                    {'loss': 2.8409, 'grad_norm': 18.20081329345703, 'learning_rate': 4.527118644067797e-06, 'epoch': 0.11}
 11%|â–ˆ         | 658/6000 [31:26<4:25:46,  2.99s/it] 11%|â–ˆ         | 659/6000 [31:29<4:19:42,  2.92s/it]                                                    {'loss': 2.7451, 'grad_norm': 28.613155364990234, 'learning_rate': 4.526271186440679e-06, 'epoch': 0.11}
 11%|â–ˆ         | 659/6000 [31:29<4:19:42,  2.92s/it] 11%|â–ˆ         | 660/6000 [31:31<4:15:30,  2.87s/it]                                                    {'loss': 2.7658, 'grad_norm': 24.392616271972656, 'learning_rate': 4.52542372881356e-06, 'epoch': 0.11}
 11%|â–ˆ         | 660/6000 [31:31<4:15:30,  2.87s/it] 11%|â–ˆ         | 661/6000 [31:34<4:12:09,  2.83s/it]                                                    {'loss': 2.7853, 'grad_norm': 10.34438419342041, 'learning_rate': 4.524576271186441e-06, 'epoch': 0.11}
 11%|â–ˆ         | 661/6000 [31:34<4:12:09,  2.83s/it] 11%|â–ˆ         | 662/6000 [31:37<4:09:29,  2.80s/it]                                                    {'loss': 2.7917, 'grad_norm': 13.793092727661133, 'learning_rate': 4.523728813559322e-06, 'epoch': 0.11}
 11%|â–ˆ         | 662/6000 [31:37<4:09:29,  2.80s/it] 11%|â–ˆ         | 663/6000 [31:40<4:07:32,  2.78s/it]                                                    {'loss': 2.7372, 'grad_norm': 17.989620208740234, 'learning_rate': 4.5228813559322035e-06, 'epoch': 0.11}
 11%|â–ˆ         | 663/6000 [31:40<4:07:32,  2.78s/it] 11%|â–ˆ         | 664/6000 [31:42<4:09:55,  2.81s/it]                                                    {'loss': 2.8637, 'grad_norm': 14.941610336303711, 'learning_rate': 4.522033898305085e-06, 'epoch': 0.11}
 11%|â–ˆ         | 664/6000 [31:42<4:09:55,  2.81s/it] 11%|â–ˆ         | 665/6000 [31:45<4:08:49,  2.80s/it]                                                    {'loss': 2.7634, 'grad_norm': 15.618426322937012, 'learning_rate': 4.521186440677967e-06, 'epoch': 0.11}
 11%|â–ˆ         | 665/6000 [31:45<4:08:49,  2.80s/it] 11%|â–ˆ         | 666/6000 [31:48<4:06:24,  2.77s/it]                                                    {'loss': 2.7835, 'grad_norm': 12.677122116088867, 'learning_rate': 4.520338983050848e-06, 'epoch': 0.11}
 11%|â–ˆ         | 666/6000 [31:48<4:06:24,  2.77s/it] 11%|â–ˆ         | 667/6000 [31:51<4:06:32,  2.77s/it]                                                    {'loss': 2.8112, 'grad_norm': 12.288159370422363, 'learning_rate': 4.519491525423729e-06, 'epoch': 0.11}
 11%|â–ˆ         | 667/6000 [31:51<4:06:32,  2.77s/it] 11%|â–ˆ         | 668/6000 [31:54<4:06:33,  2.77s/it]                                                    {'loss': 2.7894, 'grad_norm': 17.336578369140625, 'learning_rate': 4.51864406779661e-06, 'epoch': 0.11}
 11%|â–ˆ         | 668/6000 [31:54<4:06:33,  2.77s/it] 11%|â–ˆ         | 669/6000 [31:56<4:05:55,  2.77s/it]                                                    {'loss': 2.8152, 'grad_norm': 16.74467658996582, 'learning_rate': 4.517796610169492e-06, 'epoch': 0.11}
 11%|â–ˆ         | 669/6000 [31:56<4:05:55,  2.77s/it] 11%|â–ˆ         | 670/6000 [31:59<4:09:06,  2.80s/it]                                                    {'loss': 2.7805, 'grad_norm': 18.42304039001465, 'learning_rate': 4.516949152542373e-06, 'epoch': 0.11}
 11%|â–ˆ         | 670/6000 [31:59<4:09:06,  2.80s/it] 11%|â–ˆ         | 671/6000 [32:02<4:07:35,  2.79s/it]                                                    {'loss': 2.7977, 'grad_norm': 9.424188613891602, 'learning_rate': 4.516101694915255e-06, 'epoch': 0.11}
 11%|â–ˆ         | 671/6000 [32:02<4:07:35,  2.79s/it] 11%|â–ˆ         | 672/6000 [32:05<4:03:02,  2.74s/it]                                                    {'loss': 2.806, 'grad_norm': 12.58365535736084, 'learning_rate': 4.515254237288136e-06, 'epoch': 0.11}
 11%|â–ˆ         | 672/6000 [32:05<4:03:02,  2.74s/it] 11%|â–ˆ         | 673/6000 [32:07<4:04:12,  2.75s/it]                                                    {'loss': 2.8223, 'grad_norm': 12.353463172912598, 'learning_rate': 4.514406779661017e-06, 'epoch': 0.11}
 11%|â–ˆ         | 673/6000 [32:07<4:04:12,  2.75s/it] 11%|â–ˆ         | 674/6000 [32:10<4:04:30,  2.75s/it]                                                    {'loss': 2.7897, 'grad_norm': 26.39097023010254, 'learning_rate': 4.513559322033898e-06, 'epoch': 0.11}
 11%|â–ˆ         | 674/6000 [32:10<4:04:30,  2.75s/it] 11%|â–ˆâ–        | 675/6000 [32:13<4:08:23,  2.80s/it]                                                    {'loss': 2.7987, 'grad_norm': 16.404956817626953, 'learning_rate': 4.51271186440678e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 675/6000 [32:13<4:08:23,  2.80s/it] 11%|â–ˆâ–        | 676/6000 [32:16<4:06:37,  2.78s/it]                                                    {'loss': 2.8078, 'grad_norm': 10.571148872375488, 'learning_rate': 4.5118644067796614e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 676/6000 [32:16<4:06:37,  2.78s/it] 11%|â–ˆâ–        | 677/6000 [32:18<4:03:36,  2.75s/it]                                                    {'loss': 2.7425, 'grad_norm': 27.44520378112793, 'learning_rate': 4.511016949152543e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 677/6000 [32:18<4:03:36,  2.75s/it] 11%|â–ˆâ–        | 678/6000 [32:21<4:03:11,  2.74s/it]                                                    {'loss': 2.7804, 'grad_norm': 8.39767074584961, 'learning_rate': 4.510169491525424e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 678/6000 [32:21<4:03:11,  2.74s/it] 11%|â–ˆâ–        | 679/6000 [32:24<4:01:24,  2.72s/it]                                                    {'loss': 2.7807, 'grad_norm': 15.981856346130371, 'learning_rate': 4.5093220338983055e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 679/6000 [32:24<4:01:24,  2.72s/it] 11%|â–ˆâ–        | 680/6000 [32:27<4:02:45,  2.74s/it]                                                    {'loss': 2.7814, 'grad_norm': 15.223491668701172, 'learning_rate': 4.508474576271187e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 680/6000 [32:27<4:02:45,  2.74s/it] 11%|â–ˆâ–        | 681/6000 [32:30<4:12:57,  2.85s/it]                                                    {'loss': 2.791, 'grad_norm': 12.118036270141602, 'learning_rate': 4.507627118644069e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 681/6000 [32:30<4:12:57,  2.85s/it] 11%|â–ˆâ–        | 682/6000 [32:33<4:11:59,  2.84s/it]                                                    {'loss': 2.7654, 'grad_norm': 11.295238494873047, 'learning_rate': 4.5067796610169496e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 682/6000 [32:33<4:11:59,  2.84s/it] 11%|â–ˆâ–        | 683/6000 [32:35<4:09:48,  2.82s/it]                                                    {'loss': 2.7415, 'grad_norm': 17.9238338470459, 'learning_rate': 4.50593220338983e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 683/6000 [32:35<4:09:48,  2.82s/it] 11%|â–ˆâ–        | 684/6000 [32:38<4:08:04,  2.80s/it]                                                    {'loss': 2.7887, 'grad_norm': 17.53689193725586, 'learning_rate': 4.505084745762712e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 684/6000 [32:38<4:08:04,  2.80s/it] 11%|â–ˆâ–        | 685/6000 [32:41<4:06:18,  2.78s/it]                                                    {'loss': 2.7711, 'grad_norm': 13.15273666381836, 'learning_rate': 4.504237288135594e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 685/6000 [32:41<4:06:18,  2.78s/it] 11%|â–ˆâ–        | 686/6000 [32:44<4:19:43,  2.93s/it]                                                    {'loss': 2.7666, 'grad_norm': 11.822258949279785, 'learning_rate': 4.503389830508475e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 686/6000 [32:44<4:19:43,  2.93s/it] 11%|â–ˆâ–        | 687/6000 [32:47<4:13:21,  2.86s/it]                                                    {'loss': 2.78, 'grad_norm': 13.646543502807617, 'learning_rate': 4.502542372881356e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 687/6000 [32:47<4:13:21,  2.86s/it] 11%|â–ˆâ–        | 688/6000 [32:49<4:09:45,  2.82s/it]                                                    {'loss': 2.8073, 'grad_norm': 7.887094974517822, 'learning_rate': 4.501694915254238e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 688/6000 [32:49<4:09:45,  2.82s/it] 11%|â–ˆâ–        | 689/6000 [32:52<4:07:51,  2.80s/it]                                                    {'loss': 2.7635, 'grad_norm': 35.609230041503906, 'learning_rate': 4.5008474576271185e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 689/6000 [32:52<4:07:51,  2.80s/it] 12%|â–ˆâ–        | 690/6000 [32:55<4:09:57,  2.82s/it]                                                    {'loss': 2.7537, 'grad_norm': 24.039531707763672, 'learning_rate': 4.5e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 690/6000 [32:55<4:09:57,  2.82s/it] 12%|â–ˆâ–        | 691/6000 [32:58<4:09:34,  2.82s/it]                                                    {'loss': 2.755, 'grad_norm': 12.793052673339844, 'learning_rate': 4.499152542372882e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 691/6000 [32:58<4:09:34,  2.82s/it] 12%|â–ˆâ–        | 692/6000 [33:01<4:07:03,  2.79s/it]                                                    {'loss': 2.7229, 'grad_norm': 23.138452529907227, 'learning_rate': 4.498305084745763e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 692/6000 [33:01<4:07:03,  2.79s/it] 12%|â–ˆâ–        | 693/6000 [33:03<4:04:44,  2.77s/it]                                                    {'loss': 2.7845, 'grad_norm': 9.529597282409668, 'learning_rate': 4.497457627118644e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 693/6000 [33:03<4:04:44,  2.77s/it] 12%|â–ˆâ–        | 694/6000 [33:06<4:05:00,  2.77s/it]                                                    {'loss': 2.7648, 'grad_norm': 22.841588973999023, 'learning_rate': 4.496610169491526e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 694/6000 [33:06<4:05:00,  2.77s/it] 12%|â–ˆâ–        | 695/6000 [33:09<4:04:51,  2.77s/it]                                                    {'loss': 2.8035, 'grad_norm': 11.105262756347656, 'learning_rate': 4.4957627118644075e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 695/6000 [33:09<4:04:51,  2.77s/it] 12%|â–ˆâ–        | 696/6000 [33:12<4:15:26,  2.89s/it]                                                    {'loss': 2.7751, 'grad_norm': 13.478241920471191, 'learning_rate': 4.494915254237289e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 696/6000 [33:12<4:15:26,  2.89s/it] 12%|â–ˆâ–        | 697/6000 [33:15<4:09:21,  2.82s/it]                                                    {'loss': 2.7799, 'grad_norm': 14.38400650024414, 'learning_rate': 4.49406779661017e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 697/6000 [33:15<4:09:21,  2.82s/it] 12%|â–ˆâ–        | 698/6000 [33:17<4:05:38,  2.78s/it]                                                    {'loss': 2.844, 'grad_norm': 9.726154327392578, 'learning_rate': 4.4932203389830515e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 698/6000 [33:17<4:05:38,  2.78s/it] 12%|â–ˆâ–        | 699/6000 [33:20<4:04:12,  2.76s/it]                                                    {'loss': 2.7585, 'grad_norm': 15.296712875366211, 'learning_rate': 4.492372881355932e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 699/6000 [33:20<4:04:12,  2.76s/it] 12%|â–ˆâ–        | 700/6000 [33:23<4:05:04,  2.77s/it]                                                    {'loss': 2.7673, 'grad_norm': 10.687076568603516, 'learning_rate': 4.491525423728814e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 700/6000 [33:23<4:05:04,  2.77s/it][2025-10-23 00:59:52,241] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700
[2025-10-23 00:59:52,256] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 00:59:52,870] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 12%|â–ˆâ–        | 701/6000 [33:28<5:05:43,  3.46s/it]                                                    {'loss': 2.8119, 'grad_norm': 23.15589714050293, 'learning_rate': 4.490677966101696e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 701/6000 [33:28<5:05:43,  3.46s/it] 12%|â–ˆâ–        | 702/6000 [33:31<4:57:54,  3.37s/it]                                                    {'loss': 2.7694, 'grad_norm': 10.536613464355469, 'learning_rate': 4.489830508474577e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 702/6000 [33:31<4:57:54,  3.37s/it] 12%|â–ˆâ–        | 703/6000 [33:34<4:41:15,  3.19s/it]                                                    {'loss': 2.7569, 'grad_norm': 34.28505325317383, 'learning_rate': 4.488983050847458e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 703/6000 [33:34<4:41:15,  3.19s/it] 12%|â–ˆâ–        | 704/6000 [33:37<4:38:46,  3.16s/it]                                                    {'loss': 2.736, 'grad_norm': 14.231633186340332, 'learning_rate': 4.488135593220339e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 704/6000 [33:37<4:38:46,  3.16s/it] 12%|â–ˆâ–        | 705/6000 [33:40<4:29:54,  3.06s/it]                                                    {'loss': 2.8319, 'grad_norm': 30.56585693359375, 'learning_rate': 4.4872881355932204e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 705/6000 [33:40<4:29:54,  3.06s/it] 12%|â–ˆâ–        | 706/6000 [33:43<4:20:33,  2.95s/it]                                                    {'loss': 2.7722, 'grad_norm': 19.56463623046875, 'learning_rate': 4.486440677966102e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 706/6000 [33:43<4:20:33,  2.95s/it] 12%|â–ˆâ–        | 707/6000 [33:45<4:15:40,  2.90s/it]                                                    {'loss': 2.7911, 'grad_norm': 18.298030853271484, 'learning_rate': 4.485593220338984e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 707/6000 [33:45<4:15:40,  2.90s/it] 12%|â–ˆâ–        | 708/6000 [33:48<4:11:35,  2.85s/it]                                                    {'loss': 2.7814, 'grad_norm': 18.274415969848633, 'learning_rate': 4.4847457627118645e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 708/6000 [33:48<4:11:35,  2.85s/it] 12%|â–ˆâ–        | 709/6000 [33:51<4:08:24,  2.82s/it]                                                    {'loss': 2.7694, 'grad_norm': 19.184537887573242, 'learning_rate': 4.483898305084746e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 709/6000 [33:51<4:08:24,  2.82s/it] 12%|â–ˆâ–        | 710/6000 [33:54<4:14:41,  2.89s/it]                                                    {'loss': 2.7502, 'grad_norm': 24.63996124267578, 'learning_rate': 4.483050847457627e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 710/6000 [33:54<4:14:41,  2.89s/it] 12%|â–ˆâ–        | 711/6000 [33:57<4:08:42,  2.82s/it]                                                    {'loss': 2.7756, 'grad_norm': 13.266539573669434, 'learning_rate': 4.4822033898305086e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 711/6000 [33:57<4:08:42,  2.82s/it] 12%|â–ˆâ–        | 712/6000 [33:59<4:07:48,  2.81s/it]                                                    {'loss': 2.7614, 'grad_norm': 11.512826919555664, 'learning_rate': 4.48135593220339e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 712/6000 [33:59<4:07:48,  2.81s/it] 12%|â–ˆâ–        | 713/6000 [34:02<4:06:18,  2.80s/it]                                                    {'loss': 2.7586, 'grad_norm': 10.893099784851074, 'learning_rate': 4.480508474576272e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 713/6000 [34:02<4:06:18,  2.80s/it] 12%|â–ˆâ–        | 714/6000 [34:05<4:04:48,  2.78s/it]                                                    {'loss': 2.7622, 'grad_norm': 23.342348098754883, 'learning_rate': 4.479661016949153e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 714/6000 [34:05<4:04:48,  2.78s/it] 12%|â–ˆâ–        | 715/6000 [34:07<4:01:50,  2.75s/it]                                                    {'loss': 2.7682, 'grad_norm': 7.731756210327148, 'learning_rate': 4.478813559322034e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 715/6000 [34:07<4:01:50,  2.75s/it] 12%|â–ˆâ–        | 716/6000 [34:10<4:01:36,  2.74s/it]                                                    {'loss': 2.8196, 'grad_norm': 12.734463691711426, 'learning_rate': 4.477966101694916e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 716/6000 [34:10<4:01:36,  2.74s/it] 12%|â–ˆâ–        | 717/6000 [34:13<4:10:56,  2.85s/it]                                                    {'loss': 2.7952, 'grad_norm': 11.803013801574707, 'learning_rate': 4.4771186440677975e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 717/6000 [34:13<4:10:56,  2.85s/it] 12%|â–ˆâ–        | 718/6000 [34:16<4:06:15,  2.80s/it]                                                    {'loss': 2.798, 'grad_norm': 11.640069007873535, 'learning_rate': 4.476271186440678e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 718/6000 [34:16<4:06:15,  2.80s/it] 12%|â–ˆâ–        | 719/6000 [34:19<4:06:16,  2.80s/it]                                                    {'loss': 2.7369, 'grad_norm': 10.88271713256836, 'learning_rate': 4.47542372881356e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 719/6000 [34:19<4:06:16,  2.80s/it] 12%|â–ˆâ–        | 720/6000 [34:21<4:03:14,  2.76s/it]                                                    {'loss': 2.7605, 'grad_norm': 8.013315200805664, 'learning_rate': 4.474576271186441e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 720/6000 [34:21<4:03:14,  2.76s/it] 12%|â–ˆâ–        | 721/6000 [34:24<4:01:53,  2.75s/it]                                                    {'loss': 2.7458, 'grad_norm': 14.768156051635742, 'learning_rate': 4.473728813559322e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 721/6000 [34:24<4:01:53,  2.75s/it] 12%|â–ˆâ–        | 722/6000 [34:27<4:02:46,  2.76s/it]                                                    {'loss': 2.7767, 'grad_norm': 21.3214168548584, 'learning_rate': 4.472881355932204e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 722/6000 [34:27<4:02:46,  2.76s/it] 12%|â–ˆâ–        | 723/6000 [34:30<4:13:48,  2.89s/it]                                                    {'loss': 2.7709, 'grad_norm': 14.351557731628418, 'learning_rate': 4.472033898305086e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 723/6000 [34:30<4:13:48,  2.89s/it] 12%|â–ˆâ–        | 724/6000 [34:33<4:09:35,  2.84s/it]                                                    {'loss': 2.8033, 'grad_norm': 19.16634178161621, 'learning_rate': 4.4711864406779664e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 724/6000 [34:33<4:09:35,  2.84s/it] 12%|â–ˆâ–        | 725/6000 [34:36<4:09:53,  2.84s/it]                                                    {'loss': 2.7284, 'grad_norm': 17.064775466918945, 'learning_rate': 4.470338983050847e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 725/6000 [34:36<4:09:53,  2.84s/it] 12%|â–ˆâ–        | 726/6000 [34:38<4:05:20,  2.79s/it]                                                    {'loss': 2.749, 'grad_norm': 45.371360778808594, 'learning_rate': 4.469491525423729e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 726/6000 [34:38<4:05:20,  2.79s/it] 12%|â–ˆâ–        | 727/6000 [34:41<4:03:31,  2.77s/it]                                                    {'loss': 2.7732, 'grad_norm': 17.005062103271484, 'learning_rate': 4.4686440677966105e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 727/6000 [34:41<4:03:31,  2.77s/it] 12%|â–ˆâ–        | 728/6000 [34:44<4:02:01,  2.75s/it]                                                    {'loss': 2.7765, 'grad_norm': 13.855362892150879, 'learning_rate': 4.467796610169492e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 728/6000 [34:44<4:02:01,  2.75s/it] 12%|â–ˆâ–        | 729/6000 [34:47<4:01:57,  2.75s/it]                                                    {'loss': 2.7611, 'grad_norm': 15.530409812927246, 'learning_rate': 4.466949152542373e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 729/6000 [34:47<4:01:57,  2.75s/it] 12%|â–ˆâ–        | 730/6000 [34:49<4:01:54,  2.75s/it]                                                    {'loss': 2.8203, 'grad_norm': 36.11384582519531, 'learning_rate': 4.4661016949152546e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 730/6000 [34:49<4:01:54,  2.75s/it] 12%|â–ˆâ–        | 731/6000 [34:52<4:01:09,  2.75s/it]                                                    {'loss': 2.7633, 'grad_norm': 11.9879789352417, 'learning_rate': 4.465254237288135e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 731/6000 [34:52<4:01:09,  2.75s/it] 12%|â–ˆâ–        | 732/6000 [34:55<4:06:36,  2.81s/it]                                                    {'loss': 2.7739, 'grad_norm': 24.433609008789062, 'learning_rate': 4.464406779661018e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 732/6000 [34:55<4:06:36,  2.81s/it] 12%|â–ˆâ–        | 733/6000 [34:58<4:08:08,  2.83s/it]                                                    {'loss': 2.8427, 'grad_norm': 14.978140830993652, 'learning_rate': 4.463559322033899e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 733/6000 [34:58<4:08:08,  2.83s/it] 12%|â–ˆâ–        | 734/6000 [35:01<4:17:09,  2.93s/it]                                                    {'loss': 2.7326, 'grad_norm': 16.831087112426758, 'learning_rate': 4.46271186440678e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 734/6000 [35:01<4:17:09,  2.93s/it] 12%|â–ˆâ–        | 735/6000 [35:04<4:11:08,  2.86s/it]                                                    {'loss': 2.7598, 'grad_norm': 42.41432189941406, 'learning_rate': 4.461864406779661e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 735/6000 [35:04<4:11:08,  2.86s/it] 12%|â–ˆâ–        | 736/6000 [35:06<4:05:33,  2.80s/it]                                                    {'loss': 2.8052, 'grad_norm': 23.815534591674805, 'learning_rate': 4.461016949152543e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 736/6000 [35:06<4:05:33,  2.80s/it] 12%|â–ˆâ–        | 737/6000 [35:09<4:04:13,  2.78s/it]                                                    {'loss': 2.8496, 'grad_norm': 20.329431533813477, 'learning_rate': 4.460169491525424e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 737/6000 [35:09<4:04:13,  2.78s/it] 12%|â–ˆâ–        | 738/6000 [35:12<4:02:41,  2.77s/it]                                                    {'loss': 2.8031, 'grad_norm': 19.65924072265625, 'learning_rate': 4.459322033898306e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 738/6000 [35:12<4:02:41,  2.77s/it] 12%|â–ˆâ–        | 739/6000 [35:15<4:01:17,  2.75s/it]                                                    {'loss': 2.7365, 'grad_norm': 29.608858108520508, 'learning_rate': 4.458474576271187e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 739/6000 [35:15<4:01:17,  2.75s/it] 12%|â–ˆâ–        | 740/6000 [35:17<4:04:31,  2.79s/it]                                                    {'loss': 2.7285, 'grad_norm': 26.56669044494629, 'learning_rate': 4.457627118644068e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 740/6000 [35:17<4:04:31,  2.79s/it] 12%|â–ˆâ–        | 741/6000 [35:20<4:04:27,  2.79s/it]                                                    {'loss': 2.7937, 'grad_norm': 21.193572998046875, 'learning_rate': 4.456779661016949e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 741/6000 [35:20<4:04:27,  2.79s/it] 12%|â–ˆâ–        | 742/6000 [35:23<4:02:26,  2.77s/it]                                                    {'loss': 2.8149, 'grad_norm': 12.15975570678711, 'learning_rate': 4.455932203389831e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 742/6000 [35:23<4:02:26,  2.77s/it] 12%|â–ˆâ–        | 743/6000 [35:26<4:00:22,  2.74s/it]                                                    {'loss': 2.7286, 'grad_norm': 11.562406539916992, 'learning_rate': 4.4550847457627125e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 743/6000 [35:26<4:00:22,  2.74s/it] 12%|â–ˆâ–        | 744/6000 [35:28<3:57:32,  2.71s/it]                                                    {'loss': 2.7554, 'grad_norm': 17.95307731628418, 'learning_rate': 4.454237288135594e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 744/6000 [35:28<3:57:32,  2.71s/it] 12%|â–ˆâ–        | 745/6000 [35:31<3:57:42,  2.71s/it]                                                    {'loss': 2.7481, 'grad_norm': 50.0625, 'learning_rate': 4.453389830508475e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 745/6000 [35:31<3:57:42,  2.71s/it] 12%|â–ˆâ–        | 746/6000 [35:34<3:57:41,  2.71s/it]                                                    {'loss': 2.7848, 'grad_norm': 21.23841094970703, 'learning_rate': 4.452542372881356e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 746/6000 [35:34<3:57:41,  2.71s/it] 12%|â–ˆâ–        | 747/6000 [35:36<3:56:54,  2.71s/it]                                                    {'loss': 2.7478, 'grad_norm': 15.967445373535156, 'learning_rate': 4.451694915254237e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 747/6000 [35:36<3:56:54,  2.71s/it] 12%|â–ˆâ–        | 748/6000 [35:39<4:00:58,  2.75s/it]                                                    {'loss': 2.7711, 'grad_norm': 25.1334228515625, 'learning_rate': 4.450847457627119e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 748/6000 [35:39<4:00:58,  2.75s/it] 12%|â–ˆâ–        | 749/6000 [35:43<4:20:53,  2.98s/it]                                                    {'loss': 2.7535, 'grad_norm': 33.67390060424805, 'learning_rate': 4.450000000000001e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 749/6000 [35:43<4:20:53,  2.98s/it] 12%|â–ˆâ–Ž        | 750/6000 [35:46<4:23:25,  3.01s/it]                                                    {'loss': 2.7902, 'grad_norm': 14.278565406799316, 'learning_rate': 4.449152542372881e-06, 'epoch': 0.12}
 12%|â–ˆâ–Ž        | 750/6000 [35:46<4:23:25,  3.01s/it][2025-10-23 01:02:15,201] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750
[2025-10-23 01:02:15,212] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:02:15,832] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 751/6000 [35:51<5:15:03,  3.60s/it]                                                    {'loss': 2.8047, 'grad_norm': 12.908365249633789, 'learning_rate': 4.448305084745763e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 751/6000 [35:51<5:15:03,  3.60s/it] 13%|â–ˆâ–Ž        | 752/6000 [35:54<5:02:33,  3.46s/it]                                                    {'loss': 2.8488, 'grad_norm': 20.908340454101562, 'learning_rate': 4.447457627118645e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 752/6000 [35:54<5:02:33,  3.46s/it] 13%|â–ˆâ–Ž        | 753/6000 [35:57<4:45:34,  3.27s/it]                                                    {'loss': 2.8024, 'grad_norm': 13.644247055053711, 'learning_rate': 4.446610169491526e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 753/6000 [35:57<4:45:34,  3.27s/it] 13%|â–ˆâ–Ž        | 754/6000 [36:00<4:33:07,  3.12s/it]                                                    {'loss': 2.7956, 'grad_norm': 32.94960403442383, 'learning_rate': 4.445762711864407e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 754/6000 [36:00<4:33:07,  3.12s/it] 13%|â–ˆâ–Ž        | 755/6000 [36:02<4:23:15,  3.01s/it]                                                    {'loss': 2.786, 'grad_norm': 26.119911193847656, 'learning_rate': 4.444915254237289e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 755/6000 [36:02<4:23:15,  3.01s/it] 13%|â–ˆâ–Ž        | 756/6000 [36:05<4:14:43,  2.91s/it]                                                    {'loss': 2.7976, 'grad_norm': 22.56969451904297, 'learning_rate': 4.4440677966101695e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 756/6000 [36:05<4:14:43,  2.91s/it] 13%|â–ˆâ–Ž        | 757/6000 [36:08<4:15:12,  2.92s/it]                                                    {'loss': 2.7698, 'grad_norm': 15.057051658630371, 'learning_rate': 4.443220338983051e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 757/6000 [36:08<4:15:12,  2.92s/it] 13%|â–ˆâ–Ž        | 758/6000 [36:11<4:13:34,  2.90s/it]                                                    {'loss': 2.7936, 'grad_norm': 13.277602195739746, 'learning_rate': 4.442372881355933e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 758/6000 [36:11<4:13:34,  2.90s/it] 13%|â–ˆâ–Ž        | 759/6000 [36:14<4:09:30,  2.86s/it]                                                    {'loss': 2.7493, 'grad_norm': 18.482208251953125, 'learning_rate': 4.441525423728814e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 759/6000 [36:14<4:09:30,  2.86s/it] 13%|â–ˆâ–Ž        | 760/6000 [36:17<4:11:12,  2.88s/it]                                                    {'loss': 2.7841, 'grad_norm': 23.75162124633789, 'learning_rate': 4.440677966101695e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 760/6000 [36:17<4:11:12,  2.88s/it] 13%|â–ˆâ–Ž        | 761/6000 [36:19<4:10:14,  2.87s/it]                                                    {'loss': 2.8252, 'grad_norm': 24.62855339050293, 'learning_rate': 4.439830508474577e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 761/6000 [36:19<4:10:14,  2.87s/it] 13%|â–ˆâ–Ž        | 762/6000 [36:22<4:10:32,  2.87s/it]                                                    {'loss': 2.7475, 'grad_norm': 33.11167526245117, 'learning_rate': 4.438983050847458e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 762/6000 [36:22<4:10:32,  2.87s/it] 13%|â–ˆâ–Ž        | 763/6000 [36:25<4:05:56,  2.82s/it]                                                    {'loss': 2.79, 'grad_norm': 13.309586524963379, 'learning_rate': 4.438135593220339e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 763/6000 [36:25<4:05:56,  2.82s/it] 13%|â–ˆâ–Ž        | 764/6000 [36:28<4:06:24,  2.82s/it]                                                    {'loss': 2.7728, 'grad_norm': 11.459165573120117, 'learning_rate': 4.437288135593221e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 764/6000 [36:28<4:06:24,  2.82s/it] 13%|â–ˆâ–Ž        | 765/6000 [36:31<4:08:05,  2.84s/it]                                                    {'loss': 2.8206, 'grad_norm': 8.961382865905762, 'learning_rate': 4.436440677966102e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 765/6000 [36:31<4:08:05,  2.84s/it] 13%|â–ˆâ–Ž        | 766/6000 [36:33<4:07:20,  2.84s/it]                                                    {'loss': 2.7601, 'grad_norm': 11.495622634887695, 'learning_rate': 4.435593220338983e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 766/6000 [36:33<4:07:20,  2.84s/it] 13%|â–ˆâ–Ž        | 767/6000 [36:37<4:13:52,  2.91s/it]                                                    {'loss': 2.7243, 'grad_norm': 16.61652374267578, 'learning_rate': 4.434745762711864e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 767/6000 [36:37<4:13:52,  2.91s/it] 13%|â–ˆâ–Ž        | 768/6000 [36:39<4:07:26,  2.84s/it]                                                    {'loss': 2.7607, 'grad_norm': 11.145071029663086, 'learning_rate': 4.433898305084746e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 768/6000 [36:39<4:07:26,  2.84s/it] 13%|â–ˆâ–Ž        | 769/6000 [36:42<4:09:38,  2.86s/it]                                                    {'loss': 2.737, 'grad_norm': 12.420398712158203, 'learning_rate': 4.433050847457627e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 769/6000 [36:42<4:09:38,  2.86s/it] 13%|â–ˆâ–Ž        | 770/6000 [36:45<4:05:43,  2.82s/it]                                                    {'loss': 2.8279, 'grad_norm': 13.249039649963379, 'learning_rate': 4.432203389830509e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 770/6000 [36:45<4:05:43,  2.82s/it] 13%|â–ˆâ–Ž        | 771/6000 [36:48<4:02:00,  2.78s/it]                                                    {'loss': 2.7714, 'grad_norm': 11.154972076416016, 'learning_rate': 4.43135593220339e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 771/6000 [36:48<4:02:00,  2.78s/it] 13%|â–ˆâ–Ž        | 772/6000 [36:50<3:58:51,  2.74s/it]                                                    {'loss': 2.7537, 'grad_norm': 18.781654357910156, 'learning_rate': 4.4305084745762715e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 772/6000 [36:50<3:58:51,  2.74s/it] 13%|â–ˆâ–Ž        | 773/6000 [36:53<3:57:48,  2.73s/it]                                                    {'loss': 2.7715, 'grad_norm': 14.477871894836426, 'learning_rate': 4.429661016949153e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 773/6000 [36:53<3:57:48,  2.73s/it] 13%|â–ˆâ–Ž        | 774/6000 [36:56<3:57:35,  2.73s/it]                                                    {'loss': 2.7835, 'grad_norm': 9.962494850158691, 'learning_rate': 4.428813559322035e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 774/6000 [36:56<3:57:35,  2.73s/it] 13%|â–ˆâ–Ž        | 775/6000 [36:58<4:00:38,  2.76s/it]                                                    {'loss': 2.8179, 'grad_norm': 14.615589141845703, 'learning_rate': 4.4279661016949155e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 775/6000 [36:58<4:00:38,  2.76s/it] 13%|â–ˆâ–Ž        | 776/6000 [37:01<4:01:41,  2.78s/it]                                                    {'loss': 2.7928, 'grad_norm': 25.49945640563965, 'learning_rate': 4.427118644067797e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 776/6000 [37:01<4:01:41,  2.78s/it] 13%|â–ˆâ–Ž        | 777/6000 [37:04<4:05:05,  2.82s/it]                                                    {'loss': 2.7025, 'grad_norm': 22.06081771850586, 'learning_rate': 4.426271186440678e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 777/6000 [37:04<4:05:05,  2.82s/it] 13%|â–ˆâ–Ž        | 778/6000 [37:07<4:12:15,  2.90s/it]                                                    {'loss': 2.774, 'grad_norm': 9.800169944763184, 'learning_rate': 4.42542372881356e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 778/6000 [37:07<4:12:15,  2.90s/it] 13%|â–ˆâ–Ž        | 779/6000 [37:10<4:06:41,  2.83s/it]                                                    {'loss': 2.787, 'grad_norm': 9.567365646362305, 'learning_rate': 4.424576271186441e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 779/6000 [37:10<4:06:41,  2.83s/it] 13%|â–ˆâ–Ž        | 780/6000 [37:13<4:03:58,  2.80s/it]                                                    {'loss': 2.8006, 'grad_norm': 13.795906066894531, 'learning_rate': 4.423728813559323e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 780/6000 [37:13<4:03:58,  2.80s/it] 13%|â–ˆâ–Ž        | 781/6000 [37:16<4:03:56,  2.80s/it]                                                    {'loss': 2.7569, 'grad_norm': 8.523283958435059, 'learning_rate': 4.422881355932204e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 781/6000 [37:16<4:03:56,  2.80s/it] 13%|â–ˆâ–Ž        | 782/6000 [37:18<4:00:52,  2.77s/it]                                                    {'loss': 2.7572, 'grad_norm': 6.473145008087158, 'learning_rate': 4.422033898305085e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 782/6000 [37:18<4:00:52,  2.77s/it] 13%|â–ˆâ–Ž        | 783/6000 [37:21<3:59:41,  2.76s/it]                                                    {'loss': 2.7691, 'grad_norm': 10.472360610961914, 'learning_rate': 4.421186440677966e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 783/6000 [37:21<3:59:41,  2.76s/it] 13%|â–ˆâ–Ž        | 784/6000 [37:24<3:57:30,  2.73s/it]                                                    {'loss': 2.7634, 'grad_norm': 12.447806358337402, 'learning_rate': 4.420338983050848e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 784/6000 [37:24<3:57:30,  2.73s/it] 13%|â–ˆâ–Ž        | 785/6000 [37:26<3:55:22,  2.71s/it]                                                    {'loss': 2.7704, 'grad_norm': 8.461711883544922, 'learning_rate': 4.419491525423729e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 785/6000 [37:26<3:55:22,  2.71s/it] 13%|â–ˆâ–Ž        | 786/6000 [37:29<3:55:20,  2.71s/it]                                                    {'loss': 2.7565, 'grad_norm': 11.999520301818848, 'learning_rate': 4.41864406779661e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 786/6000 [37:29<3:55:20,  2.71s/it] 13%|â–ˆâ–Ž        | 787/6000 [37:32<3:56:21,  2.72s/it]                                                    {'loss': 2.843, 'grad_norm': 10.621753692626953, 'learning_rate': 4.417796610169492e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 787/6000 [37:32<3:56:21,  2.72s/it] 13%|â–ˆâ–Ž        | 788/6000 [37:34<3:56:19,  2.72s/it]                                                    {'loss': 2.7504, 'grad_norm': 10.699581146240234, 'learning_rate': 4.416949152542373e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 788/6000 [37:34<3:56:19,  2.72s/it] 13%|â–ˆâ–Ž        | 789/6000 [37:37<4:00:14,  2.77s/it]                                                    {'loss': 2.7795, 'grad_norm': 14.937397003173828, 'learning_rate': 4.416101694915255e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 789/6000 [37:37<4:00:14,  2.77s/it] 13%|â–ˆâ–Ž        | 790/6000 [37:40<3:57:50,  2.74s/it]                                                    {'loss': 2.7723, 'grad_norm': 26.001249313354492, 'learning_rate': 4.415254237288136e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 790/6000 [37:40<3:57:50,  2.74s/it] 13%|â–ˆâ–Ž        | 791/6000 [37:43<3:55:49,  2.72s/it]                                                    {'loss': 2.8283, 'grad_norm': 9.071690559387207, 'learning_rate': 4.4144067796610175e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 791/6000 [37:43<3:55:49,  2.72s/it] 13%|â–ˆâ–Ž        | 792/6000 [37:45<3:56:30,  2.72s/it]                                                    {'loss': 2.7757, 'grad_norm': 8.665919303894043, 'learning_rate': 4.413559322033898e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 792/6000 [37:45<3:56:30,  2.72s/it] 13%|â–ˆâ–Ž        | 793/6000 [37:48<3:56:03,  2.72s/it]                                                    {'loss': 2.7789, 'grad_norm': 13.100326538085938, 'learning_rate': 4.41271186440678e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 793/6000 [37:48<3:56:03,  2.72s/it] 13%|â–ˆâ–Ž        | 794/6000 [37:51<3:58:43,  2.75s/it]                                                    {'loss': 2.7946, 'grad_norm': 22.107547760009766, 'learning_rate': 4.4118644067796615e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 794/6000 [37:51<3:58:43,  2.75s/it] 13%|â–ˆâ–Ž        | 795/6000 [37:54<4:07:29,  2.85s/it]                                                    {'loss': 2.7706, 'grad_norm': 19.760337829589844, 'learning_rate': 4.411016949152543e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 795/6000 [37:54<4:07:29,  2.85s/it] 13%|â–ˆâ–Ž        | 796/6000 [37:57<4:03:30,  2.81s/it]                                                    {'loss': 2.7528, 'grad_norm': 11.997931480407715, 'learning_rate': 4.410169491525424e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 796/6000 [37:57<4:03:30,  2.81s/it] 13%|â–ˆâ–Ž        | 797/6000 [37:59<4:01:20,  2.78s/it]                                                    {'loss': 2.7531, 'grad_norm': 19.800098419189453, 'learning_rate': 4.409322033898306e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 797/6000 [37:59<4:01:20,  2.78s/it] 13%|â–ˆâ–Ž        | 798/6000 [38:02<3:57:34,  2.74s/it]                                                    {'loss': 2.7858, 'grad_norm': 14.870820999145508, 'learning_rate': 4.408474576271186e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 798/6000 [38:02<3:57:34,  2.74s/it] 13%|â–ˆâ–Ž        | 799/6000 [38:05<3:58:41,  2.75s/it]                                                    {'loss': 2.7844, 'grad_norm': 10.563982963562012, 'learning_rate': 4.407627118644068e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 799/6000 [38:05<3:58:41,  2.75s/it] 13%|â–ˆâ–Ž        | 800/6000 [38:08<3:58:42,  2.75s/it]                                                    {'loss': 2.7597, 'grad_norm': 22.782926559448242, 'learning_rate': 4.40677966101695e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 800/6000 [38:08<3:58:42,  2.75s/it][2025-10-23 01:04:36,925] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800
[2025-10-23 01:04:36,935] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:04:37,535] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 801/6000 [38:13<4:56:19,  3.42s/it]                                                    {'loss': 2.749, 'grad_norm': 15.034092903137207, 'learning_rate': 4.405932203389831e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 801/6000 [38:13<4:56:19,  3.42s/it] 13%|â–ˆâ–Ž        | 802/6000 [38:15<4:39:45,  3.23s/it]                                                    {'loss': 2.8253, 'grad_norm': 22.5010986328125, 'learning_rate': 4.405084745762712e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 802/6000 [38:15<4:39:45,  3.23s/it] 13%|â–ˆâ–Ž        | 803/6000 [38:18<4:26:53,  3.08s/it]                                                    {'loss': 2.7428, 'grad_norm': 24.812185287475586, 'learning_rate': 4.404237288135594e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 803/6000 [38:18<4:26:53,  3.08s/it] 13%|â–ˆâ–Ž        | 804/6000 [38:21<4:17:17,  2.97s/it]                                                    {'loss': 2.7428, 'grad_norm': 23.991731643676758, 'learning_rate': 4.4033898305084745e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 804/6000 [38:21<4:17:17,  2.97s/it] 13%|â–ˆâ–Ž        | 805/6000 [38:24<4:09:45,  2.88s/it]                                                    {'loss': 2.8255, 'grad_norm': 30.441116333007812, 'learning_rate': 4.402542372881356e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 805/6000 [38:24<4:09:45,  2.88s/it] 13%|â–ˆâ–Ž        | 806/6000 [38:26<4:06:40,  2.85s/it]                                                    {'loss': 2.8259, 'grad_norm': 18.076066970825195, 'learning_rate': 4.401694915254238e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 806/6000 [38:26<4:06:40,  2.85s/it] 13%|â–ˆâ–Ž        | 807/6000 [38:29<4:02:24,  2.80s/it]                                                    {'loss': 2.7688, 'grad_norm': 36.91884994506836, 'learning_rate': 4.400847457627119e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 807/6000 [38:29<4:02:24,  2.80s/it] 13%|â–ˆâ–Ž        | 808/6000 [38:32<4:00:40,  2.78s/it]                                                    {'loss': 2.7947, 'grad_norm': 8.13835334777832, 'learning_rate': 4.4e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 808/6000 [38:32<4:00:40,  2.78s/it] 13%|â–ˆâ–Ž        | 809/6000 [38:34<3:58:28,  2.76s/it]                                                    {'loss': 2.7778, 'grad_norm': 16.87904167175293, 'learning_rate': 4.399152542372882e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 809/6000 [38:34<3:58:28,  2.76s/it] 14%|â–ˆâ–Ž        | 810/6000 [38:37<4:02:44,  2.81s/it]                                                    {'loss': 2.7731, 'grad_norm': 14.158507347106934, 'learning_rate': 4.3983050847457635e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 810/6000 [38:37<4:02:44,  2.81s/it] 14%|â–ˆâ–Ž        | 811/6000 [38:40<4:00:49,  2.78s/it]                                                    {'loss': 2.7725, 'grad_norm': 16.104183197021484, 'learning_rate': 4.397457627118644e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 811/6000 [38:40<4:00:49,  2.78s/it] 14%|â–ˆâ–Ž        | 812/6000 [38:43<3:58:25,  2.76s/it]                                                    {'loss': 2.8245, 'grad_norm': 20.204544067382812, 'learning_rate': 4.396610169491526e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 812/6000 [38:43<3:58:25,  2.76s/it] 14%|â–ˆâ–Ž        | 813/6000 [38:46<4:15:46,  2.96s/it]                                                    {'loss': 2.7658, 'grad_norm': 16.54338264465332, 'learning_rate': 4.395762711864407e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 813/6000 [38:46<4:15:46,  2.96s/it] 14%|â–ˆâ–Ž        | 814/6000 [38:49<4:08:36,  2.88s/it]                                                    {'loss': 2.7909, 'grad_norm': 24.420869827270508, 'learning_rate': 4.394915254237288e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 814/6000 [38:49<4:08:36,  2.88s/it] 14%|â–ˆâ–Ž        | 815/6000 [38:51<4:02:15,  2.80s/it]                                                    {'loss': 2.8068, 'grad_norm': 27.412141799926758, 'learning_rate': 4.39406779661017e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 815/6000 [38:51<4:02:15,  2.80s/it] 14%|â–ˆâ–Ž        | 816/6000 [38:54<3:58:49,  2.76s/it]                                                    {'loss': 2.7793, 'grad_norm': 8.432846069335938, 'learning_rate': 4.393220338983052e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 816/6000 [38:54<3:58:49,  2.76s/it] 14%|â–ˆâ–Ž        | 817/6000 [38:58<4:16:40,  2.97s/it]                                                    {'loss': 2.7764, 'grad_norm': 22.289066314697266, 'learning_rate': 4.392372881355932e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 817/6000 [38:58<4:16:40,  2.97s/it] 14%|â–ˆâ–Ž        | 818/6000 [39:00<4:10:19,  2.90s/it]                                                    {'loss': 2.7628, 'grad_norm': 14.263528823852539, 'learning_rate': 4.391525423728814e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 818/6000 [39:00<4:10:19,  2.90s/it] 14%|â–ˆâ–Ž        | 819/6000 [39:03<4:04:33,  2.83s/it]                                                    {'loss': 2.7236, 'grad_norm': 30.174955368041992, 'learning_rate': 4.390677966101695e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 819/6000 [39:03<4:04:33,  2.83s/it] 14%|â–ˆâ–Ž        | 820/6000 [39:06<4:02:36,  2.81s/it]                                                    {'loss': 2.7612, 'grad_norm': 12.31113052368164, 'learning_rate': 4.3898305084745765e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 820/6000 [39:06<4:02:36,  2.81s/it] 14%|â–ˆâ–Ž        | 821/6000 [39:09<4:02:30,  2.81s/it]                                                    {'loss': 2.7958, 'grad_norm': 14.550305366516113, 'learning_rate': 4.388983050847458e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 821/6000 [39:09<4:02:30,  2.81s/it] 14%|â–ˆâ–Ž        | 822/6000 [39:11<4:00:07,  2.78s/it]                                                    {'loss': 2.7725, 'grad_norm': 15.633511543273926, 'learning_rate': 4.38813559322034e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 822/6000 [39:11<4:00:07,  2.78s/it] 14%|â–ˆâ–Ž        | 823/6000 [39:14<3:58:24,  2.76s/it]                                                    {'loss': 2.7776, 'grad_norm': 9.864579200744629, 'learning_rate': 4.3872881355932205e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 823/6000 [39:14<3:58:24,  2.76s/it] 14%|â–ˆâ–Ž        | 824/6000 [39:17<3:56:47,  2.74s/it]                                                    {'loss': 2.7765, 'grad_norm': 12.001590728759766, 'learning_rate': 4.386440677966102e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 824/6000 [39:17<3:56:47,  2.74s/it] 14%|â–ˆâ–        | 825/6000 [39:20<4:02:46,  2.81s/it]                                                    {'loss': 2.8466, 'grad_norm': 10.175360679626465, 'learning_rate': 4.385593220338983e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 825/6000 [39:20<4:02:46,  2.81s/it] 14%|â–ˆâ–        | 826/6000 [39:22<4:00:28,  2.79s/it]                                                    {'loss': 2.7575, 'grad_norm': 20.86540412902832, 'learning_rate': 4.384745762711865e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 826/6000 [39:22<4:00:28,  2.79s/it] 14%|â–ˆâ–        | 827/6000 [39:25<3:59:40,  2.78s/it]                                                    {'loss': 2.794, 'grad_norm': 13.07050895690918, 'learning_rate': 4.383898305084746e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 827/6000 [39:25<3:59:40,  2.78s/it] 14%|â–ˆâ–        | 828/6000 [39:28<3:59:30,  2.78s/it]                                                    {'loss': 2.7975, 'grad_norm': 11.012345314025879, 'learning_rate': 4.383050847457627e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 828/6000 [39:28<3:59:30,  2.78s/it] 14%|â–ˆâ–        | 829/6000 [39:31<3:57:59,  2.76s/it]                                                    {'loss': 2.8107, 'grad_norm': 14.319787979125977, 'learning_rate': 4.382203389830509e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 829/6000 [39:31<3:57:59,  2.76s/it] 14%|â–ˆâ–        | 830/6000 [39:34<4:19:12,  3.01s/it]                                                    {'loss': 2.8286, 'grad_norm': 19.27587890625, 'learning_rate': 4.38135593220339e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 830/6000 [39:34<4:19:12,  3.01s/it] 14%|â–ˆâ–        | 831/6000 [39:37<4:14:33,  2.95s/it]                                                    {'loss': 2.8239, 'grad_norm': 22.125019073486328, 'learning_rate': 4.380508474576272e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 831/6000 [39:37<4:14:33,  2.95s/it] 14%|â–ˆâ–        | 832/6000 [39:40<4:06:51,  2.87s/it]                                                    {'loss': 2.7756, 'grad_norm': 15.530987739562988, 'learning_rate': 4.379661016949153e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 832/6000 [39:40<4:06:51,  2.87s/it] 14%|â–ˆâ–        | 833/6000 [39:42<4:02:54,  2.82s/it]                                                    {'loss': 2.7288, 'grad_norm': 32.657657623291016, 'learning_rate': 4.378813559322034e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 833/6000 [39:42<4:02:54,  2.82s/it] 14%|â–ˆâ–        | 834/6000 [39:45<3:59:16,  2.78s/it]                                                    {'loss': 2.7712, 'grad_norm': 11.877036094665527, 'learning_rate': 4.377966101694915e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 834/6000 [39:45<3:59:16,  2.78s/it] 14%|â–ˆâ–        | 835/6000 [39:48<4:08:47,  2.89s/it]                                                    {'loss': 2.7894, 'grad_norm': 15.806997299194336, 'learning_rate': 4.377118644067797e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 835/6000 [39:48<4:08:47,  2.89s/it] 14%|â–ˆâ–        | 836/6000 [39:51<4:02:14,  2.81s/it]                                                    {'loss': 2.757, 'grad_norm': 12.492788314819336, 'learning_rate': 4.3762711864406784e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 836/6000 [39:51<4:02:14,  2.81s/it] 14%|â–ˆâ–        | 837/6000 [39:55<4:21:37,  3.04s/it]                                                    {'loss': 2.7269, 'grad_norm': 16.25204849243164, 'learning_rate': 4.37542372881356e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 837/6000 [39:55<4:21:37,  3.04s/it] 14%|â–ˆâ–        | 838/6000 [39:58<4:23:27,  3.06s/it]                                                    {'loss': 2.7369, 'grad_norm': 17.283828735351562, 'learning_rate': 4.374576271186441e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 838/6000 [39:58<4:23:27,  3.06s/it] 14%|â–ˆâ–        | 839/6000 [40:00<4:14:48,  2.96s/it]                                                    {'loss': 2.7799, 'grad_norm': 8.322321891784668, 'learning_rate': 4.3737288135593225e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 839/6000 [40:00<4:14:48,  2.96s/it] 14%|â–ˆâ–        | 840/6000 [40:03<4:10:21,  2.91s/it]                                                    {'loss': 2.7376, 'grad_norm': 19.322769165039062, 'learning_rate': 4.372881355932203e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 840/6000 [40:03<4:10:21,  2.91s/it] 14%|â–ˆâ–        | 841/6000 [40:06<4:17:04,  2.99s/it]                                                    {'loss': 2.766, 'grad_norm': 10.034132957458496, 'learning_rate': 4.372033898305085e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 841/6000 [40:06<4:17:04,  2.99s/it] 14%|â–ˆâ–        | 842/6000 [40:09<4:10:20,  2.91s/it]                                                    {'loss': 2.7471, 'grad_norm': 10.042150497436523, 'learning_rate': 4.3711864406779666e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 842/6000 [40:09<4:10:20,  2.91s/it] 14%|â–ˆâ–        | 843/6000 [40:12<4:04:49,  2.85s/it]                                                    {'loss': 2.7874, 'grad_norm': 11.67403793334961, 'learning_rate': 4.370338983050848e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 843/6000 [40:12<4:04:49,  2.85s/it] 14%|â–ˆâ–        | 844/6000 [40:15<4:06:54,  2.87s/it]                                                    {'loss': 2.8267, 'grad_norm': 26.496553421020508, 'learning_rate': 4.369491525423729e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 844/6000 [40:15<4:06:54,  2.87s/it] 14%|â–ˆâ–        | 845/6000 [40:17<4:02:12,  2.82s/it]                                                    {'loss': 2.8566, 'grad_norm': 7.7875189781188965, 'learning_rate': 4.368644067796611e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 845/6000 [40:17<4:02:12,  2.82s/it] 14%|â–ˆâ–        | 846/6000 [40:20<4:02:40,  2.83s/it]                                                    {'loss': 2.7312, 'grad_norm': 9.750309944152832, 'learning_rate': 4.367796610169492e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 846/6000 [40:20<4:02:40,  2.83s/it] 14%|â–ˆâ–        | 847/6000 [40:23<4:05:59,  2.86s/it]                                                    {'loss': 2.7042, 'grad_norm': 28.505098342895508, 'learning_rate': 4.366949152542374e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 847/6000 [40:23<4:05:59,  2.86s/it] 14%|â–ˆâ–        | 848/6000 [40:26<4:11:37,  2.93s/it]                                                    {'loss': 2.7756, 'grad_norm': 10.897415161132812, 'learning_rate': 4.366101694915255e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 848/6000 [40:26<4:11:37,  2.93s/it] 14%|â–ˆâ–        | 849/6000 [40:29<4:09:15,  2.90s/it]                                                    {'loss': 2.7659, 'grad_norm': 10.945359230041504, 'learning_rate': 4.3652542372881355e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 849/6000 [40:29<4:09:15,  2.90s/it] 14%|â–ˆâ–        | 850/6000 [40:32<4:04:14,  2.85s/it]                                                    {'loss': 2.7994, 'grad_norm': 11.687378883361816, 'learning_rate': 4.364406779661017e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 850/6000 [40:32<4:04:14,  2.85s/it][2025-10-23 01:07:01,121] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850
[2025-10-23 01:07:01,136] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:07:01,762] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 14%|â–ˆâ–        | 851/6000 [40:37<4:56:06,  3.45s/it]                                                    {'loss': 2.7897, 'grad_norm': 11.216644287109375, 'learning_rate': 4.363559322033899e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 851/6000 [40:37<4:56:06,  3.45s/it] 14%|â–ˆâ–        | 852/6000 [40:39<4:36:41,  3.22s/it]                                                    {'loss': 2.7633, 'grad_norm': 13.909370422363281, 'learning_rate': 4.36271186440678e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 852/6000 [40:39<4:36:41,  3.22s/it] 14%|â–ˆâ–        | 853/6000 [40:42<4:24:03,  3.08s/it]                                                    {'loss': 2.8157, 'grad_norm': 17.866233825683594, 'learning_rate': 4.361864406779661e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 853/6000 [40:42<4:24:03,  3.08s/it] 14%|â–ˆâ–        | 854/6000 [40:45<4:16:31,  2.99s/it]                                                    {'loss': 2.8087, 'grad_norm': 15.048924446105957, 'learning_rate': 4.361016949152543e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 854/6000 [40:45<4:16:31,  2.99s/it] 14%|â–ˆâ–        | 855/6000 [40:48<4:10:55,  2.93s/it]                                                    {'loss': 2.8305, 'grad_norm': 8.922679901123047, 'learning_rate': 4.360169491525424e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 855/6000 [40:48<4:10:55,  2.93s/it] 14%|â–ˆâ–        | 856/6000 [40:50<4:07:49,  2.89s/it]                                                    {'loss': 2.8234, 'grad_norm': 8.670919418334961, 'learning_rate': 4.359322033898305e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 856/6000 [40:51<4:07:49,  2.89s/it] 14%|â–ˆâ–        | 857/6000 [40:53<4:05:01,  2.86s/it]                                                    {'loss': 2.7769, 'grad_norm': 11.853813171386719, 'learning_rate': 4.358474576271187e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 857/6000 [40:53<4:05:01,  2.86s/it] 14%|â–ˆâ–        | 858/6000 [40:56<4:01:06,  2.81s/it]                                                    {'loss': 2.7347, 'grad_norm': 9.794548988342285, 'learning_rate': 4.3576271186440685e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 858/6000 [40:56<4:01:06,  2.81s/it] 14%|â–ˆâ–        | 859/6000 [40:59<3:57:15,  2.77s/it]                                                    {'loss': 2.7259, 'grad_norm': 21.798404693603516, 'learning_rate': 4.356779661016949e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 859/6000 [40:59<3:57:15,  2.77s/it] 14%|â–ˆâ–        | 860/6000 [41:02<4:06:53,  2.88s/it]                                                    {'loss': 2.8027, 'grad_norm': 24.984128952026367, 'learning_rate': 4.355932203389831e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 860/6000 [41:02<4:06:53,  2.88s/it] 14%|â–ˆâ–        | 861/6000 [41:04<4:01:18,  2.82s/it]                                                    {'loss': 2.8077, 'grad_norm': 14.042811393737793, 'learning_rate': 4.355084745762712e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 861/6000 [41:04<4:01:18,  2.82s/it] 14%|â–ˆâ–        | 862/6000 [41:07<3:58:46,  2.79s/it]                                                    {'loss': 2.8056, 'grad_norm': 23.14588165283203, 'learning_rate': 4.354237288135593e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 862/6000 [41:07<3:58:46,  2.79s/it] 14%|â–ˆâ–        | 863/6000 [41:10<3:58:36,  2.79s/it]                                                    {'loss': 2.7823, 'grad_norm': 14.304097175598145, 'learning_rate': 4.353389830508475e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 863/6000 [41:10<3:58:36,  2.79s/it] 14%|â–ˆâ–        | 864/6000 [41:13<3:55:58,  2.76s/it]                                                    {'loss': 2.7474, 'grad_norm': 15.726058959960938, 'learning_rate': 4.352542372881357e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 864/6000 [41:13<3:55:58,  2.76s/it] 14%|â–ˆâ–        | 865/6000 [41:16<4:04:14,  2.85s/it]                                                    {'loss': 2.7021, 'grad_norm': 20.43364906311035, 'learning_rate': 4.3516949152542374e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 865/6000 [41:16<4:04:14,  2.85s/it] 14%|â–ˆâ–        | 866/6000 [41:18<4:01:19,  2.82s/it]                                                    {'loss': 2.764, 'grad_norm': 17.467418670654297, 'learning_rate': 4.350847457627119e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 866/6000 [41:18<4:01:19,  2.82s/it] 14%|â–ˆâ–        | 867/6000 [41:21<4:00:46,  2.81s/it]                                                    {'loss': 2.7892, 'grad_norm': 18.699806213378906, 'learning_rate': 4.350000000000001e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 867/6000 [41:21<4:00:46,  2.81s/it] 14%|â–ˆâ–        | 868/6000 [41:24<3:56:18,  2.76s/it]                                                    {'loss': 2.7827, 'grad_norm': 8.398886680603027, 'learning_rate': 4.349152542372882e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 868/6000 [41:24<3:56:18,  2.76s/it] 14%|â–ˆâ–        | 869/6000 [41:27<3:54:59,  2.75s/it]                                                    {'loss': 2.7132, 'grad_norm': 30.923303604125977, 'learning_rate': 4.348305084745763e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 869/6000 [41:27<3:54:59,  2.75s/it] 14%|â–ˆâ–        | 870/6000 [41:29<3:53:44,  2.73s/it]                                                    {'loss': 2.7381, 'grad_norm': 15.218750953674316, 'learning_rate': 4.347457627118644e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 870/6000 [41:29<3:53:44,  2.73s/it] 15%|â–ˆâ–        | 871/6000 [41:32<3:55:18,  2.75s/it]                                                    {'loss': 2.8137, 'grad_norm': 21.328371047973633, 'learning_rate': 4.3466101694915256e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 871/6000 [41:32<3:55:18,  2.75s/it] 15%|â–ˆâ–        | 872/6000 [41:35<3:56:02,  2.76s/it]                                                    {'loss': 2.7982, 'grad_norm': 18.722057342529297, 'learning_rate': 4.345762711864407e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 872/6000 [41:35<3:56:02,  2.76s/it] 15%|â–ˆâ–        | 873/6000 [41:38<3:56:25,  2.77s/it]                                                    {'loss': 2.7286, 'grad_norm': 20.370744705200195, 'learning_rate': 4.344915254237289e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 873/6000 [41:38<3:56:25,  2.77s/it] 15%|â–ˆâ–        | 874/6000 [41:40<3:54:35,  2.75s/it]                                                    {'loss': 2.9441, 'grad_norm': 22.89667510986328, 'learning_rate': 4.34406779661017e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 874/6000 [41:40<3:54:35,  2.75s/it] 15%|â–ˆâ–        | 875/6000 [41:43<4:00:38,  2.82s/it]                                                    {'loss': 2.7821, 'grad_norm': 29.062786102294922, 'learning_rate': 4.343220338983051e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 875/6000 [41:43<4:00:38,  2.82s/it] 15%|â–ˆâ–        | 876/6000 [41:46<3:57:51,  2.79s/it]                                                    {'loss': 2.8025, 'grad_norm': 22.689672470092773, 'learning_rate': 4.342372881355932e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 876/6000 [41:46<3:57:51,  2.79s/it] 15%|â–ˆâ–        | 877/6000 [41:49<3:57:37,  2.78s/it]                                                    {'loss': 2.7854, 'grad_norm': 13.296908378601074, 'learning_rate': 4.341525423728814e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 877/6000 [41:49<3:57:37,  2.78s/it] 15%|â–ˆâ–        | 878/6000 [41:52<3:57:17,  2.78s/it]                                                    {'loss': 2.7516, 'grad_norm': 10.41353702545166, 'learning_rate': 4.340677966101695e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 878/6000 [41:52<3:57:17,  2.78s/it] 15%|â–ˆâ–        | 879/6000 [41:55<4:12:21,  2.96s/it]                                                    {'loss': 2.7314, 'grad_norm': 17.734237670898438, 'learning_rate': 4.339830508474577e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 879/6000 [41:55<4:12:21,  2.96s/it] 15%|â–ˆâ–        | 880/6000 [41:58<4:05:08,  2.87s/it]                                                    {'loss': 2.7457, 'grad_norm': 11.436969757080078, 'learning_rate': 4.338983050847458e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 880/6000 [41:58<4:05:08,  2.87s/it] 15%|â–ˆâ–        | 881/6000 [42:00<4:01:01,  2.83s/it]                                                    {'loss': 2.7705, 'grad_norm': 12.12775707244873, 'learning_rate': 4.338135593220339e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 881/6000 [42:00<4:01:01,  2.83s/it] 15%|â–ˆâ–        | 882/6000 [42:03<3:59:06,  2.80s/it]                                                    {'loss': 2.7919, 'grad_norm': 18.60652732849121, 'learning_rate': 4.33728813559322e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 882/6000 [42:03<3:59:06,  2.80s/it] 15%|â–ˆâ–        | 883/6000 [42:06<3:58:13,  2.79s/it]                                                    {'loss': 2.815, 'grad_norm': 20.653179168701172, 'learning_rate': 4.336440677966103e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 883/6000 [42:06<3:58:13,  2.79s/it] 15%|â–ˆâ–        | 884/6000 [42:09<4:02:52,  2.85s/it]                                                    {'loss': 2.7073, 'grad_norm': 19.29924201965332, 'learning_rate': 4.3355932203389834e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 884/6000 [42:09<4:02:52,  2.85s/it] 15%|â–ˆâ–        | 885/6000 [42:12<4:01:18,  2.83s/it]                                                    {'loss': 2.7814, 'grad_norm': 15.292818069458008, 'learning_rate': 4.334745762711865e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 885/6000 [42:12<4:01:18,  2.83s/it] 15%|â–ˆâ–        | 886/6000 [42:15<4:01:20,  2.83s/it]                                                    {'loss': 2.7208, 'grad_norm': 15.849032402038574, 'learning_rate': 4.333898305084746e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 886/6000 [42:15<4:01:20,  2.83s/it] 15%|â–ˆâ–        | 887/6000 [42:17<4:00:30,  2.82s/it]                                                    {'loss': 2.6934, 'grad_norm': 19.290630340576172, 'learning_rate': 4.3330508474576275e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 887/6000 [42:17<4:00:30,  2.82s/it] 15%|â–ˆâ–        | 888/6000 [42:20<4:00:11,  2.82s/it]                                                    {'loss': 2.7782, 'grad_norm': 21.275863647460938, 'learning_rate': 4.332203389830509e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 888/6000 [42:20<4:00:11,  2.82s/it] 15%|â–ˆâ–        | 889/6000 [42:23<3:56:49,  2.78s/it]                                                    {'loss': 2.7733, 'grad_norm': 12.44589614868164, 'learning_rate': 4.331355932203391e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 889/6000 [42:23<3:56:49,  2.78s/it] 15%|â–ˆâ–        | 890/6000 [42:26<3:56:10,  2.77s/it]                                                    {'loss': 2.7834, 'grad_norm': 30.694358825683594, 'learning_rate': 4.3305084745762716e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 890/6000 [42:26<3:56:10,  2.77s/it] 15%|â–ˆâ–        | 891/6000 [42:28<3:54:21,  2.75s/it]                                                    {'loss': 2.7991, 'grad_norm': 24.820289611816406, 'learning_rate': 4.329661016949152e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 891/6000 [42:28<3:54:21,  2.75s/it] 15%|â–ˆâ–        | 892/6000 [42:31<3:53:56,  2.75s/it]                                                    {'loss': 2.7587, 'grad_norm': 14.845244407653809, 'learning_rate': 4.328813559322034e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 892/6000 [42:31<3:53:56,  2.75s/it] 15%|â–ˆâ–        | 893/6000 [42:34<3:52:37,  2.73s/it]                                                    {'loss': 2.8256, 'grad_norm': 18.051185607910156, 'learning_rate': 4.327966101694916e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 893/6000 [42:34<3:52:37,  2.73s/it] 15%|â–ˆâ–        | 894/6000 [42:36<3:53:35,  2.74s/it]                                                    {'loss': 2.7816, 'grad_norm': 13.456531524658203, 'learning_rate': 4.327118644067797e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 894/6000 [42:36<3:53:35,  2.74s/it] 15%|â–ˆâ–        | 895/6000 [42:39<3:52:41,  2.73s/it]                                                    {'loss': 2.7652, 'grad_norm': 17.222850799560547, 'learning_rate': 4.326271186440678e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 895/6000 [42:39<3:52:41,  2.73s/it] 15%|â–ˆâ–        | 896/6000 [42:42<3:51:12,  2.72s/it]                                                    {'loss': 2.7496, 'grad_norm': 30.257596969604492, 'learning_rate': 4.32542372881356e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 896/6000 [42:42<3:51:12,  2.72s/it] 15%|â–ˆâ–        | 897/6000 [42:45<3:52:29,  2.73s/it]                                                    {'loss': 2.7218, 'grad_norm': 31.735570907592773, 'learning_rate': 4.3245762711864405e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 897/6000 [42:45<3:52:29,  2.73s/it] 15%|â–ˆâ–        | 898/6000 [42:47<3:51:58,  2.73s/it]                                                    {'loss': 2.7554, 'grad_norm': 24.174896240234375, 'learning_rate': 4.323728813559322e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 898/6000 [42:47<3:51:58,  2.73s/it] 15%|â–ˆâ–        | 899/6000 [42:51<4:13:57,  2.99s/it]                                                    {'loss': 2.7236, 'grad_norm': 13.100532531738281, 'learning_rate': 4.322881355932204e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 899/6000 [42:51<4:13:57,  2.99s/it] 15%|â–ˆâ–Œ        | 900/6000 [42:54<4:07:32,  2.91s/it]                                                    {'loss': 2.8606, 'grad_norm': 17.85236167907715, 'learning_rate': 4.322033898305085e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 900/6000 [42:54<4:07:32,  2.91s/it][2025-10-23 01:09:22,977] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-900
[2025-10-23 01:09:22,987] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:09:23,609] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-900/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 15%|â–ˆâ–Œ        | 901/6000 [42:59<5:07:29,  3.62s/it]                                                    {'loss': 2.7175, 'grad_norm': 12.833048820495605, 'learning_rate': 4.321186440677966e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 901/6000 [42:59<5:07:29,  3.62s/it] 15%|â–ˆâ–Œ        | 902/6000 [43:02<4:44:18,  3.35s/it]                                                    {'loss': 2.7705, 'grad_norm': 8.653197288513184, 'learning_rate': 4.320338983050848e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 902/6000 [43:02<4:44:18,  3.35s/it] 15%|â–ˆâ–Œ        | 903/6000 [43:04<4:28:35,  3.16s/it]                                                    {'loss': 2.7358, 'grad_norm': 15.759943008422852, 'learning_rate': 4.3194915254237295e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 903/6000 [43:04<4:28:35,  3.16s/it] 15%|â–ˆâ–Œ        | 904/6000 [43:07<4:16:36,  3.02s/it]                                                    {'loss': 2.8778, 'grad_norm': 19.53588104248047, 'learning_rate': 4.318644067796611e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 904/6000 [43:07<4:16:36,  3.02s/it] 15%|â–ˆâ–Œ        | 905/6000 [43:10<4:24:20,  3.11s/it]                                                    {'loss': 2.765, 'grad_norm': 17.516183853149414, 'learning_rate': 4.317796610169492e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 905/6000 [43:10<4:24:20,  3.11s/it] 15%|â–ˆâ–Œ        | 906/6000 [43:13<4:19:30,  3.06s/it]                                                    {'loss': 2.7484, 'grad_norm': 27.954774856567383, 'learning_rate': 4.3169491525423735e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 906/6000 [43:13<4:19:30,  3.06s/it] 15%|â–ˆâ–Œ        | 907/6000 [43:16<4:21:09,  3.08s/it]                                                    {'loss': 2.8498, 'grad_norm': 15.928007125854492, 'learning_rate': 4.316101694915254e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 907/6000 [43:16<4:21:09,  3.08s/it] 15%|â–ˆâ–Œ        | 908/6000 [43:20<4:22:12,  3.09s/it]                                                    {'loss': 2.8035, 'grad_norm': 16.664827346801758, 'learning_rate': 4.315254237288136e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 908/6000 [43:20<4:22:12,  3.09s/it] 15%|â–ˆâ–Œ        | 909/6000 [43:22<4:12:24,  2.97s/it]                                                    {'loss': 2.7504, 'grad_norm': 21.192995071411133, 'learning_rate': 4.314406779661018e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 909/6000 [43:22<4:12:24,  2.97s/it] 15%|â–ˆâ–Œ        | 910/6000 [43:25<4:05:39,  2.90s/it]                                                    {'loss': 2.7357, 'grad_norm': 23.710182189941406, 'learning_rate': 4.313559322033899e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 910/6000 [43:25<4:05:39,  2.90s/it] 15%|â–ˆâ–Œ        | 911/6000 [43:28<4:02:41,  2.86s/it]                                                    {'loss': 2.7236, 'grad_norm': 17.288036346435547, 'learning_rate': 4.31271186440678e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 911/6000 [43:28<4:02:41,  2.86s/it] 15%|â–ˆâ–Œ        | 912/6000 [43:30<3:58:31,  2.81s/it]                                                    {'loss': 2.7739, 'grad_norm': 11.026533126831055, 'learning_rate': 4.311864406779661e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 912/6000 [43:30<3:58:31,  2.81s/it] 15%|â–ˆâ–Œ        | 913/6000 [43:33<3:55:00,  2.77s/it]                                                    {'loss': 2.7929, 'grad_norm': 11.44228458404541, 'learning_rate': 4.3110169491525424e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 913/6000 [43:33<3:55:00,  2.77s/it] 15%|â–ˆâ–Œ        | 914/6000 [43:36<4:04:07,  2.88s/it]                                                    {'loss': 2.8273, 'grad_norm': 16.663484573364258, 'learning_rate': 4.310169491525424e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 914/6000 [43:36<4:04:07,  2.88s/it] 15%|â–ˆâ–Œ        | 915/6000 [43:40<4:24:01,  3.12s/it]                                                    {'loss': 2.7924, 'grad_norm': 15.172625541687012, 'learning_rate': 4.309322033898306e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 915/6000 [43:40<4:24:01,  3.12s/it] 15%|â–ˆâ–Œ        | 916/6000 [43:43<4:15:01,  3.01s/it]                                                    {'loss': 2.8194, 'grad_norm': 25.364582061767578, 'learning_rate': 4.3084745762711865e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 916/6000 [43:43<4:15:01,  3.01s/it] 15%|â–ˆâ–Œ        | 917/6000 [43:45<4:06:29,  2.91s/it]                                                    {'loss': 2.7448, 'grad_norm': 14.690587043762207, 'learning_rate': 4.307627118644068e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 917/6000 [43:45<4:06:29,  2.91s/it] 15%|â–ˆâ–Œ        | 918/6000 [43:48<4:01:37,  2.85s/it]                                                    {'loss': 2.8049, 'grad_norm': 27.217723846435547, 'learning_rate': 4.306779661016949e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 918/6000 [43:48<4:01:37,  2.85s/it] 15%|â–ˆâ–Œ        | 919/6000 [43:51<4:00:19,  2.84s/it]                                                    {'loss': 2.8256, 'grad_norm': 29.993282318115234, 'learning_rate': 4.3059322033898306e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 919/6000 [43:51<4:00:19,  2.84s/it] 15%|â–ˆâ–Œ        | 920/6000 [43:54<3:57:32,  2.81s/it]                                                    {'loss': 2.7851, 'grad_norm': 19.391719818115234, 'learning_rate': 4.305084745762712e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 920/6000 [43:54<3:57:32,  2.81s/it] 15%|â–ˆâ–Œ        | 921/6000 [43:56<3:55:26,  2.78s/it]                                                    {'loss': 2.7686, 'grad_norm': 19.03629493713379, 'learning_rate': 4.304237288135594e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 921/6000 [43:56<3:55:26,  2.78s/it] 15%|â–ˆâ–Œ        | 922/6000 [43:59<3:58:37,  2.82s/it]                                                    {'loss': 2.7243, 'grad_norm': 23.238502502441406, 'learning_rate': 4.303389830508475e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 922/6000 [43:59<3:58:37,  2.82s/it] 15%|â–ˆâ–Œ        | 923/6000 [44:02<3:57:56,  2.81s/it]                                                    {'loss': 2.8078, 'grad_norm': 16.288633346557617, 'learning_rate': 4.302542372881356e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 923/6000 [44:02<3:57:56,  2.81s/it] 15%|â–ˆâ–Œ        | 924/6000 [44:05<3:54:02,  2.77s/it]                                                    {'loss': 2.8052, 'grad_norm': 17.554433822631836, 'learning_rate': 4.301694915254238e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 924/6000 [44:05<3:54:02,  2.77s/it] 15%|â–ˆâ–Œ        | 925/6000 [44:07<3:52:59,  2.75s/it]                                                    {'loss': 2.7785, 'grad_norm': 27.86890411376953, 'learning_rate': 4.3008474576271195e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 925/6000 [44:07<3:52:59,  2.75s/it] 15%|â–ˆâ–Œ        | 926/6000 [44:10<3:55:16,  2.78s/it]                                                    {'loss': 2.7941, 'grad_norm': 13.993181228637695, 'learning_rate': 4.3e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 926/6000 [44:10<3:55:16,  2.78s/it] 15%|â–ˆâ–Œ        | 927/6000 [44:13<3:54:05,  2.77s/it]                                                    {'loss': 2.7522, 'grad_norm': 16.255962371826172, 'learning_rate': 4.299152542372882e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 927/6000 [44:13<3:54:05,  2.77s/it] 15%|â–ˆâ–Œ        | 928/6000 [44:16<3:54:01,  2.77s/it]                                                    {'loss': 2.7903, 'grad_norm': 11.065937042236328, 'learning_rate': 4.298305084745763e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 928/6000 [44:16<3:54:01,  2.77s/it] 15%|â–ˆâ–Œ        | 929/6000 [44:19<3:55:58,  2.79s/it]                                                    {'loss': 2.7497, 'grad_norm': 39.67924499511719, 'learning_rate': 4.297457627118644e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 929/6000 [44:19<3:55:58,  2.79s/it] 16%|â–ˆâ–Œ        | 930/6000 [44:21<3:53:27,  2.76s/it]                                                    {'loss': 2.8008, 'grad_norm': 21.375028610229492, 'learning_rate': 4.296610169491526e-06, 'epoch': 0.15}
 16%|â–ˆâ–Œ        | 930/6000 [44:21<3:53:27,  2.76s/it] 16%|â–ˆâ–Œ        | 931/6000 [44:24<3:52:57,  2.76s/it]                                                    {'loss': 2.763, 'grad_norm': 10.574041366577148, 'learning_rate': 4.295762711864407e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 931/6000 [44:24<3:52:57,  2.76s/it] 16%|â–ˆâ–Œ        | 932/6000 [44:27<3:50:36,  2.73s/it]                                                    {'loss': 2.7615, 'grad_norm': 9.34859848022461, 'learning_rate': 4.2949152542372885e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 932/6000 [44:27<3:50:36,  2.73s/it] 16%|â–ˆâ–Œ        | 933/6000 [44:29<3:48:48,  2.71s/it]                                                    {'loss': 2.7768, 'grad_norm': 14.971918106079102, 'learning_rate': 4.294067796610169e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 933/6000 [44:29<3:48:48,  2.71s/it] 16%|â–ˆâ–Œ        | 934/6000 [44:33<4:01:09,  2.86s/it]                                                    {'loss': 2.7484, 'grad_norm': 11.57759952545166, 'learning_rate': 4.293220338983051e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 934/6000 [44:33<4:01:09,  2.86s/it] 16%|â–ˆâ–Œ        | 935/6000 [44:35<3:57:25,  2.81s/it]                                                    {'loss': 2.7603, 'grad_norm': 15.053238868713379, 'learning_rate': 4.2923728813559325e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 935/6000 [44:35<3:57:25,  2.81s/it] 16%|â–ˆâ–Œ        | 936/6000 [44:38<3:54:28,  2.78s/it]                                                    {'loss': 2.8479, 'grad_norm': 26.228235244750977, 'learning_rate': 4.291525423728814e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 936/6000 [44:38<3:54:28,  2.78s/it] 16%|â–ˆâ–Œ        | 937/6000 [44:41<3:52:50,  2.76s/it]                                                    {'loss': 2.8067, 'grad_norm': 12.228221893310547, 'learning_rate': 4.290677966101695e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 937/6000 [44:41<3:52:50,  2.76s/it] 16%|â–ˆâ–Œ        | 938/6000 [44:44<3:53:41,  2.77s/it]                                                    {'loss': 2.7246, 'grad_norm': 28.506532669067383, 'learning_rate': 4.289830508474577e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 938/6000 [44:44<3:53:41,  2.77s/it] 16%|â–ˆâ–Œ        | 939/6000 [44:46<3:53:20,  2.77s/it]                                                    {'loss': 2.7892, 'grad_norm': 7.946423053741455, 'learning_rate': 4.288983050847458e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 939/6000 [44:46<3:53:20,  2.77s/it] 16%|â–ˆâ–Œ        | 940/6000 [44:49<3:53:46,  2.77s/it]                                                    {'loss': 2.7635, 'grad_norm': 15.933538436889648, 'learning_rate': 4.28813559322034e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 940/6000 [44:49<3:53:46,  2.77s/it] 16%|â–ˆâ–Œ        | 941/6000 [44:52<4:08:21,  2.95s/it]                                                    {'loss': 2.8091, 'grad_norm': 18.117219924926758, 'learning_rate': 4.287288135593221e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 941/6000 [44:52<4:08:21,  2.95s/it] 16%|â–ˆâ–Œ        | 942/6000 [44:55<4:04:08,  2.90s/it]                                                    {'loss': 2.7837, 'grad_norm': 9.494834899902344, 'learning_rate': 4.286440677966102e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 942/6000 [44:55<4:04:08,  2.90s/it] 16%|â–ˆâ–Œ        | 943/6000 [44:58<4:02:57,  2.88s/it]                                                    {'loss': 2.7643, 'grad_norm': 9.504227638244629, 'learning_rate': 4.285593220338983e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 943/6000 [44:58<4:02:57,  2.88s/it] 16%|â–ˆâ–Œ        | 944/6000 [45:01<3:59:13,  2.84s/it]                                                    {'loss': 2.7491, 'grad_norm': 11.816166877746582, 'learning_rate': 4.284745762711865e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 944/6000 [45:01<3:59:13,  2.84s/it] 16%|â–ˆâ–Œ        | 945/6000 [45:04<4:06:17,  2.92s/it]                                                    {'loss': 2.8255, 'grad_norm': 8.249272346496582, 'learning_rate': 4.283898305084746e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 945/6000 [45:04<4:06:17,  2.92s/it] 16%|â–ˆâ–Œ        | 946/6000 [45:07<4:04:08,  2.90s/it]                                                    {'loss': 2.7558, 'grad_norm': 8.479708671569824, 'learning_rate': 4.283050847457628e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 946/6000 [45:07<4:04:08,  2.90s/it] 16%|â–ˆâ–Œ        | 947/6000 [45:09<3:59:26,  2.84s/it]                                                    {'loss': 2.7656, 'grad_norm': 14.790388107299805, 'learning_rate': 4.282203389830509e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 947/6000 [45:09<3:59:26,  2.84s/it] 16%|â–ˆâ–Œ        | 948/6000 [45:12<4:00:26,  2.86s/it]                                                    {'loss': 2.8085, 'grad_norm': 11.85533332824707, 'learning_rate': 4.28135593220339e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 948/6000 [45:12<4:00:26,  2.86s/it] 16%|â–ˆâ–Œ        | 949/6000 [45:15<4:06:57,  2.93s/it]                                                    {'loss': 2.7961, 'grad_norm': 14.67006778717041, 'learning_rate': 4.280508474576271e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 949/6000 [45:15<4:06:57,  2.93s/it] 16%|â–ˆâ–Œ        | 950/6000 [45:18<4:02:18,  2.88s/it]                                                    {'loss': 2.8088, 'grad_norm': 18.914125442504883, 'learning_rate': 4.279661016949153e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 950/6000 [45:18<4:02:18,  2.88s/it][2025-10-23 01:11:47,525] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-950
[2025-10-23 01:11:47,536] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:11:48,153] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-950/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 16%|â–ˆâ–Œ        | 951/6000 [45:23<4:51:59,  3.47s/it]                                                    {'loss': 2.768, 'grad_norm': 10.387421607971191, 'learning_rate': 4.2788135593220345e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 951/6000 [45:23<4:51:59,  3.47s/it] 16%|â–ˆâ–Œ        | 952/6000 [45:26<4:34:05,  3.26s/it]                                                    {'loss': 2.7539, 'grad_norm': 11.73692512512207, 'learning_rate': 4.277966101694915e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 952/6000 [45:26<4:34:05,  3.26s/it] 16%|â–ˆâ–Œ        | 953/6000 [45:29<4:26:20,  3.17s/it]                                                    {'loss': 2.7656, 'grad_norm': 10.227946281433105, 'learning_rate': 4.277118644067797e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 953/6000 [45:29<4:26:20,  3.17s/it] 16%|â–ˆâ–Œ        | 954/6000 [45:32<4:15:22,  3.04s/it]                                                    {'loss': 2.804, 'grad_norm': 14.171358108520508, 'learning_rate': 4.276271186440678e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 954/6000 [45:32<4:15:22,  3.04s/it] 16%|â–ˆâ–Œ        | 955/6000 [45:34<4:07:11,  2.94s/it]                                                    {'loss': 2.7894, 'grad_norm': 9.246504783630371, 'learning_rate': 4.275423728813559e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 955/6000 [45:34<4:07:11,  2.94s/it] 16%|â–ˆâ–Œ        | 956/6000 [45:37<4:07:04,  2.94s/it]                                                    {'loss': 2.8083, 'grad_norm': 18.37837791442871, 'learning_rate': 4.274576271186441e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 956/6000 [45:37<4:07:04,  2.94s/it] 16%|â–ˆâ–Œ        | 957/6000 [45:40<3:59:59,  2.86s/it]                                                    {'loss': 2.76, 'grad_norm': 11.051734924316406, 'learning_rate': 4.273728813559323e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 957/6000 [45:40<3:59:59,  2.86s/it] 16%|â–ˆâ–Œ        | 958/6000 [45:43<4:00:39,  2.86s/it]                                                    {'loss': 2.7997, 'grad_norm': 8.404860496520996, 'learning_rate': 4.272881355932203e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 958/6000 [45:43<4:00:39,  2.86s/it] 16%|â–ˆâ–Œ        | 959/6000 [45:45<3:55:33,  2.80s/it]                                                    {'loss': 2.7684, 'grad_norm': 6.534584999084473, 'learning_rate': 4.272033898305085e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 959/6000 [45:45<3:55:33,  2.80s/it] 16%|â–ˆâ–Œ        | 960/6000 [45:48<3:55:03,  2.80s/it]                                                    {'loss': 2.7934, 'grad_norm': 16.241304397583008, 'learning_rate': 4.271186440677967e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 960/6000 [45:48<3:55:03,  2.80s/it] 16%|â–ˆâ–Œ        | 961/6000 [45:51<3:56:45,  2.82s/it]                                                    {'loss': 2.9486, 'grad_norm': 10.698156356811523, 'learning_rate': 4.270338983050848e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 961/6000 [45:51<3:56:45,  2.82s/it] 16%|â–ˆâ–Œ        | 962/6000 [45:54<4:04:15,  2.91s/it]                                                    {'loss': 2.7578, 'grad_norm': 9.01785659790039, 'learning_rate': 4.269491525423729e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 962/6000 [45:54<4:04:15,  2.91s/it] 16%|â–ˆâ–Œ        | 963/6000 [45:57<3:59:53,  2.86s/it]                                                    {'loss': 2.8524, 'grad_norm': 7.783466339111328, 'learning_rate': 4.268644067796611e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 963/6000 [45:57<3:59:53,  2.86s/it] 16%|â–ˆâ–Œ        | 964/6000 [46:00<3:55:19,  2.80s/it]                                                    {'loss': 2.7809, 'grad_norm': 8.724297523498535, 'learning_rate': 4.2677966101694915e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 964/6000 [46:00<3:55:19,  2.80s/it] 16%|â–ˆâ–Œ        | 965/6000 [46:02<3:57:00,  2.82s/it]                                                    {'loss': 2.7479, 'grad_norm': 11.795207977294922, 'learning_rate': 4.266949152542373e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 965/6000 [46:02<3:57:00,  2.82s/it] 16%|â–ˆâ–Œ        | 966/6000 [46:05<3:55:59,  2.81s/it]                                                    {'loss': 2.7819, 'grad_norm': 8.228225708007812, 'learning_rate': 4.266101694915255e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 966/6000 [46:05<3:55:59,  2.81s/it] 16%|â–ˆâ–Œ        | 967/6000 [46:08<3:56:37,  2.82s/it]                                                    {'loss': 2.7488, 'grad_norm': 14.03434944152832, 'learning_rate': 4.2652542372881364e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 967/6000 [46:08<3:56:37,  2.82s/it] 16%|â–ˆâ–Œ        | 968/6000 [46:11<4:02:48,  2.90s/it]                                                    {'loss': 2.7827, 'grad_norm': 14.517931938171387, 'learning_rate': 4.264406779661017e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 968/6000 [46:11<4:02:48,  2.90s/it] 16%|â–ˆâ–Œ        | 969/6000 [46:14<3:58:32,  2.84s/it]                                                    {'loss': 2.7338, 'grad_norm': 16.420475006103516, 'learning_rate': 4.263559322033899e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 969/6000 [46:14<3:58:32,  2.84s/it] 16%|â–ˆâ–Œ        | 970/6000 [46:17<3:57:55,  2.84s/it]                                                    {'loss': 2.8373, 'grad_norm': 17.03469467163086, 'learning_rate': 4.26271186440678e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 970/6000 [46:17<3:57:55,  2.84s/it] 16%|â–ˆâ–Œ        | 971/6000 [46:19<3:55:01,  2.80s/it]                                                    {'loss': 2.8062, 'grad_norm': 10.826746940612793, 'learning_rate': 4.261864406779661e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 971/6000 [46:19<3:55:01,  2.80s/it] 16%|â–ˆâ–Œ        | 972/6000 [46:22<3:53:37,  2.79s/it]                                                    {'loss': 2.7261, 'grad_norm': 16.116172790527344, 'learning_rate': 4.261016949152543e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 972/6000 [46:22<3:53:37,  2.79s/it] 16%|â–ˆâ–Œ        | 973/6000 [46:25<3:52:24,  2.77s/it]                                                    {'loss': 2.757, 'grad_norm': 12.551669120788574, 'learning_rate': 4.260169491525424e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 973/6000 [46:25<3:52:24,  2.77s/it] 16%|â–ˆâ–Œ        | 974/6000 [46:28<3:52:30,  2.78s/it]                                                    {'loss': 2.9263, 'grad_norm': 21.714073181152344, 'learning_rate': 4.259322033898305e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 974/6000 [46:28<3:52:30,  2.78s/it] 16%|â–ˆâ–‹        | 975/6000 [46:30<3:50:19,  2.75s/it]                                                    {'loss': 2.8049, 'grad_norm': 12.596670150756836, 'learning_rate': 4.258474576271186e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 975/6000 [46:30<3:50:19,  2.75s/it] 16%|â–ˆâ–‹        | 976/6000 [46:33<3:52:12,  2.77s/it]                                                    {'loss': 2.7845, 'grad_norm': 14.327933311462402, 'learning_rate': 4.257627118644068e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 976/6000 [46:33<3:52:12,  2.77s/it] 16%|â–ˆâ–‹        | 977/6000 [46:36<3:49:02,  2.74s/it]                                                    {'loss': 2.8526, 'grad_norm': 12.421164512634277, 'learning_rate': 4.256779661016949e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 977/6000 [46:36<3:49:02,  2.74s/it] 16%|â–ˆâ–‹        | 978/6000 [46:38<3:46:42,  2.71s/it]                                                    {'loss': 2.7824, 'grad_norm': 11.10525894165039, 'learning_rate': 4.255932203389831e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 978/6000 [46:38<3:46:42,  2.71s/it] 16%|â–ˆâ–‹        | 979/6000 [46:41<3:45:54,  2.70s/it]                                                    {'loss': 2.7564, 'grad_norm': 12.320548057556152, 'learning_rate': 4.255084745762712e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 979/6000 [46:41<3:45:54,  2.70s/it] 16%|â–ˆâ–‹        | 980/6000 [46:44<3:48:57,  2.74s/it]                                                    {'loss': 2.8049, 'grad_norm': 11.037840843200684, 'learning_rate': 4.2542372881355935e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 980/6000 [46:44<3:48:57,  2.74s/it] 16%|â–ˆâ–‹        | 981/6000 [46:47<3:47:43,  2.72s/it]                                                    {'loss': 2.7564, 'grad_norm': 12.214807510375977, 'learning_rate': 4.253389830508475e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 981/6000 [46:47<3:47:43,  2.72s/it] 16%|â–ˆâ–‹        | 982/6000 [46:49<3:48:40,  2.73s/it]                                                    {'loss': 2.7552, 'grad_norm': 13.589396476745605, 'learning_rate': 4.252542372881357e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 982/6000 [46:49<3:48:40,  2.73s/it] 16%|â–ˆâ–‹        | 983/6000 [46:52<3:48:56,  2.74s/it]                                                    {'loss': 2.8074, 'grad_norm': 8.707019805908203, 'learning_rate': 4.2516949152542375e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 983/6000 [46:52<3:48:56,  2.74s/it] 16%|â–ˆâ–‹        | 984/6000 [46:55<3:47:52,  2.73s/it]                                                    {'loss': 2.7738, 'grad_norm': 11.909096717834473, 'learning_rate': 4.250847457627119e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 984/6000 [46:55<3:47:52,  2.73s/it] 16%|â–ˆâ–‹        | 985/6000 [46:58<3:51:46,  2.77s/it]                                                    {'loss': 2.7514, 'grad_norm': 18.463077545166016, 'learning_rate': 4.25e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 985/6000 [46:58<3:51:46,  2.77s/it] 16%|â–ˆâ–‹        | 986/6000 [47:00<3:49:11,  2.74s/it]                                                    {'loss': 2.7562, 'grad_norm': 17.747974395751953, 'learning_rate': 4.249152542372882e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 986/6000 [47:00<3:49:11,  2.74s/it] 16%|â–ˆâ–‹        | 987/6000 [47:03<3:52:58,  2.79s/it]                                                    {'loss': 2.7514, 'grad_norm': 18.09513282775879, 'learning_rate': 4.248305084745763e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 987/6000 [47:03<3:52:58,  2.79s/it] 16%|â–ˆâ–‹        | 988/6000 [47:06<3:50:53,  2.76s/it]                                                    {'loss': 2.7844, 'grad_norm': 8.810083389282227, 'learning_rate': 4.247457627118645e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 988/6000 [47:06<3:50:53,  2.76s/it] 16%|â–ˆâ–‹        | 989/6000 [47:09<3:49:23,  2.75s/it]                                                    {'loss': 2.7732, 'grad_norm': 8.833700180053711, 'learning_rate': 4.246610169491526e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 989/6000 [47:09<3:49:23,  2.75s/it] 16%|â–ˆâ–‹        | 990/6000 [47:11<3:48:26,  2.74s/it]                                                    {'loss': 2.8316, 'grad_norm': 39.837425231933594, 'learning_rate': 4.245762711864407e-06, 'epoch': 0.17}
 16%|â–ˆâ–‹        | 990/6000 [47:11<3:48:26,  2.74s/it] 17%|â–ˆâ–‹        | 991/6000 [47:15<4:01:09,  2.89s/it]                                                    {'loss': 2.8001, 'grad_norm': 12.592382431030273, 'learning_rate': 4.244915254237288e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 991/6000 [47:15<4:01:09,  2.89s/it] 17%|â–ˆâ–‹        | 992/6000 [47:17<3:58:10,  2.85s/it]                                                    {'loss': 2.7628, 'grad_norm': 15.434447288513184, 'learning_rate': 4.24406779661017e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 992/6000 [47:17<3:58:10,  2.85s/it] 17%|â–ˆâ–‹        | 993/6000 [47:20<3:57:24,  2.84s/it]                                                    {'loss': 2.766, 'grad_norm': 13.960697174072266, 'learning_rate': 4.243220338983051e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 993/6000 [47:20<3:57:24,  2.84s/it] 17%|â–ˆâ–‹        | 994/6000 [47:23<3:57:09,  2.84s/it]                                                    {'loss': 2.7332, 'grad_norm': 24.953920364379883, 'learning_rate': 4.242372881355932e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 994/6000 [47:23<3:57:09,  2.84s/it] 17%|â–ˆâ–‹        | 995/6000 [47:26<3:54:49,  2.82s/it]                                                    {'loss': 2.7317, 'grad_norm': 37.469139099121094, 'learning_rate': 4.241525423728814e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 995/6000 [47:26<3:54:49,  2.82s/it] 17%|â–ˆâ–‹        | 996/6000 [47:29<3:52:20,  2.79s/it]                                                    {'loss': 2.8143, 'grad_norm': 12.34004020690918, 'learning_rate': 4.2406779661016954e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 996/6000 [47:29<3:52:20,  2.79s/it] 17%|â–ˆâ–‹        | 997/6000 [47:31<3:53:35,  2.80s/it]                                                    {'loss': 2.7779, 'grad_norm': 21.275516510009766, 'learning_rate': 4.239830508474577e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 997/6000 [47:31<3:53:35,  2.80s/it] 17%|â–ˆâ–‹        | 998/6000 [47:34<3:50:40,  2.77s/it]                                                    {'loss': 2.7395, 'grad_norm': 16.016569137573242, 'learning_rate': 4.238983050847458e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 998/6000 [47:34<3:50:40,  2.77s/it] 17%|â–ˆâ–‹        | 999/6000 [47:37<3:48:58,  2.75s/it]                                                    {'loss': 2.743, 'grad_norm': 13.152037620544434, 'learning_rate': 4.2381355932203395e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 999/6000 [47:37<3:48:58,  2.75s/it] 17%|â–ˆâ–‹        | 1000/6000 [47:40<3:49:35,  2.76s/it]                                                     {'loss': 2.7869, 'grad_norm': 15.656323432922363, 'learning_rate': 4.23728813559322e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1000/6000 [47:40<3:49:35,  2.76s/it][2025-10-23 01:14:08,917] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1000
[2025-10-23 01:14:08,958] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:14:09,794] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1000/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 17%|â–ˆâ–‹        | 1001/6000 [47:45<4:50:23,  3.49s/it]                                                     {'loss': 2.739, 'grad_norm': 22.514270782470703, 'learning_rate': 4.236440677966102e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1001/6000 [47:45<4:50:23,  3.49s/it] 17%|â–ˆâ–‹        | 1002/6000 [47:48<4:31:22,  3.26s/it]                                                     {'loss': 2.8211, 'grad_norm': 10.746953964233398, 'learning_rate': 4.2355932203389836e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1002/6000 [47:48<4:31:22,  3.26s/it] 17%|â–ˆâ–‹        | 1003/6000 [47:50<4:17:31,  3.09s/it]                                                     {'loss': 2.8028, 'grad_norm': 16.817012786865234, 'learning_rate': 4.234745762711865e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1003/6000 [47:50<4:17:31,  3.09s/it] 17%|â–ˆâ–‹        | 1004/6000 [47:53<4:07:43,  2.97s/it]                                                     {'loss': 2.7707, 'grad_norm': 15.681093215942383, 'learning_rate': 4.233898305084746e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1004/6000 [47:53<4:07:43,  2.97s/it] 17%|â–ˆâ–‹        | 1005/6000 [47:56<4:01:31,  2.90s/it]                                                     {'loss': 2.7906, 'grad_norm': 9.681764602661133, 'learning_rate': 4.233050847457628e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1005/6000 [47:56<4:01:31,  2.90s/it] 17%|â–ˆâ–‹        | 1006/6000 [47:58<3:57:00,  2.85s/it]                                                     {'loss': 2.7675, 'grad_norm': 8.086797714233398, 'learning_rate': 4.232203389830508e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1006/6000 [47:58<3:57:00,  2.85s/it] 17%|â–ˆâ–‹        | 1007/6000 [48:01<3:53:10,  2.80s/it]                                                     {'loss': 2.7606, 'grad_norm': 19.073711395263672, 'learning_rate': 4.23135593220339e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1007/6000 [48:01<3:53:10,  2.80s/it] 17%|â–ˆâ–‹        | 1008/6000 [48:04<3:52:03,  2.79s/it]                                                     {'loss': 2.7882, 'grad_norm': 11.798908233642578, 'learning_rate': 4.230508474576272e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1008/6000 [48:04<3:52:03,  2.79s/it] 17%|â–ˆâ–‹        | 1009/6000 [48:07<3:52:14,  2.79s/it]                                                     {'loss': 2.7685, 'grad_norm': 15.355843544006348, 'learning_rate': 4.229661016949153e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1009/6000 [48:07<3:52:14,  2.79s/it] 17%|â–ˆâ–‹        | 1010/6000 [48:09<3:49:40,  2.76s/it]                                                     {'loss': 2.7621, 'grad_norm': 15.164637565612793, 'learning_rate': 4.228813559322034e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1010/6000 [48:09<3:49:40,  2.76s/it] 17%|â–ˆâ–‹        | 1011/6000 [48:12<3:49:34,  2.76s/it]                                                     {'loss': 2.7582, 'grad_norm': 10.739022254943848, 'learning_rate': 4.227966101694916e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1011/6000 [48:12<3:49:34,  2.76s/it] 17%|â–ˆâ–‹        | 1012/6000 [48:15<3:52:08,  2.79s/it]                                                     {'loss': 2.8021, 'grad_norm': 11.18630599975586, 'learning_rate': 4.2271186440677965e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1012/6000 [48:15<3:52:08,  2.79s/it] 17%|â–ˆâ–‹        | 1013/6000 [48:18<3:54:29,  2.82s/it]                                                     {'loss': 2.7759, 'grad_norm': 9.70665168762207, 'learning_rate': 4.226271186440678e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1013/6000 [48:18<3:54:29,  2.82s/it] 17%|â–ˆâ–‹        | 1014/6000 [48:20<3:50:20,  2.77s/it]                                                     {'loss': 2.7652, 'grad_norm': 10.82457447052002, 'learning_rate': 4.22542372881356e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1014/6000 [48:21<3:50:20,  2.77s/it] 17%|â–ˆâ–‹        | 1015/6000 [48:23<3:49:29,  2.76s/it]                                                     {'loss': 2.7651, 'grad_norm': 12.294382095336914, 'learning_rate': 4.224576271186441e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1015/6000 [48:23<3:49:29,  2.76s/it] 17%|â–ˆâ–‹        | 1016/6000 [48:26<3:49:18,  2.76s/it]                                                     {'loss': 2.8314, 'grad_norm': 16.59097671508789, 'learning_rate': 4.223728813559322e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1016/6000 [48:26<3:49:18,  2.76s/it] 17%|â–ˆâ–‹        | 1017/6000 [48:29<3:50:44,  2.78s/it]                                                     {'loss': 2.7979, 'grad_norm': 18.6771240234375, 'learning_rate': 4.222881355932204e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1017/6000 [48:29<3:50:44,  2.78s/it] 17%|â–ˆâ–‹        | 1018/6000 [48:32<3:50:16,  2.77s/it]                                                     {'loss': 2.7795, 'grad_norm': 8.909357070922852, 'learning_rate': 4.2220338983050855e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1018/6000 [48:32<3:50:16,  2.77s/it] 17%|â–ˆâ–‹        | 1019/6000 [48:34<3:48:22,  2.75s/it]                                                     {'loss': 2.7811, 'grad_norm': 14.49366283416748, 'learning_rate': 4.221186440677966e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1019/6000 [48:34<3:48:22,  2.75s/it] 17%|â–ˆâ–‹        | 1020/6000 [48:37<3:48:10,  2.75s/it]                                                     {'loss': 2.7552, 'grad_norm': 10.345792770385742, 'learning_rate': 4.220338983050848e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1020/6000 [48:37<3:48:10,  2.75s/it] 17%|â–ˆâ–‹        | 1021/6000 [48:40<3:47:20,  2.74s/it]                                                     {'loss': 2.7931, 'grad_norm': 7.778672695159912, 'learning_rate': 4.219491525423729e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1021/6000 [48:40<3:47:20,  2.74s/it] 17%|â–ˆâ–‹        | 1022/6000 [48:43<3:51:45,  2.79s/it]                                                     {'loss': 2.7736, 'grad_norm': 9.938237190246582, 'learning_rate': 4.21864406779661e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1022/6000 [48:43<3:51:45,  2.79s/it] 17%|â–ˆâ–‹        | 1023/6000 [48:45<3:50:27,  2.78s/it]                                                     {'loss': 2.7903, 'grad_norm': 12.765033721923828, 'learning_rate': 4.217796610169492e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1023/6000 [48:45<3:50:27,  2.78s/it] 17%|â–ˆâ–‹        | 1024/6000 [48:48<3:48:23,  2.75s/it]                                                     {'loss': 2.7655, 'grad_norm': 11.098976135253906, 'learning_rate': 4.216949152542374e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1024/6000 [48:48<3:48:23,  2.75s/it] 17%|â–ˆâ–‹        | 1025/6000 [48:51<3:46:39,  2.73s/it]                                                     {'loss': 2.8037, 'grad_norm': 16.758930206298828, 'learning_rate': 4.2161016949152544e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1025/6000 [48:51<3:46:39,  2.73s/it] 17%|â–ˆâ–‹        | 1026/6000 [48:54<3:46:20,  2.73s/it]                                                     {'loss': 2.7507, 'grad_norm': 12.554423332214355, 'learning_rate': 4.215254237288136e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1026/6000 [48:54<3:46:20,  2.73s/it] 17%|â–ˆâ–‹        | 1027/6000 [48:56<3:45:39,  2.72s/it]                                                     {'loss': 2.7318, 'grad_norm': 15.083881378173828, 'learning_rate': 4.214406779661017e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1027/6000 [48:56<3:45:39,  2.72s/it] 17%|â–ˆâ–‹        | 1028/6000 [48:59<3:44:30,  2.71s/it]                                                     {'loss': 2.7947, 'grad_norm': 9.484975814819336, 'learning_rate': 4.2135593220338985e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1028/6000 [48:59<3:44:30,  2.71s/it] 17%|â–ˆâ–‹        | 1029/6000 [49:02<3:43:58,  2.70s/it]                                                     {'loss': 2.7893, 'grad_norm': 9.668103218078613, 'learning_rate': 4.21271186440678e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1029/6000 [49:02<3:43:58,  2.70s/it] 17%|â–ˆâ–‹        | 1030/6000 [49:04<3:42:30,  2.69s/it]                                                     {'loss': 2.8006, 'grad_norm': 11.599231719970703, 'learning_rate': 4.211864406779662e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1030/6000 [49:04<3:42:30,  2.69s/it] 17%|â–ˆâ–‹        | 1031/6000 [49:07<3:43:55,  2.70s/it]                                                     {'loss': 2.8544, 'grad_norm': 15.114534378051758, 'learning_rate': 4.2110169491525426e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1031/6000 [49:07<3:43:55,  2.70s/it] 17%|â–ˆâ–‹        | 1032/6000 [49:10<3:43:47,  2.70s/it]                                                     {'loss': 2.7305, 'grad_norm': 14.46557331085205, 'learning_rate': 4.210169491525424e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1032/6000 [49:10<3:43:47,  2.70s/it] 17%|â–ˆâ–‹        | 1033/6000 [49:12<3:43:24,  2.70s/it]                                                     {'loss': 2.7329, 'grad_norm': 14.854531288146973, 'learning_rate': 4.209322033898305e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1033/6000 [49:12<3:43:24,  2.70s/it] 17%|â–ˆâ–‹        | 1034/6000 [49:15<3:43:12,  2.70s/it]                                                     {'loss': 2.7619, 'grad_norm': 11.099424362182617, 'learning_rate': 4.2084745762711875e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1034/6000 [49:15<3:43:12,  2.70s/it] 17%|â–ˆâ–‹        | 1035/6000 [49:18<3:45:02,  2.72s/it]                                                     {'loss': 2.7729, 'grad_norm': 8.84653091430664, 'learning_rate': 4.207627118644068e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1035/6000 [49:18<3:45:02,  2.72s/it] 17%|â–ˆâ–‹        | 1036/6000 [49:21<3:44:55,  2.72s/it]                                                     {'loss': 2.7963, 'grad_norm': 16.44308853149414, 'learning_rate': 4.206779661016949e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1036/6000 [49:21<3:44:55,  2.72s/it] 17%|â–ˆâ–‹        | 1037/6000 [49:23<3:44:33,  2.71s/it]                                                     {'loss': 2.8021, 'grad_norm': 13.186888694763184, 'learning_rate': 4.205932203389831e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1037/6000 [49:23<3:44:33,  2.71s/it] 17%|â–ˆâ–‹        | 1038/6000 [49:26<3:54:05,  2.83s/it]                                                     {'loss': 2.7845, 'grad_norm': 12.873538970947266, 'learning_rate': 4.205084745762712e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1038/6000 [49:26<3:54:05,  2.83s/it] 17%|â–ˆâ–‹        | 1039/6000 [49:29<3:53:44,  2.83s/it]                                                     {'loss': 2.8134, 'grad_norm': 11.601115226745605, 'learning_rate': 4.204237288135594e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1039/6000 [49:29<3:53:44,  2.83s/it] 17%|â–ˆâ–‹        | 1040/6000 [49:32<3:54:43,  2.84s/it]                                                     {'loss': 2.7774, 'grad_norm': 16.438535690307617, 'learning_rate': 4.203389830508475e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1040/6000 [49:32<3:54:43,  2.84s/it] 17%|â–ˆâ–‹        | 1041/6000 [49:35<3:51:13,  2.80s/it]                                                     {'loss': 2.7675, 'grad_norm': 12.930146217346191, 'learning_rate': 4.202542372881356e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1041/6000 [49:35<3:51:13,  2.80s/it] 17%|â–ˆâ–‹        | 1042/6000 [49:37<3:49:21,  2.78s/it]                                                     {'loss': 2.7878, 'grad_norm': 8.063666343688965, 'learning_rate': 4.201694915254237e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1042/6000 [49:37<3:49:21,  2.78s/it] 17%|â–ˆâ–‹        | 1043/6000 [49:40<3:47:36,  2.76s/it]                                                     {'loss': 2.8105, 'grad_norm': 13.31589412689209, 'learning_rate': 4.200847457627119e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1043/6000 [49:40<3:47:36,  2.76s/it] 17%|â–ˆâ–‹        | 1044/6000 [49:43<3:47:21,  2.75s/it]                                                     {'loss': 2.8062, 'grad_norm': 13.047139167785645, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1044/6000 [49:43<3:47:21,  2.75s/it] 17%|â–ˆâ–‹        | 1045/6000 [49:46<3:50:47,  2.79s/it]                                                     {'loss': 2.8707, 'grad_norm': 29.1933536529541, 'learning_rate': 4.199152542372882e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1045/6000 [49:46<3:50:47,  2.79s/it] 17%|â–ˆâ–‹        | 1046/6000 [49:49<3:49:51,  2.78s/it]                                                     {'loss': 2.7919, 'grad_norm': 11.62380313873291, 'learning_rate': 4.198305084745763e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1046/6000 [49:49<3:49:51,  2.78s/it] 17%|â–ˆâ–‹        | 1047/6000 [49:52<3:58:25,  2.89s/it]                                                     {'loss': 2.8215, 'grad_norm': 13.524459838867188, 'learning_rate': 4.1974576271186445e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1047/6000 [49:52<3:58:25,  2.89s/it] 17%|â–ˆâ–‹        | 1048/6000 [49:54<3:55:01,  2.85s/it]                                                     {'loss': 2.7851, 'grad_norm': 9.728861808776855, 'learning_rate': 4.196610169491525e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1048/6000 [49:54<3:55:01,  2.85s/it] 17%|â–ˆâ–‹        | 1049/6000 [49:57<3:52:37,  2.82s/it]                                                     {'loss': 2.8479, 'grad_norm': 14.68646240234375, 'learning_rate': 4.195762711864407e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1049/6000 [49:57<3:52:37,  2.82s/it] 18%|â–ˆâ–Š        | 1050/6000 [50:01<4:06:55,  2.99s/it]                                                     {'loss': 2.8205, 'grad_norm': 30.310686111450195, 'learning_rate': 4.1949152542372886e-06, 'epoch': 0.17}
 18%|â–ˆâ–Š        | 1050/6000 [50:01<4:06:55,  2.99s/it][2025-10-23 01:16:29,900] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1050
[2025-10-23 01:16:29,911] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:16:30,519] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1050/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1051/6000 [50:05<4:53:29,  3.56s/it]                                                     {'loss': 2.7972, 'grad_norm': 5.313319206237793, 'learning_rate': 4.19406779661017e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1051/6000 [50:05<4:53:29,  3.56s/it] 18%|â–ˆâ–Š        | 1052/6000 [50:08<4:33:30,  3.32s/it]                                                     {'loss': 2.7852, 'grad_norm': 12.27852725982666, 'learning_rate': 4.193220338983051e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1052/6000 [50:08<4:33:30,  3.32s/it] 18%|â–ˆâ–Š        | 1053/6000 [50:11<4:26:42,  3.23s/it]                                                     {'loss': 2.7921, 'grad_norm': 19.665925979614258, 'learning_rate': 4.192372881355933e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1053/6000 [50:11<4:26:42,  3.23s/it] 18%|â–ˆâ–Š        | 1054/6000 [50:14<4:16:24,  3.11s/it]                                                     {'loss': 2.7569, 'grad_norm': 11.745382308959961, 'learning_rate': 4.191525423728814e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1054/6000 [50:14<4:16:24,  3.11s/it] 18%|â–ˆâ–Š        | 1055/6000 [50:17<4:07:07,  3.00s/it]                                                     {'loss': 2.7883, 'grad_norm': 7.042226791381836, 'learning_rate': 4.190677966101696e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1055/6000 [50:17<4:07:07,  3.00s/it] 18%|â–ˆâ–Š        | 1056/6000 [50:20<4:01:12,  2.93s/it]                                                     {'loss': 2.7777, 'grad_norm': 7.136880397796631, 'learning_rate': 4.189830508474577e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1056/6000 [50:20<4:01:12,  2.93s/it] 18%|â–ˆâ–Š        | 1057/6000 [50:22<3:55:25,  2.86s/it]                                                     {'loss': 2.7891, 'grad_norm': 8.811406135559082, 'learning_rate': 4.1889830508474575e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1057/6000 [50:22<3:55:25,  2.86s/it] 18%|â–ˆâ–Š        | 1058/6000 [50:25<3:51:13,  2.81s/it]                                                     {'loss': 2.7685, 'grad_norm': 9.047051429748535, 'learning_rate': 4.188135593220339e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1058/6000 [50:25<3:51:13,  2.81s/it] 18%|â–ˆâ–Š        | 1059/6000 [50:28<3:50:11,  2.80s/it]                                                     {'loss': 2.8022, 'grad_norm': 10.06809139251709, 'learning_rate': 4.187288135593221e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1059/6000 [50:28<3:50:11,  2.80s/it] 18%|â–ˆâ–Š        | 1060/6000 [50:30<3:48:07,  2.77s/it]                                                     {'loss': 2.7861, 'grad_norm': 6.777825832366943, 'learning_rate': 4.186440677966102e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1060/6000 [50:30<3:48:07,  2.77s/it] 18%|â–ˆâ–Š        | 1061/6000 [50:34<3:56:08,  2.87s/it]                                                     {'loss': 2.7551, 'grad_norm': 9.515941619873047, 'learning_rate': 4.185593220338983e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1061/6000 [50:34<3:56:08,  2.87s/it] 18%|â–ˆâ–Š        | 1062/6000 [50:36<3:55:32,  2.86s/it]                                                     {'loss': 2.8097, 'grad_norm': 6.156381130218506, 'learning_rate': 4.184745762711865e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1062/6000 [50:36<3:55:32,  2.86s/it] 18%|â–ˆâ–Š        | 1063/6000 [50:39<3:51:50,  2.82s/it]                                                     {'loss': 2.7791, 'grad_norm': 7.500310897827148, 'learning_rate': 4.183898305084746e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1063/6000 [50:39<3:51:50,  2.82s/it] 18%|â–ˆâ–Š        | 1064/6000 [50:42<3:52:43,  2.83s/it]                                                     {'loss': 2.7627, 'grad_norm': 6.772305488586426, 'learning_rate': 4.183050847457627e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1064/6000 [50:42<3:52:43,  2.83s/it] 18%|â–ˆâ–Š        | 1065/6000 [50:45<3:49:00,  2.78s/it]                                                     {'loss': 2.794, 'grad_norm': 10.347946166992188, 'learning_rate': 4.182203389830509e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1065/6000 [50:45<3:49:00,  2.78s/it] 18%|â–ˆâ–Š        | 1066/6000 [50:47<3:48:52,  2.78s/it]                                                     {'loss': 2.8266, 'grad_norm': 6.712559700012207, 'learning_rate': 4.1813559322033905e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1066/6000 [50:47<3:48:52,  2.78s/it] 18%|â–ˆâ–Š        | 1067/6000 [50:50<3:48:47,  2.78s/it]                                                     {'loss': 2.7862, 'grad_norm': 7.372115135192871, 'learning_rate': 4.180508474576271e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1067/6000 [50:50<3:48:47,  2.78s/it] 18%|â–ˆâ–Š        | 1068/6000 [50:53<3:45:04,  2.74s/it]                                                     {'loss': 2.772, 'grad_norm': 5.897263050079346, 'learning_rate': 4.179661016949153e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1068/6000 [50:53<3:45:04,  2.74s/it] 18%|â–ˆâ–Š        | 1069/6000 [50:56<3:43:45,  2.72s/it]                                                     {'loss': 2.76, 'grad_norm': 5.505989074707031, 'learning_rate': 4.178813559322034e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1069/6000 [50:56<3:43:45,  2.72s/it] 18%|â–ˆâ–Š        | 1070/6000 [50:58<3:43:37,  2.72s/it]                                                     {'loss': 2.7767, 'grad_norm': 7.13153076171875, 'learning_rate': 4.177966101694915e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1070/6000 [50:58<3:43:37,  2.72s/it] 18%|â–ˆâ–Š        | 1071/6000 [51:01<3:42:10,  2.70s/it]                                                     {'loss': 2.7678, 'grad_norm': 7.664999008178711, 'learning_rate': 4.177118644067797e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1071/6000 [51:01<3:42:10,  2.70s/it] 18%|â–ˆâ–Š        | 1072/6000 [51:04<3:42:38,  2.71s/it]                                                     {'loss': 2.7677, 'grad_norm': 7.609492778778076, 'learning_rate': 4.176271186440679e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1072/6000 [51:04<3:42:38,  2.71s/it] 18%|â–ˆâ–Š        | 1073/6000 [51:06<3:45:44,  2.75s/it]                                                     {'loss': 2.7474, 'grad_norm': 10.087374687194824, 'learning_rate': 4.1754237288135594e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1073/6000 [51:06<3:45:44,  2.75s/it] 18%|â–ˆâ–Š        | 1074/6000 [51:09<3:50:56,  2.81s/it]                                                     {'loss': 2.7394, 'grad_norm': 8.356669425964355, 'learning_rate': 4.174576271186441e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1074/6000 [51:09<3:50:56,  2.81s/it] 18%|â–ˆâ–Š        | 1075/6000 [51:12<3:49:15,  2.79s/it]                                                     {'loss': 2.7692, 'grad_norm': 10.144207954406738, 'learning_rate': 4.173728813559323e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1075/6000 [51:12<3:49:15,  2.79s/it] 18%|â–ˆâ–Š        | 1076/6000 [51:15<3:46:44,  2.76s/it]                                                     {'loss': 2.7548, 'grad_norm': 8.115588188171387, 'learning_rate': 4.172881355932204e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1076/6000 [51:15<3:46:44,  2.76s/it] 18%|â–ˆâ–Š        | 1077/6000 [51:18<3:45:23,  2.75s/it]                                                     {'loss': 2.7748, 'grad_norm': 5.371203422546387, 'learning_rate': 4.172033898305085e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1077/6000 [51:18<3:45:23,  2.75s/it] 18%|â–ˆâ–Š        | 1078/6000 [51:20<3:43:59,  2.73s/it]                                                     {'loss': 2.7457, 'grad_norm': 8.756318092346191, 'learning_rate': 4.171186440677966e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1078/6000 [51:20<3:43:59,  2.73s/it] 18%|â–ˆâ–Š        | 1079/6000 [51:23<3:54:48,  2.86s/it]                                                     {'loss': 2.7748, 'grad_norm': 9.246750831604004, 'learning_rate': 4.1703389830508476e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1079/6000 [51:23<3:54:48,  2.86s/it] 18%|â–ˆâ–Š        | 1080/6000 [51:26<3:52:14,  2.83s/it]                                                     {'loss': 2.7853, 'grad_norm': 7.68762731552124, 'learning_rate': 4.169491525423729e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1080/6000 [51:26<3:52:14,  2.83s/it] 18%|â–ˆâ–Š        | 1081/6000 [51:29<3:47:45,  2.78s/it]                                                     {'loss': 2.7919, 'grad_norm': 10.57340145111084, 'learning_rate': 4.168644067796611e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1081/6000 [51:29<3:47:45,  2.78s/it] 18%|â–ˆâ–Š        | 1082/6000 [51:32<3:47:25,  2.77s/it]                                                     {'loss': 2.8106, 'grad_norm': 16.029233932495117, 'learning_rate': 4.167796610169492e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1082/6000 [51:32<3:47:25,  2.77s/it] 18%|â–ˆâ–Š        | 1083/6000 [51:34<3:44:35,  2.74s/it]                                                     {'loss': 2.7342, 'grad_norm': 14.501870155334473, 'learning_rate': 4.166949152542373e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1083/6000 [51:34<3:44:35,  2.74s/it] 18%|â–ˆâ–Š        | 1084/6000 [51:37<3:45:11,  2.75s/it]                                                     {'loss': 2.7841, 'grad_norm': 10.316411018371582, 'learning_rate': 4.166101694915254e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1084/6000 [51:37<3:45:11,  2.75s/it] 18%|â–ˆâ–Š        | 1085/6000 [51:40<3:43:37,  2.73s/it]                                                     {'loss': 2.7924, 'grad_norm': 6.306018352508545, 'learning_rate': 4.165254237288136e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1085/6000 [51:40<3:43:37,  2.73s/it] 18%|â–ˆâ–Š        | 1086/6000 [51:43<3:49:56,  2.81s/it]                                                     {'loss': 2.808, 'grad_norm': 13.516946792602539, 'learning_rate': 4.164406779661017e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1086/6000 [51:43<3:49:56,  2.81s/it] 18%|â–ˆâ–Š        | 1087/6000 [51:46<3:49:34,  2.80s/it]                                                     {'loss': 2.8108, 'grad_norm': 11.272574424743652, 'learning_rate': 4.163559322033899e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1087/6000 [51:46<3:49:34,  2.80s/it] 18%|â–ˆâ–Š        | 1088/6000 [51:48<3:48:18,  2.79s/it]                                                     {'loss': 2.8039, 'grad_norm': 13.32840633392334, 'learning_rate': 4.16271186440678e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1088/6000 [51:48<3:48:18,  2.79s/it] 18%|â–ˆâ–Š        | 1089/6000 [51:52<4:07:40,  3.03s/it]                                                     {'loss': 2.7897, 'grad_norm': 16.212078094482422, 'learning_rate': 4.161864406779661e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1089/6000 [51:52<4:07:40,  3.03s/it] 18%|â–ˆâ–Š        | 1090/6000 [51:55<3:59:29,  2.93s/it]                                                     {'loss': 2.7823, 'grad_norm': 9.548698425292969, 'learning_rate': 4.161016949152543e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1090/6000 [51:55<3:59:29,  2.93s/it] 18%|â–ˆâ–Š        | 1091/6000 [51:57<3:54:47,  2.87s/it]                                                     {'loss': 2.7572, 'grad_norm': 12.4581298828125, 'learning_rate': 4.160169491525425e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1091/6000 [51:57<3:54:47,  2.87s/it] 18%|â–ˆâ–Š        | 1092/6000 [52:00<3:50:56,  2.82s/it]                                                     {'loss': 2.7851, 'grad_norm': 7.896092414855957, 'learning_rate': 4.1593220338983055e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1092/6000 [52:00<3:50:56,  2.82s/it] 18%|â–ˆâ–Š        | 1093/6000 [52:03<3:48:09,  2.79s/it]                                                     {'loss': 2.7655, 'grad_norm': 10.255650520324707, 'learning_rate': 4.158474576271187e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1093/6000 [52:03<3:48:09,  2.79s/it] 18%|â–ˆâ–Š        | 1094/6000 [52:05<3:45:36,  2.76s/it]                                                     {'loss': 2.8337, 'grad_norm': 28.63060188293457, 'learning_rate': 4.157627118644068e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1094/6000 [52:05<3:45:36,  2.76s/it] 18%|â–ˆâ–Š        | 1095/6000 [52:08<3:44:57,  2.75s/it]                                                     {'loss': 2.7653, 'grad_norm': 11.799101829528809, 'learning_rate': 4.1567796610169495e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1095/6000 [52:08<3:44:57,  2.75s/it] 18%|â–ˆâ–Š        | 1096/6000 [52:11<3:48:25,  2.79s/it]                                                     {'loss': 2.7133, 'grad_norm': 21.00992202758789, 'learning_rate': 4.155932203389831e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1096/6000 [52:11<3:48:25,  2.79s/it] 18%|â–ˆâ–Š        | 1097/6000 [52:14<3:47:13,  2.78s/it]                                                     {'loss': 2.7597, 'grad_norm': 8.575279235839844, 'learning_rate': 4.155084745762713e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1097/6000 [52:14<3:47:13,  2.78s/it] 18%|â–ˆâ–Š        | 1098/6000 [52:17<3:59:30,  2.93s/it]                                                     {'loss': 2.7928, 'grad_norm': 13.195751190185547, 'learning_rate': 4.154237288135594e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1098/6000 [52:17<3:59:30,  2.93s/it] 18%|â–ˆâ–Š        | 1099/6000 [52:20<4:10:05,  3.06s/it]                                                     {'loss': 2.7808, 'grad_norm': 24.63008689880371, 'learning_rate': 4.153389830508474e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1099/6000 [52:20<4:10:05,  3.06s/it] 18%|â–ˆâ–Š        | 1100/6000 [52:23<4:00:50,  2.95s/it]                                                     {'loss': 2.7959, 'grad_norm': 11.808716773986816, 'learning_rate': 4.152542372881356e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1100/6000 [52:23<4:00:50,  2.95s/it][2025-10-23 01:18:52,426] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1100
[2025-10-23 01:18:52,441] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:18:53,056] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1101/6000 [52:28<4:48:32,  3.53s/it]                                                     {'loss': 2.8144, 'grad_norm': 14.40110969543457, 'learning_rate': 4.151694915254238e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1101/6000 [52:28<4:48:32,  3.53s/it] 18%|â–ˆâ–Š        | 1102/6000 [52:31<4:28:37,  3.29s/it]                                                     {'loss': 2.8, 'grad_norm': 7.461546897888184, 'learning_rate': 4.150847457627119e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1102/6000 [52:31<4:28:37,  3.29s/it] 18%|â–ˆâ–Š        | 1103/6000 [52:34<4:16:34,  3.14s/it]                                                     {'loss': 2.7884, 'grad_norm': 9.199913024902344, 'learning_rate': 4.15e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1103/6000 [52:34<4:16:34,  3.14s/it] 18%|â–ˆâ–Š        | 1104/6000 [52:36<4:05:10,  3.00s/it]                                                     {'loss': 2.7902, 'grad_norm': 7.01961612701416, 'learning_rate': 4.149152542372882e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1104/6000 [52:36<4:05:10,  3.00s/it] 18%|â–ˆâ–Š        | 1105/6000 [52:39<3:58:36,  2.92s/it]                                                     {'loss': 2.7755, 'grad_norm': 9.243110656738281, 'learning_rate': 4.1483050847457625e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1105/6000 [52:39<3:58:36,  2.92s/it] 18%|â–ˆâ–Š        | 1106/6000 [52:42<3:53:29,  2.86s/it]                                                     {'loss': 2.7755, 'grad_norm': 9.285369873046875, 'learning_rate': 4.147457627118644e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1106/6000 [52:42<3:53:29,  2.86s/it] 18%|â–ˆâ–Š        | 1107/6000 [52:44<3:48:51,  2.81s/it]                                                     {'loss': 2.7557, 'grad_norm': 7.252719879150391, 'learning_rate': 4.146610169491526e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1107/6000 [52:44<3:48:51,  2.81s/it] 18%|â–ˆâ–Š        | 1108/6000 [52:47<3:48:53,  2.81s/it]                                                     {'loss': 2.7676, 'grad_norm': 15.022354125976562, 'learning_rate': 4.145762711864407e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1108/6000 [52:47<3:48:53,  2.81s/it] 18%|â–ˆâ–Š        | 1109/6000 [52:50<3:45:57,  2.77s/it]                                                     {'loss': 2.7894, 'grad_norm': 13.007497787475586, 'learning_rate': 4.144915254237288e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1109/6000 [52:50<3:45:57,  2.77s/it] 18%|â–ˆâ–Š        | 1110/6000 [52:53<3:46:09,  2.77s/it]                                                     {'loss': 2.762, 'grad_norm': 7.877254009246826, 'learning_rate': 4.14406779661017e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1110/6000 [52:53<3:46:09,  2.77s/it] 19%|â–ˆâ–Š        | 1111/6000 [52:55<3:42:50,  2.73s/it]                                                     {'loss': 2.8355, 'grad_norm': 13.851106643676758, 'learning_rate': 4.1432203389830515e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1111/6000 [52:55<3:42:50,  2.73s/it] 19%|â–ˆâ–Š        | 1112/6000 [52:58<3:42:43,  2.73s/it]                                                     {'loss': 2.7745, 'grad_norm': 11.596089363098145, 'learning_rate': 4.142372881355933e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1112/6000 [52:58<3:42:43,  2.73s/it] 19%|â–ˆâ–Š        | 1113/6000 [53:01<3:42:10,  2.73s/it]                                                     {'loss': 2.8094, 'grad_norm': 9.624211311340332, 'learning_rate': 4.141525423728814e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1113/6000 [53:01<3:42:10,  2.73s/it] 19%|â–ˆâ–Š        | 1114/6000 [53:03<3:42:31,  2.73s/it]                                                     {'loss': 2.787, 'grad_norm': 11.056841850280762, 'learning_rate': 4.1406779661016955e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1114/6000 [53:03<3:42:31,  2.73s/it] 19%|â–ˆâ–Š        | 1115/6000 [53:06<3:43:05,  2.74s/it]                                                     {'loss': 2.7527, 'grad_norm': 8.617372512817383, 'learning_rate': 4.139830508474576e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1115/6000 [53:06<3:43:05,  2.74s/it] 19%|â–ˆâ–Š        | 1116/6000 [53:09<3:42:55,  2.74s/it]                                                     {'loss': 2.7559, 'grad_norm': 9.080107688903809, 'learning_rate': 4.138983050847458e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1116/6000 [53:09<3:42:55,  2.74s/it] 19%|â–ˆâ–Š        | 1117/6000 [53:12<3:46:22,  2.78s/it]                                                     {'loss': 2.7788, 'grad_norm': 13.040799140930176, 'learning_rate': 4.13813559322034e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1117/6000 [53:12<3:46:22,  2.78s/it] 19%|â–ˆâ–Š        | 1118/6000 [53:15<3:46:16,  2.78s/it]                                                     {'loss': 2.7653, 'grad_norm': 7.306927680969238, 'learning_rate': 4.13728813559322e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1118/6000 [53:15<3:46:16,  2.78s/it] 19%|â–ˆâ–Š        | 1119/6000 [53:17<3:48:03,  2.80s/it]                                                     {'loss': 2.7636, 'grad_norm': 8.49800968170166, 'learning_rate': 4.136440677966102e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1119/6000 [53:17<3:48:03,  2.80s/it] 19%|â–ˆâ–Š        | 1120/6000 [53:20<3:46:16,  2.78s/it]                                                     {'loss': 2.8076, 'grad_norm': 10.755806922912598, 'learning_rate': 4.135593220338983e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1120/6000 [53:20<3:46:16,  2.78s/it] 19%|â–ˆâ–Š        | 1121/6000 [53:23<3:45:36,  2.77s/it]                                                     {'loss': 2.7441, 'grad_norm': 10.248833656311035, 'learning_rate': 4.1347457627118645e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1121/6000 [53:23<3:45:36,  2.77s/it] 19%|â–ˆâ–Š        | 1122/6000 [53:26<3:43:36,  2.75s/it]                                                     {'loss': 2.7944, 'grad_norm': 8.746499061584473, 'learning_rate': 4.133898305084746e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1122/6000 [53:26<3:43:36,  2.75s/it] 19%|â–ˆâ–Š        | 1123/6000 [53:28<3:43:12,  2.75s/it]                                                     {'loss': 2.7244, 'grad_norm': 11.853252410888672, 'learning_rate': 4.133050847457628e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1123/6000 [53:28<3:43:12,  2.75s/it] 19%|â–ˆâ–Š        | 1124/6000 [53:32<3:52:13,  2.86s/it]                                                     {'loss': 2.7762, 'grad_norm': 8.044803619384766, 'learning_rate': 4.1322033898305085e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1124/6000 [53:32<3:52:13,  2.86s/it] 19%|â–ˆâ–‰        | 1125/6000 [53:34<3:52:48,  2.87s/it]                                                     {'loss': 2.7768, 'grad_norm': 9.16359806060791, 'learning_rate': 4.13135593220339e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1125/6000 [53:34<3:52:48,  2.87s/it] 19%|â–ˆâ–‰        | 1126/6000 [53:37<3:50:20,  2.84s/it]                                                     {'loss': 2.833, 'grad_norm': 15.097972869873047, 'learning_rate': 4.130508474576271e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1126/6000 [53:37<3:50:20,  2.84s/it] 19%|â–ˆâ–‰        | 1127/6000 [53:40<3:49:09,  2.82s/it]                                                     {'loss': 2.7622, 'grad_norm': 9.995567321777344, 'learning_rate': 4.129661016949153e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1127/6000 [53:40<3:49:09,  2.82s/it] 19%|â–ˆâ–‰        | 1128/6000 [53:43<3:46:35,  2.79s/it]                                                     {'loss': 2.7637, 'grad_norm': 7.309544086456299, 'learning_rate': 4.128813559322034e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1128/6000 [53:43<3:46:35,  2.79s/it] 19%|â–ˆâ–‰        | 1129/6000 [53:45<3:44:20,  2.76s/it]                                                     {'loss': 2.7626, 'grad_norm': 6.881967067718506, 'learning_rate': 4.127966101694916e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1129/6000 [53:45<3:44:20,  2.76s/it] 19%|â–ˆâ–‰        | 1130/6000 [53:48<3:45:17,  2.78s/it]                                                     {'loss': 2.7344, 'grad_norm': 10.287910461425781, 'learning_rate': 4.127118644067797e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1130/6000 [53:48<3:45:17,  2.78s/it] 19%|â–ˆâ–‰        | 1131/6000 [53:51<3:43:37,  2.76s/it]                                                     {'loss': 2.7778, 'grad_norm': 10.119117736816406, 'learning_rate': 4.126271186440678e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1131/6000 [53:51<3:43:37,  2.76s/it] 19%|â–ˆâ–‰        | 1132/6000 [53:54<3:41:39,  2.73s/it]                                                     {'loss': 2.7763, 'grad_norm': 10.765297889709473, 'learning_rate': 4.12542372881356e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1132/6000 [53:54<3:41:39,  2.73s/it] 19%|â–ˆâ–‰        | 1133/6000 [53:57<3:50:57,  2.85s/it]                                                     {'loss': 2.7772, 'grad_norm': 14.818486213684082, 'learning_rate': 4.1245762711864416e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1133/6000 [53:57<3:50:57,  2.85s/it] 19%|â–ˆâ–‰        | 1134/6000 [54:00<3:59:04,  2.95s/it]                                                     {'loss': 2.7467, 'grad_norm': 11.609380722045898, 'learning_rate': 4.123728813559322e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1134/6000 [54:00<3:59:04,  2.95s/it] 19%|â–ˆâ–‰        | 1135/6000 [54:03<3:53:03,  2.87s/it]                                                     {'loss': 2.7734, 'grad_norm': 8.697344779968262, 'learning_rate': 4.122881355932204e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1135/6000 [54:03<3:53:03,  2.87s/it] 19%|â–ˆâ–‰        | 1136/6000 [54:05<3:49:28,  2.83s/it]                                                     {'loss': 2.7728, 'grad_norm': 12.703545570373535, 'learning_rate': 4.122033898305085e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1136/6000 [54:05<3:49:28,  2.83s/it] 19%|â–ˆâ–‰        | 1137/6000 [54:08<3:48:36,  2.82s/it]                                                     {'loss': 2.7922, 'grad_norm': 8.898085594177246, 'learning_rate': 4.121186440677966e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1137/6000 [54:08<3:48:36,  2.82s/it] 19%|â–ˆâ–‰        | 1138/6000 [54:11<3:45:50,  2.79s/it]                                                     {'loss': 2.8048, 'grad_norm': 9.499984741210938, 'learning_rate': 4.120338983050848e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1138/6000 [54:11<3:45:50,  2.79s/it] 19%|â–ˆâ–‰        | 1139/6000 [54:14<3:45:52,  2.79s/it]                                                     {'loss': 2.7599, 'grad_norm': 14.77092456817627, 'learning_rate': 4.119491525423729e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1139/6000 [54:14<3:45:52,  2.79s/it] 19%|â–ˆâ–‰        | 1140/6000 [54:16<3:47:41,  2.81s/it]                                                     {'loss': 2.7412, 'grad_norm': 13.884462356567383, 'learning_rate': 4.1186440677966105e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1140/6000 [54:16<3:47:41,  2.81s/it] 19%|â–ˆâ–‰        | 1141/6000 [54:19<3:47:23,  2.81s/it]                                                     {'loss': 2.7744, 'grad_norm': 9.907116889953613, 'learning_rate': 4.117796610169491e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1141/6000 [54:19<3:47:23,  2.81s/it] 19%|â–ˆâ–‰        | 1142/6000 [54:22<3:47:54,  2.81s/it]                                                     {'loss': 2.8098, 'grad_norm': 12.43704605102539, 'learning_rate': 4.116949152542373e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1142/6000 [54:22<3:47:54,  2.81s/it] 19%|â–ˆâ–‰        | 1143/6000 [54:25<3:44:50,  2.78s/it]                                                     {'loss': 2.8111, 'grad_norm': 16.299148559570312, 'learning_rate': 4.1161016949152545e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1143/6000 [54:25<3:44:50,  2.78s/it] 19%|â–ˆâ–‰        | 1144/6000 [54:27<3:43:11,  2.76s/it]                                                     {'loss': 2.8471, 'grad_norm': 12.35585880279541, 'learning_rate': 4.115254237288136e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1144/6000 [54:27<3:43:11,  2.76s/it] 19%|â–ˆâ–‰        | 1145/6000 [54:30<3:44:17,  2.77s/it]                                                     {'loss': 2.8246, 'grad_norm': 14.635329246520996, 'learning_rate': 4.114406779661017e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1145/6000 [54:30<3:44:17,  2.77s/it] 19%|â–ˆâ–‰        | 1146/6000 [54:33<3:47:01,  2.81s/it]                                                     {'loss': 2.7555, 'grad_norm': 17.08785629272461, 'learning_rate': 4.113559322033899e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1146/6000 [54:33<3:47:01,  2.81s/it] 19%|â–ˆâ–‰        | 1147/6000 [54:36<3:44:22,  2.77s/it]                                                     {'loss': 2.8013, 'grad_norm': 14.684926986694336, 'learning_rate': 4.11271186440678e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1147/6000 [54:36<3:44:22,  2.77s/it] 19%|â–ˆâ–‰        | 1148/6000 [54:39<3:42:23,  2.75s/it]                                                     {'loss': 2.8147, 'grad_norm': 15.632302284240723, 'learning_rate': 4.111864406779662e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1148/6000 [54:39<3:42:23,  2.75s/it] 19%|â–ˆâ–‰        | 1149/6000 [54:42<3:51:16,  2.86s/it]                                                     {'loss': 2.8009, 'grad_norm': 14.527715682983398, 'learning_rate': 4.111016949152543e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1149/6000 [54:42<3:51:16,  2.86s/it] 19%|â–ˆâ–‰        | 1150/6000 [54:44<3:49:42,  2.84s/it]                                                     {'loss': 2.7875, 'grad_norm': 10.815058708190918, 'learning_rate': 4.110169491525424e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1150/6000 [54:44<3:49:42,  2.84s/it][2025-10-23 01:21:13,789] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1150
[2025-10-23 01:21:13,800] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:21:14,424] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 19%|â–ˆâ–‰        | 1151/6000 [54:50<4:45:44,  3.54s/it]                                                     {'loss': 2.7292, 'grad_norm': 19.199569702148438, 'learning_rate': 4.109322033898305e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1151/6000 [54:50<4:45:44,  3.54s/it] 19%|â–ˆâ–‰        | 1152/6000 [54:52<4:25:42,  3.29s/it]                                                     {'loss': 2.7761, 'grad_norm': 17.683860778808594, 'learning_rate': 4.108474576271187e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1152/6000 [54:52<4:25:42,  3.29s/it] 19%|â–ˆâ–‰        | 1153/6000 [54:55<4:13:20,  3.14s/it]                                                     {'loss': 2.7872, 'grad_norm': 8.397658348083496, 'learning_rate': 4.107627118644068e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1153/6000 [54:55<4:13:20,  3.14s/it] 19%|â–ˆâ–‰        | 1154/6000 [54:58<4:04:39,  3.03s/it]                                                     {'loss': 2.7548, 'grad_norm': 6.836688995361328, 'learning_rate': 4.10677966101695e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1154/6000 [54:58<4:04:39,  3.03s/it] 19%|â–ˆâ–‰        | 1155/6000 [55:01<3:59:00,  2.96s/it]                                                     {'loss': 2.7567, 'grad_norm': 7.814626216888428, 'learning_rate': 4.105932203389831e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1155/6000 [55:01<3:59:00,  2.96s/it] 19%|â–ˆâ–‰        | 1156/6000 [55:04<3:55:22,  2.92s/it]                                                     {'loss': 2.73, 'grad_norm': 14.569159507751465, 'learning_rate': 4.1050847457627124e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1156/6000 [55:04<3:55:22,  2.92s/it] 19%|â–ˆâ–‰        | 1157/6000 [55:06<3:53:51,  2.90s/it]                                                     {'loss': 2.7451, 'grad_norm': 9.120309829711914, 'learning_rate': 4.104237288135593e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1157/6000 [55:06<3:53:51,  2.90s/it] 19%|â–ˆâ–‰        | 1158/6000 [55:09<3:50:48,  2.86s/it]                                                     {'loss': 2.7348, 'grad_norm': 10.293176651000977, 'learning_rate': 4.103389830508475e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1158/6000 [55:09<3:50:48,  2.86s/it] 19%|â–ˆâ–‰        | 1159/6000 [55:12<3:47:08,  2.82s/it]                                                     {'loss': 2.9402, 'grad_norm': 12.408815383911133, 'learning_rate': 4.1025423728813565e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1159/6000 [55:12<3:47:08,  2.82s/it] 19%|â–ˆâ–‰        | 1160/6000 [55:15<3:44:49,  2.79s/it]                                                     {'loss': 2.8242, 'grad_norm': 13.809383392333984, 'learning_rate': 4.101694915254237e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1160/6000 [55:15<3:44:49,  2.79s/it] 19%|â–ˆâ–‰        | 1161/6000 [55:17<3:42:48,  2.76s/it]                                                     {'loss': 2.74, 'grad_norm': 10.09664249420166, 'learning_rate': 4.100847457627119e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1161/6000 [55:17<3:42:48,  2.76s/it] 19%|â–ˆâ–‰        | 1162/6000 [55:20<3:41:20,  2.75s/it]                                                     {'loss': 2.7894, 'grad_norm': 8.870677947998047, 'learning_rate': 4.1e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1162/6000 [55:20<3:41:20,  2.75s/it] 19%|â–ˆâ–‰        | 1163/6000 [55:23<3:39:12,  2.72s/it]                                                     {'loss': 2.7429, 'grad_norm': 12.641660690307617, 'learning_rate': 4.099152542372881e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1163/6000 [55:23<3:39:12,  2.72s/it] 19%|â–ˆâ–‰        | 1164/6000 [55:25<3:37:51,  2.70s/it]                                                     {'loss': 2.7345, 'grad_norm': 11.190389633178711, 'learning_rate': 4.098305084745763e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1164/6000 [55:25<3:37:51,  2.70s/it] 19%|â–ˆâ–‰        | 1165/6000 [55:28<3:39:44,  2.73s/it]                                                     {'loss': 2.8022, 'grad_norm': 11.129807472229004, 'learning_rate': 4.097457627118645e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1165/6000 [55:28<3:39:44,  2.73s/it] 19%|â–ˆâ–‰        | 1166/6000 [55:31<3:39:33,  2.73s/it]                                                     {'loss': 2.97, 'grad_norm': 9.432573318481445, 'learning_rate': 4.096610169491525e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1166/6000 [55:31<3:39:33,  2.73s/it] 19%|â–ˆâ–‰        | 1167/6000 [55:34<3:52:44,  2.89s/it]                                                     {'loss': 2.7471, 'grad_norm': 12.360803604125977, 'learning_rate': 4.095762711864407e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1167/6000 [55:34<3:52:44,  2.89s/it] 19%|â–ˆâ–‰        | 1168/6000 [55:37<3:47:24,  2.82s/it]                                                     {'loss': 2.8249, 'grad_norm': 9.692111015319824, 'learning_rate': 4.094915254237289e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1168/6000 [55:37<3:47:24,  2.82s/it] 19%|â–ˆâ–‰        | 1169/6000 [55:39<3:43:39,  2.78s/it]                                                     {'loss': 2.8123, 'grad_norm': 11.29882526397705, 'learning_rate': 4.09406779661017e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1169/6000 [55:39<3:43:39,  2.78s/it] 20%|â–ˆâ–‰        | 1170/6000 [55:42<3:41:50,  2.76s/it]                                                     {'loss': 2.822, 'grad_norm': 11.901595115661621, 'learning_rate': 4.093220338983051e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1170/6000 [55:42<3:41:50,  2.76s/it] 20%|â–ˆâ–‰        | 1171/6000 [55:45<3:44:22,  2.79s/it]                                                     {'loss': 2.7457, 'grad_norm': 12.219056129455566, 'learning_rate': 4.092372881355933e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1171/6000 [55:45<3:44:22,  2.79s/it] 20%|â–ˆâ–‰        | 1172/6000 [55:48<3:42:10,  2.76s/it]                                                     {'loss': 2.7169, 'grad_norm': 14.147261619567871, 'learning_rate': 4.0915254237288135e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1172/6000 [55:48<3:42:10,  2.76s/it] 20%|â–ˆâ–‰        | 1173/6000 [55:50<3:39:39,  2.73s/it]                                                     {'loss': 2.7322, 'grad_norm': 15.120342254638672, 'learning_rate': 4.090677966101695e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1173/6000 [55:50<3:39:39,  2.73s/it] 20%|â–ˆâ–‰        | 1174/6000 [55:53<3:39:34,  2.73s/it]                                                     {'loss': 2.7218, 'grad_norm': 9.264358520507812, 'learning_rate': 4.089830508474577e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1174/6000 [55:53<3:39:34,  2.73s/it] 20%|â–ˆâ–‰        | 1175/6000 [55:56<3:41:34,  2.76s/it]                                                     {'loss': 2.7703, 'grad_norm': 10.613585472106934, 'learning_rate': 4.0889830508474584e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1175/6000 [55:56<3:41:34,  2.76s/it] 20%|â–ˆâ–‰        | 1176/6000 [55:59<3:40:13,  2.74s/it]                                                     {'loss': 2.7872, 'grad_norm': 9.742353439331055, 'learning_rate': 4.088135593220339e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1176/6000 [55:59<3:40:13,  2.74s/it] 20%|â–ˆâ–‰        | 1177/6000 [56:01<3:40:22,  2.74s/it]                                                     {'loss': 2.757, 'grad_norm': 13.954724311828613, 'learning_rate': 4.087288135593221e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1177/6000 [56:01<3:40:22,  2.74s/it] 20%|â–ˆâ–‰        | 1178/6000 [56:05<3:50:50,  2.87s/it]                                                     {'loss': 2.7865, 'grad_norm': 14.258453369140625, 'learning_rate': 4.086440677966102e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1178/6000 [56:05<3:50:50,  2.87s/it] 20%|â–ˆâ–‰        | 1179/6000 [56:08<3:57:01,  2.95s/it]                                                     {'loss': 2.7894, 'grad_norm': 15.720337867736816, 'learning_rate': 4.085593220338983e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1179/6000 [56:08<3:57:01,  2.95s/it] 20%|â–ˆâ–‰        | 1180/6000 [56:11<4:00:53,  3.00s/it]                                                     {'loss': 2.9566, 'grad_norm': 14.206680297851562, 'learning_rate': 4.084745762711865e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1180/6000 [56:11<4:00:53,  3.00s/it] 20%|â–ˆâ–‰        | 1181/6000 [56:14<3:54:15,  2.92s/it]                                                     {'loss': 2.7677, 'grad_norm': 7.9481120109558105, 'learning_rate': 4.083898305084746e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1181/6000 [56:14<3:54:15,  2.92s/it] 20%|â–ˆâ–‰        | 1182/6000 [56:17<4:10:29,  3.12s/it]                                                     {'loss': 2.7942, 'grad_norm': 22.99606704711914, 'learning_rate': 4.083050847457627e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1182/6000 [56:17<4:10:29,  3.12s/it] 20%|â–ˆâ–‰        | 1183/6000 [56:20<4:05:25,  3.06s/it]                                                     {'loss': 2.8014, 'grad_norm': 11.805181503295898, 'learning_rate': 4.082203389830508e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1183/6000 [56:20<4:05:25,  3.06s/it] 20%|â–ˆâ–‰        | 1184/6000 [56:23<3:56:18,  2.94s/it]                                                     {'loss': 2.7633, 'grad_norm': 10.852204322814941, 'learning_rate': 4.081355932203391e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1184/6000 [56:23<3:56:18,  2.94s/it] 20%|â–ˆâ–‰        | 1185/6000 [56:25<3:50:34,  2.87s/it]                                                     {'loss': 2.7648, 'grad_norm': 10.459711074829102, 'learning_rate': 4.0805084745762714e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1185/6000 [56:25<3:50:34,  2.87s/it] 20%|â–ˆâ–‰        | 1186/6000 [56:28<3:48:09,  2.84s/it]                                                     {'loss': 2.763, 'grad_norm': 9.911389350891113, 'learning_rate': 4.079661016949153e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1186/6000 [56:28<3:48:09,  2.84s/it] 20%|â–ˆâ–‰        | 1187/6000 [56:31<3:45:31,  2.81s/it]                                                     {'loss': 2.841, 'grad_norm': 16.074539184570312, 'learning_rate': 4.078813559322034e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1187/6000 [56:31<3:45:31,  2.81s/it] 20%|â–ˆâ–‰        | 1188/6000 [56:34<3:41:39,  2.76s/it]                                                     {'loss': 2.8595, 'grad_norm': 8.877920150756836, 'learning_rate': 4.0779661016949155e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1188/6000 [56:34<3:41:39,  2.76s/it] 20%|â–ˆâ–‰        | 1189/6000 [56:36<3:39:50,  2.74s/it]                                                     {'loss': 2.8012, 'grad_norm': 10.65059757232666, 'learning_rate': 4.077118644067797e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1189/6000 [56:36<3:39:50,  2.74s/it] 20%|â–ˆâ–‰        | 1190/6000 [56:39<3:42:04,  2.77s/it]                                                     {'loss': 2.7837, 'grad_norm': 13.256484031677246, 'learning_rate': 4.076271186440679e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1190/6000 [56:39<3:42:04,  2.77s/it] 20%|â–ˆâ–‰        | 1191/6000 [56:42<3:42:26,  2.78s/it]                                                     {'loss': 2.7463, 'grad_norm': 12.430227279663086, 'learning_rate': 4.0754237288135596e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1191/6000 [56:42<3:42:26,  2.78s/it] 20%|â–ˆâ–‰        | 1192/6000 [56:45<3:43:49,  2.79s/it]                                                     {'loss': 2.7329, 'grad_norm': 17.89017677307129, 'learning_rate': 4.074576271186441e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1192/6000 [56:45<3:43:49,  2.79s/it] 20%|â–ˆâ–‰        | 1193/6000 [56:47<3:43:28,  2.79s/it]                                                     {'loss': 2.7767, 'grad_norm': 10.193662643432617, 'learning_rate': 4.073728813559322e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1193/6000 [56:47<3:43:28,  2.79s/it] 20%|â–ˆâ–‰        | 1194/6000 [56:50<3:43:45,  2.79s/it]                                                     {'loss': 2.7522, 'grad_norm': 14.393744468688965, 'learning_rate': 4.072881355932204e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1194/6000 [56:50<3:43:45,  2.79s/it] 20%|â–ˆâ–‰        | 1195/6000 [56:53<3:41:38,  2.77s/it]                                                     {'loss': 2.775, 'grad_norm': 8.813115119934082, 'learning_rate': 4.072033898305085e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1195/6000 [56:53<3:41:38,  2.77s/it] 20%|â–ˆâ–‰        | 1196/6000 [56:56<3:41:07,  2.76s/it]                                                     {'loss': 2.7216, 'grad_norm': 8.819716453552246, 'learning_rate': 4.071186440677967e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1196/6000 [56:56<3:41:07,  2.76s/it] 20%|â–ˆâ–‰        | 1197/6000 [56:59<3:44:25,  2.80s/it]                                                     {'loss': 2.7842, 'grad_norm': 7.605645179748535, 'learning_rate': 4.070338983050848e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1197/6000 [56:59<3:44:25,  2.80s/it] 20%|â–ˆâ–‰        | 1198/6000 [57:01<3:44:03,  2.80s/it]                                                     {'loss': 2.7771, 'grad_norm': 12.14630126953125, 'learning_rate': 4.069491525423729e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1198/6000 [57:01<3:44:03,  2.80s/it] 20%|â–ˆâ–‰        | 1199/6000 [57:04<3:43:43,  2.80s/it]                                                     {'loss': 2.8206, 'grad_norm': 13.966309547424316, 'learning_rate': 4.06864406779661e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1199/6000 [57:04<3:43:43,  2.80s/it] 20%|â–ˆâ–ˆ        | 1200/6000 [57:07<3:41:09,  2.76s/it]                                                     {'loss': 2.7615, 'grad_norm': 9.407768249511719, 'learning_rate': 4.067796610169492e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1200/6000 [57:07<3:41:09,  2.76s/it][2025-10-23 01:23:36,214] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1200
[2025-10-23 01:23:36,225] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:23:36,865] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 20%|â–ˆâ–ˆ        | 1201/6000 [57:12<4:35:06,  3.44s/it]                                                     {'loss': 2.7778, 'grad_norm': 11.533797264099121, 'learning_rate': 4.066949152542373e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1201/6000 [57:12<4:35:06,  3.44s/it] 20%|â–ˆâ–ˆ        | 1202/6000 [57:15<4:17:55,  3.23s/it]                                                     {'loss': 2.7592, 'grad_norm': 7.866242408752441, 'learning_rate': 4.066101694915254e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1202/6000 [57:15<4:17:55,  3.23s/it] 20%|â–ˆâ–ˆ        | 1203/6000 [57:17<4:05:01,  3.06s/it]                                                     {'loss': 2.7898, 'grad_norm': 10.812979698181152, 'learning_rate': 4.065254237288136e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1203/6000 [57:17<4:05:01,  3.06s/it] 20%|â–ˆâ–ˆ        | 1204/6000 [57:21<4:07:55,  3.10s/it]                                                     {'loss': 2.7605, 'grad_norm': 15.838869094848633, 'learning_rate': 4.0644067796610174e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1204/6000 [57:21<4:07:55,  3.10s/it] 20%|â–ˆâ–ˆ        | 1205/6000 [57:23<4:01:37,  3.02s/it]                                                     {'loss': 2.782, 'grad_norm': 9.145169258117676, 'learning_rate': 4.063559322033899e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1205/6000 [57:23<4:01:37,  3.02s/it] 20%|â–ˆâ–ˆ        | 1206/6000 [57:26<3:55:54,  2.95s/it]                                                     {'loss': 2.8178, 'grad_norm': 13.149556159973145, 'learning_rate': 4.06271186440678e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1206/6000 [57:26<3:55:54,  2.95s/it] 20%|â–ˆâ–ˆ        | 1207/6000 [57:29<3:49:46,  2.88s/it]                                                     {'loss': 2.7354, 'grad_norm': 15.19058609008789, 'learning_rate': 4.0618644067796615e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1207/6000 [57:29<3:49:46,  2.88s/it] 20%|â–ˆâ–ˆ        | 1208/6000 [57:32<3:45:02,  2.82s/it]                                                     {'loss': 2.7826, 'grad_norm': 11.39135456085205, 'learning_rate': 4.061016949152542e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1208/6000 [57:32<3:45:02,  2.82s/it] 20%|â–ˆâ–ˆ        | 1209/6000 [57:34<3:42:05,  2.78s/it]                                                     {'loss': 2.7537, 'grad_norm': 10.516786575317383, 'learning_rate': 4.060169491525424e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1209/6000 [57:34<3:42:05,  2.78s/it] 20%|â–ˆâ–ˆ        | 1210/6000 [57:37<3:42:50,  2.79s/it]                                                     {'loss': 2.8256, 'grad_norm': 19.80512046813965, 'learning_rate': 4.0593220338983056e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1210/6000 [57:37<3:42:50,  2.79s/it] 20%|â–ˆâ–ˆ        | 1211/6000 [57:40<3:41:54,  2.78s/it]                                                     {'loss': 2.7598, 'grad_norm': 7.677234649658203, 'learning_rate': 4.058474576271187e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1211/6000 [57:40<3:41:54,  2.78s/it] 20%|â–ˆâ–ˆ        | 1212/6000 [57:43<3:40:06,  2.76s/it]                                                     {'loss': 2.7283, 'grad_norm': 10.5970458984375, 'learning_rate': 4.057627118644068e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1212/6000 [57:43<3:40:06,  2.76s/it] 20%|â–ˆâ–ˆ        | 1213/6000 [57:45<3:38:39,  2.74s/it]                                                     {'loss': 2.8141, 'grad_norm': 15.52763843536377, 'learning_rate': 4.05677966101695e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1213/6000 [57:45<3:38:39,  2.74s/it] 20%|â–ˆâ–ˆ        | 1214/6000 [57:48<3:40:06,  2.76s/it]                                                     {'loss': 2.7631, 'grad_norm': 16.847782135009766, 'learning_rate': 4.0559322033898304e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1214/6000 [57:48<3:40:06,  2.76s/it] 20%|â–ˆâ–ˆ        | 1215/6000 [57:51<3:39:09,  2.75s/it]                                                     {'loss': 2.7391, 'grad_norm': 15.326147079467773, 'learning_rate': 4.055084745762712e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1215/6000 [57:51<3:39:09,  2.75s/it] 20%|â–ˆâ–ˆ        | 1216/6000 [57:53<3:36:49,  2.72s/it]                                                     {'loss': 2.7915, 'grad_norm': 10.340187072753906, 'learning_rate': 4.054237288135594e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1216/6000 [57:53<3:36:49,  2.72s/it] 20%|â–ˆâ–ˆ        | 1217/6000 [57:57<3:46:56,  2.85s/it]                                                     {'loss': 2.8039, 'grad_norm': 13.225528717041016, 'learning_rate': 4.053389830508475e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1217/6000 [57:57<3:46:56,  2.85s/it] 20%|â–ˆâ–ˆ        | 1218/6000 [57:59<3:44:26,  2.82s/it]                                                     {'loss': 2.7897, 'grad_norm': 14.244672775268555, 'learning_rate': 4.052542372881356e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1218/6000 [57:59<3:44:26,  2.82s/it] 20%|â–ˆâ–ˆ        | 1219/6000 [58:02<3:48:37,  2.87s/it]                                                     {'loss': 2.7884, 'grad_norm': 13.951982498168945, 'learning_rate': 4.051694915254238e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1219/6000 [58:02<3:48:37,  2.87s/it] 20%|â–ˆâ–ˆ        | 1220/6000 [58:05<3:45:41,  2.83s/it]                                                     {'loss': 2.7906, 'grad_norm': 8.380849838256836, 'learning_rate': 4.0508474576271186e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1220/6000 [58:05<3:45:41,  2.83s/it] 20%|â–ˆâ–ˆ        | 1221/6000 [58:08<3:44:35,  2.82s/it]                                                     {'loss': 2.771, 'grad_norm': 16.054298400878906, 'learning_rate': 4.05e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1221/6000 [58:08<3:44:35,  2.82s/it] 20%|â–ˆâ–ˆ        | 1222/6000 [58:11<3:45:12,  2.83s/it]                                                     {'loss': 2.7509, 'grad_norm': 9.297952651977539, 'learning_rate': 4.049152542372882e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1222/6000 [58:11<3:45:12,  2.83s/it] 20%|â–ˆâ–ˆ        | 1223/6000 [58:13<3:42:32,  2.80s/it]                                                     {'loss': 2.7683, 'grad_norm': 8.163382530212402, 'learning_rate': 4.048305084745763e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1223/6000 [58:13<3:42:32,  2.80s/it] 20%|â–ˆâ–ˆ        | 1224/6000 [58:16<3:41:51,  2.79s/it]                                                     {'loss': 2.8343, 'grad_norm': 9.459288597106934, 'learning_rate': 4.047457627118644e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1224/6000 [58:16<3:41:51,  2.79s/it] 20%|â–ˆâ–ˆ        | 1225/6000 [58:19<3:46:04,  2.84s/it]                                                     {'loss': 2.7869, 'grad_norm': 8.56827163696289, 'learning_rate': 4.046610169491526e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1225/6000 [58:19<3:46:04,  2.84s/it] 20%|â–ˆâ–ˆ        | 1226/6000 [58:22<3:50:05,  2.89s/it]                                                     {'loss': 2.753, 'grad_norm': 15.158008575439453, 'learning_rate': 4.0457627118644075e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1226/6000 [58:22<3:50:05,  2.89s/it] 20%|â–ˆâ–ˆ        | 1227/6000 [58:25<3:54:49,  2.95s/it]                                                     {'loss': 2.7338, 'grad_norm': 17.99371910095215, 'learning_rate': 4.044915254237288e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1227/6000 [58:25<3:54:49,  2.95s/it] 20%|â–ˆâ–ˆ        | 1228/6000 [58:28<3:54:18,  2.95s/it]                                                     {'loss': 2.8053, 'grad_norm': 10.505538940429688, 'learning_rate': 4.04406779661017e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1228/6000 [58:28<3:54:18,  2.95s/it] 20%|â–ˆâ–ˆ        | 1229/6000 [58:31<3:47:02,  2.86s/it]                                                     {'loss': 2.7733, 'grad_norm': 11.80193042755127, 'learning_rate': 4.043220338983051e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1229/6000 [58:31<3:47:02,  2.86s/it] 20%|â–ˆâ–ˆ        | 1230/6000 [58:34<3:44:06,  2.82s/it]                                                     {'loss': 2.7766, 'grad_norm': 8.927644729614258, 'learning_rate': 4.042372881355932e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1230/6000 [58:34<3:44:06,  2.82s/it] 21%|â–ˆâ–ˆ        | 1231/6000 [58:36<3:42:07,  2.79s/it]                                                     {'loss': 2.7961, 'grad_norm': 10.524155616760254, 'learning_rate': 4.041525423728814e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1231/6000 [58:36<3:42:07,  2.79s/it] 21%|â–ˆâ–ˆ        | 1232/6000 [58:39<3:39:33,  2.76s/it]                                                     {'loss': 2.7335, 'grad_norm': 8.140021324157715, 'learning_rate': 4.040677966101696e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1232/6000 [58:39<3:39:33,  2.76s/it] 21%|â–ˆâ–ˆ        | 1233/6000 [58:42<3:45:52,  2.84s/it]                                                     {'loss': 2.7961, 'grad_norm': 12.037238121032715, 'learning_rate': 4.0398305084745764e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1233/6000 [58:42<3:45:52,  2.84s/it] 21%|â–ˆâ–ˆ        | 1234/6000 [58:45<3:53:09,  2.94s/it]                                                     {'loss': 2.7539, 'grad_norm': 9.58534049987793, 'learning_rate': 4.038983050847458e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1234/6000 [58:45<3:53:09,  2.94s/it] 21%|â–ˆâ–ˆ        | 1235/6000 [58:48<3:48:39,  2.88s/it]                                                     {'loss': 2.7567, 'grad_norm': 7.889684677124023, 'learning_rate': 4.038135593220339e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1235/6000 [58:48<3:48:39,  2.88s/it] 21%|â–ˆâ–ˆ        | 1236/6000 [58:51<3:44:52,  2.83s/it]                                                     {'loss': 2.8252, 'grad_norm': 6.824163436889648, 'learning_rate': 4.0372881355932205e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1236/6000 [58:51<3:44:52,  2.83s/it] 21%|â–ˆâ–ˆ        | 1237/6000 [58:53<3:42:53,  2.81s/it]                                                     {'loss': 2.7624, 'grad_norm': 6.774433612823486, 'learning_rate': 4.036440677966102e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1237/6000 [58:53<3:42:53,  2.81s/it] 21%|â–ˆâ–ˆ        | 1238/6000 [58:56<3:41:02,  2.78s/it]                                                     {'loss': 2.7412, 'grad_norm': 17.11113166809082, 'learning_rate': 4.035593220338984e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1238/6000 [58:56<3:41:02,  2.78s/it] 21%|â–ˆâ–ˆ        | 1239/6000 [58:59<3:38:53,  2.76s/it]                                                     {'loss': 2.7749, 'grad_norm': 6.695555686950684, 'learning_rate': 4.0347457627118646e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1239/6000 [58:59<3:38:53,  2.76s/it] 21%|â–ˆâ–ˆ        | 1240/6000 [59:02<3:38:36,  2.76s/it]                                                     {'loss': 2.7592, 'grad_norm': 7.220369815826416, 'learning_rate': 4.033898305084746e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1240/6000 [59:02<3:38:36,  2.76s/it] 21%|â–ˆâ–ˆ        | 1241/6000 [59:04<3:36:09,  2.73s/it]                                                     {'loss': 2.7379, 'grad_norm': 20.494548797607422, 'learning_rate': 4.033050847457628e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1241/6000 [59:04<3:36:09,  2.73s/it] 21%|â–ˆâ–ˆ        | 1242/6000 [59:07<3:39:25,  2.77s/it]                                                     {'loss': 2.767, 'grad_norm': 12.021984100341797, 'learning_rate': 4.0322033898305095e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1242/6000 [59:07<3:39:25,  2.77s/it] 21%|â–ˆâ–ˆ        | 1243/6000 [59:10<3:39:49,  2.77s/it]                                                     {'loss': 2.7901, 'grad_norm': 9.904083251953125, 'learning_rate': 4.03135593220339e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1243/6000 [59:10<3:39:49,  2.77s/it] 21%|â–ˆâ–ˆ        | 1244/6000 [59:13<3:38:47,  2.76s/it]                                                     {'loss': 2.7163, 'grad_norm': 11.0619478225708, 'learning_rate': 4.030508474576271e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1244/6000 [59:13<3:38:47,  2.76s/it] 21%|â–ˆâ–ˆ        | 1245/6000 [59:15<3:38:05,  2.75s/it]                                                     {'loss': 2.758, 'grad_norm': 8.384634017944336, 'learning_rate': 4.029661016949153e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1245/6000 [59:15<3:38:05,  2.75s/it] 21%|â–ˆâ–ˆ        | 1246/6000 [59:18<3:36:44,  2.74s/it]                                                     {'loss': 2.7895, 'grad_norm': 7.3700690269470215, 'learning_rate': 4.028813559322034e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1246/6000 [59:18<3:36:44,  2.74s/it] 21%|â–ˆâ–ˆ        | 1247/6000 [59:21<3:38:34,  2.76s/it]                                                     {'loss': 2.8462, 'grad_norm': 12.213135719299316, 'learning_rate': 4.027966101694916e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1247/6000 [59:21<3:38:34,  2.76s/it] 21%|â–ˆâ–ˆ        | 1248/6000 [59:24<3:38:53,  2.76s/it]                                                     {'loss': 2.763, 'grad_norm': 13.866438865661621, 'learning_rate': 4.027118644067797e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1248/6000 [59:24<3:38:53,  2.76s/it] 21%|â–ˆâ–ˆ        | 1249/6000 [59:26<3:38:01,  2.75s/it]                                                     {'loss': 2.7636, 'grad_norm': 10.3563871383667, 'learning_rate': 4.026271186440678e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1249/6000 [59:26<3:38:01,  2.75s/it] 21%|â–ˆâ–ˆ        | 1250/6000 [59:29<3:35:42,  2.72s/it]                                                     {'loss': 2.7549, 'grad_norm': 8.999597549438477, 'learning_rate': 4.025423728813559e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1250/6000 [59:29<3:35:42,  2.72s/it][2025-10-23 01:25:58,268] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1250
[2025-10-23 01:25:58,284] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:25:58,913] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 21%|â–ˆâ–ˆ        | 1251/6000 [59:34<4:28:36,  3.39s/it]                                                     {'loss': 2.7903, 'grad_norm': 12.348910331726074, 'learning_rate': 4.024576271186441e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1251/6000 [59:34<4:28:36,  3.39s/it] 21%|â–ˆâ–ˆ        | 1252/6000 [59:37<4:11:42,  3.18s/it]                                                     {'loss': 2.7722, 'grad_norm': 17.253084182739258, 'learning_rate': 4.0237288135593225e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1252/6000 [59:37<4:11:42,  3.18s/it] 21%|â–ˆâ–ˆ        | 1253/6000 [59:40<4:14:19,  3.21s/it]                                                     {'loss': 2.8119, 'grad_norm': 12.419187545776367, 'learning_rate': 4.022881355932204e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1253/6000 [59:40<4:14:19,  3.21s/it] 21%|â–ˆâ–ˆ        | 1254/6000 [59:43<4:01:40,  3.06s/it]                                                     {'loss': 2.7286, 'grad_norm': 8.240923881530762, 'learning_rate': 4.022033898305085e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1254/6000 [59:43<4:01:40,  3.06s/it] 21%|â–ˆâ–ˆ        | 1255/6000 [59:45<3:53:17,  2.95s/it]                                                     {'loss': 2.7131, 'grad_norm': 21.496322631835938, 'learning_rate': 4.0211864406779665e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1255/6000 [59:45<3:53:17,  2.95s/it] 21%|â–ˆâ–ˆ        | 1256/6000 [59:48<3:47:57,  2.88s/it]                                                     {'loss': 2.8352, 'grad_norm': 15.047905921936035, 'learning_rate': 4.020338983050847e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1256/6000 [59:48<3:47:57,  2.88s/it] 21%|â–ˆâ–ˆ        | 1257/6000 [59:51<3:42:43,  2.82s/it]                                                     {'loss': 2.8061, 'grad_norm': 12.012898445129395, 'learning_rate': 4.019491525423729e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1257/6000 [59:51<3:42:43,  2.82s/it] 21%|â–ˆâ–ˆ        | 1258/6000 [59:53<3:42:41,  2.82s/it]                                                     {'loss': 2.8485, 'grad_norm': 26.306474685668945, 'learning_rate': 4.018644067796611e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1258/6000 [59:54<3:42:41,  2.82s/it] 21%|â–ˆâ–ˆ        | 1259/6000 [59:56<3:40:26,  2.79s/it]                                                     {'loss': 2.8187, 'grad_norm': 11.157896995544434, 'learning_rate': 4.017796610169492e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1259/6000 [59:56<3:40:26,  2.79s/it] 21%|â–ˆâ–ˆ        | 1260/6000 [59:59<3:38:08,  2.76s/it]                                                     {'loss': 2.7637, 'grad_norm': 16.37080192565918, 'learning_rate': 4.016949152542373e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1260/6000 [59:59<3:38:08,  2.76s/it] 21%|â–ˆâ–ˆ        | 1261/6000 [1:00:02<3:37:01,  2.75s/it]                                                       {'loss': 2.7243, 'grad_norm': 23.40699005126953, 'learning_rate': 4.016101694915255e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1261/6000 [1:00:02<3:37:01,  2.75s/it] 21%|â–ˆâ–ˆ        | 1262/6000 [1:00:05<3:44:47,  2.85s/it]                                                       {'loss': 2.7844, 'grad_norm': 12.916358947753906, 'learning_rate': 4.015254237288136e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1262/6000 [1:00:05<3:44:47,  2.85s/it] 21%|â–ˆâ–ˆ        | 1263/6000 [1:00:07<3:43:20,  2.83s/it]                                                       {'loss': 2.8609, 'grad_norm': 27.79307746887207, 'learning_rate': 4.014406779661018e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1263/6000 [1:00:08<3:43:20,  2.83s/it] 21%|â–ˆâ–ˆ        | 1264/6000 [1:00:10<3:43:11,  2.83s/it]                                                       {'loss': 2.7678, 'grad_norm': 8.719095230102539, 'learning_rate': 4.013559322033899e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1264/6000 [1:00:10<3:43:11,  2.83s/it] 21%|â–ˆâ–ˆ        | 1265/6000 [1:00:13<3:42:38,  2.82s/it]                                                       {'loss': 2.7482, 'grad_norm': 10.245672225952148, 'learning_rate': 4.0127118644067795e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1265/6000 [1:00:13<3:42:38,  2.82s/it] 21%|â–ˆâ–ˆ        | 1266/6000 [1:00:16<3:43:55,  2.84s/it]                                                       {'loss': 2.7684, 'grad_norm': 15.287981986999512, 'learning_rate': 4.011864406779661e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1266/6000 [1:00:16<3:43:55,  2.84s/it] 21%|â–ˆâ–ˆ        | 1267/6000 [1:00:19<3:48:17,  2.89s/it]                                                       {'loss': 2.789, 'grad_norm': 9.54421329498291, 'learning_rate': 4.011016949152543e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1267/6000 [1:00:19<3:48:17,  2.89s/it] 21%|â–ˆâ–ˆ        | 1268/6000 [1:00:22<3:44:08,  2.84s/it]                                                       {'loss': 2.7337, 'grad_norm': 14.028464317321777, 'learning_rate': 4.010169491525424e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1268/6000 [1:00:22<3:44:08,  2.84s/it] 21%|â–ˆâ–ˆ        | 1269/6000 [1:00:25<3:43:31,  2.83s/it]                                                       {'loss': 2.793, 'grad_norm': 11.577402114868164, 'learning_rate': 4.009322033898305e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1269/6000 [1:00:25<3:43:31,  2.83s/it] 21%|â–ˆâ–ˆ        | 1270/6000 [1:00:27<3:41:05,  2.80s/it]                                                       {'loss': 2.8067, 'grad_norm': 17.97649383544922, 'learning_rate': 4.008474576271187e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1270/6000 [1:00:27<3:41:05,  2.80s/it] 21%|â–ˆâ–ˆ        | 1271/6000 [1:00:30<3:43:35,  2.84s/it]                                                       {'loss': 2.7126, 'grad_norm': 16.9288330078125, 'learning_rate': 4.007627118644068e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1271/6000 [1:00:30<3:43:35,  2.84s/it] 21%|â–ˆâ–ˆ        | 1272/6000 [1:00:33<3:49:00,  2.91s/it]                                                       {'loss': 2.7537, 'grad_norm': 13.822159767150879, 'learning_rate': 4.006779661016949e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1272/6000 [1:00:33<3:49:00,  2.91s/it] 21%|â–ˆâ–ˆ        | 1273/6000 [1:00:36<3:46:24,  2.87s/it]                                                       {'loss': 2.7601, 'grad_norm': 12.666173934936523, 'learning_rate': 4.005932203389831e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1273/6000 [1:00:36<3:46:24,  2.87s/it] 21%|â–ˆâ–ˆ        | 1274/6000 [1:00:39<3:40:42,  2.80s/it]                                                       {'loss': 2.8458, 'grad_norm': 22.659046173095703, 'learning_rate': 4.0050847457627125e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1274/6000 [1:00:39<3:40:42,  2.80s/it] 21%|â–ˆâ–ˆâ–       | 1275/6000 [1:00:41<3:39:18,  2.78s/it]                                                       {'loss': 2.6905, 'grad_norm': 23.939054489135742, 'learning_rate': 4.004237288135593e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1275/6000 [1:00:41<3:39:18,  2.78s/it] 21%|â–ˆâ–ˆâ–       | 1276/6000 [1:00:44<3:40:13,  2.80s/it]                                                       {'loss': 2.7221, 'grad_norm': 26.58302879333496, 'learning_rate': 4.003389830508475e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1276/6000 [1:00:44<3:40:13,  2.80s/it] 21%|â–ˆâ–ˆâ–       | 1277/6000 [1:00:47<3:39:19,  2.79s/it]                                                       {'loss': 2.8081, 'grad_norm': 14.505925178527832, 'learning_rate': 4.002542372881356e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1277/6000 [1:00:47<3:39:19,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1278/6000 [1:00:50<3:39:10,  2.78s/it]                                                       {'loss': 2.7618, 'grad_norm': 12.455907821655273, 'learning_rate': 4.001694915254237e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1278/6000 [1:00:50<3:39:10,  2.78s/it] 21%|â–ˆâ–ˆâ–       | 1279/6000 [1:00:53<3:37:02,  2.76s/it]                                                       {'loss': 2.7487, 'grad_norm': 10.65434455871582, 'learning_rate': 4.000847457627119e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1279/6000 [1:00:53<3:37:02,  2.76s/it] 21%|â–ˆâ–ˆâ–       | 1280/6000 [1:00:55<3:36:47,  2.76s/it]                                                       {'loss': 2.8302, 'grad_norm': 20.57706642150879, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1280/6000 [1:00:55<3:36:47,  2.76s/it] 21%|â–ˆâ–ˆâ–       | 1281/6000 [1:00:58<3:37:17,  2.76s/it]                                                       {'loss': 2.6497, 'grad_norm': 34.863624572753906, 'learning_rate': 3.9991525423728815e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1281/6000 [1:00:58<3:37:17,  2.76s/it] 21%|â–ˆâ–ˆâ–       | 1282/6000 [1:01:01<3:35:32,  2.74s/it]                                                       {'loss': 2.8473, 'grad_norm': 20.519847869873047, 'learning_rate': 3.998305084745763e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1282/6000 [1:01:01<3:35:32,  2.74s/it] 21%|â–ˆâ–ˆâ–       | 1283/6000 [1:01:03<3:33:19,  2.71s/it]                                                       {'loss': 2.7345, 'grad_norm': 21.060396194458008, 'learning_rate': 3.997457627118645e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1283/6000 [1:01:03<3:33:19,  2.71s/it] 21%|â–ˆâ–ˆâ–       | 1284/6000 [1:01:06<3:34:53,  2.73s/it]                                                       {'loss': 2.7701, 'grad_norm': 15.803753852844238, 'learning_rate': 3.996610169491526e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1284/6000 [1:01:06<3:34:53,  2.73s/it] 21%|â–ˆâ–ˆâ–       | 1285/6000 [1:01:09<3:35:05,  2.74s/it]                                                       {'loss': 2.8478, 'grad_norm': 21.226350784301758, 'learning_rate': 3.995762711864407e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1285/6000 [1:01:09<3:35:05,  2.74s/it] 21%|â–ˆâ–ˆâ–       | 1286/6000 [1:01:12<3:37:35,  2.77s/it]                                                       {'loss': 2.6478, 'grad_norm': 19.652164459228516, 'learning_rate': 3.994915254237288e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1286/6000 [1:01:12<3:37:35,  2.77s/it] 21%|â–ˆâ–ˆâ–       | 1287/6000 [1:01:15<3:39:43,  2.80s/it]                                                       {'loss': 2.7997, 'grad_norm': 14.155375480651855, 'learning_rate': 3.99406779661017e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1287/6000 [1:01:15<3:39:43,  2.80s/it] 21%|â–ˆâ–ˆâ–       | 1288/6000 [1:01:17<3:38:44,  2.79s/it]                                                       {'loss': 2.8196, 'grad_norm': 16.929105758666992, 'learning_rate': 3.993220338983051e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1288/6000 [1:01:17<3:38:44,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1289/6000 [1:01:20<3:35:32,  2.75s/it]                                                       {'loss': 2.8013, 'grad_norm': 18.67522621154785, 'learning_rate': 3.992372881355933e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1289/6000 [1:01:20<3:35:32,  2.75s/it] 22%|â–ˆâ–ˆâ–       | 1290/6000 [1:01:23<3:36:56,  2.76s/it]                                                       {'loss': 2.7535, 'grad_norm': 26.811769485473633, 'learning_rate': 3.991525423728814e-06, 'epoch': 0.21}
 22%|â–ˆâ–ˆâ–       | 1290/6000 [1:01:23<3:36:56,  2.76s/it] 22%|â–ˆâ–ˆâ–       | 1291/6000 [1:01:26<3:34:34,  2.73s/it]                                                       {'loss': 2.797, 'grad_norm': 13.08281421661377, 'learning_rate': 3.990677966101695e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1291/6000 [1:01:26<3:34:34,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1292/6000 [1:01:28<3:33:33,  2.72s/it]                                                       {'loss': 2.753, 'grad_norm': 23.279338836669922, 'learning_rate': 3.989830508474576e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1292/6000 [1:01:28<3:33:33,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1293/6000 [1:01:31<3:36:17,  2.76s/it]                                                       {'loss': 2.7418, 'grad_norm': 9.03260326385498, 'learning_rate': 3.988983050847458e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1293/6000 [1:01:31<3:36:17,  2.76s/it] 22%|â–ˆâ–ˆâ–       | 1294/6000 [1:01:34<3:34:58,  2.74s/it]                                                       {'loss': 2.777, 'grad_norm': 18.408418655395508, 'learning_rate': 3.988135593220339e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1294/6000 [1:01:34<3:34:58,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1295/6000 [1:01:36<3:33:53,  2.73s/it]                                                       {'loss': 2.7928, 'grad_norm': 15.54373836517334, 'learning_rate': 3.987288135593221e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1295/6000 [1:01:36<3:33:53,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1296/6000 [1:01:39<3:35:19,  2.75s/it]                                                       {'loss': 2.7813, 'grad_norm': 9.301858901977539, 'learning_rate': 3.986440677966102e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1296/6000 [1:01:39<3:35:19,  2.75s/it] 22%|â–ˆâ–ˆâ–       | 1297/6000 [1:01:42<3:41:30,  2.83s/it]                                                       {'loss': 2.7489, 'grad_norm': 14.415176391601562, 'learning_rate': 3.985593220338983e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1297/6000 [1:01:42<3:41:30,  2.83s/it] 22%|â–ˆâ–ˆâ–       | 1298/6000 [1:01:45<3:38:39,  2.79s/it]                                                       {'loss': 2.806, 'grad_norm': 10.924962043762207, 'learning_rate': 3.984745762711865e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1298/6000 [1:01:45<3:38:39,  2.79s/it] 22%|â–ˆâ–ˆâ–       | 1299/6000 [1:01:48<3:38:10,  2.78s/it]                                                       {'loss': 2.7541, 'grad_norm': 8.435687065124512, 'learning_rate': 3.983898305084747e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1299/6000 [1:01:48<3:38:10,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1300/6000 [1:01:50<3:37:02,  2.77s/it]                                                       {'loss': 2.7857, 'grad_norm': 10.180536270141602, 'learning_rate': 3.9830508474576275e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1300/6000 [1:01:50<3:37:02,  2.77s/it][2025-10-23 01:28:19,768] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1300
[2025-10-23 01:28:19,781] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:28:20,412] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 22%|â–ˆâ–ˆâ–       | 1301/6000 [1:01:56<4:30:42,  3.46s/it]                                                       {'loss': 2.8458, 'grad_norm': 12.74909782409668, 'learning_rate': 3.982203389830509e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1301/6000 [1:01:56<4:30:42,  3.46s/it] 22%|â–ˆâ–ˆâ–       | 1302/6000 [1:01:58<4:18:08,  3.30s/it]                                                       {'loss': 2.7803, 'grad_norm': 18.94158363342285, 'learning_rate': 3.98135593220339e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1302/6000 [1:01:58<4:18:08,  3.30s/it] 22%|â–ˆâ–ˆâ–       | 1303/6000 [1:02:01<4:03:49,  3.11s/it]                                                       {'loss': 2.7994, 'grad_norm': 9.274360656738281, 'learning_rate': 3.9805084745762715e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1303/6000 [1:02:01<4:03:49,  3.11s/it] 22%|â–ˆâ–ˆâ–       | 1304/6000 [1:02:04<3:55:08,  3.00s/it]                                                       {'loss': 2.7938, 'grad_norm': 13.887825965881348, 'learning_rate': 3.979661016949153e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1304/6000 [1:02:04<3:55:08,  3.00s/it] 22%|â–ˆâ–ˆâ–       | 1305/6000 [1:02:07<3:46:44,  2.90s/it]                                                       {'loss': 2.8089, 'grad_norm': 9.286626815795898, 'learning_rate': 3.978813559322034e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1305/6000 [1:02:07<3:46:44,  2.90s/it] 22%|â–ˆâ–ˆâ–       | 1306/6000 [1:02:09<3:44:33,  2.87s/it]                                                       {'loss': 2.7679, 'grad_norm': 8.912604331970215, 'learning_rate': 3.977966101694916e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1306/6000 [1:02:09<3:44:33,  2.87s/it] 22%|â–ˆâ–ˆâ–       | 1307/6000 [1:02:12<3:40:20,  2.82s/it]                                                       {'loss': 2.7704, 'grad_norm': 11.97972583770752, 'learning_rate': 3.977118644067796e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1307/6000 [1:02:12<3:40:20,  2.82s/it] 22%|â–ˆâ–ˆâ–       | 1308/6000 [1:02:15<3:40:00,  2.81s/it]                                                       {'loss': 2.7474, 'grad_norm': 9.566326141357422, 'learning_rate': 3.976271186440678e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1308/6000 [1:02:15<3:40:00,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1309/6000 [1:02:18<3:37:10,  2.78s/it]                                                       {'loss': 2.7589, 'grad_norm': 7.4042181968688965, 'learning_rate': 3.97542372881356e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1309/6000 [1:02:18<3:37:10,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1310/6000 [1:02:20<3:37:13,  2.78s/it]                                                       {'loss': 2.7223, 'grad_norm': 13.522359848022461, 'learning_rate': 3.974576271186441e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1310/6000 [1:02:20<3:37:13,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1311/6000 [1:02:23<3:34:58,  2.75s/it]                                                       {'loss': 2.8433, 'grad_norm': 11.108424186706543, 'learning_rate': 3.973728813559322e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1311/6000 [1:02:23<3:34:58,  2.75s/it] 22%|â–ˆâ–ˆâ–       | 1312/6000 [1:02:26<3:33:22,  2.73s/it]                                                       {'loss': 2.8008, 'grad_norm': 11.25778579711914, 'learning_rate': 3.972881355932204e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1312/6000 [1:02:26<3:33:22,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1313/6000 [1:02:28<3:32:41,  2.72s/it]                                                       {'loss': 2.8139, 'grad_norm': 19.89704132080078, 'learning_rate': 3.9720338983050845e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1313/6000 [1:02:28<3:32:41,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1314/6000 [1:02:31<3:31:00,  2.70s/it]                                                       {'loss': 2.7696, 'grad_norm': 8.012975692749023, 'learning_rate': 3.971186440677966e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1314/6000 [1:02:31<3:31:00,  2.70s/it] 22%|â–ˆâ–ˆâ–       | 1315/6000 [1:02:34<3:30:47,  2.70s/it]                                                       {'loss': 2.7779, 'grad_norm': 10.190255165100098, 'learning_rate': 3.970338983050848e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1315/6000 [1:02:34<3:30:47,  2.70s/it] 22%|â–ˆâ–ˆâ–       | 1316/6000 [1:02:37<3:32:56,  2.73s/it]                                                       {'loss': 2.8117, 'grad_norm': 13.11198616027832, 'learning_rate': 3.9694915254237294e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1316/6000 [1:02:37<3:32:56,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1317/6000 [1:02:39<3:31:57,  2.72s/it]                                                       {'loss': 2.716, 'grad_norm': 12.873653411865234, 'learning_rate': 3.96864406779661e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1317/6000 [1:02:39<3:31:57,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1318/6000 [1:02:42<3:32:29,  2.72s/it]                                                       {'loss': 2.7849, 'grad_norm': 10.464298248291016, 'learning_rate': 3.967796610169492e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1318/6000 [1:02:42<3:32:29,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1319/6000 [1:02:45<3:30:23,  2.70s/it]                                                       {'loss': 2.7951, 'grad_norm': 6.580348968505859, 'learning_rate': 3.9669491525423735e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1319/6000 [1:02:45<3:30:23,  2.70s/it] 22%|â–ˆâ–ˆâ–       | 1320/6000 [1:02:47<3:31:24,  2.71s/it]                                                       {'loss': 2.7831, 'grad_norm': 9.544878959655762, 'learning_rate': 3.966101694915255e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1320/6000 [1:02:47<3:31:24,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1321/6000 [1:02:50<3:31:44,  2.72s/it]                                                       {'loss': 2.7481, 'grad_norm': 11.568605422973633, 'learning_rate': 3.965254237288136e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1321/6000 [1:02:50<3:31:44,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1322/6000 [1:02:53<3:43:06,  2.86s/it]                                                       {'loss': 2.7596, 'grad_norm': 11.1865816116333, 'learning_rate': 3.9644067796610176e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1322/6000 [1:02:53<3:43:06,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1323/6000 [1:02:56<3:42:38,  2.86s/it]                                                       {'loss': 2.7705, 'grad_norm': 9.613897323608398, 'learning_rate': 3.963559322033898e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1323/6000 [1:02:56<3:42:38,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1324/6000 [1:02:59<3:38:55,  2.81s/it]                                                       {'loss': 2.8604, 'grad_norm': 6.549341678619385, 'learning_rate': 3.96271186440678e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1324/6000 [1:02:59<3:38:55,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1325/6000 [1:03:02<3:36:59,  2.78s/it]                                                       {'loss': 2.8059, 'grad_norm': 7.85504674911499, 'learning_rate': 3.961864406779662e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1325/6000 [1:03:02<3:36:59,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1326/6000 [1:03:04<3:36:27,  2.78s/it]                                                       {'loss': 2.7452, 'grad_norm': 9.058672904968262, 'learning_rate': 3.961016949152542e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1326/6000 [1:03:04<3:36:27,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1327/6000 [1:03:07<3:36:37,  2.78s/it]                                                       {'loss': 2.7253, 'grad_norm': 9.38473129272461, 'learning_rate': 3.960169491525424e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1327/6000 [1:03:07<3:36:37,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1328/6000 [1:03:10<3:36:33,  2.78s/it]                                                       {'loss': 2.834, 'grad_norm': 14.200714111328125, 'learning_rate': 3.959322033898305e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1328/6000 [1:03:10<3:36:33,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1329/6000 [1:03:13<3:44:05,  2.88s/it]                                                       {'loss': 2.8054, 'grad_norm': 15.076180458068848, 'learning_rate': 3.9584745762711865e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1329/6000 [1:03:13<3:44:05,  2.88s/it] 22%|â–ˆâ–ˆâ–       | 1330/6000 [1:03:16<3:40:45,  2.84s/it]                                                       {'loss': 2.8391, 'grad_norm': 7.524864196777344, 'learning_rate': 3.957627118644068e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1330/6000 [1:03:16<3:40:45,  2.84s/it] 22%|â–ˆâ–ˆâ–       | 1331/6000 [1:03:18<3:38:31,  2.81s/it]                                                       {'loss': 2.7693, 'grad_norm': 8.600481986999512, 'learning_rate': 3.95677966101695e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1331/6000 [1:03:18<3:38:31,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1332/6000 [1:03:21<3:35:14,  2.77s/it]                                                       {'loss': 2.7761, 'grad_norm': 8.873353004455566, 'learning_rate': 3.9559322033898305e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1332/6000 [1:03:21<3:35:14,  2.77s/it] 22%|â–ˆâ–ˆâ–       | 1333/6000 [1:03:24<3:35:56,  2.78s/it]                                                       {'loss': 2.7658, 'grad_norm': 9.66427993774414, 'learning_rate': 3.955084745762712e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1333/6000 [1:03:24<3:35:56,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1334/6000 [1:03:27<3:35:20,  2.77s/it]                                                       {'loss': 2.7697, 'grad_norm': 7.108273029327393, 'learning_rate': 3.954237288135593e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1334/6000 [1:03:27<3:35:20,  2.77s/it] 22%|â–ˆâ–ˆâ–       | 1335/6000 [1:03:29<3:33:48,  2.75s/it]                                                       {'loss': 2.7818, 'grad_norm': 13.637503623962402, 'learning_rate': 3.9533898305084754e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1335/6000 [1:03:29<3:33:48,  2.75s/it] 22%|â–ˆâ–ˆâ–       | 1336/6000 [1:03:32<3:32:17,  2.73s/it]                                                       {'loss': 2.7583, 'grad_norm': 10.51673412322998, 'learning_rate': 3.952542372881356e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1336/6000 [1:03:32<3:32:17,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1337/6000 [1:03:35<3:31:42,  2.72s/it]                                                       {'loss': 2.7741, 'grad_norm': 12.099225044250488, 'learning_rate': 3.951694915254238e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1337/6000 [1:03:35<3:31:42,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1338/6000 [1:03:37<3:31:46,  2.73s/it]                                                       {'loss': 2.7612, 'grad_norm': 10.137809753417969, 'learning_rate': 3.950847457627119e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1338/6000 [1:03:37<3:31:46,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1339/6000 [1:03:40<3:30:56,  2.72s/it]                                                       {'loss': 2.7963, 'grad_norm': 9.427721977233887, 'learning_rate': 3.95e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1339/6000 [1:03:40<3:30:56,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1340/6000 [1:03:43<3:30:20,  2.71s/it]                                                       {'loss': 2.7598, 'grad_norm': 9.17480754852295, 'learning_rate': 3.949152542372882e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1340/6000 [1:03:43<3:30:20,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1341/6000 [1:03:46<3:30:08,  2.71s/it]                                                       {'loss': 2.7815, 'grad_norm': 10.638667106628418, 'learning_rate': 3.9483050847457636e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1341/6000 [1:03:46<3:30:08,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1342/6000 [1:03:49<3:38:22,  2.81s/it]                                                       {'loss': 2.8216, 'grad_norm': 10.184176445007324, 'learning_rate': 3.947457627118644e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1342/6000 [1:03:49<3:38:22,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1343/6000 [1:03:52<3:44:17,  2.89s/it]                                                       {'loss': 2.7457, 'grad_norm': 14.705023765563965, 'learning_rate': 3.946610169491526e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1343/6000 [1:03:52<3:44:17,  2.89s/it] 22%|â–ˆâ–ˆâ–       | 1344/6000 [1:03:54<3:40:00,  2.84s/it]                                                       {'loss': 2.8136, 'grad_norm': 12.378963470458984, 'learning_rate': 3.945762711864407e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1344/6000 [1:03:54<3:40:00,  2.84s/it] 22%|â–ˆâ–ˆâ–       | 1345/6000 [1:03:57<3:45:22,  2.90s/it]                                                       {'loss': 2.8028, 'grad_norm': 13.718011856079102, 'learning_rate': 3.9449152542372884e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1345/6000 [1:03:57<3:45:22,  2.90s/it] 22%|â–ˆâ–ˆâ–       | 1346/6000 [1:04:00<3:42:03,  2.86s/it]                                                       {'loss': 2.8017, 'grad_norm': 10.56856918334961, 'learning_rate': 3.94406779661017e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1346/6000 [1:04:00<3:42:03,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1347/6000 [1:04:03<3:41:39,  2.86s/it]                                                       {'loss': 2.7603, 'grad_norm': 14.46640682220459, 'learning_rate': 3.943220338983051e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1347/6000 [1:04:03<3:41:39,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1348/6000 [1:04:06<3:38:50,  2.82s/it]                                                       {'loss': 2.7977, 'grad_norm': 14.77863597869873, 'learning_rate': 3.9423728813559325e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1348/6000 [1:04:06<3:38:50,  2.82s/it] 22%|â–ˆâ–ˆâ–       | 1349/6000 [1:04:09<3:45:21,  2.91s/it]                                                       {'loss': 2.7261, 'grad_norm': 11.779464721679688, 'learning_rate': 3.941525423728813e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1349/6000 [1:04:09<3:45:21,  2.91s/it] 22%|â–ˆâ–ˆâ–Ž       | 1350/6000 [1:04:12<3:44:33,  2.90s/it]                                                       {'loss': 2.8461, 'grad_norm': 9.958648681640625, 'learning_rate': 3.940677966101695e-06, 'epoch': 0.23}
 22%|â–ˆâ–ˆâ–Ž       | 1350/6000 [1:04:12<3:44:33,  2.90s/it][2025-10-23 01:30:41,118] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1350
[2025-10-23 01:30:41,128] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-23 01:30:41,728] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test6-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1350/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 23%|â–ˆâ–ˆâ–Ž       | 1351/6000 [1:04:17<4:35:37,  3.56s/it]                                                       {'loss': 2.7457, 'grad_norm': 14.261907577514648, 'learning_rate': 3.9398305084745766e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1351/6000 [1:04:17<4:35:37,  3.56s/it] 23%|â–ˆâ–ˆâ–Ž       | 1352/6000 [1:04:20<4:16:44,  3.31s/it]                                                       {'loss': 2.7729, 'grad_norm': 11.370880126953125, 'learning_rate': 3.938983050847458e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1352/6000 [1:04:20<4:16:44,  3.31s/it] 23%|â–ˆâ–ˆâ–Ž       | 1353/6000 [1:04:23<4:07:24,  3.19s/it]                                                       {'loss': 2.7544, 'grad_norm': 9.680749893188477, 'learning_rate': 3.938135593220339e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1353/6000 [1:04:23<4:07:24,  3.19s/it] 23%|â–ˆâ–ˆâ–Ž       | 1354/6000 [1:04:25<3:57:20,  3.07s/it]                                                       {'loss': 2.8007, 'grad_norm': 9.99197006225586, 'learning_rate': 3.937288135593221e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1354/6000 [1:04:25<3:57:20,  3.07s/it] 23%|â–ˆâ–ˆâ–Ž       | 1355/6000 [1:04:28<3:52:18,  3.00s/it]                                                       {'loss': 2.7564, 'grad_norm': 9.646175384521484, 'learning_rate': 3.936440677966102e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1355/6000 [1:04:28<3:52:18,  3.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1356/6000 [1:04:31<3:59:21,  3.09s/it]                                                       {'loss': 2.758, 'grad_norm': 18.369821548461914, 'learning_rate': 3.935593220338984e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1356/6000 [1:04:32<3:59:21,  3.09s/it]