==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-6 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 14:48:40.804000 134457859213120 torch/distributed/run.py:779] 
W1103 14:48:40.804000 134457859213120 torch/distributed/run.py:779] *****************************************
W1103 14:48:40.804000 134457859213120 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 14:48:40.804000 134457859213120 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 14:48:50,861] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.63it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_144851-6yztbbx6
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/6yztbbx6
[2025-11-03 14:48:52,345] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.13it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.92it/s]
[2025-11-03 14:48:52,977] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 14:49:02,016] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 14:49:03,191] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 14:49:03,192] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 14:49:07,567] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 14:49:07,567] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 14:49:08,410] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 14:49:08,411] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 14:49:08,411] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 14:49:08,412] INFO [src.utils:19] ==================================================
[2025-11-03 14:49:08,413] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 14:49:08,413] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 14:49:08,414] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 14:49:08,414] INFO [src.utils:19] ==================================================
[2025-11-03 14:49:10,179] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 14:49:10,179] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 14:49:10,180] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 14:49:10,180] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 14:49:10,180] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 14:49:10,180] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 14:49:10,180] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 14:49:10,180] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 14:49:10,180] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 14:49:10,181] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 14:49:10,181] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 14:49:10,181] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 14:49:10,181] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 14:49:10,182] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 14:49:10,188] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 14:49:10,190] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 14:49:10,194] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 14:49:10,197] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1103 14:49:13.915528424 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1103 14:49:13.952933907 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:58:04,  4.18s/it]                                                  {'loss': 8.1137, 'grad_norm': 4374.57568359375, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:58:04,  4.18s/it]  0%|          | 2/6000 [00:06<5:32:34,  3.33s/it]                                                  {'loss': 8.5053, 'grad_norm': 3840.6572265625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:32:34,  3.33s/it]  0%|          | 3/6000 [00:09<5:10:30,  3.11s/it]                                                  {'loss': 8.9073, 'grad_norm': 4211.60205078125, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:10:30,  3.11s/it]  0%|          | 4/6000 [00:12<4:58:49,  2.99s/it]                                                  {'loss': 10.6459, 'grad_norm': 3485.071533203125, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:58:49,  2.99s/it]  0%|          | 5/6000 [00:15<4:51:52,  2.92s/it]                                                  {'loss': 8.2008, 'grad_norm': 4495.890625, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:15<4:51:52,  2.92s/it]  0%|          | 6/6000 [00:18<4:48:10,  2.88s/it]                                                  {'loss': 8.5586, 'grad_norm': 3600.57861328125, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:18<4:48:10,  2.88s/it]  0%|          | 7/6000 [00:20<4:44:36,  2.85s/it]                                                  {'loss': 8.0947, 'grad_norm': 3055.528076171875, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:44:36,  2.85s/it]  0%|          | 8/6000 [00:23<4:40:34,  2.81s/it]                                                  {'loss': 7.8984, 'grad_norm': 3273.618896484375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:23<4:40:34,  2.81s/it]  0%|          | 9/6000 [00:26<4:41:06,  2.82s/it]                                                  {'loss': 7.0845, 'grad_norm': 3317.48583984375, 'learning_rate': 4.5000000000000003e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:26<4:41:06,  2.82s/it]  0%|          | 10/6000 [00:29<4:37:29,  2.78s/it]                                                   {'loss': 7.7092, 'grad_norm': 2289.737548828125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 10/6000 [00:29<4:37:29,  2.78s/it]  0%|          | 11/6000 [00:32<4:48:08,  2.89s/it]                                                   {'loss': 9.0033, 'grad_norm': 2273.176513671875, 'learning_rate': 5.5e-07, 'epoch': 0.0}
  0%|          | 11/6000 [00:32<4:48:08,  2.89s/it]  0%|          | 12/6000 [00:35<4:51:20,  2.92s/it]                                                   {'loss': 6.9638, 'grad_norm': 2027.307861328125, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 12/6000 [00:35<4:51:20,  2.92s/it]  0%|          | 13/6000 [00:38<4:47:50,  2.88s/it]                                                   {'loss': 7.838, 'grad_norm': 1806.8653564453125, 'learning_rate': 6.5e-07, 'epoch': 0.0}
  0%|          | 13/6000 [00:38<4:47:50,  2.88s/it]  0%|          | 14/6000 [00:41<4:49:22,  2.90s/it]                                                   {'loss': 7.021, 'grad_norm': 2121.187255859375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 14/6000 [00:41<4:49:22,  2.90s/it]  0%|          | 15/6000 [00:43<4:45:37,  2.86s/it]                                                   {'loss': 6.3865, 'grad_norm': 1703.759765625, 'learning_rate': 7.5e-07, 'epoch': 0.0}
  0%|          | 15/6000 [00:43<4:45:37,  2.86s/it]  0%|          | 16/6000 [00:46<4:43:03,  2.84s/it]                                                   {'loss': 6.4296, 'grad_norm': 1617.376220703125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 16/6000 [00:46<4:43:03,  2.84s/it]  0%|          | 17/6000 [00:49<4:43:20,  2.84s/it]                                                   {'loss': 6.1627, 'grad_norm': 912.775390625, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.0}
  0%|          | 17/6000 [00:49<4:43:20,  2.84s/it]  0%|          | 18/6000 [00:52<4:42:08,  2.83s/it]                                                   {'loss': 5.5673, 'grad_norm': 860.681396484375, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 18/6000 [00:52<4:42:08,  2.83s/it]  0%|          | 19/6000 [00:55<4:39:59,  2.81s/it]                                                   {'loss': 6.4192, 'grad_norm': 1439.509521484375, 'learning_rate': 9.500000000000001e-07, 'epoch': 0.0}
  0%|          | 19/6000 [00:55<4:39:59,  2.81s/it]  0%|          | 20/6000 [00:57<4:39:48,  2.81s/it]                                                   {'loss': 5.7089, 'grad_norm': 886.6140747070312, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:57<4:39:48,  2.81s/it]  0%|          | 21/6000 [01:00<4:44:10,  2.85s/it]                                                   {'loss': 5.7379, 'grad_norm': 701.7762451171875, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:00<4:44:10,  2.85s/it]  0%|          | 22/6000 [01:03<4:44:44,  2.86s/it]                                                   {'loss': 5.0309, 'grad_norm': 382.8799743652344, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:03<4:44:44,  2.86s/it]  0%|          | 23/6000 [01:06<4:42:08,  2.83s/it]                                                   {'loss': 5.0079, 'grad_norm': 462.3936462402344, 'learning_rate': 1.1500000000000002e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:06<4:42:08,  2.83s/it]  0%|          | 24/6000 [01:09<4:44:41,  2.86s/it]                                                   {'loss': 4.706, 'grad_norm': 390.1495666503906, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:09<4:44:41,  2.86s/it]  0%|          | 25/6000 [01:12<4:43:23,  2.85s/it]                                                   {'loss': 5.3473, 'grad_norm': 545.1377563476562, 'learning_rate': 1.25e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:12<4:43:23,  2.85s/it]  0%|          | 26/6000 [01:15<4:42:42,  2.84s/it]                                                   {'loss': 4.8318, 'grad_norm': 281.0380859375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:15<4:42:42,  2.84s/it]  0%|          | 27/6000 [01:17<4:42:22,  2.84s/it]                                                   {'loss': 5.6042, 'grad_norm': 859.5421752929688, 'learning_rate': 1.3500000000000002e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:17<4:42:22,  2.84s/it]  0%|          | 28/6000 [01:21<5:08:32,  3.10s/it]                                                   {'loss': 4.8673, 'grad_norm': 987.4522094726562, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:21<5:08:32,  3.10s/it]  0%|          | 29/6000 [01:24<4:58:39,  3.00s/it]                                                   {'loss': 4.4564, 'grad_norm': 171.9636688232422, 'learning_rate': 1.45e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:24<4:58:39,  3.00s/it]  0%|          | 30/6000 [01:27<4:54:05,  2.96s/it]                                                   {'loss': 4.4506, 'grad_norm': 156.89862060546875, 'learning_rate': 1.5e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:27<4:54:05,  2.96s/it]  1%|          | 31/6000 [01:29<4:48:58,  2.90s/it]                                                   {'loss': 4.3785, 'grad_norm': 125.29014587402344, 'learning_rate': 1.5500000000000002e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:29<4:48:58,  2.90s/it]  1%|          | 32/6000 [01:32<4:44:54,  2.86s/it]                                                   {'loss': 4.5031, 'grad_norm': 146.13851928710938, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:32<4:44:54,  2.86s/it]  1%|          | 33/6000 [01:35<4:42:55,  2.84s/it]                                                   {'loss': 4.2913, 'grad_norm': 122.33780670166016, 'learning_rate': 1.6500000000000003e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:35<4:42:55,  2.84s/it]  1%|          | 34/6000 [01:38<4:39:55,  2.82s/it]                                                   {'loss': 4.7393, 'grad_norm': 225.31597900390625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:38<4:39:55,  2.82s/it]  1%|          | 35/6000 [01:41<4:40:48,  2.82s/it]                                                   {'loss': 4.0876, 'grad_norm': 94.78582763671875, 'learning_rate': 1.75e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:41<4:40:48,  2.82s/it]  1%|          | 36/6000 [01:43<4:38:34,  2.80s/it]                                                   {'loss': 4.3114, 'grad_norm': 156.8831787109375, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:43<4:38:34,  2.80s/it]  1%|          | 37/6000 [01:46<4:38:15,  2.80s/it]                                                   {'loss': 3.9977, 'grad_norm': 71.8266372680664, 'learning_rate': 1.85e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:46<4:38:15,  2.80s/it]  1%|          | 38/6000 [01:49<4:35:33,  2.77s/it]                                                   {'loss': 4.0267, 'grad_norm': 72.47679138183594, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:49<4:35:33,  2.77s/it]  1%|          | 39/6000 [01:52<4:35:07,  2.77s/it]                                                   {'loss': 3.8353, 'grad_norm': 52.40473937988281, 'learning_rate': 1.9500000000000004e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:52<4:35:07,  2.77s/it]  1%|          | 40/6000 [01:54<4:36:05,  2.78s/it]                                                   {'loss': 3.9888, 'grad_norm': 68.35420227050781, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:54<4:36:05,  2.78s/it]  1%|          | 41/6000 [01:57<4:34:59,  2.77s/it]                                                   {'loss': 3.9076, 'grad_norm': 59.08689498901367, 'learning_rate': 2.05e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:57<4:34:59,  2.77s/it]  1%|          | 42/6000 [02:00<4:35:55,  2.78s/it]                                                   {'loss': 3.9784, 'grad_norm': 80.45301818847656, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:00<4:35:55,  2.78s/it]  1%|          | 43/6000 [02:04<5:08:01,  3.10s/it]                                                   {'loss': 3.8309, 'grad_norm': 120.62353515625, 'learning_rate': 2.15e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:04<5:08:01,  3.10s/it]  1%|          | 44/6000 [02:07<5:12:43,  3.15s/it]                                                   {'loss': 3.783, 'grad_norm': 50.12131881713867, 'learning_rate': 2.2e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:07<5:12:43,  3.15s/it]  1%|          | 45/6000 [02:10<5:03:31,  3.06s/it]                                                   {'loss': 3.8315, 'grad_norm': 53.638973236083984, 'learning_rate': 2.25e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:10<5:03:31,  3.06s/it]  1%|          | 46/6000 [02:13<4:58:59,  3.01s/it]                                                   {'loss': 3.7105, 'grad_norm': 42.5142936706543, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:13<4:58:59,  3.01s/it]  1%|          | 47/6000 [02:16<4:53:18,  2.96s/it]                                                   {'loss': 3.6852, 'grad_norm': 47.345306396484375, 'learning_rate': 2.35e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:16<4:53:18,  2.96s/it]  1%|          | 48/6000 [02:19<4:49:46,  2.92s/it]                                                   {'loss': 3.6531, 'grad_norm': 42.93743133544922, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:19<4:49:46,  2.92s/it]  1%|          | 49/6000 [02:21<4:44:01,  2.86s/it]                                                   {'loss': 3.6383, 'grad_norm': 42.70656967163086, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:21<4:44:01,  2.86s/it]  1%|          | 50/6000 [02:24<4:48:05,  2.91s/it]                                                   {'loss': 3.6343, 'grad_norm': 47.64583969116211, 'learning_rate': 2.5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:24<4:48:05,  2.91s/it][2025-11-03 14:51:35,142] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-03 14:51:35,154] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 14:51:35,763] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:38<10:09:50,  6.15s/it]                                                    {'loss': 3.5475, 'grad_norm': 42.46356964111328, 'learning_rate': 2.55e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:38<10:09:50,  6.15s/it]  1%|          | 52/6000 [02:41<8:28:02,  5.12s/it]                                                    {'loss': 3.4962, 'grad_norm': 42.744842529296875, 'learning_rate': 2.6e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:41<8:28:02,  5.12s/it]  1%|          | 53/6000 [02:44<7:20:34,  4.45s/it]                                                   {'loss': 3.5209, 'grad_norm': 51.7285270690918, 'learning_rate': 2.6500000000000005e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:44<7:20:34,  4.45s/it]  1%|          | 54/6000 [02:46<6:29:13,  3.93s/it]                                                   {'loss': 3.3795, 'grad_norm': 53.192100524902344, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:46<6:29:13,  3.93s/it]  1%|          | 55/6000 [02:49<5:56:34,  3.60s/it]                                                   {'loss': 3.3127, 'grad_norm': 49.20261001586914, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:49<5:56:34,  3.60s/it]  1%|          | 56/6000 [02:52<5:33:40,  3.37s/it]                                                   {'loss': 3.3109, 'grad_norm': 48.607421875, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:52<5:33:40,  3.37s/it]  1%|          | 57/6000 [02:55<5:15:27,  3.18s/it]                                                   {'loss': 3.2603, 'grad_norm': 47.43796157836914, 'learning_rate': 2.85e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:55<5:15:27,  3.18s/it]  1%|          | 58/6000 [02:57<5:03:47,  3.07s/it]                                                   {'loss': 3.1942, 'grad_norm': 49.870574951171875, 'learning_rate': 2.9e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:57<5:03:47,  3.07s/it]  1%|          | 59/6000 [03:00<4:55:35,  2.99s/it]                                                   {'loss': 3.1163, 'grad_norm': 45.283409118652344, 'learning_rate': 2.95e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:00<4:55:35,  2.99s/it]  1%|          | 60/6000 [03:03<4:49:49,  2.93s/it]                                                   {'loss': 3.0726, 'grad_norm': 58.20132064819336, 'learning_rate': 3e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:03<4:49:49,  2.93s/it]  1%|          | 61/6000 [03:06<4:44:08,  2.87s/it]                                                   {'loss': 2.9996, 'grad_norm': 38.53870391845703, 'learning_rate': 3.05e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:06<4:44:08,  2.87s/it]  1%|          | 62/6000 [03:09<4:42:41,  2.86s/it]                                                   {'loss': 2.9445, 'grad_norm': 37.191497802734375, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:09<4:42:41,  2.86s/it]  1%|          | 63/6000 [03:11<4:40:12,  2.83s/it]                                                   {'loss': 2.9355, 'grad_norm': 40.54234313964844, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:11<4:40:12,  2.83s/it]  1%|          | 64/6000 [03:14<4:38:26,  2.81s/it]                                                   {'loss': 2.8587, 'grad_norm': 27.661348342895508, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:14<4:38:26,  2.81s/it]  1%|          | 65/6000 [03:17<4:35:39,  2.79s/it]                                                   {'loss': 2.8617, 'grad_norm': 25.862197875976562, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:17<4:35:39,  2.79s/it]  1%|          | 66/6000 [03:20<4:49:06,  2.92s/it]                                                   {'loss': 2.8676, 'grad_norm': 25.31098747253418, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:20<4:49:06,  2.92s/it]  1%|          | 67/6000 [03:23<4:44:41,  2.88s/it]                                                   {'loss': 2.8659, 'grad_norm': 24.81817626953125, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:23<4:44:41,  2.88s/it]  1%|          | 68/6000 [03:26<4:41:40,  2.85s/it]                                                   {'loss': 2.8421, 'grad_norm': 21.396820068359375, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:26<4:41:40,  2.85s/it]  1%|          | 69/6000 [03:29<4:40:09,  2.83s/it]                                                   {'loss': 2.8511, 'grad_norm': 12.568493843078613, 'learning_rate': 3.45e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:29<4:40:09,  2.83s/it]  1%|          | 70/6000 [03:31<4:41:53,  2.85s/it]                                                   {'loss': 2.8242, 'grad_norm': 13.784266471862793, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:31<4:41:53,  2.85s/it]  1%|          | 71/6000 [03:34<4:38:58,  2.82s/it]                                                   {'loss': 2.8147, 'grad_norm': 13.692594528198242, 'learning_rate': 3.5500000000000003e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:34<4:38:58,  2.82s/it]  1%|          | 72/6000 [03:37<4:50:33,  2.94s/it]                                                   {'loss': 2.801, 'grad_norm': 9.330714225769043, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:37<4:50:33,  2.94s/it]  1%|          | 73/6000 [03:40<4:48:09,  2.92s/it]                                                   {'loss': 2.8429, 'grad_norm': 11.114867210388184, 'learning_rate': 3.65e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:40<4:48:09,  2.92s/it]  1%|          | 74/6000 [03:43<4:49:52,  2.93s/it]                                                   {'loss': 2.8102, 'grad_norm': 14.553277015686035, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:43<4:49:52,  2.93s/it]  1%|â–         | 75/6000 [03:46<4:46:00,  2.90s/it]                                                   {'loss': 2.8025, 'grad_norm': 8.389419555664062, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:46<4:46:00,  2.90s/it]  1%|â–         | 76/6000 [03:49<4:48:03,  2.92s/it]                                                   {'loss': 2.8303, 'grad_norm': 8.782379150390625, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:49<4:48:03,  2.92s/it]  1%|â–         | 77/6000 [03:52<4:46:47,  2.91s/it]                                                   {'loss': 2.8701, 'grad_norm': 9.279983520507812, 'learning_rate': 3.85e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:52<4:46:47,  2.91s/it]  1%|â–         | 78/6000 [03:55<4:55:32,  2.99s/it]                                                   {'loss': 2.7971, 'grad_norm': 12.184714317321777, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:55<4:55:32,  2.99s/it]  1%|â–         | 79/6000 [03:58<4:50:48,  2.95s/it]                                                   {'loss': 2.7984, 'grad_norm': 12.479896545410156, 'learning_rate': 3.95e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:58<4:50:48,  2.95s/it]  1%|â–         | 80/6000 [04:01<4:53:47,  2.98s/it]                                                   {'loss': 2.8176, 'grad_norm': 25.278549194335938, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:01<4:53:47,  2.98s/it]  1%|â–         | 81/6000 [04:04<4:47:27,  2.91s/it]                                                   {'loss': 2.7957, 'grad_norm': 13.842500686645508, 'learning_rate': 4.05e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:04<4:47:27,  2.91s/it]  1%|â–         | 82/6000 [04:07<4:45:09,  2.89s/it]                                                   {'loss': 2.7849, 'grad_norm': 9.056225776672363, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:07<4:45:09,  2.89s/it]  1%|â–         | 83/6000 [04:09<4:43:27,  2.87s/it]                                                   {'loss': 2.8161, 'grad_norm': 12.951116561889648, 'learning_rate': 4.15e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:09<4:43:27,  2.87s/it]  1%|â–         | 84/6000 [04:12<4:44:26,  2.88s/it]                                                   {'loss': 2.8047, 'grad_norm': 12.666398048400879, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:12<4:44:26,  2.88s/it]  1%|â–         | 85/6000 [04:15<4:53:10,  2.97s/it]                                                   {'loss': 2.7941, 'grad_norm': 18.9163818359375, 'learning_rate': 4.25e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:15<4:53:10,  2.97s/it]  1%|â–         | 86/6000 [04:18<4:48:22,  2.93s/it]                                                   {'loss': 2.8019, 'grad_norm': 15.378854751586914, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:18<4:48:22,  2.93s/it]  1%|â–         | 87/6000 [04:21<4:44:16,  2.88s/it]                                                   {'loss': 2.8058, 'grad_norm': 11.441511154174805, 'learning_rate': 4.350000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:21<4:44:16,  2.88s/it]  1%|â–         | 88/6000 [04:24<4:40:16,  2.84s/it]                                                   {'loss': 2.8169, 'grad_norm': 25.761821746826172, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:24<4:40:16,  2.84s/it]  1%|â–         | 89/6000 [04:27<4:37:29,  2.82s/it]                                                   {'loss': 2.7974, 'grad_norm': 16.03120231628418, 'learning_rate': 4.450000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:27<4:37:29,  2.82s/it]  2%|â–         | 90/6000 [04:29<4:38:38,  2.83s/it]                                                   {'loss': 2.8364, 'grad_norm': 13.044828414916992, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:29<4:38:38,  2.83s/it]  2%|â–         | 91/6000 [04:32<4:39:26,  2.84s/it]                                                   {'loss': 2.7821, 'grad_norm': 22.46821403503418, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:32<4:39:26,  2.84s/it]  2%|â–         | 92/6000 [04:36<4:50:15,  2.95s/it]                                                   {'loss': 2.7963, 'grad_norm': 28.980710983276367, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:36<4:50:15,  2.95s/it]  2%|â–         | 93/6000 [04:38<4:49:57,  2.95s/it]                                                   {'loss': 2.7866, 'grad_norm': 12.221209526062012, 'learning_rate': 4.65e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:38<4:49:57,  2.95s/it]  2%|â–         | 94/6000 [04:41<4:44:19,  2.89s/it]                                                   {'loss': 2.8084, 'grad_norm': 12.463467597961426, 'learning_rate': 4.7e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:41<4:44:19,  2.89s/it]  2%|â–         | 95/6000 [04:44<4:41:27,  2.86s/it]                                                   {'loss': 2.7809, 'grad_norm': 7.9149489402771, 'learning_rate': 4.75e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:44<4:41:27,  2.86s/it]  2%|â–         | 96/6000 [04:47<4:39:59,  2.85s/it]                                                   {'loss': 2.7796, 'grad_norm': 13.598151206970215, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:47<4:39:59,  2.85s/it]  2%|â–         | 97/6000 [04:50<4:40:25,  2.85s/it]                                                   {'loss': 2.8031, 'grad_norm': 12.006573677062988, 'learning_rate': 4.85e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:50<4:40:25,  2.85s/it]  2%|â–         | 98/6000 [04:52<4:38:17,  2.83s/it]                                                   {'loss': 2.7879, 'grad_norm': 14.883868217468262, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:52<4:38:17,  2.83s/it]  2%|â–         | 99/6000 [04:55<4:35:28,  2.80s/it]                                                   {'loss': 2.7714, 'grad_norm': 6.513140678405762, 'learning_rate': 4.95e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:55<4:35:28,  2.80s/it]  2%|â–         | 100/6000 [04:58<4:34:00,  2.79s/it]                                                    {'loss': 2.7677, 'grad_norm': 8.418109893798828, 'learning_rate': 5e-06, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:58<4:34:00,  2.79s/it][2025-11-03 14:54:08,827] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-03 14:54:08,838] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 14:54:09,426] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [05:13<10:24:26,  6.35s/it]                                                     {'loss': 3.0078, 'grad_norm': 469.04986572265625, 'learning_rate': 4.999152542372881e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:13<10:24:26,  6.35s/it]  2%|â–         | 102/6000 [05:15<8:38:31,  5.27s/it]                                                     {'loss': 2.7884, 'grad_norm': 12.289170265197754, 'learning_rate': 4.998305084745763e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:15<8:38:31,  5.27s/it]  2%|â–         | 103/6000 [05:18<7:28:10,  4.56s/it]                                                    {'loss': 2.7922, 'grad_norm': 7.435093402862549, 'learning_rate': 4.9974576271186445e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:18<7:28:10,  4.56s/it]  2%|â–         | 104/6000 [05:21<6:44:08,  4.11s/it]                                                    {'loss': 2.7767, 'grad_norm': 4.539255142211914, 'learning_rate': 4.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [05:21<6:44:08,  4.11s/it]  2%|â–         | 105/6000 [05:24<6:08:33,  3.75s/it]                                                    {'loss': 2.8001, 'grad_norm': 10.304484367370605, 'learning_rate': 4.995762711864407e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [05:24<6:08:33,  3.75s/it]  2%|â–         | 106/6000 [05:27<5:44:51,  3.51s/it]                                                    {'loss': 2.7784, 'grad_norm': 6.231786727905273, 'learning_rate': 4.9949152542372885e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:27<5:44:51,  3.51s/it]  2%|â–         | 107/6000 [05:30<5:23:58,  3.30s/it]                                                    {'loss': 2.8041, 'grad_norm': 6.62013053894043, 'learning_rate': 4.994067796610169e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:30<5:23:58,  3.30s/it]  2%|â–         | 108/6000 [05:33<5:14:15,  3.20s/it]                                                    {'loss': 2.7867, 'grad_norm': 8.772807121276855, 'learning_rate': 4.993220338983051e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:33<5:14:15,  3.20s/it]  2%|â–         | 109/6000 [05:36<5:16:21,  3.22s/it]                                                    {'loss': 2.8271, 'grad_norm': 5.2254486083984375, 'learning_rate': 4.992372881355933e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:36<5:16:21,  3.22s/it]  2%|â–         | 110/6000 [05:39<5:04:05,  3.10s/it]                                                    {'loss': 2.858, 'grad_norm': 10.929226875305176, 'learning_rate': 4.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:39<5:04:05,  3.10s/it]  2%|â–         | 111/6000 [05:42<4:58:28,  3.04s/it]                                                    {'loss': 2.7823, 'grad_norm': 9.964618682861328, 'learning_rate': 4.990677966101695e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:42<4:58:28,  3.04s/it]  2%|â–         | 112/6000 [05:45<5:09:34,  3.15s/it]                                                    {'loss': 2.7793, 'grad_norm': 12.611917495727539, 'learning_rate': 4.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:45<5:09:34,  3.15s/it]  2%|â–         | 113/6000 [05:48<4:59:03,  3.05s/it]                                                    {'loss': 2.7777, 'grad_norm': 4.722336292266846, 'learning_rate': 4.988983050847458e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:48<4:59:03,  3.05s/it]  2%|â–         | 114/6000 [05:51<4:54:06,  3.00s/it]                                                    {'loss': 2.7765, 'grad_norm': 4.191383361816406, 'learning_rate': 4.98813559322034e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:51<4:54:06,  3.00s/it]  2%|â–         | 115/6000 [05:54<4:48:43,  2.94s/it]                                                    {'loss': 2.8533, 'grad_norm': 10.633378028869629, 'learning_rate': 4.987288135593221e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:54<4:48:43,  2.94s/it]  2%|â–         | 116/6000 [05:57<4:45:23,  2.91s/it]                                                    {'loss': 2.7944, 'grad_norm': 6.252507209777832, 'learning_rate': 4.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:57<4:45:23,  2.91s/it]  2%|â–         | 117/6000 [06:00<4:44:11,  2.90s/it]                                                    {'loss': 2.9138, 'grad_norm': 6.900057315826416, 'learning_rate': 4.985593220338983e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [06:00<4:44:11,  2.90s/it]  2%|â–         | 118/6000 [06:03<4:58:27,  3.04s/it]                                                    {'loss': 2.7828, 'grad_norm': 5.422316074371338, 'learning_rate': 4.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:03<4:58:27,  3.04s/it]  2%|â–         | 119/6000 [06:06<4:50:48,  2.97s/it]                                                    {'loss': 2.7824, 'grad_norm': 8.008174896240234, 'learning_rate': 4.9838983050847464e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:06<4:50:48,  2.97s/it]  2%|â–         | 120/6000 [06:09<4:47:01,  2.93s/it]                                                    {'loss': 2.7857, 'grad_norm': 4.996394157409668, 'learning_rate': 4.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:09<4:47:01,  2.93s/it]  2%|â–         | 121/6000 [06:11<4:45:08,  2.91s/it]                                                    {'loss': 2.7708, 'grad_norm': 4.59171199798584, 'learning_rate': 4.982203389830509e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [06:11<4:45:08,  2.91s/it]  2%|â–         | 122/6000 [06:14<4:46:23,  2.92s/it]                                                    {'loss': 2.7762, 'grad_norm': 4.495120048522949, 'learning_rate': 4.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [06:14<4:46:23,  2.92s/it]  2%|â–         | 123/6000 [06:17<4:41:40,  2.88s/it]                                                    {'loss': 2.7882, 'grad_norm': 5.940728664398193, 'learning_rate': 4.980508474576271e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [06:17<4:41:40,  2.88s/it]  2%|â–         | 124/6000 [06:20<4:38:20,  2.84s/it]                                                    {'loss': 2.8202, 'grad_norm': 8.219278335571289, 'learning_rate': 4.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [06:20<4:38:20,  2.84s/it]  2%|â–         | 125/6000 [06:23<4:44:54,  2.91s/it]                                                    {'loss': 2.951, 'grad_norm': 7.086045265197754, 'learning_rate': 4.9788135593220346e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [06:23<4:44:54,  2.91s/it]  2%|â–         | 126/6000 [06:26<4:44:37,  2.91s/it]                                                    {'loss': 2.7744, 'grad_norm': 4.245541095733643, 'learning_rate': 4.977966101694915e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [06:26<4:44:37,  2.91s/it]  2%|â–         | 127/6000 [06:29<4:46:21,  2.93s/it]                                                    {'loss': 2.7778, 'grad_norm': 7.09375524520874, 'learning_rate': 4.977118644067797e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:29<4:46:21,  2.93s/it]  2%|â–         | 128/6000 [06:32<4:42:37,  2.89s/it]                                                    {'loss': 2.7903, 'grad_norm': 4.122981548309326, 'learning_rate': 4.976271186440678e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:32<4:42:37,  2.89s/it]  2%|â–         | 129/6000 [06:35<4:41:59,  2.88s/it]                                                    {'loss': 2.8529, 'grad_norm': 5.368526458740234, 'learning_rate': 4.97542372881356e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:35<4:41:59,  2.88s/it]  2%|â–         | 130/6000 [06:37<4:41:18,  2.88s/it]                                                    {'loss': 2.7732, 'grad_norm': 7.398993492126465, 'learning_rate': 4.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:37<4:41:18,  2.88s/it]  2%|â–         | 131/6000 [06:40<4:40:53,  2.87s/it]                                                    {'loss': 2.7867, 'grad_norm': 9.704733848571777, 'learning_rate': 4.973728813559323e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:40<4:40:53,  2.87s/it]  2%|â–         | 132/6000 [06:43<4:38:26,  2.85s/it]                                                    {'loss': 2.7957, 'grad_norm': 7.819603443145752, 'learning_rate': 4.9728813559322035e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:43<4:38:26,  2.85s/it]  2%|â–         | 133/6000 [06:46<4:38:22,  2.85s/it]                                                    {'loss': 2.7754, 'grad_norm': 4.244176387786865, 'learning_rate': 4.972033898305085e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:46<4:38:22,  2.85s/it]  2%|â–         | 134/6000 [06:49<4:43:19,  2.90s/it]                                                    {'loss': 2.7764, 'grad_norm': 6.056174278259277, 'learning_rate': 4.971186440677967e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:49<4:43:19,  2.90s/it]  2%|â–         | 135/6000 [06:52<4:42:42,  2.89s/it]                                                    {'loss': 2.7928, 'grad_norm': 5.747432708740234, 'learning_rate': 4.970338983050848e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:52<4:42:42,  2.89s/it]  2%|â–         | 136/6000 [06:55<4:41:53,  2.88s/it]                                                    {'loss': 2.7811, 'grad_norm': 3.807202100753784, 'learning_rate': 4.969491525423729e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:55<4:41:53,  2.88s/it]  2%|â–         | 137/6000 [06:58<4:56:42,  3.04s/it]                                                    {'loss': 2.7705, 'grad_norm': 6.210425853729248, 'learning_rate': 4.968644067796611e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:58<4:56:42,  3.04s/it]  2%|â–         | 138/6000 [07:01<4:48:27,  2.95s/it]                                                    {'loss': 2.7746, 'grad_norm': 9.210269927978516, 'learning_rate': 4.967796610169492e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [07:01<4:48:27,  2.95s/it]  2%|â–         | 139/6000 [07:04<4:49:11,  2.96s/it]                                                    {'loss': 2.8234, 'grad_norm': 4.644256114959717, 'learning_rate': 4.966949152542373e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [07:04<4:49:11,  2.96s/it]  2%|â–         | 140/6000 [07:07<4:56:21,  3.03s/it]                                                    {'loss': 2.7704, 'grad_norm': 6.1619553565979, 'learning_rate': 4.966101694915255e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [07:07<4:56:21,  3.03s/it]  2%|â–         | 141/6000 [07:10<4:52:46,  3.00s/it]                                                    {'loss': 2.7946, 'grad_norm': 9.115836143493652, 'learning_rate': 4.9652542372881365e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [07:10<4:52:46,  3.00s/it]  2%|â–         | 142/6000 [07:13<4:45:25,  2.92s/it]                                                    {'loss': 2.7779, 'grad_norm': 5.2881388664245605, 'learning_rate': 4.964406779661017e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [07:13<4:45:25,  2.92s/it]  2%|â–         | 143/6000 [07:15<4:42:06,  2.89s/it]                                                    {'loss': 2.774, 'grad_norm': 3.462836980819702, 'learning_rate': 4.963559322033898e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [07:15<4:42:06,  2.89s/it]  2%|â–         | 144/6000 [07:18<4:41:01,  2.88s/it]                                                    {'loss': 2.7905, 'grad_norm': 5.362385272979736, 'learning_rate': 4.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [07:18<4:41:01,  2.88s/it]  2%|â–         | 145/6000 [07:21<4:37:33,  2.84s/it]                                                    {'loss': 2.8042, 'grad_norm': 9.333250999450684, 'learning_rate': 4.961864406779661e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [07:21<4:37:33,  2.84s/it]  2%|â–         | 146/6000 [07:24<4:38:35,  2.86s/it]                                                    {'loss': 2.8109, 'grad_norm': 9.593966484069824, 'learning_rate': 4.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [07:24<4:38:35,  2.86s/it]  2%|â–         | 147/6000 [07:27<4:36:59,  2.84s/it]                                                    {'loss': 2.7783, 'grad_norm': 3.7603509426116943, 'learning_rate': 4.960169491525424e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [07:27<4:36:59,  2.84s/it]  2%|â–         | 148/6000 [07:30<4:36:34,  2.84s/it]                                                    {'loss': 2.7916, 'grad_norm': 6.2934465408325195, 'learning_rate': 4.9593220338983054e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [07:30<4:36:34,  2.84s/it]  2%|â–         | 149/6000 [07:32<4:34:29,  2.81s/it]                                                    {'loss': 2.7742, 'grad_norm': 6.136297225952148, 'learning_rate': 4.958474576271187e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:32<4:34:29,  2.81s/it]  2%|â–Ž         | 150/6000 [07:35<4:33:01,  2.80s/it]                                                    {'loss': 2.7875, 'grad_norm': 5.820621967315674, 'learning_rate': 4.957627118644069e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:35<4:33:01,  2.80s/it][2025-11-03 14:56:46,016] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-03 14:56:46,031] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 14:56:46,661] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:49<10:00:12,  6.16s/it]                                                     {'loss': 2.7827, 'grad_norm': 4.7700676918029785, 'learning_rate': 4.9567796610169495e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:49<10:00:12,  6.16s/it]  3%|â–Ž         | 152/6000 [07:52<8:21:38,  5.15s/it]                                                     {'loss': 2.7893, 'grad_norm': 6.6036200523376465, 'learning_rate': 4.955932203389831e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:52<8:21:38,  5.15s/it]  3%|â–Ž         | 153/6000 [07:55<7:12:17,  4.44s/it]                                                    {'loss': 2.7805, 'grad_norm': 6.9127092361450195, 'learning_rate': 4.955084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:55<7:12:17,  4.44s/it]  3%|â–Ž         | 154/6000 [07:58<6:27:41,  3.98s/it]                                                    {'loss': 2.7757, 'grad_norm': 10.83020305633545, 'learning_rate': 4.9542372881355936e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:58<6:27:41,  3.98s/it]  3%|â–Ž         | 155/6000 [08:00<5:54:29,  3.64s/it]                                                    {'loss': 2.7969, 'grad_norm': 5.053638935089111, 'learning_rate': 4.953389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [08:00<5:54:29,  3.64s/it]  3%|â–Ž         | 156/6000 [08:03<5:32:56,  3.42s/it]                                                    {'loss': 2.8366, 'grad_norm': 7.470155239105225, 'learning_rate': 4.952542372881357e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [08:03<5:32:56,  3.42s/it]  3%|â–Ž         | 157/6000 [08:07<5:29:49,  3.39s/it]                                                    {'loss': 2.7657, 'grad_norm': 5.83917236328125, 'learning_rate': 4.951694915254238e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [08:07<5:29:49,  3.39s/it]  3%|â–Ž         | 158/6000 [08:09<5:11:41,  3.20s/it]                                                    {'loss': 2.7784, 'grad_norm': 6.448885917663574, 'learning_rate': 4.950847457627119e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [08:09<5:11:41,  3.20s/it]  3%|â–Ž         | 159/6000 [08:12<5:01:12,  3.09s/it]                                                    {'loss': 2.7978, 'grad_norm': 6.07752799987793, 'learning_rate': 4.95e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [08:12<5:01:12,  3.09s/it]  3%|â–Ž         | 160/6000 [08:16<5:10:37,  3.19s/it]                                                    {'loss': 2.8097, 'grad_norm': 5.943489074707031, 'learning_rate': 4.949152542372882e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [08:16<5:10:37,  3.19s/it]  3%|â–Ž         | 161/6000 [08:19<5:04:30,  3.13s/it]                                                    {'loss': 2.7771, 'grad_norm': 8.615198135375977, 'learning_rate': 4.948305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [08:19<5:04:30,  3.13s/it]  3%|â–Ž         | 162/6000 [08:21<4:55:16,  3.03s/it]                                                    {'loss': 2.7679, 'grad_norm': 9.552380561828613, 'learning_rate': 4.947457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [08:21<4:55:16,  3.03s/it]  3%|â–Ž         | 163/6000 [08:25<5:05:58,  3.15s/it]                                                    {'loss': 2.7951, 'grad_norm': 6.897193431854248, 'learning_rate': 4.946610169491526e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [08:25<5:05:58,  3.15s/it]  3%|â–Ž         | 164/6000 [08:28<4:55:08,  3.03s/it]                                                    {'loss': 2.9036, 'grad_norm': 9.973732948303223, 'learning_rate': 4.9457627118644065e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [08:28<4:55:08,  3.03s/it]  3%|â–Ž         | 165/6000 [08:31<4:52:52,  3.01s/it]                                                    {'loss': 2.8064, 'grad_norm': 12.761724472045898, 'learning_rate': 4.944915254237288e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [08:31<4:52:52,  3.01s/it]  3%|â–Ž         | 166/6000 [08:33<4:45:12,  2.93s/it]                                                    {'loss': 2.7804, 'grad_norm': 7.887058734893799, 'learning_rate': 4.94406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [08:33<4:45:12,  2.93s/it]  3%|â–Ž         | 167/6000 [08:36<4:42:44,  2.91s/it]                                                    {'loss': 2.7969, 'grad_norm': 11.012528419494629, 'learning_rate': 4.9432203389830514e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [08:36<4:42:44,  2.91s/it]  3%|â–Ž         | 168/6000 [08:39<4:40:38,  2.89s/it]                                                    {'loss': 2.826, 'grad_norm': 6.095000743865967, 'learning_rate': 4.942372881355932e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [08:39<4:40:38,  2.89s/it]  3%|â–Ž         | 169/6000 [08:42<4:51:52,  3.00s/it]                                                    {'loss': 2.7965, 'grad_norm': 8.30765151977539, 'learning_rate': 4.941525423728814e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:42<4:51:52,  3.00s/it]  3%|â–Ž         | 170/6000 [08:45<4:45:42,  2.94s/it]                                                    {'loss': 2.7886, 'grad_norm': 10.101373672485352, 'learning_rate': 4.9406779661016955e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:45<4:45:42,  2.94s/it]  3%|â–Ž         | 171/6000 [08:48<4:40:36,  2.89s/it]                                                    {'loss': 2.7904, 'grad_norm': 8.185938835144043, 'learning_rate': 4.939830508474577e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:48<4:40:36,  2.89s/it]  3%|â–Ž         | 172/6000 [08:51<4:44:06,  2.92s/it]                                                    {'loss': 2.7959, 'grad_norm': 8.900410652160645, 'learning_rate': 4.938983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:51<4:44:06,  2.92s/it]  3%|â–Ž         | 173/6000 [08:54<4:42:38,  2.91s/it]                                                    {'loss': 2.7894, 'grad_norm': 6.881721019744873, 'learning_rate': 4.9381355932203396e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:54<4:42:38,  2.91s/it]  3%|â–Ž         | 174/6000 [08:57<4:59:48,  3.09s/it]                                                    {'loss': 2.7689, 'grad_norm': 16.137500762939453, 'learning_rate': 4.93728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:57<4:59:48,  3.09s/it]  3%|â–Ž         | 175/6000 [09:00<4:53:22,  3.02s/it]                                                    {'loss': 2.7926, 'grad_norm': 5.120983123779297, 'learning_rate': 4.936440677966102e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [09:00<4:53:22,  3.02s/it]  3%|â–Ž         | 176/6000 [09:03<4:56:09,  3.05s/it]                                                    {'loss': 2.7827, 'grad_norm': 9.230664253234863, 'learning_rate': 4.935593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [09:03<4:56:09,  3.05s/it]  3%|â–Ž         | 177/6000 [09:06<4:49:53,  2.99s/it]                                                    {'loss': 2.7848, 'grad_norm': 5.016544818878174, 'learning_rate': 4.934745762711865e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [09:06<4:49:53,  2.99s/it]  3%|â–Ž         | 178/6000 [09:09<4:46:30,  2.95s/it]                                                    {'loss': 2.7834, 'grad_norm': 5.492335796356201, 'learning_rate': 4.933898305084746e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [09:09<4:46:30,  2.95s/it]  3%|â–Ž         | 179/6000 [09:12<4:42:04,  2.91s/it]                                                    {'loss': 2.7959, 'grad_norm': 4.261064052581787, 'learning_rate': 4.933050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [09:12<4:42:04,  2.91s/it]  3%|â–Ž         | 180/6000 [09:15<4:44:38,  2.93s/it]                                                    {'loss': 2.785, 'grad_norm': 4.1519575119018555, 'learning_rate': 4.9322033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [09:15<4:44:38,  2.93s/it]  3%|â–Ž         | 181/6000 [09:18<4:42:32,  2.91s/it]                                                    {'loss': 2.8107, 'grad_norm': 4.0316081047058105, 'learning_rate': 4.93135593220339e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [09:18<4:42:32,  2.91s/it]  3%|â–Ž         | 182/6000 [09:20<4:39:01,  2.88s/it]                                                    {'loss': 2.8206, 'grad_norm': 4.004499435424805, 'learning_rate': 4.930508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [09:20<4:39:01,  2.88s/it]  3%|â–Ž         | 183/6000 [09:23<4:36:29,  2.85s/it]                                                    {'loss': 2.7774, 'grad_norm': 5.692561149597168, 'learning_rate': 4.929661016949153e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [09:23<4:36:29,  2.85s/it]  3%|â–Ž         | 184/6000 [09:26<4:35:51,  2.85s/it]                                                    {'loss': 2.7804, 'grad_norm': 3.93430233001709, 'learning_rate': 4.928813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [09:26<4:35:51,  2.85s/it]  3%|â–Ž         | 185/6000 [09:29<4:35:00,  2.84s/it]                                                    {'loss': 2.791, 'grad_norm': 2.904313564300537, 'learning_rate': 4.927966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [09:29<4:35:00,  2.84s/it]  3%|â–Ž         | 186/6000 [09:32<4:35:26,  2.84s/it]                                                    {'loss': 2.786, 'grad_norm': 23.13170623779297, 'learning_rate': 4.9271186440677975e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [09:32<4:35:26,  2.84s/it]  3%|â–Ž         | 187/6000 [09:35<4:34:32,  2.83s/it]                                                    {'loss': 2.7795, 'grad_norm': 6.435272216796875, 'learning_rate': 4.926271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [09:35<4:34:32,  2.83s/it]  3%|â–Ž         | 188/6000 [09:37<4:33:43,  2.83s/it]                                                    {'loss': 2.7796, 'grad_norm': 3.5012598037719727, 'learning_rate': 4.92542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [09:37<4:33:43,  2.83s/it]  3%|â–Ž         | 189/6000 [09:40<4:33:34,  2.82s/it]                                                    {'loss': 2.769, 'grad_norm': 5.711324214935303, 'learning_rate': 4.924576271186441e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [09:40<4:33:34,  2.82s/it]  3%|â–Ž         | 190/6000 [09:43<4:31:31,  2.80s/it]                                                    {'loss': 2.785, 'grad_norm': 5.841082572937012, 'learning_rate': 4.923728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [09:43<4:31:31,  2.80s/it]  3%|â–Ž         | 191/6000 [09:46<4:34:30,  2.84s/it]                                                    {'loss': 2.7825, 'grad_norm': 4.407632827758789, 'learning_rate': 4.922881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:46<4:34:30,  2.84s/it]  3%|â–Ž         | 192/6000 [09:49<4:52:34,  3.02s/it]                                                    {'loss': 2.871, 'grad_norm': 5.028336048126221, 'learning_rate': 4.922033898305086e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:49<4:52:34,  3.02s/it]  3%|â–Ž         | 193/6000 [09:52<4:56:28,  3.06s/it]                                                    {'loss': 2.7802, 'grad_norm': 7.12747049331665, 'learning_rate': 4.921186440677966e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:52<4:56:28,  3.06s/it]  3%|â–Ž         | 194/6000 [09:55<4:46:48,  2.96s/it]                                                    {'loss': 2.778, 'grad_norm': 6.062986373901367, 'learning_rate': 4.920338983050848e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:55<4:46:48,  2.96s/it]  3%|â–Ž         | 195/6000 [09:58<4:45:37,  2.95s/it]                                                    {'loss': 2.8101, 'grad_norm': 3.7885162830352783, 'learning_rate': 4.919491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:58<4:45:37,  2.95s/it]  3%|â–Ž         | 196/6000 [10:01<4:42:28,  2.92s/it]                                                    {'loss': 2.7925, 'grad_norm': 7.619420051574707, 'learning_rate': 4.9186440677966104e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [10:01<4:42:28,  2.92s/it]  3%|â–Ž         | 197/6000 [10:04<4:44:46,  2.94s/it]                                                    {'loss': 2.7914, 'grad_norm': 4.710766315460205, 'learning_rate': 4.917796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [10:04<4:44:46,  2.94s/it]  3%|â–Ž         | 198/6000 [10:07<4:45:23,  2.95s/it]                                                    {'loss': 2.788, 'grad_norm': 7.223742961883545, 'learning_rate': 4.916949152542374e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [10:07<4:45:23,  2.95s/it]  3%|â–Ž         | 199/6000 [10:10<4:39:37,  2.89s/it]                                                    {'loss': 2.7735, 'grad_norm': 4.181607723236084, 'learning_rate': 4.9161016949152545e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [10:10<4:39:37,  2.89s/it]  3%|â–Ž         | 200/6000 [10:13<4:52:55,  3.03s/it]                                                    {'loss': 2.7722, 'grad_norm': 4.333156108856201, 'learning_rate': 4.915254237288136e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [10:13<4:52:55,  3.03s/it][2025-11-03 14:59:23,929] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-11-03 14:59:23,940] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 14:59:24,538] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [10:27<10:04:17,  6.25s/it]                                                     {'loss': 2.798, 'grad_norm': 5.268631458282471, 'learning_rate': 4.914406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [10:27<10:04:17,  6.25s/it]  3%|â–Ž         | 202/6000 [10:30<8:23:59,  5.22s/it]                                                     {'loss': 2.7741, 'grad_norm': 4.034248352050781, 'learning_rate': 4.9135593220338986e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [10:30<8:23:59,  5.22s/it]  3%|â–Ž         | 203/6000 [10:32<7:14:37,  4.50s/it]                                                    {'loss': 2.7957, 'grad_norm': 3.0445713996887207, 'learning_rate': 4.91271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [10:32<7:14:37,  4.50s/it]  3%|â–Ž         | 204/6000 [10:35<6:29:12,  4.03s/it]                                                    {'loss': 2.781, 'grad_norm': 3.0290639400482178, 'learning_rate': 4.911864406779661e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [10:35<6:29:12,  4.03s/it]  3%|â–Ž         | 205/6000 [10:38<5:52:01,  3.64s/it]                                                    {'loss': 2.7712, 'grad_norm': 3.635162115097046, 'learning_rate': 4.911016949152543e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [10:38<5:52:01,  3.64s/it]  3%|â–Ž         | 206/6000 [10:41<5:26:17,  3.38s/it]                                                    {'loss': 2.8258, 'grad_norm': 3.9481265544891357, 'learning_rate': 4.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [10:41<5:26:17,  3.38s/it]  3%|â–Ž         | 207/6000 [10:44<5:08:40,  3.20s/it]                                                    {'loss': 2.8746, 'grad_norm': 5.309704303741455, 'learning_rate': 4.909322033898306e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [10:44<5:08:40,  3.20s/it]  3%|â–Ž         | 208/6000 [10:46<4:57:00,  3.08s/it]                                                    {'loss': 2.7816, 'grad_norm': 3.8953890800476074, 'learning_rate': 4.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [10:46<4:57:00,  3.08s/it]  3%|â–Ž         | 209/6000 [10:49<4:50:16,  3.01s/it]                                                    {'loss': 2.7758, 'grad_norm': 2.964317798614502, 'learning_rate': 4.907627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [10:49<4:50:16,  3.01s/it]  4%|â–Ž         | 210/6000 [10:53<4:57:22,  3.08s/it]                                                    {'loss': 2.7775, 'grad_norm': 6.065883636474609, 'learning_rate': 4.906779661016949e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [10:53<4:57:22,  3.08s/it]  4%|â–Ž         | 211/6000 [10:56<4:55:47,  3.07s/it]                                                    {'loss': 2.7897, 'grad_norm': 7.929286003112793, 'learning_rate': 4.905932203389831e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:56<4:55:47,  3.07s/it]  4%|â–Ž         | 212/6000 [10:59<4:59:58,  3.11s/it]                                                    {'loss': 2.7754, 'grad_norm': 5.584216594696045, 'learning_rate': 4.905084745762712e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:59<4:59:58,  3.11s/it]  4%|â–Ž         | 213/6000 [11:02<4:53:44,  3.05s/it]                                                    {'loss': 2.7948, 'grad_norm': 6.6253886222839355, 'learning_rate': 4.904237288135594e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [11:02<4:53:44,  3.05s/it]  4%|â–Ž         | 214/6000 [11:04<4:45:39,  2.96s/it]                                                    {'loss': 2.7835, 'grad_norm': 4.813338756561279, 'learning_rate': 4.903389830508475e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [11:04<4:45:39,  2.96s/it]  4%|â–Ž         | 215/6000 [11:07<4:42:47,  2.93s/it]                                                    {'loss': 2.8065, 'grad_norm': 4.070914268493652, 'learning_rate': 4.9025423728813565e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [11:07<4:42:47,  2.93s/it]  4%|â–Ž         | 216/6000 [11:10<4:42:03,  2.93s/it]                                                    {'loss': 2.7777, 'grad_norm': 4.934111595153809, 'learning_rate': 4.901694915254237e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [11:10<4:42:03,  2.93s/it]  4%|â–Ž         | 217/6000 [11:13<4:41:43,  2.92s/it]                                                    {'loss': 2.8241, 'grad_norm': 2.751490831375122, 'learning_rate': 4.900847457627119e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [11:13<4:41:43,  2.92s/it]  4%|â–Ž         | 218/6000 [11:16<4:41:56,  2.93s/it]                                                    {'loss': 2.7896, 'grad_norm': 7.808224201202393, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [11:16<4:41:56,  2.93s/it]  4%|â–Ž         | 219/6000 [11:19<4:39:00,  2.90s/it]                                                    {'loss': 2.8179, 'grad_norm': 3.8567616939544678, 'learning_rate': 4.899152542372882e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [11:19<4:39:00,  2.90s/it]