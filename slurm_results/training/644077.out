==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailToken-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 4 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 5000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/train.log
W1019 16:53:33.418000 136919728248640 torch/distributed/run.py:779] 
W1019 16:53:33.418000 136919728248640 torch/distributed/run.py:779] *****************************************
W1019 16:53:33.418000 136919728248640 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1019 16:53:33.418000 136919728248640 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-19 16:53:43,731] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.06it/s]
wandb: setting up run a7rkfsm4
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_165344-a7rkfsm4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailToken-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/a7rkfsm4
[2025-10-19 16:53:45,275] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.24it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.14it/s]
[2025-10-19 16:53:45,903] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-19 16:53:45,910] INFO [src.utils:19] Loading lora adapter from TailTokenWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-19 16:53:55,159] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-19 16:53:56,048] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-19 16:53:56,048] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-19 16:54:00,359] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-19 16:54:00,360] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-19 16:54:01,336] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-19 16:54:01,337] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-19 16:54:01,337] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=8
		estimated num step per epoch=26660.875
		interleave_batch_size=0.0
[2025-10-19 16:54:01,339] INFO [src.utils:19] ==================================================
[2025-10-19 16:54:01,339] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-19 16:54:01,340] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 16:54:01,341] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-19 16:54:01,341] INFO [src.utils:19] ==================================================
[2025-10-19 16:54:02,637] INFO [src.utils:19] âœ… Custom optimizer (gme.learnable_token only) enabled
[2025-10-19 16:54:03,091] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 16:54:03,091] INFO [src.trainer:342] ***** Running training *****
[2025-10-19 16:54:03,091] INFO [src.trainer:343]   Num examples = 40,000
[2025-10-19 16:54:03,091] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 16:54:03,091] INFO [src.trainer:345]   Instantaneous batch size per device = 4
[2025-10-19 16:54:03,091] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 8
[2025-10-19 16:54:03,091] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 16:54:03,091] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 16:54:03,092] INFO [src.trainer:343]   Num examples = 40,000
[2025-10-19 16:54:03,092] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-19 16:54:03,092] INFO [src.trainer:345]   Instantaneous batch size per device = 4
[2025-10-19 16:54:03,093] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 8
[2025-10-19 16:54:03,093] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-19 16:54:03,093] INFO [src.trainer:350]   Total optimization steps = 5,000
[2025-10-19 16:54:03,102] INFO [src.trainer:351]   Number of trainable parameters = 1,536
[2025-10-19 16:54:03,105] INFO [src.trainer:351]   Number of trainable parameters = 1,536
  0%|          | 0/5000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 112, in <module>
[rank1]:     main()
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 104, in main
[rank1]:     trainer.train(resume_from_checkpoint=resume_checkpoint_dir)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 510, in _inner_training_loop
[rank1]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 708, in training_step
[rank1]:     loss = self.gc(queries, targets, no_sync_except_last=_distributed)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 71, in __call__
[rank1]:     return self.cache_step(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 285, in cache_step
[rank1]:     model_reps, rnd_states = self.forward_no_grad(model, x)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 187, in forward_no_grad
[rank1]:     y = self.model_call(model, x)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 141, in model_call
[rank1]:     return model(**model_input)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank1]:     return model_forward(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank1]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 333, in forward
[rank1]:     qry_reps = self.encode_input(qry, self.qry_chosen_layer) if qry else None  # (bsz_per_device, dim)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 110, in encode_input
[rank1]:     hidden_states = self.encoder(**input, return_dict=True, output_hidden_states=True)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/peft/peft_model.py", line 881, in forward
[rank1]:     return self.get_base_model()(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/tail_token_wrapper.py", line 77, in forward
[rank1]:     outputs = self.base.model(
[rank1]: TypeError: Qwen2VLModel(
[rank1]:   (embed_tokens): Embedding(151936, 1536)
[rank1]:   (layers): ModuleList(
[rank1]:     (0-27): 28 x Qwen2VLDecoderLayer(
[rank1]:       (self_attn): Qwen2VLFlashAttention2(
[rank1]:         (q_proj): lora.Linear(
[rank1]:           (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
[rank1]:           (lora_dropout): ModuleDict(
[rank1]:             (default): Dropout(p=0.1, inplace=False)
[rank1]:           )
[rank1]:           (lora_A): ModuleDict(
[rank1]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank1]:           )
[rank1]:           (lora_B): ModuleDict(
[rank1]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank1]:           )
[rank1]:           (lora_embedding_A): ParameterDict()
[rank1]:           (lora_embedding_B): ParameterDict()
[rank1]:           (lora_magnitude_vector): ModuleDict(
[rank1]:             (default): lora.dora.DoraLinearLayer()
[rank1]:           )
[rank1]:         )
[rank1]:         (k_proj): lora.Linear(
[rank1]:           (base_layer): Linear(in_features=1536, out_features=256, bias=True)
[rank1]:           (lora_dropout): ModuleDict(
[rank1]:             (default): Dropout(p=0.1, inplace=False)
[rank1]:           )
[rank1]:           (lora_A): ModuleDict(
[rank1]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank1]:           )
[rank1]:           (lora_B): ModuleDict(
[rank1]:             (default): Linear(in_features=16, out_features=256, bias=False)
[rank1]:           )
[rank1]:           (lora_embedding_A): ParameterDict()
[rank1]:           (lora_embedding_B): ParameterDict()
[rank1]:           (lora_magnitude_vector): ModuleDict(
[rank1]:             (default): lora.dora.DoraLinearLayer()
[rank1]:           )
[rank1]:         )
[rank1]:         (v_proj): lora.Linear(
[rank1]:           (base_layer): Linear(in_features=1536, out_features=256, bias=True)
[rank1]:           (lora_dropout): ModuleDict(
[rank1]:             (default): Dropout(p=0.1, inplace=False)
[rank1]:           )
[rank1]:           (lora_A): ModuleDict(
[rank1]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank1]:           )
[rank1]:           (lora_B): ModuleDict(
[rank1]:             (default): Linear(in_features=16, out_features=256, bias=False)
[rank1]:           )
[rank1]:           (lora_embedding_A): ParameterDict()
[rank1]:           (lora_embedding_B): ParameterDict()
[rank1]:           (lora_magnitude_vector): ModuleDict(
[rank1]:             (default): lora.dora.DoraLinearLayer()
[rank1]:           )
[rank1]:         )
[rank1]:         (o_proj): lora.Linear(
[rank1]:           (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
[rank1]:           (lora_dropout): ModuleDict(
[rank1]:             (default): Dropout(p=0.1, inplace=False)
[rank1]:           )
[rank1]:           (lora_A): ModuleDict(
[rank1]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank1]:           )
[rank1]:           (lora_B): ModuleDict(
[rank1]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank1]:           )
[rank1]:           (lora_embedding_A): ParameterDict()
[rank1]:           (lora_embedding_B): ParameterDict()
[rank1]:           (lora_magnitude_vector): ModuleDict(
[rank1]:             (default): lora.dora.DoraLinearLayer()
[rank1]:           )
[rank1]:         )
[rank1]:         (rotary_emb): Qwen2VLRotaryEmbedding()
[rank1]:       )
[rank1]:       (mlp): Qwen2MLP(
[rank1]:         (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
[rank1]:         (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
[rank1]:         (down_proj): lora.Linear(
[rank1]:           (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
[rank1]:           (lora_dropout): ModuleDict(
[rank1]:             (default): Dropout(p=0.1, inplace=False)
[rank1]:           )
[rank1]:           (lora_A): ModuleDict(
[rank1]:             (default): Linear(in_features=8960, out_features=16, bias=False)
[rank1]:           )
[rank1]:           (lora_B): ModuleDict(
[rank1]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank1]:           )
[rank1]:           (lora_embedding_A): ParameterDict()
[rank1]:           (lora_embedding_B): ParameterDict()
[rank1]:           (lora_magnitude_vector): ModuleDict(
[rank1]:             (default): lora.dora.DoraLinearLayer()
[rank1]:           )
[rank1]:         )
[rank1]:         (act_fn): SiLU()
[rank1]:       )
[rank1]:       (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank1]:       (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank1]:     )
[rank1]:   )
[rank1]:   (norm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank1]:   (rotary_emb): Qwen2VLRotaryEmbedding()
[rank1]: ) got multiple values for keyword argument 'return_dict'
Traceback (most recent call last):
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 112, in <module>
    main()
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 104, in main
    trainer.train(resume_from_checkpoint=resume_checkpoint_dir)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 510, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 708, in training_step
    loss = self.gc(queries, targets, no_sync_except_last=_distributed)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 71, in __call__
    return self.cache_step(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 285, in cache_step
    model_reps, rnd_states = self.forward_no_grad(model, x)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 187, in forward_no_grad
    y = self.model_call(model, x)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 141, in model_call
    return model(**model_input)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
    return model_forward(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
    return func(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 333, in forward
    qry_reps = self.encode_input(qry, self.qry_chosen_layer) if qry else None  # (bsz_per_device, dim)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 110, in encode_input
    hidden_states = self.encoder(**input, return_dict=True, output_hidden_states=True)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/peft/peft_model.py", line 881, in forward
    return self.get_base_model()(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/tail_token_wrapper.py", line 77, in forward
    outputs = self.base.model(
TypeError: Qwen2VLModel(
  (embed_tokens): Embedding(151936, 1536)
  (layers): ModuleList(
    (0-27): 28 x Qwen2VLDecoderLayer(
      (self_attn): Qwen2VLFlashAttention2(
        (q_proj): lora.Linear(
          (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1536, out_features=16, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=16, out_features=1536, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
          (lora_magnitude_vector): ModuleDict(
            (default): lora.dora.DoraLinearLayer()
          )
        )
        (k_proj): lora.Linear(
          (base_layer): Linear(in_features=1536, out_features=256, bias=True)
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1536, out_features=16, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=16, out_features=256, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
          (lora_magnitude_vector): ModuleDict(
            (default): lora.dora.DoraLinearLayer()
          )
        )
        (v_proj): lora.Linear(
          (base_layer): Linear(in_features=1536, out_features=256, bias=True)
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1536, out_features=16, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=16, out_features=256, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
          (lora_magnitude_vector): ModuleDict(
            (default): lora.dora.DoraLinearLayer()
          )
        )
        (o_proj): lora.Linear(
          (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=1536, out_features=16, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=16, out_features=1536, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
          (lora_magnitude_vector): ModuleDict(
            (default): lora.dora.DoraLinearLayer()
          )
        )
        (rotary_emb): Qwen2VLRotaryEmbedding()
      )
      (mlp): Qwen2MLP(
        (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
        (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
        (down_proj): lora.Linear(
          (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
          (lora_dropout): ModuleDict(
            (default): Dropout(p=0.1, inplace=False)
          )
          (lora_A): ModuleDict(
            (default): Linear(in_features=8960, out_features=16, bias=False)
          )
          (lora_B): ModuleDict(
            (default): Linear(in_features=16, out_features=1536, bias=False)
          )
          (lora_embedding_A): ParameterDict()
          (lora_embedding_B): ParameterDict()
          (lora_magnitude_vector): ModuleDict(
            (default): lora.dora.DoraLinearLayer()
          )
        )
        (act_fn): SiLU()
      )
      (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
    )
  )
  (norm): Qwen2RMSNorm((1536,), eps=1e-06)
  (rotary_emb): Qwen2VLRotaryEmbedding()
) got multiple values for keyword argument 'return_dict'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 112, in <module>
[rank0]:     main()
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/train.py", line 104, in main
[rank0]:     trainer.train(resume_from_checkpoint=resume_checkpoint_dir)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 510, in _inner_training_loop
[rank0]:     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/trainer.py", line 708, in training_step
[rank0]:     loss = self.gc(queries, targets, no_sync_except_last=_distributed)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 71, in __call__
[rank0]:     return self.cache_step(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 285, in cache_step
[rank0]:     model_reps, rnd_states = self.forward_no_grad(model, x)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 187, in forward_no_grad
[rank0]:     y = self.model_call(model, x)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/grad_cache/grad_cache.py", line 141, in model_call
[rank0]:     return model(**model_input)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 818, in forward
[rank0]:     return model_forward(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/accelerate/utils/operations.py", line 806, in __call__
[rank0]:     return convert_to_fp32(self.model_forward(*args, **kwargs))
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/amp/autocast_mode.py", line 43, in decorate_autocast
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 333, in forward
[rank0]:     qry_reps = self.encode_input(qry, self.qry_chosen_layer) if qry else None  # (bsz_per_device, dim)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/model.py", line 110, in encode_input
[rank0]:     hidden_states = self.encoder(**input, return_dict=True, output_hidden_states=True)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/peft/peft_model.py", line 881, in forward
[rank0]:     return self.get_base_model()(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/tail_token_wrapper.py", line 77, in forward
[rank0]:     outputs = self.base.model(
[rank0]: TypeError: Qwen2VLModel(
[rank0]:   (embed_tokens): Embedding(151936, 1536)
[rank0]:   (layers): ModuleList(
[rank0]:     (0-27): 28 x Qwen2VLDecoderLayer(
[rank0]:       (self_attn): Qwen2VLFlashAttention2(
[rank0]:         (q_proj): lora.Linear(
[rank0]:           (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
[rank0]:           (lora_dropout): ModuleDict(
[rank0]:             (default): Dropout(p=0.1, inplace=False)
[rank0]:           )
[rank0]:           (lora_A): ModuleDict(
[rank0]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank0]:           )
[rank0]:           (lora_B): ModuleDict(
[rank0]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank0]:           )
[rank0]:           (lora_embedding_A): ParameterDict()
[rank0]:           (lora_embedding_B): ParameterDict()
[rank0]:           (lora_magnitude_vector): ModuleDict(
[rank0]:             (default): lora.dora.DoraLinearLayer()
[rank0]:           )
[rank0]:         )
[rank0]:         (k_proj): lora.Linear(
[rank0]:           (base_layer): Linear(in_features=1536, out_features=256, bias=True)
[rank0]:           (lora_dropout): ModuleDict(
[rank0]:             (default): Dropout(p=0.1, inplace=False)
[rank0]:           )
[rank0]:           (lora_A): ModuleDict(
[rank0]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank0]:           )
[rank0]:           (lora_B): ModuleDict(
[rank0]:             (default): Linear(in_features=16, out_features=256, bias=False)
[rank0]:           )
[rank0]:           (lora_embedding_A): ParameterDict()
[rank0]:           (lora_embedding_B): ParameterDict()
[rank0]:           (lora_magnitude_vector): ModuleDict(
[rank0]:             (default): lora.dora.DoraLinearLayer()
[rank0]:           )
[rank0]:         )
[rank0]:         (v_proj): lora.Linear(
[rank0]:           (base_layer): Linear(in_features=1536, out_features=256, bias=True)
[rank0]:           (lora_dropout): ModuleDict(
[rank0]:             (default): Dropout(p=0.1, inplace=False)
[rank0]:           )
[rank0]:           (lora_A): ModuleDict(
[rank0]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank0]:           )
[rank0]:           (lora_B): ModuleDict(
[rank0]:             (default): Linear(in_features=16, out_features=256, bias=False)
[rank0]:           )
[rank0]:           (lora_embedding_A): ParameterDict()
[rank0]:           (lora_embedding_B): ParameterDict()
[rank0]:           (lora_magnitude_vector): ModuleDict(
[rank0]:             (default): lora.dora.DoraLinearLayer()
[rank0]:           )
[rank0]:         )
[rank0]:         (o_proj): lora.Linear(
[rank0]:           (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
[rank0]:           (lora_dropout): ModuleDict(
[rank0]:             (default): Dropout(p=0.1, inplace=False)
[rank0]:           )
[rank0]:           (lora_A): ModuleDict(
[rank0]:             (default): Linear(in_features=1536, out_features=16, bias=False)
[rank0]:           )
[rank0]:           (lora_B): ModuleDict(
[rank0]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank0]:           )
[rank0]:           (lora_embedding_A): ParameterDict()
[rank0]:           (lora_embedding_B): ParameterDict()
[rank0]:           (lora_magnitude_vector): ModuleDict(
[rank0]:             (default): lora.dora.DoraLinearLayer()
[rank0]:           )
[rank0]:         )
[rank0]:         (rotary_emb): Qwen2VLRotaryEmbedding()
[rank0]:       )
[rank0]:       (mlp): Qwen2MLP(
[rank0]:         (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
[rank0]:         (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
[rank0]:         (down_proj): lora.Linear(
[rank0]:           (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
[rank0]:           (lora_dropout): ModuleDict(
[rank0]:             (default): Dropout(p=0.1, inplace=False)
[rank0]:           )
[rank0]:           (lora_A): ModuleDict(
[rank0]:             (default): Linear(in_features=8960, out_features=16, bias=False)
[rank0]:           )
[rank0]:           (lora_B): ModuleDict(
[rank0]:             (default): Linear(in_features=16, out_features=1536, bias=False)
[rank0]:           )
[rank0]:           (lora_embedding_A): ParameterDict()
[rank0]:           (lora_embedding_B): ParameterDict()
[rank0]:           (lora_magnitude_vector): ModuleDict(
[rank0]:             (default): lora.dora.DoraLinearLayer()
[rank0]:           )
[rank0]:         )
[rank0]:         (act_fn): SiLU()
[rank0]:       )
[rank0]:       (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank0]:       (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank0]:     )
[rank0]:   )
[rank0]:   (norm): Qwen2RMSNorm((1536,), eps=1e-06)
[rank0]:   (rotary_emb): Qwen2VLRotaryEmbedding()
[rank0]: ) got multiple values for keyword argument 'return_dict'
[1;34mwandb[0m: 
[1;34mwandb[0m: ðŸš€ View run [33mTailToken-Qwen/Qwen2-VL-2B-Instruct[0m at: [34m[0m
[1;34mwandb[0m: Find logs at: [1;35mexperiments/public/exps/train/TailToken-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251019_165344-a7rkfsm4/logs[0m
W1019 16:54:05.309000 136919728248640 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 184859 closing signal SIGTERM
E1019 16:54:05.623000 136919728248640 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 184860) of binary: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python3.10
Traceback (most recent call last):
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-10-19_16:54:05
  host      : node40.enst.fr
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 184860)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
Job finished at: dim. 19 oct. 2025 16:54:05 CEST
