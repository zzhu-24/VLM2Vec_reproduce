==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 15:15:26.176000 130375345551168 torch/distributed/run.py:779] 
W1103 15:15:26.176000 130375345551168 torch/distributed/run.py:779] *****************************************
W1103 15:15:26.176000 130375345551168 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 15:15:26.176000 130375345551168 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 15:15:36,503] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.94it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.60it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_151536-roa8yzbc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/roa8yzbc
[2025-11-03 15:15:38,100] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.24it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.13it/s]
[2025-11-03 15:15:38,709] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 15:15:47,657] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 15:15:48,844] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 15:15:48,845] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 15:15:53,636] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 15:15:53,637] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 15:15:54,502] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 15:15:54,502] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 15:15:54,503] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 15:15:54,504] INFO [src.utils:19] ==================================================
[2025-11-03 15:15:54,505] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 15:15:54,505] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:15:54,506] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:15:54,507] INFO [src.utils:19] ==================================================
[2025-11-03 15:15:56,279] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:15:56,279] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:15:56,280] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:15:56,280] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:15:56,280] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:15:56,280] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:15:56,280] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:15:56,280] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:15:56,280] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:15:56,281] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:15:56,281] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:15:56,281] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:15:56,282] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:15:56,282] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:15:56,288] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 15:15:56,290] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 15:15:56,294] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 15:15:56,297] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:04<6:59:06,  4.19s/it]                                                  {'loss': 21.6478, 'grad_norm': 790.16650390625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:59:06,  4.19s/it]  0%|          | 2/6000 [00:06<5:32:42,  3.33s/it]                                                  {'loss': 18.9604, 'grad_norm': 767.9754638671875, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:32:42,  3.33s/it]  0%|          | 3/6000 [00:09<5:10:00,  3.10s/it]                                                  {'loss': 18.7105, 'grad_norm': 807.0392456054688, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:10:00,  3.10s/it]  0%|          | 4/6000 [00:12<4:59:40,  3.00s/it]                                                  {'loss': 19.6779, 'grad_norm': 781.15380859375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:59:40,  3.00s/it]  0%|          | 5/6000 [00:15<4:53:37,  2.94s/it]                                                  {'loss': 20.5937, 'grad_norm': 659.2492065429688, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:15<4:53:37,  2.94s/it]  0%|          | 6/6000 [00:18<4:50:15,  2.91s/it]                                                  {'loss': 20.3337, 'grad_norm': 712.4927368164062, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:18<4:50:15,  2.91s/it]  0%|          | 7/6000 [00:21<4:46:16,  2.87s/it]                                                  {'loss': 20.4217, 'grad_norm': 765.8165283203125, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:21<4:46:16,  2.87s/it]  0%|          | 8/6000 [00:23<4:40:35,  2.81s/it]                                                  {'loss': 19.6395, 'grad_norm': 695.7813720703125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:23<4:40:35,  2.81s/it]  0%|          | 9/6000 [00:26<4:40:52,  2.81s/it]                                                  {'loss': 14.4991, 'grad_norm': 680.3450927734375, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:26<4:40:52,  2.81s/it]  0%|          | 10/6000 [00:29<4:39:14,  2.80s/it]                                                   {'loss': 18.8216, 'grad_norm': 672.177978515625, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:29<4:39:14,  2.80s/it]  0%|          | 11/6000 [00:32<4:49:34,  2.90s/it]                                                   {'loss': 23.0424, 'grad_norm': 682.3521118164062, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:32<4:49:34,  2.90s/it]  0%|          | 12/6000 [00:35<4:52:15,  2.93s/it]                                                   {'loss': 19.4723, 'grad_norm': 607.9486694335938, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:35<4:52:15,  2.93s/it]  0%|          | 13/6000 [00:38<4:49:24,  2.90s/it]                                                   {'loss': 19.1919, 'grad_norm': 598.6888427734375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:38<4:49:24,  2.90s/it]  0%|          | 14/6000 [00:41<4:50:45,  2.91s/it]                                                   {'loss': 21.7062, 'grad_norm': 600.6580810546875, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:41<4:50:45,  2.91s/it]  0%|          | 15/6000 [00:44<4:47:46,  2.88s/it]                                                   {'loss': 17.5691, 'grad_norm': 562.4127807617188, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:44<4:47:46,  2.88s/it]  0%|          | 16/6000 [00:46<4:44:23,  2.85s/it]                                                   {'loss': 18.0592, 'grad_norm': 555.23583984375, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:46<4:44:23,  2.85s/it]  0%|          | 17/6000 [00:49<4:43:20,  2.84s/it]                                                   {'loss': 18.0297, 'grad_norm': 536.729248046875, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:49<4:43:20,  2.84s/it]  0%|          | 18/6000 [00:52<4:44:07,  2.85s/it]                                                   {'loss': 15.5804, 'grad_norm': 421.3643798828125, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:52<4:44:07,  2.85s/it]  0%|          | 19/6000 [00:55<4:42:55,  2.84s/it]                                                   {'loss': 17.8197, 'grad_norm': 487.77154541015625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:55<4:42:55,  2.84s/it]  0%|          | 20/6000 [00:58<4:41:49,  2.83s/it]                                                   {'loss': 18.6688, 'grad_norm': 542.0296020507812, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:58<4:41:49,  2.83s/it]  0%|          | 21/6000 [01:01<4:46:20,  2.87s/it]                                                   {'loss': 15.538, 'grad_norm': 468.9342041015625, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:01<4:46:20,  2.87s/it]  0%|          | 22/6000 [01:04<4:48:42,  2.90s/it]                                                   {'loss': 15.5759, 'grad_norm': 414.92669677734375, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:04<4:48:42,  2.90s/it]  0%|          | 23/6000 [01:06<4:46:12,  2.87s/it]                                                   {'loss': 14.1791, 'grad_norm': 436.73858642578125, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:06<4:46:12,  2.87s/it]  0%|          | 24/6000 [01:09<4:47:34,  2.89s/it]                                                   {'loss': 11.6231, 'grad_norm': 410.7353515625, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:09<4:47:34,  2.89s/it]  0%|          | 25/6000 [01:12<4:45:52,  2.87s/it]                                                   {'loss': 13.8312, 'grad_norm': 535.7279052734375, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:12<4:45:52,  2.87s/it]  0%|          | 26/6000 [01:15<4:46:26,  2.88s/it]                                                   {'loss': 14.8986, 'grad_norm': 670.6940307617188, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:15<4:46:26,  2.88s/it]  0%|          | 27/6000 [01:18<4:45:52,  2.87s/it]                                                   {'loss': 13.4003, 'grad_norm': 767.90283203125, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:18<4:45:52,  2.87s/it]  0%|          | 28/6000 [01:22<5:13:12,  3.15s/it]                                                   {'loss': 12.4315, 'grad_norm': 530.1488037109375, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:22<5:13:12,  3.15s/it]  0%|          | 29/6000 [01:24<5:01:57,  3.03s/it]                                                   {'loss': 11.8041, 'grad_norm': 540.8189086914062, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:24<5:01:57,  3.03s/it]  0%|          | 30/6000 [01:27<4:55:41,  2.97s/it]                                                   {'loss': 11.5165, 'grad_norm': 643.4359741210938, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:27<4:55:41,  2.97s/it]  1%|          | 31/6000 [01:30<4:50:16,  2.92s/it]                                                   {'loss': 10.5206, 'grad_norm': 421.8796081542969, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:30<4:50:16,  2.92s/it]  1%|          | 32/6000 [01:33<4:48:12,  2.90s/it]                                                   {'loss': 10.0597, 'grad_norm': 517.5205078125, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:33<4:48:12,  2.90s/it]  1%|          | 33/6000 [01:36<4:48:13,  2.90s/it]                                                   {'loss': 10.5649, 'grad_norm': 416.2618713378906, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:36<4:48:13,  2.90s/it]  1%|          | 34/6000 [01:39<4:46:00,  2.88s/it]                                                   {'loss': 9.649, 'grad_norm': 333.39129638671875, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:39<4:46:00,  2.88s/it]  1%|          | 35/6000 [01:42<4:46:51,  2.89s/it]                                                   {'loss': 8.5102, 'grad_norm': 327.0218200683594, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:42<4:46:51,  2.89s/it]  1%|          | 36/6000 [01:44<4:43:43,  2.85s/it]                                                   {'loss': 8.3405, 'grad_norm': 556.1475830078125, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:44<4:43:43,  2.85s/it]  1%|          | 37/6000 [01:47<4:43:33,  2.85s/it]                                                   {'loss': 7.649, 'grad_norm': 501.08807373046875, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:47<4:43:33,  2.85s/it]  1%|          | 38/6000 [01:50<4:41:04,  2.83s/it]                                                   {'loss': 7.2849, 'grad_norm': 340.5802001953125, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:50<4:41:04,  2.83s/it]  1%|          | 39/6000 [01:53<4:41:27,  2.83s/it]                                                   {'loss': 6.9423, 'grad_norm': 276.2848815917969, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:53<4:41:27,  2.83s/it]  1%|          | 40/6000 [01:56<4:41:43,  2.84s/it]                                                   {'loss': 6.9441, 'grad_norm': 443.16412353515625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:56<4:41:43,  2.84s/it]  1%|          | 41/6000 [01:58<4:41:27,  2.83s/it]                                                   {'loss': 7.8235, 'grad_norm': 408.7929382324219, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:58<4:41:27,  2.83s/it]  1%|          | 42/6000 [02:01<4:40:46,  2.83s/it]                                                   {'loss': 7.414, 'grad_norm': 265.90325927734375, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:01<4:40:46,  2.83s/it]  1%|          | 43/6000 [02:05<5:11:18,  3.14s/it]                                                   {'loss': 6.18, 'grad_norm': 191.3489990234375, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:05<5:11:18,  3.14s/it]  1%|          | 44/6000 [02:08<5:16:09,  3.18s/it]                                                   {'loss': 6.0385, 'grad_norm': 224.4851837158203, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:08<5:16:09,  3.18s/it]  1%|          | 45/6000 [02:11<5:06:33,  3.09s/it]                                                   {'loss': 6.7096, 'grad_norm': 304.4216613769531, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:11<5:06:33,  3.09s/it]  1%|          | 46/6000 [02:14<5:01:57,  3.04s/it]                                                   {'loss': 5.7548, 'grad_norm': 268.5943908691406, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:14<5:01:57,  3.04s/it]  1%|          | 47/6000 [02:17<4:55:39,  2.98s/it]                                                   {'loss': 5.5215, 'grad_norm': 344.896484375, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:17<4:55:39,  2.98s/it]  1%|          | 48/6000 [02:20<4:53:48,  2.96s/it]                                                   {'loss': 5.5486, 'grad_norm': 173.90516662597656, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:20<4:53:48,  2.96s/it]  1%|          | 49/6000 [02:23<4:47:48,  2.90s/it]                                                   {'loss': 5.251, 'grad_norm': 435.532470703125, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:23<4:47:48,  2.90s/it]  1%|          | 50/6000 [02:26<4:51:03,  2.93s/it]                                                   {'loss': 4.3319, 'grad_norm': 175.3483428955078, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:26<4:51:03,  2.93s/it][2025-11-03 15:18:22,744] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-03 15:18:22,755] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:18:23,370] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:39<10:11:34,  6.17s/it]                                                    {'loss': 5.0788, 'grad_norm': 183.08786010742188, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:39<10:11:34,  6.17s/it]  1%|          | 52/6000 [02:42<8:30:03,  5.15s/it]                                                    {'loss': 4.4153, 'grad_norm': 310.71563720703125, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:42<8:30:03,  5.15s/it]  1%|          | 53/6000 [02:45<7:23:17,  4.47s/it]                                                   {'loss': 5.7363, 'grad_norm': 234.75942993164062, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:45<7:23:17,  4.47s/it]  1%|          | 54/6000 [02:48<6:32:16,  3.96s/it]                                                   {'loss': 4.4748, 'grad_norm': 166.23814392089844, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:48<6:32:16,  3.96s/it]  1%|          | 55/6000 [02:51<5:58:14,  3.62s/it]                                                   {'loss': 4.6261, 'grad_norm': 155.4097442626953, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:51<5:58:14,  3.62s/it]  1%|          | 56/6000 [02:54<5:35:42,  3.39s/it]                                                   {'loss': 3.7058, 'grad_norm': 100.24266052246094, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:54<5:35:42,  3.39s/it]  1%|          | 57/6000 [02:56<5:18:01,  3.21s/it]                                                   {'loss': 3.9216, 'grad_norm': 125.8536376953125, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:56<5:18:01,  3.21s/it]  1%|          | 58/6000 [02:59<5:05:39,  3.09s/it]                                                   {'loss': 3.4206, 'grad_norm': 91.2566909790039, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:59<5:05:39,  3.09s/it]  1%|          | 59/6000 [03:02<4:56:32,  2.99s/it]                                                   {'loss': 4.4937, 'grad_norm': 156.79917907714844, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:02<4:56:32,  2.99s/it]  1%|          | 60/6000 [03:05<4:49:48,  2.93s/it]                                                   {'loss': 3.9321, 'grad_norm': 107.45348358154297, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:05<4:49:48,  2.93s/it]  1%|          | 61/6000 [03:07<4:44:40,  2.88s/it]                                                   {'loss': 3.1738, 'grad_norm': 91.82768249511719, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:07<4:44:40,  2.88s/it]  1%|          | 62/6000 [03:10<4:44:30,  2.87s/it]                                                   {'loss': 3.6249, 'grad_norm': 148.06068420410156, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:10<4:44:30,  2.87s/it]  1%|          | 63/6000 [03:13<4:42:32,  2.86s/it]                                                   {'loss': 3.4151, 'grad_norm': 114.31485748291016, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:13<4:42:32,  2.86s/it]  1%|          | 64/6000 [03:16<4:39:04,  2.82s/it]                                                   {'loss': 3.1487, 'grad_norm': 68.73657989501953, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:16<4:39:04,  2.82s/it]  1%|          | 65/6000 [03:19<4:36:58,  2.80s/it]                                                   {'loss': 3.5908, 'grad_norm': 151.26129150390625, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:19<4:36:58,  2.80s/it]  1%|          | 66/6000 [03:22<4:50:13,  2.93s/it]                                                   {'loss': 3.4396, 'grad_norm': 125.32089233398438, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:22<4:50:13,  2.93s/it]  1%|          | 67/6000 [03:25<4:46:41,  2.90s/it]                                                   {'loss': 3.5004, 'grad_norm': 289.7903137207031, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:25<4:46:41,  2.90s/it]  1%|          | 68/6000 [03:27<4:43:12,  2.86s/it]                                                   {'loss': 3.1437, 'grad_norm': 93.8497085571289, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:27<4:43:12,  2.86s/it]  1%|          | 69/6000 [03:30<4:40:51,  2.84s/it]                                                   {'loss': 3.1383, 'grad_norm': 127.55726623535156, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:30<4:40:51,  2.84s/it]  1%|          | 70/6000 [03:33<4:42:27,  2.86s/it]                                                   {'loss': 2.9624, 'grad_norm': 43.829017639160156, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:33<4:42:27,  2.86s/it]  1%|          | 71/6000 [03:36<4:40:05,  2.83s/it]                                                   {'loss': 3.1259, 'grad_norm': 72.43782806396484, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:36<4:40:05,  2.83s/it]  1%|          | 72/6000 [03:39<4:59:05,  3.03s/it]                                                   {'loss': 3.1682, 'grad_norm': 100.25580596923828, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:39<4:59:05,  3.03s/it]  1%|          | 73/6000 [03:42<4:53:44,  2.97s/it]                                                   {'loss': 2.9251, 'grad_norm': 38.47449493408203, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:42<4:53:44,  2.97s/it]  1%|          | 74/6000 [03:45<4:47:29,  2.91s/it]                                                   {'loss': 3.0769, 'grad_norm': 49.16478729248047, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:45<4:47:29,  2.91s/it]  1%|â–         | 75/6000 [03:48<4:43:38,  2.87s/it]                                                   {'loss': 2.929, 'grad_norm': 59.03264617919922, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:48<4:43:38,  2.87s/it]  1%|â–         | 76/6000 [03:51<4:45:33,  2.89s/it]                                                   {'loss': 3.1321, 'grad_norm': 54.727081298828125, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:51<4:45:33,  2.89s/it]  1%|â–         | 77/6000 [03:54<4:45:23,  2.89s/it]                                                   {'loss': 3.4254, 'grad_norm': 80.78983306884766, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:54<4:45:23,  2.89s/it]  1%|â–         | 78/6000 [03:57<4:55:42,  3.00s/it]                                                   {'loss': 3.4111, 'grad_norm': 105.87373352050781, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:57<4:55:42,  3.00s/it]  1%|â–         | 79/6000 [04:00<4:50:25,  2.94s/it]                                                   {'loss': 2.9749, 'grad_norm': 57.13746643066406, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [04:00<4:50:25,  2.94s/it]  1%|â–         | 80/6000 [04:03<4:55:24,  2.99s/it]                                                   {'loss': 3.0797, 'grad_norm': 74.45137786865234, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:03<4:55:24,  2.99s/it]  1%|â–         | 81/6000 [04:06<4:49:33,  2.94s/it]                                                   {'loss': 2.9362, 'grad_norm': 45.57278823852539, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:06<4:49:33,  2.94s/it]  1%|â–         | 82/6000 [04:08<4:47:55,  2.92s/it]                                                   {'loss': 2.9751, 'grad_norm': 45.917415618896484, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:08<4:47:55,  2.92s/it]  1%|â–         | 83/6000 [04:11<4:45:09,  2.89s/it]                                                   {'loss': 3.2565, 'grad_norm': 128.90174865722656, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:11<4:45:09,  2.89s/it]  1%|â–         | 84/6000 [04:14<4:45:23,  2.89s/it]                                                   {'loss': 3.1579, 'grad_norm': 156.99952697753906, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:14<4:45:23,  2.89s/it]  1%|â–         | 85/6000 [04:17<4:54:22,  2.99s/it]                                                   {'loss': 3.0523, 'grad_norm': 42.807289123535156, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:17<4:54:22,  2.99s/it]  1%|â–         | 86/6000 [04:20<4:50:04,  2.94s/it]                                                   {'loss': 3.2212, 'grad_norm': 81.4217300415039, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:20<4:50:04,  2.94s/it]  1%|â–         | 87/6000 [04:23<4:46:36,  2.91s/it]                                                   {'loss': 3.1875, 'grad_norm': 102.06836700439453, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:23<4:46:36,  2.91s/it]  1%|â–         | 88/6000 [04:26<4:43:46,  2.88s/it]                                                   {'loss': 3.379, 'grad_norm': 81.14615631103516, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:26<4:43:46,  2.88s/it]  1%|â–         | 89/6000 [04:29<4:40:04,  2.84s/it]                                                   {'loss': 3.0173, 'grad_norm': 68.51410675048828, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:29<4:40:04,  2.84s/it]  2%|â–         | 90/6000 [04:32<4:40:59,  2.85s/it]                                                   {'loss': 2.8824, 'grad_norm': 39.7028694152832, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:32<4:40:59,  2.85s/it]  2%|â–         | 91/6000 [04:34<4:41:55,  2.86s/it]                                                   {'loss': 2.8907, 'grad_norm': 36.00093460083008, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:34<4:41:55,  2.86s/it]  2%|â–         | 92/6000 [04:38<4:52:14,  2.97s/it]                                                   {'loss': 2.9665, 'grad_norm': 77.3813705444336, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:38<4:52:14,  2.97s/it]  2%|â–         | 93/6000 [04:41<4:52:29,  2.97s/it]                                                   {'loss': 2.9449, 'grad_norm': 72.71320343017578, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:41<4:52:29,  2.97s/it]  2%|â–         | 94/6000 [04:43<4:48:08,  2.93s/it]                                                   {'loss': 2.9788, 'grad_norm': 33.67250442504883, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:43<4:48:08,  2.93s/it]  2%|â–         | 95/6000 [04:46<4:44:38,  2.89s/it]                                                   {'loss': 2.8986, 'grad_norm': 37.349639892578125, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:46<4:44:38,  2.89s/it]  2%|â–         | 96/6000 [04:49<4:41:34,  2.86s/it]                                                   {'loss': 2.9105, 'grad_norm': 23.58387565612793, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:49<4:41:34,  2.86s/it]  2%|â–         | 97/6000 [04:52<4:41:33,  2.86s/it]                                                   {'loss': 3.0287, 'grad_norm': 59.91700744628906, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:52<4:41:33,  2.86s/it]  2%|â–         | 98/6000 [04:55<4:40:32,  2.85s/it]                                                   {'loss': 2.8589, 'grad_norm': 30.726301193237305, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:55<4:40:32,  2.85s/it]  2%|â–         | 99/6000 [04:58<4:39:04,  2.84s/it]                                                   {'loss': 2.8228, 'grad_norm': 20.850698471069336, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:58<4:39:04,  2.84s/it]  2%|â–         | 100/6000 [05:00<4:36:43,  2.81s/it]                                                    {'loss': 2.8915, 'grad_norm': 23.848222732543945, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [05:00<4:36:43,  2.81s/it][2025-11-03 15:20:57,290] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-03 15:20:57,301] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:20:57,910] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [05:15<10:25:57,  6.37s/it]                                                     {'loss': 2.9276, 'grad_norm': 53.048336029052734, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:15<10:25:57,  6.37s/it]  2%|â–         | 102/6000 [05:18<8:40:52,  5.30s/it]                                                     {'loss': 2.868, 'grad_norm': 26.631303787231445, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:18<8:40:52,  5.30s/it]  2%|â–         | 103/6000 [05:21<7:29:45,  4.58s/it]                                                    {'loss': 2.8506, 'grad_norm': 34.03281784057617, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:21<7:29:45,  4.58s/it]  2%|â–         | 104/6000 [05:24<6:45:19,  4.12s/it]                                                    {'loss': 2.882, 'grad_norm': 23.72303581237793, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [05:24<6:45:19,  4.12s/it]  2%|â–         | 105/6000 [05:27<6:09:35,  3.76s/it]                                                    {'loss': 2.986, 'grad_norm': 103.02629089355469, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [05:27<6:09:35,  3.76s/it]  2%|â–         | 106/6000 [05:30<5:46:11,  3.52s/it]                                                    {'loss': 2.819, 'grad_norm': 23.323532104492188, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:30<5:46:11,  3.52s/it]  2%|â–         | 107/6000 [05:32<5:23:54,  3.30s/it]                                                    {'loss': 2.872, 'grad_norm': 27.219371795654297, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:32<5:23:54,  3.30s/it]  2%|â–         | 108/6000 [05:35<5:13:47,  3.20s/it]                                                    {'loss': 2.8204, 'grad_norm': 36.017330169677734, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:35<5:13:47,  3.20s/it]  2%|â–         | 109/6000 [05:39<5:16:08,  3.22s/it]                                                    {'loss': 2.8941, 'grad_norm': 53.83388900756836, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:39<5:16:08,  3.22s/it]  2%|â–         | 110/6000 [05:42<5:07:18,  3.13s/it]                                                    {'loss': 2.9148, 'grad_norm': 28.416242599487305, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:42<5:07:18,  3.13s/it]  2%|â–         | 111/6000 [05:44<5:02:30,  3.08s/it]                                                    {'loss': 2.8287, 'grad_norm': 27.97972869873047, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:45<5:02:30,  3.08s/it]  2%|â–         | 112/6000 [05:48<5:12:17,  3.18s/it]                                                    {'loss': 3.1484, 'grad_norm': 64.04417419433594, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:48<5:12:17,  3.18s/it]  2%|â–         | 113/6000 [05:51<5:02:07,  3.08s/it]                                                    {'loss': 2.8977, 'grad_norm': 45.07197952270508, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:51<5:02:07,  3.08s/it]  2%|â–         | 114/6000 [05:54<4:56:14,  3.02s/it]                                                    {'loss': 2.8664, 'grad_norm': 35.95351028442383, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:54<4:56:14,  3.02s/it]  2%|â–         | 115/6000 [05:57<4:51:58,  2.98s/it]                                                    {'loss': 2.9014, 'grad_norm': 31.08959197998047, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:57<4:51:58,  2.98s/it]  2%|â–         | 116/6000 [05:59<4:47:32,  2.93s/it]                                                    {'loss': 2.8842, 'grad_norm': 41.65694046020508, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:59<4:47:32,  2.93s/it]  2%|â–         | 117/6000 [06:02<4:46:20,  2.92s/it]                                                    {'loss': 3.1485, 'grad_norm': 62.62651443481445, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [06:02<4:46:20,  2.92s/it]  2%|â–         | 118/6000 [06:06<5:02:06,  3.08s/it]                                                    {'loss': 2.8202, 'grad_norm': 22.541757583618164, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:06<5:02:06,  3.08s/it]  2%|â–         | 119/6000 [06:09<4:55:07,  3.01s/it]                                                    {'loss': 2.8408, 'grad_norm': 22.949420928955078, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:09<4:55:07,  3.01s/it]  2%|â–         | 120/6000 [06:11<4:51:10,  2.97s/it]                                                    {'loss': 2.8514, 'grad_norm': 26.789419174194336, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:11<4:51:10,  2.97s/it]  2%|â–         | 121/6000 [06:14<4:48:10,  2.94s/it]                                                    {'loss': 2.8502, 'grad_norm': 20.93491554260254, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [06:14<4:48:10,  2.94s/it]  2%|â–         | 122/6000 [06:17<4:50:28,  2.97s/it]                                                    {'loss': 2.808, 'grad_norm': 23.15884780883789, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [06:17<4:50:28,  2.97s/it]  2%|â–         | 123/6000 [06:20<4:45:16,  2.91s/it]                                                    {'loss': 2.9454, 'grad_norm': 36.42900466918945, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [06:20<4:45:16,  2.91s/it]  2%|â–         | 124/6000 [06:23<4:41:25,  2.87s/it]                                                    {'loss': 2.839, 'grad_norm': 19.34121322631836, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [06:23<4:41:25,  2.87s/it]  2%|â–         | 125/6000 [06:26<4:47:39,  2.94s/it]                                                    {'loss': 2.9802, 'grad_norm': 13.707035064697266, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [06:26<4:47:39,  2.94s/it]  2%|â–         | 126/6000 [06:29<4:47:12,  2.93s/it]                                                    {'loss': 2.8535, 'grad_norm': 39.1800651550293, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [06:29<4:47:12,  2.93s/it]  2%|â–         | 127/6000 [06:32<4:48:32,  2.95s/it]                                                    {'loss': 2.853, 'grad_norm': 20.280874252319336, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:32<4:48:32,  2.95s/it]  2%|â–         | 128/6000 [06:35<4:44:43,  2.91s/it]                                                    {'loss': 2.7934, 'grad_norm': 14.953656196594238, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:35<4:44:43,  2.91s/it]  2%|â–         | 129/6000 [06:38<4:42:44,  2.89s/it]                                                    {'loss': 2.9916, 'grad_norm': 34.65314865112305, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:38<4:42:44,  2.89s/it]  2%|â–         | 130/6000 [06:40<4:41:22,  2.88s/it]                                                    {'loss': 2.8748, 'grad_norm': 37.217498779296875, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:40<4:41:22,  2.88s/it]  2%|â–         | 131/6000 [06:43<4:41:18,  2.88s/it]                                                    {'loss': 2.8111, 'grad_norm': 27.731204986572266, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:43<4:41:18,  2.88s/it]  2%|â–         | 132/6000 [06:46<4:38:29,  2.85s/it]                                                    {'loss': 2.7942, 'grad_norm': 23.816814422607422, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:46<4:38:29,  2.85s/it]  2%|â–         | 133/6000 [06:49<4:39:19,  2.86s/it]                                                    {'loss': 2.8287, 'grad_norm': 25.435779571533203, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:49<4:39:19,  2.86s/it]  2%|â–         | 134/6000 [06:52<4:44:37,  2.91s/it]                                                    {'loss': 2.8412, 'grad_norm': 22.514524459838867, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:52<4:44:37,  2.91s/it]  2%|â–         | 135/6000 [06:55<4:42:29,  2.89s/it]                                                    {'loss': 2.8446, 'grad_norm': 17.782779693603516, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:55<4:42:29,  2.89s/it]  2%|â–         | 136/6000 [06:58<4:42:36,  2.89s/it]                                                    {'loss': 2.883, 'grad_norm': 31.188352584838867, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:58<4:42:36,  2.89s/it]  2%|â–         | 137/6000 [07:01<4:57:01,  3.04s/it]                                                    {'loss': 2.8365, 'grad_norm': 30.958120346069336, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [07:01<4:57:01,  3.04s/it]  2%|â–         | 138/6000 [07:04<4:48:40,  2.95s/it]                                                    {'loss': 2.8151, 'grad_norm': 27.801998138427734, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [07:04<4:48:40,  2.95s/it]  2%|â–         | 139/6000 [07:07<4:48:18,  2.95s/it]                                                    {'loss': 2.8547, 'grad_norm': 14.74551010131836, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [07:07<4:48:18,  2.95s/it]  2%|â–         | 140/6000 [07:10<4:58:06,  3.05s/it]                                                    {'loss': 2.8131, 'grad_norm': 18.616008758544922, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [07:10<4:58:06,  3.05s/it]  2%|â–         | 141/6000 [07:13<4:53:11,  3.00s/it]                                                    {'loss': 2.8431, 'grad_norm': 27.901493072509766, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [07:13<4:53:11,  3.00s/it]  2%|â–         | 142/6000 [07:16<4:44:58,  2.92s/it]                                                    {'loss': 2.8145, 'grad_norm': 22.304122924804688, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [07:16<4:44:58,  2.92s/it]  2%|â–         | 143/6000 [07:18<4:40:58,  2.88s/it]                                                    {'loss': 2.8157, 'grad_norm': 19.01237678527832, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [07:18<4:40:58,  2.88s/it]  2%|â–         | 144/6000 [07:21<4:41:35,  2.89s/it]                                                    {'loss': 2.8394, 'grad_norm': 23.871870040893555, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [07:21<4:41:35,  2.89s/it]  2%|â–         | 145/6000 [07:24<4:38:16,  2.85s/it]                                                    {'loss': 2.8312, 'grad_norm': 14.274036407470703, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [07:24<4:38:16,  2.85s/it]  2%|â–         | 146/6000 [07:27<4:39:37,  2.87s/it]                                                    {'loss': 2.8633, 'grad_norm': 17.51130485534668, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [07:27<4:39:37,  2.87s/it]  2%|â–         | 147/6000 [07:30<4:36:56,  2.84s/it]                                                    {'loss': 2.8081, 'grad_norm': 15.491432189941406, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [07:30<4:36:56,  2.84s/it]  2%|â–         | 148/6000 [07:33<4:42:05,  2.89s/it]                                                    {'loss': 2.8321, 'grad_norm': 19.210521697998047, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [07:33<4:42:05,  2.89s/it]  2%|â–         | 149/6000 [07:36<4:39:47,  2.87s/it]                                                    {'loss': 2.7985, 'grad_norm': 20.6895809173584, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:36<4:39:47,  2.87s/it]  2%|â–Ž         | 150/6000 [07:38<4:38:15,  2.85s/it]                                                    {'loss': 2.8293, 'grad_norm': 20.39100456237793, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:38<4:38:15,  2.85s/it][2025-11-03 15:23:35,450] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-03 15:23:35,462] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:23:36,088] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:52<9:58:56,  6.14s/it]                                                    {'loss': 2.8028, 'grad_norm': 14.604085922241211, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:52<9:58:56,  6.14s/it]  3%|â–Ž         | 152/6000 [07:55<8:21:53,  5.15s/it]                                                    {'loss': 2.8621, 'grad_norm': 42.93046951293945, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:55<8:21:53,  5.15s/it]  3%|â–Ž         | 153/6000 [07:58<7:13:28,  4.45s/it]                                                    {'loss': 2.7927, 'grad_norm': 13.359777450561523, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:58<7:13:28,  4.45s/it]  3%|â–Ž         | 154/6000 [08:01<6:30:21,  4.01s/it]                                                    {'loss': 2.8193, 'grad_norm': 22.173952102661133, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [08:01<6:30:21,  4.01s/it]  3%|â–Ž         | 155/6000 [08:04<5:57:34,  3.67s/it]                                                    {'loss': 2.8602, 'grad_norm': 25.76512336730957, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [08:04<5:57:34,  3.67s/it]  3%|â–Ž         | 156/6000 [08:07<5:36:14,  3.45s/it]                                                    {'loss': 2.8835, 'grad_norm': 16.763301849365234, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [08:07<5:36:14,  3.45s/it]  3%|â–Ž         | 157/6000 [08:10<5:33:32,  3.43s/it]                                                    {'loss': 2.7917, 'grad_norm': 13.490262985229492, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [08:10<5:33:32,  3.43s/it]  3%|â–Ž         | 158/6000 [08:13<5:16:58,  3.26s/it]                                                    {'loss': 2.8332, 'grad_norm': 18.45282745361328, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [08:13<5:16:58,  3.26s/it]  3%|â–Ž         | 159/6000 [08:16<5:04:28,  3.13s/it]                                                    {'loss': 2.812, 'grad_norm': 17.05282974243164, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [08:16<5:04:28,  3.13s/it]  3%|â–Ž         | 160/6000 [08:19<5:13:24,  3.22s/it]                                                    {'loss': 2.8182, 'grad_norm': 24.988956451416016, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [08:19<5:13:24,  3.22s/it]  3%|â–Ž         | 161/6000 [08:22<5:05:35,  3.14s/it]                                                    {'loss': 2.8, 'grad_norm': 20.727792739868164, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [08:22<5:05:35,  3.14s/it]  3%|â–Ž         | 162/6000 [08:25<4:56:26,  3.05s/it]                                                    {'loss': 2.8219, 'grad_norm': 16.646347045898438, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [08:25<4:56:26,  3.05s/it]  3%|â–Ž         | 163/6000 [08:28<5:08:10,  3.17s/it]                                                    {'loss': 2.7909, 'grad_norm': 16.444454193115234, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [08:28<5:08:10,  3.17s/it]  3%|â–Ž         | 164/6000 [08:31<4:57:51,  3.06s/it]                                                    {'loss': 2.9658, 'grad_norm': 23.122827529907227, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [08:31<4:57:51,  3.06s/it]  3%|â–Ž         | 165/6000 [08:34<4:55:12,  3.04s/it]                                                    {'loss': 2.8294, 'grad_norm': 24.546558380126953, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [08:34<4:55:12,  3.04s/it]  3%|â–Ž         | 166/6000 [08:37<4:47:59,  2.96s/it]                                                    {'loss': 2.8119, 'grad_norm': 16.48362159729004, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [08:37<4:47:59,  2.96s/it]  3%|â–Ž         | 167/6000 [08:40<4:45:58,  2.94s/it]                                                    {'loss': 2.8183, 'grad_norm': 14.75854206085205, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [08:40<4:45:58,  2.94s/it]  3%|â–Ž         | 168/6000 [08:43<4:44:19,  2.93s/it]                                                    {'loss': 2.8875, 'grad_norm': 23.869163513183594, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [08:43<4:44:19,  2.93s/it]  3%|â–Ž         | 169/6000 [08:46<4:54:56,  3.03s/it]                                                    {'loss': 2.844, 'grad_norm': 14.827253341674805, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:46<4:54:56,  3.03s/it]  3%|â–Ž         | 170/6000 [08:49<4:48:05,  2.96s/it]                                                    {'loss': 2.8086, 'grad_norm': 14.547717094421387, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:49<4:48:05,  2.96s/it]  3%|â–Ž         | 171/6000 [08:52<4:41:34,  2.90s/it]                                                    {'loss': 2.8061, 'grad_norm': 11.755121231079102, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:52<4:41:34,  2.90s/it]  3%|â–Ž         | 172/6000 [08:55<4:43:05,  2.91s/it]                                                    {'loss': 2.8372, 'grad_norm': 23.118793487548828, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:55<4:43:05,  2.91s/it]  3%|â–Ž         | 173/6000 [08:57<4:42:18,  2.91s/it]                                                    {'loss': 2.8189, 'grad_norm': 15.377946853637695, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:57<4:42:18,  2.91s/it]  3%|â–Ž         | 174/6000 [09:01<5:00:51,  3.10s/it]                                                    {'loss': 2.813, 'grad_norm': 15.582873344421387, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [09:01<5:00:51,  3.10s/it]  3%|â–Ž         | 175/6000 [09:04<4:55:34,  3.04s/it]                                                    {'loss': 2.8461, 'grad_norm': 21.4056339263916, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [09:04<4:55:34,  3.04s/it]  3%|â–Ž         | 176/6000 [09:07<4:59:00,  3.08s/it]                                                    {'loss': 2.8056, 'grad_norm': 16.9965877532959, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [09:07<4:59:00,  3.08s/it]  3%|â–Ž         | 177/6000 [09:10<4:51:37,  3.00s/it]                                                    {'loss': 2.8249, 'grad_norm': 14.062211990356445, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [09:10<4:51:37,  3.00s/it]  3%|â–Ž         | 178/6000 [09:13<4:47:50,  2.97s/it]                                                    {'loss': 2.8088, 'grad_norm': 15.598912239074707, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [09:13<4:47:50,  2.97s/it]  3%|â–Ž         | 179/6000 [09:16<4:42:48,  2.92s/it]                                                    {'loss': 2.8345, 'grad_norm': 15.862332344055176, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [09:16<4:42:48,  2.92s/it]  3%|â–Ž         | 180/6000 [09:19<4:45:20,  2.94s/it]                                                    {'loss': 2.8619, 'grad_norm': 21.614151000976562, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [09:19<4:45:20,  2.94s/it]  3%|â–Ž         | 181/6000 [09:21<4:43:30,  2.92s/it]                                                    {'loss': 2.8476, 'grad_norm': 18.397626876831055, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [09:21<4:43:30,  2.92s/it]  3%|â–Ž         | 182/6000 [09:24<4:40:13,  2.89s/it]                                                    {'loss': 2.8674, 'grad_norm': 26.342443466186523, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [09:24<4:40:13,  2.89s/it]  3%|â–Ž         | 183/6000 [09:27<4:38:52,  2.88s/it]                                                    {'loss': 2.8029, 'grad_norm': 13.286542892456055, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [09:27<4:38:52,  2.88s/it]  3%|â–Ž         | 184/6000 [09:30<4:38:03,  2.87s/it]                                                    {'loss': 2.8147, 'grad_norm': 33.39723587036133, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [09:30<4:38:03,  2.87s/it]  3%|â–Ž         | 185/6000 [09:33<4:38:37,  2.87s/it]                                                    {'loss': 2.8141, 'grad_norm': 17.48876953125, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [09:33<4:38:37,  2.87s/it]  3%|â–Ž         | 186/6000 [09:36<4:37:59,  2.87s/it]                                                    {'loss': 2.8647, 'grad_norm': 23.822389602661133, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [09:36<4:37:59,  2.87s/it]  3%|â–Ž         | 187/6000 [09:39<4:36:50,  2.86s/it]                                                    {'loss': 2.7828, 'grad_norm': 12.940397262573242, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [09:39<4:36:50,  2.86s/it]  3%|â–Ž         | 188/6000 [09:41<4:36:07,  2.85s/it]                                                    {'loss': 2.8236, 'grad_norm': 23.084213256835938, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [09:41<4:36:07,  2.85s/it]  3%|â–Ž         | 189/6000 [09:44<4:37:24,  2.86s/it]                                                    {'loss': 2.8084, 'grad_norm': 19.77975082397461, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [09:44<4:37:24,  2.86s/it]  3%|â–Ž         | 190/6000 [09:47<4:35:26,  2.84s/it]                                                    {'loss': 2.8067, 'grad_norm': 17.4343204498291, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [09:47<4:35:26,  2.84s/it]  3%|â–Ž         | 191/6000 [09:50<4:37:35,  2.87s/it]                                                    {'loss': 2.8294, 'grad_norm': 17.339807510375977, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:50<4:37:35,  2.87s/it]  3%|â–Ž         | 192/6000 [09:53<4:55:08,  3.05s/it]                                                    {'loss': 2.9336, 'grad_norm': 20.66671371459961, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:53<4:55:08,  3.05s/it]  3%|â–Ž         | 193/6000 [09:57<5:00:27,  3.10s/it]                                                    {'loss': 2.7566, 'grad_norm': 16.9007625579834, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:57<5:00:27,  3.10s/it]