==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test2-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/train.log
W1022 18:55:55.614000 130826305693504 torch/distributed/run.py:779] 
W1022 18:55:55.614000 130826305693504 torch/distributed/run.py:779] *****************************************
W1022 18:55:55.614000 130826305693504 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 18:55:55.614000 130826305693504 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-22 18:56:05,608] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.22it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.13it/s]
wandb: setting up run kqc4x193
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251022_185605-kqc4x193
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test2-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/kqc4x193
[2025-10-22 18:56:07,238] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.99it/s]
[2025-10-22 18:56:07,859] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-10-22 18:56:16,651] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-22 18:56:18,017] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-22 18:56:18,018] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-22 18:56:22,204] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-22 18:56:22,205] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-22 18:56:23,040] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-22 18:56:23,041] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-22 18:56:23,042] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-22 18:56:23,043] INFO [src.utils:19] ==================================================
[2025-10-22 18:56:23,044] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-22 18:56:23,045] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 18:56:23,046] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 18:56:23,046] INFO [src.utils:19] ==================================================
[2025-10-22 18:56:24,799] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 18:56:24,800] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 18:56:24,800] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 18:56:24,799] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 18:56:24,800] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 18:56:24,800] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 18:56:24,800] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 18:56:24,800] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 18:56:24,800] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 18:56:24,801] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 18:56:24,801] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 18:56:24,801] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 18:56:24,802] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 18:56:24,802] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 18:56:24,808] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 18:56:24,810] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 18:56:24,817] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-10-22 18:56:24,820] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1022 18:56:27.534878158 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1022 18:56:27.573649257 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:47:20,  4.07s/it]                                                  {'loss': 7.2997, 'grad_norm': 2352.937255859375, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:47:20,  4.07s/it]  0%|          | 2/6000 [00:06<5:23:59,  3.24s/it]                                                  {'loss': 6.2894, 'grad_norm': 2540.981201171875, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:23:59,  3.24s/it]  0%|          | 3/6000 [00:09<5:02:28,  3.03s/it]                                                  {'loss': 6.692, 'grad_norm': 2178.78369140625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:02:28,  3.03s/it]  0%|          | 4/6000 [00:12<4:51:35,  2.92s/it]                                                  {'loss': 6.9419, 'grad_norm': 1737.38427734375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:51:35,  2.92s/it]  0%|          | 5/6000 [00:14<4:44:02,  2.84s/it]                                                  {'loss': 6.3414, 'grad_norm': 2011.1142578125, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:44:02,  2.84s/it]  0%|          | 6/6000 [00:17<4:39:34,  2.80s/it]                                                  {'loss': 6.2474, 'grad_norm': 1450.0159912109375, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:39:34,  2.80s/it]  0%|          | 7/6000 [00:20<4:35:31,  2.76s/it]                                                  {'loss': 5.934, 'grad_norm': 1709.75732421875, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:35:31,  2.76s/it]  0%|          | 8/6000 [00:22<4:30:15,  2.71s/it]                                                  {'loss': 5.798, 'grad_norm': 2397.896728515625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:30:15,  2.71s/it]  0%|          | 9/6000 [00:25<4:34:06,  2.75s/it]                                                  {'loss': 4.6629, 'grad_norm': 1228.6268310546875, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:34:06,  2.75s/it]  0%|          | 10/6000 [00:28<4:30:58,  2.71s/it]                                                   {'loss': 5.4647, 'grad_norm': 2695.8017578125, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:30:58,  2.71s/it]  0%|          | 11/6000 [00:31<4:41:14,  2.82s/it]                                                   {'loss': 5.4998, 'grad_norm': 2529.16748046875, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:41:14,  2.82s/it]  0%|          | 12/6000 [00:34<4:45:40,  2.86s/it]                                                   {'loss': 4.4099, 'grad_norm': 1877.6778564453125, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:45:40,  2.86s/it]  0%|          | 13/6000 [00:37<4:43:14,  2.84s/it]                                                   {'loss': 4.0695, 'grad_norm': 1349.699951171875, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:43:14,  2.84s/it]  0%|          | 14/6000 [00:40<4:42:49,  2.83s/it]                                                   {'loss': 3.5572, 'grad_norm': 1209.2078857421875, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:40<4:42:49,  2.83s/it]  0%|          | 15/6000 [00:42<4:39:37,  2.80s/it]                                                   {'loss': 3.1402, 'grad_norm': 768.9515380859375, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:39:37,  2.80s/it]  0%|          | 16/6000 [00:45<4:36:06,  2.77s/it]                                                   {'loss': 3.1303, 'grad_norm': 333.23760986328125, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:36:06,  2.77s/it]  0%|          | 17/6000 [00:48<4:34:55,  2.76s/it]                                                   {'loss': 3.0861, 'grad_norm': 475.91802978515625, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:48<4:34:55,  2.76s/it]  0%|          | 18/6000 [00:50<4:35:03,  2.76s/it]                                                   {'loss': 2.9286, 'grad_norm': 254.41493225097656, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:35:03,  2.76s/it]  0%|          | 19/6000 [00:53<4:34:48,  2.76s/it]                                                   {'loss': 3.0302, 'grad_norm': 919.2161254882812, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:34:48,  2.76s/it]  0%|          | 20/6000 [00:56<4:32:47,  2.74s/it]                                                   {'loss': 2.8997, 'grad_norm': 103.19896697998047, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:56<4:32:47,  2.74s/it]  0%|          | 21/6000 [00:59<4:35:50,  2.77s/it]                                                   {'loss': 2.854, 'grad_norm': 82.64414978027344, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:59<4:35:50,  2.77s/it]  0%|          | 22/6000 [01:02<4:36:25,  2.77s/it]                                                   {'loss': 3.0222, 'grad_norm': 252.42723083496094, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:02<4:36:25,  2.77s/it]  0%|          | 23/6000 [01:04<4:35:31,  2.77s/it]                                                   {'loss': 2.8907, 'grad_norm': 294.7542724609375, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:35:31,  2.77s/it]  0%|          | 24/6000 [01:07<4:36:00,  2.77s/it]                                                   {'loss': 2.998, 'grad_norm': 247.39224243164062, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:36:00,  2.77s/it]  0%|          | 25/6000 [01:10<4:34:18,  2.75s/it]                                                   {'loss': 2.8509, 'grad_norm': 694.5546264648438, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:10<4:34:18,  2.75s/it]  0%|          | 26/6000 [01:13<4:34:11,  2.75s/it]                                                   {'loss': 2.8928, 'grad_norm': 1616.810791015625, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:13<4:34:11,  2.75s/it]  0%|          | 27/6000 [01:15<4:32:55,  2.74s/it]                                                   {'loss': 2.8573, 'grad_norm': 75.92660522460938, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:32:55,  2.74s/it]  0%|          | 28/6000 [01:19<4:59:00,  3.00s/it]                                                   {'loss': 2.8161, 'grad_norm': 98.32611083984375, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:19<4:59:00,  3.00s/it]  0%|          | 29/6000 [01:22<4:48:43,  2.90s/it]                                                   {'loss': 2.7974, 'grad_norm': 40.54967498779297, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:22<4:48:43,  2.90s/it]  0%|          | 30/6000 [01:24<4:44:10,  2.86s/it]                                                   {'loss': 2.8086, 'grad_norm': 64.97315216064453, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:44:10,  2.86s/it]  1%|          | 31/6000 [01:27<4:41:11,  2.83s/it]                                                   {'loss': 2.7888, 'grad_norm': 57.70419692993164, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:27<4:41:11,  2.83s/it]  1%|          | 32/6000 [01:30<4:38:30,  2.80s/it]                                                   {'loss': 2.7929, 'grad_norm': 110.77992248535156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:30<4:38:30,  2.80s/it]  1%|          | 33/6000 [01:32<4:35:49,  2.77s/it]                                                   {'loss': 2.8024, 'grad_norm': 98.3674087524414, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:32<4:35:49,  2.77s/it]  1%|          | 34/6000 [01:35<4:34:27,  2.76s/it]                                                   {'loss': 2.8246, 'grad_norm': 65.95852661132812, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:34:27,  2.76s/it]  1%|          | 35/6000 [01:38<4:34:02,  2.76s/it]                                                   {'loss': 2.7858, 'grad_norm': 42.16755294799805, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:38<4:34:02,  2.76s/it]  1%|          | 36/6000 [01:41<4:32:20,  2.74s/it]                                                   {'loss': 2.8043, 'grad_norm': 52.97339630126953, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:41<4:32:20,  2.74s/it]  1%|          | 37/6000 [01:43<4:30:40,  2.72s/it]                                                   {'loss': 2.7906, 'grad_norm': 32.14521789550781, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:30:40,  2.72s/it]  1%|          | 38/6000 [01:46<4:28:33,  2.70s/it]                                                   {'loss': 2.7783, 'grad_norm': 21.730016708374023, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:46<4:28:33,  2.70s/it]  1%|          | 39/6000 [01:49<4:28:03,  2.70s/it]                                                   {'loss': 2.8506, 'grad_norm': 44.71672821044922, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:49<4:28:03,  2.70s/it]  1%|          | 40/6000 [01:51<4:29:28,  2.71s/it]                                                   {'loss': 2.8605, 'grad_norm': 341.76934814453125, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:51<4:29:28,  2.71s/it]  1%|          | 41/6000 [01:54<4:29:57,  2.72s/it]                                                   {'loss': 2.7883, 'grad_norm': 64.52815246582031, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:54<4:29:57,  2.72s/it]  1%|          | 42/6000 [01:57<4:30:38,  2.73s/it]                                                   {'loss': 2.7781, 'grad_norm': 29.46623992919922, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:57<4:30:38,  2.73s/it]  1%|          | 43/6000 [02:01<5:02:16,  3.04s/it]                                                   {'loss': 2.799, 'grad_norm': 36.76968002319336, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [02:01<5:02:16,  3.04s/it]  1%|          | 44/6000 [02:04<5:07:11,  3.09s/it]                                                   {'loss': 2.7908, 'grad_norm': 80.7992935180664, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:04<5:07:11,  3.09s/it]  1%|          | 45/6000 [02:07<4:57:06,  2.99s/it]                                                   {'loss': 2.7977, 'grad_norm': 61.84972381591797, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:07<4:57:06,  2.99s/it]  1%|          | 46/6000 [02:10<4:52:49,  2.95s/it]                                                   {'loss': 2.7733, 'grad_norm': 23.99089813232422, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:10<4:52:49,  2.95s/it]  1%|          | 47/6000 [02:12<4:45:55,  2.88s/it]                                                   {'loss': 2.7837, 'grad_norm': 41.78973388671875, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:12<4:45:55,  2.88s/it]  1%|          | 48/6000 [02:15<4:44:21,  2.87s/it]                                                   {'loss': 2.7804, 'grad_norm': 22.15018653869629, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:15<4:44:21,  2.87s/it]  1%|          | 49/6000 [02:18<4:38:27,  2.81s/it]                                                   {'loss': 2.7777, 'grad_norm': 25.801830291748047, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:18<4:38:27,  2.81s/it]  1%|          | 50/6000 [02:21<4:42:09,  2.85s/it]                                                   {'loss': 2.7702, 'grad_norm': 38.283485412597656, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:21<4:42:09,  2.85s/it][2025-10-22 18:58:46,180] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:26<5:42:00,  3.45s/it]                                                   {'loss': 2.7915, 'grad_norm': 15.424574851989746, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:26<5:42:00,  3.45s/it]  1%|          | 52/6000 [02:28<5:19:27,  3.22s/it]                                                   {'loss': 2.8078, 'grad_norm': 66.15141296386719, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:28<5:19:27,  3.22s/it]  1%|          | 53/6000 [02:31<5:05:07,  3.08s/it]                                                   {'loss': 2.8647, 'grad_norm': 80.99037170410156, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:31<5:05:07,  3.08s/it]  1%|          | 54/6000 [02:34<4:51:59,  2.95s/it]                                                   {'loss': 2.7947, 'grad_norm': 64.6929702758789, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:34<4:51:59,  2.95s/it]  1%|          | 55/6000 [02:36<4:44:24,  2.87s/it]                                                   {'loss': 2.7963, 'grad_norm': 15.225555419921875, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:36<4:44:24,  2.87s/it]  1%|          | 56/6000 [02:39<4:40:42,  2.83s/it]                                                   {'loss': 2.7982, 'grad_norm': 27.340961456298828, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:39<4:40:42,  2.83s/it]  1%|          | 57/6000 [02:42<4:35:31,  2.78s/it]                                                   {'loss': 2.7912, 'grad_norm': 23.598772048950195, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:42<4:35:31,  2.78s/it]  1%|          | 58/6000 [02:44<4:33:21,  2.76s/it]                                                   {'loss': 2.7734, 'grad_norm': 11.984674453735352, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:44<4:33:21,  2.76s/it]  1%|          | 59/6000 [02:47<4:30:01,  2.73s/it]                                                   {'loss': 2.7838, 'grad_norm': 12.694616317749023, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:47<4:30:01,  2.73s/it]  1%|          | 60/6000 [02:50<4:28:45,  2.71s/it]                                                   {'loss': 2.7733, 'grad_norm': 9.370837211608887, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:50<4:28:45,  2.71s/it]  1%|          | 61/6000 [02:52<4:25:08,  2.68s/it]                                                   {'loss': 2.8063, 'grad_norm': 11.078031539916992, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:52<4:25:08,  2.68s/it]  1%|          | 62/6000 [02:55<4:27:15,  2.70s/it]                                                   {'loss': 2.7922, 'grad_norm': 7.023228168487549, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:55<4:27:15,  2.70s/it]  1%|          | 63/6000 [02:58<4:26:28,  2.69s/it]                                                   {'loss': 2.7798, 'grad_norm': 5.4014363288879395, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:58<4:26:28,  2.69s/it]  1%|          | 64/6000 [03:00<4:25:28,  2.68s/it]                                                   {'loss': 2.7754, 'grad_norm': 5.4350175857543945, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [03:00<4:25:28,  2.68s/it]  1%|          | 65/6000 [03:03<4:25:30,  2.68s/it]                                                   {'loss': 2.7926, 'grad_norm': 10.346903800964355, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [03:03<4:25:30,  2.68s/it]  1%|          | 66/6000 [03:06<4:38:47,  2.82s/it]                                                   {'loss': 2.7722, 'grad_norm': 11.677384376525879, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:06<4:38:47,  2.82s/it]  1%|          | 67/6000 [03:09<4:35:15,  2.78s/it]                                                   {'loss': 2.7936, 'grad_norm': 15.67928695678711, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:09<4:35:15,  2.78s/it]  1%|          | 68/6000 [03:12<4:30:38,  2.74s/it]                                                   {'loss': 2.7884, 'grad_norm': 9.275801658630371, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:12<4:30:38,  2.74s/it]  1%|          | 69/6000 [03:14<4:29:06,  2.72s/it]                                                   {'loss': 2.7946, 'grad_norm': 12.923961639404297, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:14<4:29:06,  2.72s/it]  1%|          | 70/6000 [03:17<4:31:02,  2.74s/it]                                                   {'loss': 2.7868, 'grad_norm': 9.606006622314453, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:17<4:31:02,  2.74s/it]  1%|          | 71/6000 [03:20<4:27:27,  2.71s/it]                                                   {'loss': 2.7781, 'grad_norm': 23.903736114501953, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:20<4:27:27,  2.71s/it]  1%|          | 72/6000 [03:23<4:46:49,  2.90s/it]                                                   {'loss': 2.771, 'grad_norm': 9.296104431152344, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:23<4:46:49,  2.90s/it]  1%|          | 73/6000 [03:26<4:42:33,  2.86s/it]                                                   {'loss': 2.8121, 'grad_norm': 20.663349151611328, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:26<4:42:33,  2.86s/it]  1%|          | 74/6000 [03:28<4:37:37,  2.81s/it]                                                   {'loss': 2.7778, 'grad_norm': 38.733333587646484, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:29<4:37:37,  2.81s/it]  1%|â–         | 75/6000 [03:31<4:33:06,  2.77s/it]                                                   {'loss': 2.7899, 'grad_norm': 13.516600608825684, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:31<4:33:06,  2.77s/it]  1%|â–         | 76/6000 [03:34<4:34:32,  2.78s/it]                                                   {'loss': 2.7943, 'grad_norm': 26.511600494384766, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:34<4:34:32,  2.78s/it]  1%|â–         | 77/6000 [03:37<4:34:06,  2.78s/it]                                                   {'loss': 2.8457, 'grad_norm': 18.215255737304688, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:37<4:34:06,  2.78s/it]  1%|â–         | 78/6000 [03:40<4:44:02,  2.88s/it]                                                   {'loss': 2.7689, 'grad_norm': 26.17811393737793, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:40<4:44:02,  2.88s/it]  1%|â–         | 79/6000 [03:43<4:38:54,  2.83s/it]                                                   {'loss': 2.7906, 'grad_norm': 22.603822708129883, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:43<4:38:54,  2.83s/it]  1%|â–         | 80/6000 [03:45<4:41:50,  2.86s/it]                                                   {'loss': 2.7902, 'grad_norm': 46.237972259521484, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:45<4:41:50,  2.86s/it]  1%|â–         | 81/6000 [03:48<4:36:31,  2.80s/it]                                                   {'loss': 2.7947, 'grad_norm': 32.60108184814453, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:48<4:36:31,  2.80s/it]  1%|â–         | 82/6000 [03:51<4:34:11,  2.78s/it]                                                   {'loss': 2.7692, 'grad_norm': 32.351112365722656, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:51<4:34:11,  2.78s/it]  1%|â–         | 83/6000 [03:54<4:31:02,  2.75s/it]                                                   {'loss': 2.7935, 'grad_norm': 110.0713882446289, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:54<4:31:02,  2.75s/it]  1%|â–         | 84/6000 [03:56<4:32:53,  2.77s/it]                                                   {'loss': 2.8092, 'grad_norm': 211.95294189453125, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:56<4:32:53,  2.77s/it]  1%|â–         | 85/6000 [03:59<4:42:26,  2.87s/it]                                                   {'loss': 2.8029, 'grad_norm': 109.6916275024414, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:59<4:42:26,  2.87s/it]  1%|â–         | 86/6000 [04:02<4:37:57,  2.82s/it]                                                   {'loss': 2.7909, 'grad_norm': 55.176387786865234, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:02<4:37:57,  2.82s/it]  1%|â–         | 87/6000 [04:05<4:33:01,  2.77s/it]                                                   {'loss': 2.8141, 'grad_norm': 78.72149658203125, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:05<4:33:01,  2.77s/it]  1%|â–         | 88/6000 [04:08<4:30:43,  2.75s/it]                                                   {'loss': 2.8194, 'grad_norm': 121.21572875976562, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:08<4:30:43,  2.75s/it]  1%|â–         | 89/6000 [04:10<4:28:00,  2.72s/it]                                                   {'loss': 2.7804, 'grad_norm': 26.106931686401367, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:10<4:28:00,  2.72s/it]  2%|â–         | 90/6000 [04:13<4:29:33,  2.74s/it]                                                   {'loss': 2.8234, 'grad_norm': 11.224903106689453, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:13<4:29:33,  2.74s/it]  2%|â–         | 91/6000 [04:16<4:29:11,  2.73s/it]                                                   {'loss': 2.7753, 'grad_norm': 8.400012016296387, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:16<4:29:11,  2.73s/it]  2%|â–         | 92/6000 [04:19<4:40:31,  2.85s/it]                                                   {'loss': 2.7747, 'grad_norm': 13.432839393615723, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:19<4:40:31,  2.85s/it]  2%|â–         | 93/6000 [04:22<4:40:33,  2.85s/it]                                                   {'loss': 2.7744, 'grad_norm': 6.003970623016357, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:22<4:40:33,  2.85s/it]  2%|â–         | 94/6000 [04:24<4:36:02,  2.80s/it]                                                   {'loss': 2.7981, 'grad_norm': 11.650945663452148, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:24<4:36:02,  2.80s/it]  2%|â–         | 95/6000 [04:27<4:33:29,  2.78s/it]                                                   {'loss': 2.7762, 'grad_norm': 5.521602630615234, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:27<4:33:29,  2.78s/it]  2%|â–         | 96/6000 [04:30<4:32:01,  2.76s/it]                                                   {'loss': 2.7727, 'grad_norm': 8.77471923828125, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:30<4:32:01,  2.76s/it]  2%|â–         | 97/6000 [04:33<4:30:20,  2.75s/it]                                                   {'loss': 2.7872, 'grad_norm': 6.862999439239502, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:33<4:30:20,  2.75s/it]  2%|â–         | 98/6000 [04:35<4:28:20,  2.73s/it]                                                   {'loss': 2.7801, 'grad_norm': 6.163372039794922, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:35<4:28:20,  2.73s/it]  2%|â–         | 99/6000 [04:38<4:26:12,  2.71s/it]                                                   {'loss': 2.7739, 'grad_norm': 6.045036792755127, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:38<4:26:12,  2.71s/it]  2%|â–         | 100/6000 [04:41<4:24:26,  2.69s/it]                                                    {'loss': 2.7672, 'grad_norm': 11.306642532348633, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:41<4:24:26,  2.69s/it][2025-10-22 19:01:06,026] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:46<5:57:31,  3.64s/it]                                                    {'loss': 2.8515, 'grad_norm': 113.70835876464844, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:46<5:57:31,  3.64s/it]  2%|â–         | 102/6000 [04:49<5:30:50,  3.37s/it]                                                    {'loss': 3.5096, 'grad_norm': 1507.80712890625, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:49<5:30:50,  3.37s/it]  2%|â–         | 103/6000 [04:52<5:14:38,  3.20s/it]                                                    {'loss': 3.1271, 'grad_norm': 641.5410766601562, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:52<5:14:38,  3.20s/it]  2%|â–         | 104/6000 [04:55<5:07:33,  3.13s/it]                                                    {'loss': 2.7822, 'grad_norm': 5.994046688079834, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:55<5:07:33,  3.13s/it]  2%|â–         | 105/6000 [04:58<4:57:59,  3.03s/it]                                                    {'loss': 2.7843, 'grad_norm': 5.594182968139648, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:58<4:57:59,  3.03s/it]  2%|â–         | 106/6000 [05:01<4:52:37,  2.98s/it]                                                    {'loss': 2.7775, 'grad_norm': 9.958834648132324, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:01<4:52:37,  2.98s/it]  2%|â–         | 107/6000 [05:03<4:43:37,  2.89s/it]                                                    {'loss': 2.7889, 'grad_norm': 3.7732372283935547, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:03<4:43:37,  2.89s/it]  2%|â–         | 108/6000 [05:06<4:42:05,  2.87s/it]                                                    {'loss': 2.7738, 'grad_norm': 5.636787414550781, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:06<4:42:05,  2.87s/it]  2%|â–         | 109/6000 [05:09<4:51:08,  2.97s/it]                                                    {'loss': 2.8279, 'grad_norm': 12.510995864868164, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:09<4:51:08,  2.97s/it]  2%|â–         | 110/6000 [05:12<4:44:55,  2.90s/it]                                                    {'loss': 2.846, 'grad_norm': 8.80198860168457, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:12<4:44:55,  2.90s/it]  2%|â–         | 111/6000 [05:15<4:43:24,  2.89s/it]                                                    {'loss': 2.7782, 'grad_norm': 7.360054969787598, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:15<4:43:24,  2.89s/it]  2%|â–         | 112/6000 [05:18<4:55:25,  3.01s/it]                                                    {'loss': 2.783, 'grad_norm': 11.577157974243164, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:18<4:55:25,  3.01s/it]  2%|â–         | 113/6000 [05:21<4:46:49,  2.92s/it]                                                    {'loss': 2.7714, 'grad_norm': 7.683633327484131, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:21<4:46:49,  2.92s/it]  2%|â–         | 114/6000 [05:24<4:43:04,  2.89s/it]                                                    {'loss': 2.7666, 'grad_norm': 11.533639907836914, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:24<4:43:04,  2.89s/it]  2%|â–         | 115/6000 [05:26<4:39:22,  2.85s/it]                                                    {'loss': 2.8658, 'grad_norm': 46.4954833984375, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:26<4:39:22,  2.85s/it]  2%|â–         | 116/6000 [05:29<4:36:02,  2.81s/it]                                                    {'loss': 2.8074, 'grad_norm': 27.909448623657227, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:29<4:36:02,  2.81s/it]  2%|â–         | 117/6000 [05:32<4:35:35,  2.81s/it]                                                    {'loss': 2.9051, 'grad_norm': 13.180002212524414, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:32<4:35:35,  2.81s/it]  2%|â–         | 118/6000 [05:35<4:50:57,  2.97s/it]                                                    {'loss': 2.7734, 'grad_norm': 11.394490242004395, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:35<4:50:57,  2.97s/it]  2%|â–         | 119/6000 [05:38<4:43:27,  2.89s/it]                                                    {'loss': 2.7735, 'grad_norm': 7.899941444396973, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:38<4:43:27,  2.89s/it]  2%|â–         | 120/6000 [05:41<4:37:59,  2.84s/it]                                                    {'loss': 2.7702, 'grad_norm': 15.809630393981934, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:41<4:37:59,  2.84s/it]  2%|â–         | 121/6000 [05:43<4:35:12,  2.81s/it]                                                    {'loss': 2.7794, 'grad_norm': 13.697225570678711, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:43<4:35:12,  2.81s/it]  2%|â–         | 122/6000 [05:46<4:38:23,  2.84s/it]                                                    {'loss': 2.7663, 'grad_norm': 13.412497520446777, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:46<4:38:23,  2.84s/it]  2%|â–         | 123/6000 [05:49<4:35:30,  2.81s/it]                                                    {'loss': 2.7872, 'grad_norm': 10.309057235717773, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:49<4:35:30,  2.81s/it]  2%|â–         | 124/6000 [05:52<4:31:16,  2.77s/it]                                                    {'loss': 2.8187, 'grad_norm': 16.153913497924805, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:52<4:31:16,  2.77s/it]  2%|â–         | 125/6000 [05:55<4:36:26,  2.82s/it]                                                    {'loss': 2.9403, 'grad_norm': 13.65854263305664, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:55<4:36:26,  2.82s/it]  2%|â–         | 126/6000 [05:58<4:35:28,  2.81s/it]                                                    {'loss': 2.7745, 'grad_norm': 13.210674285888672, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:58<4:35:28,  2.81s/it]  2%|â–         | 127/6000 [06:00<4:37:37,  2.84s/it]                                                    {'loss': 2.7687, 'grad_norm': 10.459478378295898, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:00<4:37:37,  2.84s/it]  2%|â–         | 128/6000 [06:03<4:34:17,  2.80s/it]                                                    {'loss': 2.7871, 'grad_norm': 17.222158432006836, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:03<4:34:17,  2.80s/it]  2%|â–         | 129/6000 [06:06<4:31:33,  2.78s/it]                                                    {'loss': 2.8603, 'grad_norm': 30.064483642578125, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:06<4:31:33,  2.78s/it]  2%|â–         | 130/6000 [06:09<4:29:38,  2.76s/it]                                                    {'loss': 2.7653, 'grad_norm': 20.885848999023438, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:09<4:29:38,  2.76s/it]  2%|â–         | 131/6000 [06:11<4:30:27,  2.76s/it]                                                    {'loss': 2.776, 'grad_norm': 11.735370635986328, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:11<4:30:27,  2.76s/it]  2%|â–         | 132/6000 [06:14<4:29:33,  2.76s/it]                                                    {'loss': 2.7929, 'grad_norm': 19.97711944580078, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:14<4:29:33,  2.76s/it]  2%|â–         | 133/6000 [06:17<4:28:48,  2.75s/it]                                                    {'loss': 2.7748, 'grad_norm': 16.412378311157227, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:17<4:28:48,  2.75s/it]  2%|â–         | 134/6000 [06:20<4:33:48,  2.80s/it]                                                    {'loss': 2.7561, 'grad_norm': 11.890071868896484, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:20<4:33:48,  2.80s/it]  2%|â–         | 135/6000 [06:22<4:32:26,  2.79s/it]                                                    {'loss': 2.7918, 'grad_norm': 26.131744384765625, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:22<4:32:26,  2.79s/it]  2%|â–         | 136/6000 [06:25<4:32:09,  2.78s/it]                                                    {'loss': 2.7714, 'grad_norm': 17.3778018951416, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:25<4:32:09,  2.78s/it]  2%|â–         | 137/6000 [06:29<4:47:52,  2.95s/it]                                                    {'loss': 2.7865, 'grad_norm': 24.656574249267578, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:29<4:47:52,  2.95s/it]  2%|â–         | 138/6000 [06:31<4:38:54,  2.85s/it]                                                    {'loss': 2.8112, 'grad_norm': 19.25645637512207, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:31<4:38:54,  2.85s/it]  2%|â–         | 139/6000 [06:34<4:38:41,  2.85s/it]                                                    {'loss': 2.8212, 'grad_norm': 19.2606201171875, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:34<4:38:41,  2.85s/it]  2%|â–         | 140/6000 [06:37<4:46:23,  2.93s/it]                                                    {'loss': 2.777, 'grad_norm': 12.487534523010254, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:37<4:46:23,  2.93s/it]  2%|â–         | 141/6000 [06:40<4:41:14,  2.88s/it]                                                    {'loss': 2.8065, 'grad_norm': 14.752622604370117, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:40<4:41:14,  2.88s/it]  2%|â–         | 142/6000 [06:43<4:33:56,  2.81s/it]                                                    {'loss': 2.7786, 'grad_norm': 8.225804328918457, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:43<4:33:56,  2.81s/it]  2%|â–         | 143/6000 [06:45<4:30:23,  2.77s/it]                                                    {'loss': 2.7719, 'grad_norm': 10.32594108581543, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:45<4:30:23,  2.77s/it]  2%|â–         | 144/6000 [06:48<4:29:49,  2.76s/it]                                                    {'loss': 2.7935, 'grad_norm': 8.049430847167969, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:48<4:29:49,  2.76s/it]  2%|â–         | 145/6000 [06:51<4:27:49,  2.74s/it]                                                    {'loss': 2.7989, 'grad_norm': 3.5462522506713867, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:51<4:27:49,  2.74s/it]  2%|â–         | 146/6000 [06:54<4:28:46,  2.75s/it]                                                    {'loss': 2.8095, 'grad_norm': 3.067391872406006, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:54<4:28:46,  2.75s/it]  2%|â–         | 147/6000 [06:56<4:26:23,  2.73s/it]                                                    {'loss': 2.7795, 'grad_norm': 3.455423593521118, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:56<4:26:23,  2.73s/it]  2%|â–         | 148/6000 [06:59<4:26:59,  2.74s/it]                                                    {'loss': 2.7922, 'grad_norm': 2.501218795776367, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:59<4:26:59,  2.74s/it]  2%|â–         | 149/6000 [07:02<4:25:00,  2.72s/it]                                                    {'loss': 2.7754, 'grad_norm': 2.3775100708007812, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:02<4:25:00,  2.72s/it]  2%|â–Ž         | 150/6000 [07:04<4:24:16,  2.71s/it]                                                    {'loss': 2.79, 'grad_norm': 2.0564699172973633, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:04<4:24:16,  2.71s/it][2025-10-22 19:03:29,808] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:09<5:35:31,  3.44s/it]                                                    {'loss': 2.7771, 'grad_norm': 3.9174304008483887, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:09<5:35:31,  3.44s/it]  3%|â–Ž         | 152/6000 [07:12<5:14:02,  3.22s/it]                                                    {'loss': 2.7918, 'grad_norm': 2.3954663276672363, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:12<5:14:02,  3.22s/it]  3%|â–Ž         | 153/6000 [07:15<4:58:42,  3.07s/it]                                                    {'loss': 2.78, 'grad_norm': 2.1461164951324463, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:15<4:58:42,  3.07s/it]  3%|â–Ž         | 154/6000 [07:18<4:52:12,  3.00s/it]                                                    {'loss': 2.7799, 'grad_norm': 1.514733910560608, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:18<4:52:12,  3.00s/it]  3%|â–Ž         | 155/6000 [07:20<4:44:56,  2.92s/it]                                                    {'loss': 2.8036, 'grad_norm': 1.2180665731430054, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:20<4:44:56,  2.92s/it]  3%|â–Ž         | 156/6000 [07:23<4:42:00,  2.90s/it]                                                    {'loss': 2.8413, 'grad_norm': 1.2008414268493652, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:23<4:42:00,  2.90s/it]  3%|â–Ž         | 157/6000 [07:27<4:51:46,  3.00s/it]                                                    {'loss': 2.7712, 'grad_norm': 2.196343421936035, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:27<4:51:46,  3.00s/it]  3%|â–Ž         | 158/6000 [07:29<4:42:33,  2.90s/it]                                                    {'loss': 2.7854, 'grad_norm': 3.073692560195923, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:29<4:42:33,  2.90s/it]  3%|â–Ž         | 159/6000 [07:32<4:37:46,  2.85s/it]                                                    {'loss': 2.788, 'grad_norm': 3.4039132595062256, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:32<4:37:46,  2.85s/it]  3%|â–Ž         | 160/6000 [07:35<4:51:09,  2.99s/it]                                                    {'loss': 2.8105, 'grad_norm': 5.192436218261719, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:35<4:51:09,  2.99s/it]  3%|â–Ž         | 161/6000 [07:38<4:47:54,  2.96s/it]                                                    {'loss': 2.8008, 'grad_norm': 15.71947193145752, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:38<4:47:54,  2.96s/it]  3%|â–Ž         | 162/6000 [07:41<4:40:00,  2.88s/it]                                                    {'loss': 2.8016, 'grad_norm': 8.343250274658203, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:41<4:40:00,  2.88s/it]  3%|â–Ž         | 163/6000 [07:44<4:52:56,  3.01s/it]                                                    {'loss': 2.7906, 'grad_norm': 4.011692523956299, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:44<4:52:56,  3.01s/it]  3%|â–Ž         | 164/6000 [07:47<4:44:04,  2.92s/it]                                                    {'loss': 2.915, 'grad_norm': 5.123402118682861, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:47<4:44:04,  2.92s/it]  3%|â–Ž         | 165/6000 [07:50<4:41:38,  2.90s/it]                                                    {'loss': 2.7809, 'grad_norm': 3.2216694355010986, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:50<4:41:38,  2.90s/it]  3%|â–Ž         | 166/6000 [07:52<4:35:44,  2.84s/it]                                                    {'loss': 2.7771, 'grad_norm': 1.511763095855713, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:52<4:35:44,  2.84s/it]  3%|â–Ž         | 167/6000 [07:55<4:34:07,  2.82s/it]                                                    {'loss': 2.7816, 'grad_norm': 1.7291382551193237, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:55<4:34:07,  2.82s/it]  3%|â–Ž         | 168/6000 [07:58<4:32:59,  2.81s/it]                                                    {'loss': 2.8236, 'grad_norm': 2.4381263256073, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:58<4:32:59,  2.81s/it]  3%|â–Ž         | 169/6000 [08:01<4:44:06,  2.92s/it]                                                    {'loss': 2.8085, 'grad_norm': 1.7986866235733032, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:01<4:44:06,  2.92s/it]  3%|â–Ž         | 170/6000 [08:04<4:37:39,  2.86s/it]                                                    {'loss': 2.772, 'grad_norm': 1.6390297412872314, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:04<4:37:39,  2.86s/it]  3%|â–Ž         | 171/6000 [08:07<4:33:24,  2.81s/it]                                                    {'loss': 2.7797, 'grad_norm': 1.694069504737854, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:07<4:33:24,  2.81s/it]  3%|â–Ž         | 172/6000 [08:09<4:34:06,  2.82s/it]                                                    {'loss': 2.7923, 'grad_norm': 1.127756953239441, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:09<4:34:06,  2.82s/it]  3%|â–Ž         | 173/6000 [08:12<4:31:45,  2.80s/it]                                                    {'loss': 2.7778, 'grad_norm': 2.1402008533477783, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:12<4:31:45,  2.80s/it]  3%|â–Ž         | 174/6000 [08:16<4:50:03,  2.99s/it]                                                    {'loss': 2.7754, 'grad_norm': 1.958815336227417, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:16<4:50:03,  2.99s/it]  3%|â–Ž         | 175/6000 [08:18<4:44:48,  2.93s/it]                                                    {'loss': 2.7939, 'grad_norm': 1.6650851964950562, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:18<4:44:48,  2.93s/it]  3%|â–Ž         | 176/6000 [08:21<4:47:30,  2.96s/it]                                                    {'loss': 2.7798, 'grad_norm': 1.8718973398208618, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:21<4:47:30,  2.96s/it]  3%|â–Ž         | 177/6000 [08:24<4:40:21,  2.89s/it]                                                    {'loss': 2.7946, 'grad_norm': 1.3986096382141113, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:24<4:40:21,  2.89s/it]  3%|â–Ž         | 178/6000 [08:27<4:36:21,  2.85s/it]                                                    {'loss': 2.7921, 'grad_norm': 1.5937213897705078, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:27<4:36:21,  2.85s/it]  3%|â–Ž         | 179/6000 [08:30<4:34:26,  2.83s/it]                                                    {'loss': 2.7888, 'grad_norm': 3.2303364276885986, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:30<4:34:26,  2.83s/it]  3%|â–Ž         | 180/6000 [08:33<4:35:05,  2.84s/it]                                                    {'loss': 2.7915, 'grad_norm': 1.1616926193237305, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:33<4:35:05,  2.84s/it]  3%|â–Ž         | 181/6000 [08:35<4:32:27,  2.81s/it]                                                    {'loss': 2.8049, 'grad_norm': 0.9241259694099426, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:35<4:32:27,  2.81s/it]  3%|â–Ž         | 182/6000 [08:38<4:30:46,  2.79s/it]                                                    {'loss': 2.8198, 'grad_norm': 1.2421857118606567, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:38<4:30:46,  2.79s/it]  3%|â–Ž         | 183/6000 [08:41<4:27:38,  2.76s/it]                                                    {'loss': 2.774, 'grad_norm': 0.8003140687942505, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:41<4:27:38,  2.76s/it]  3%|â–Ž         | 184/6000 [08:43<4:26:31,  2.75s/it]                                                    {'loss': 2.7831, 'grad_norm': 1.7542144060134888, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:43<4:26:31,  2.75s/it]  3%|â–Ž         | 185/6000 [08:46<4:25:46,  2.74s/it]                                                    {'loss': 2.7918, 'grad_norm': 1.682100534439087, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:46<4:25:46,  2.74s/it]  3%|â–Ž         | 186/6000 [08:49<4:25:37,  2.74s/it]                                                    {'loss': 2.7813, 'grad_norm': 1.536170244216919, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:49<4:25:37,  2.74s/it]  3%|â–Ž         | 187/6000 [08:52<4:24:14,  2.73s/it]                                                    {'loss': 2.7804, 'grad_norm': 1.1493258476257324, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:52<4:24:14,  2.73s/it]  3%|â–Ž         | 188/6000 [08:54<4:23:47,  2.72s/it]                                                    {'loss': 2.7817, 'grad_norm': 2.213597297668457, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:54<4:23:47,  2.72s/it]  3%|â–Ž         | 189/6000 [08:57<4:26:13,  2.75s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.2125145196914673, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:57<4:26:13,  2.75s/it]  3%|â–Ž         | 190/6000 [09:00<4:24:05,  2.73s/it]                                                    {'loss': 2.7878, 'grad_norm': 1.3681052923202515, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [09:00<4:24:05,  2.73s/it]  3%|â–Ž         | 191/6000 [09:03<4:25:50,  2.75s/it]                                                    {'loss': 2.7804, 'grad_norm': 1.206766963005066, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:03<4:25:50,  2.75s/it]  3%|â–Ž         | 192/6000 [09:06<4:42:43,  2.92s/it]                                                    {'loss': 2.8754, 'grad_norm': 1.215221643447876, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:06<4:42:43,  2.92s/it]  3%|â–Ž         | 193/6000 [09:09<4:46:58,  2.97s/it]                                                    {'loss': 2.776, 'grad_norm': 1.845008373260498, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:09<4:46:58,  2.97s/it]  3%|â–Ž         | 194/6000 [09:12<4:38:17,  2.88s/it]                                                    {'loss': 2.7824, 'grad_norm': 1.2583518028259277, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:12<4:38:17,  2.88s/it]  3%|â–Ž         | 195/6000 [09:14<4:35:37,  2.85s/it]                                                    {'loss': 2.8052, 'grad_norm': 0.7875691056251526, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:14<4:35:37,  2.85s/it]  3%|â–Ž         | 196/6000 [09:17<4:33:42,  2.83s/it]                                                    {'loss': 2.7919, 'grad_norm': 1.0140011310577393, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:17<4:33:42,  2.83s/it]  3%|â–Ž         | 197/6000 [09:20<4:35:15,  2.85s/it]                                                    {'loss': 2.7918, 'grad_norm': 0.8880437612533569, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:20<4:35:15,  2.85s/it]  3%|â–Ž         | 198/6000 [09:23<4:35:19,  2.85s/it]                                                    {'loss': 2.7809, 'grad_norm': 0.8916622996330261, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:23<4:35:19,  2.85s/it]  3%|â–Ž         | 199/6000 [09:26<4:30:59,  2.80s/it]                                                    {'loss': 2.7703, 'grad_norm': 1.1015384197235107, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:26<4:30:59,  2.80s/it]  3%|â–Ž         | 200/6000 [09:29<4:42:47,  2.93s/it]                                                    {'loss': 2.7816, 'grad_norm': 0.6051157116889954, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:29<4:42:47,  2.93s/it][2025-10-22 19:05:54,363] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:34<5:41:32,  3.53s/it]                                                    {'loss': 2.8074, 'grad_norm': 0.8916390538215637, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:34<5:41:32,  3.53s/it]  3%|â–Ž         | 202/6000 [09:36<5:16:49,  3.28s/it]                                                    {'loss': 2.7798, 'grad_norm': 0.6676955819129944, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:36<5:16:49,  3.28s/it]  3%|â–Ž         | 203/6000 [09:39<5:01:01,  3.12s/it]                                                    {'loss': 2.7894, 'grad_norm': 0.7858458161354065, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:39<5:01:01,  3.12s/it]  3%|â–Ž         | 204/6000 [09:42<4:52:40,  3.03s/it]                                                    {'loss': 2.7818, 'grad_norm': 0.6409152150154114, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:42<4:52:40,  3.03s/it]  3%|â–Ž         | 205/6000 [09:45<4:40:51,  2.91s/it]                                                    {'loss': 2.7695, 'grad_norm': 0.7960647344589233, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:45<4:40:51,  2.91s/it]  3%|â–Ž         | 206/6000 [09:47<4:32:48,  2.83s/it]                                                    {'loss': 2.825, 'grad_norm': 0.5676194429397583, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:47<4:32:48,  2.83s/it]  3%|â–Ž         | 207/6000 [09:50<4:29:01,  2.79s/it]                                                    {'loss': 2.8637, 'grad_norm': 1.3329501152038574, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:50<4:29:01,  2.79s/it]  3%|â–Ž         | 208/6000 [09:53<4:24:17,  2.74s/it]                                                    {'loss': 2.7863, 'grad_norm': 1.5960581302642822, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:53<4:24:17,  2.74s/it]  3%|â–Ž         | 209/6000 [09:55<4:23:24,  2.73s/it]                                                    {'loss': 2.786, 'grad_norm': 1.9898502826690674, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:55<4:23:24,  2.73s/it]  4%|â–Ž         | 210/6000 [09:58<4:35:55,  2.86s/it]                                                    {'loss': 2.7714, 'grad_norm': 1.3324577808380127, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:58<4:35:55,  2.86s/it]  4%|â–Ž         | 211/6000 [10:01<4:37:54,  2.88s/it]                                                    {'loss': 2.802, 'grad_norm': 2.306572675704956, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:01<4:37:54,  2.88s/it]  4%|â–Ž         | 212/6000 [10:05<4:45:18,  2.96s/it]                                                    {'loss': 2.7786, 'grad_norm': 1.497848391532898, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:05<4:45:18,  2.96s/it]  4%|â–Ž         | 213/6000 [10:07<4:40:49,  2.91s/it]                                                    {'loss': 2.7892, 'grad_norm': 1.4909851551055908, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:07<4:40:49,  2.91s/it]  4%|â–Ž         | 214/6000 [10:10<4:33:35,  2.84s/it]                                                    {'loss': 2.7827, 'grad_norm': 0.7405773401260376, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:10<4:33:35,  2.84s/it]  4%|â–Ž         | 215/6000 [10:13<4:31:15,  2.81s/it]                                                    {'loss': 2.8038, 'grad_norm': 0.7893161773681641, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:13<4:31:15,  2.81s/it]  4%|â–Ž         | 216/6000 [10:16<4:29:36,  2.80s/it]                                                    {'loss': 2.7849, 'grad_norm': 0.833670437335968, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:16<4:29:36,  2.80s/it]  4%|â–Ž         | 217/6000 [10:18<4:31:12,  2.81s/it]                                                    {'loss': 2.8224, 'grad_norm': 0.4721301198005676, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:18<4:31:12,  2.81s/it]  4%|â–Ž         | 218/6000 [10:21<4:31:35,  2.82s/it]                                                    {'loss': 2.7911, 'grad_norm': 0.5968907475471497, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:21<4:31:35,  2.82s/it]  4%|â–Ž         | 219/6000 [10:24<4:27:33,  2.78s/it]                                                    {'loss': 2.8248, 'grad_norm': 0.5564918518066406, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:24<4:27:33,  2.78s/it]  4%|â–Ž         | 220/6000 [10:27<4:26:50,  2.77s/it]                                                    {'loss': 2.7752, 'grad_norm': 0.5956581234931946, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:27<4:26:50,  2.77s/it]  4%|â–Ž         | 221/6000 [10:29<4:25:25,  2.76s/it]                                                    {'loss': 2.8048, 'grad_norm': 0.5984304547309875, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:29<4:25:25,  2.76s/it]  4%|â–Ž         | 222/6000 [10:32<4:24:47,  2.75s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.5402622818946838, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:32<4:24:47,  2.75s/it]  4%|â–Ž         | 223/6000 [10:35<4:24:49,  2.75s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.5954736471176147, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:35<4:24:49,  2.75s/it]  4%|â–Ž         | 224/6000 [10:38<4:21:56,  2.72s/it]                                                    {'loss': 2.8189, 'grad_norm': 0.7585514187812805, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:38<4:21:56,  2.72s/it]  4%|â–         | 225/6000 [10:40<4:23:54,  2.74s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.6446888446807861, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:40<4:23:54,  2.74s/it]  4%|â–         | 226/6000 [10:44<4:37:44,  2.89s/it]                                                    {'loss': 2.7819, 'grad_norm': 0.7449324727058411, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:44<4:37:44,  2.89s/it]  4%|â–         | 227/6000 [10:46<4:31:41,  2.82s/it]                                                    {'loss': 2.8189, 'grad_norm': 0.6341670751571655, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:46<4:31:41,  2.82s/it]  4%|â–         | 228/6000 [10:49<4:29:07,  2.80s/it]                                                    {'loss': 2.7824, 'grad_norm': 0.551701009273529, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:49<4:29:07,  2.80s/it]  4%|â–         | 229/6000 [10:52<4:24:07,  2.75s/it]                                                    {'loss': 2.7892, 'grad_norm': 0.4879872500896454, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:52<4:24:07,  2.75s/it]  4%|â–         | 230/6000 [10:54<4:24:35,  2.75s/it]                                                    {'loss': 2.8049, 'grad_norm': 0.5451579689979553, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:54<4:24:35,  2.75s/it]  4%|â–         | 231/6000 [10:57<4:22:23,  2.73s/it]                                                    {'loss': 2.7803, 'grad_norm': 0.7472339272499084, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:57<4:22:23,  2.73s/it]  4%|â–         | 232/6000 [11:00<4:26:34,  2.77s/it]                                                    {'loss': 2.7714, 'grad_norm': 0.6654379367828369, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [11:00<4:26:34,  2.77s/it]  4%|â–         | 233/6000 [11:03<4:37:06,  2.88s/it]                                                    {'loss': 2.7738, 'grad_norm': 0.48098957538604736, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:03<4:37:06,  2.88s/it]  4%|â–         | 234/6000 [11:06<4:40:45,  2.92s/it]                                                    {'loss': 2.772, 'grad_norm': 0.5122215747833252, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:06<4:40:45,  2.92s/it]  4%|â–         | 235/6000 [11:09<4:34:54,  2.86s/it]                                                    {'loss': 2.8407, 'grad_norm': 0.8716360330581665, 'learning_rate': 4.885593220338983e-05, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:09<4:34:54,  2.86s/it]  4%|â–         | 236/6000 [11:12<4:32:32,  2.84s/it]                                                    {'loss': 2.8052, 'grad_norm': 0.6562816500663757, 'learning_rate': 4.884745762711865e-05, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:12<4:32:32,  2.84s/it]  4%|â–         | 237/6000 [11:14<4:29:39,  2.81s/it]                                                    {'loss': 2.7713, 'grad_norm': 0.6647793650627136, 'learning_rate': 4.883898305084746e-05, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:14<4:29:39,  2.81s/it]  4%|â–         | 238/6000 [11:18<4:41:37,  2.93s/it]                                                    {'loss': 2.8411, 'grad_norm': 0.99228835105896, 'learning_rate': 4.883050847457628e-05, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:18<4:41:37,  2.93s/it]  4%|â–         | 239/6000 [11:20<4:38:36,  2.90s/it]                                                    {'loss': 2.7766, 'grad_norm': 0.6961161494255066, 'learning_rate': 4.882203389830508e-05, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:20<4:38:36,  2.90s/it]  4%|â–         | 240/6000 [11:23<4:34:10,  2.86s/it]                                                    {'loss': 2.7862, 'grad_norm': 0.8870830535888672, 'learning_rate': 4.88135593220339e-05, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:23<4:34:10,  2.86s/it]  4%|â–         | 241/6000 [11:26<4:32:38,  2.84s/it]                                                    {'loss': 2.7861, 'grad_norm': 0.8435547947883606, 'learning_rate': 4.880508474576271e-05, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:26<4:32:38,  2.84s/it]  4%|â–         | 242/6000 [11:29<4:28:24,  2.80s/it]                                                    {'loss': 2.8044, 'grad_norm': 0.5790929198265076, 'learning_rate': 4.879661016949153e-05, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:29<4:28:24,  2.80s/it]  4%|â–         | 243/6000 [11:32<4:36:18,  2.88s/it]                                                    {'loss': 2.7786, 'grad_norm': 0.7136828899383545, 'learning_rate': 4.878813559322034e-05, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:32<4:36:18,  2.88s/it]  4%|â–         | 244/6000 [11:34<4:30:39,  2.82s/it]                                                    {'loss': 2.7688, 'grad_norm': 0.6932761669158936, 'learning_rate': 4.877966101694916e-05, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:34<4:30:39,  2.82s/it]  4%|â–         | 245/6000 [11:37<4:29:01,  2.80s/it]                                                    {'loss': 2.7819, 'grad_norm': 1.0063008069992065, 'learning_rate': 4.877118644067797e-05, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:37<4:29:01,  2.80s/it]  4%|â–         | 246/6000 [11:40<4:30:57,  2.83s/it]                                                    {'loss': 2.779, 'grad_norm': 0.7113973498344421, 'learning_rate': 4.876271186440678e-05, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:40<4:30:57,  2.83s/it]  4%|â–         | 247/6000 [11:43<4:27:54,  2.79s/it]                                                    {'loss': 2.7702, 'grad_norm': 0.8242244720458984, 'learning_rate': 4.8754237288135593e-05, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:43<4:27:54,  2.79s/it]  4%|â–         | 248/6000 [11:46<4:34:52,  2.87s/it]                                                    {'loss': 2.7711, 'grad_norm': 0.9260421395301819, 'learning_rate': 4.8745762711864405e-05, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:46<4:34:52,  2.87s/it]  4%|â–         | 249/6000 [11:48<4:28:19,  2.80s/it]                                                    {'loss': 2.7756, 'grad_norm': 0.8400434255599976, 'learning_rate': 4.873728813559322e-05, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:48<4:28:19,  2.80s/it]  4%|â–         | 250/6000 [11:51<4:26:46,  2.78s/it]                                                    {'loss': 2.7888, 'grad_norm': 0.9029681086540222, 'learning_rate': 4.8728813559322034e-05, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:51<4:26:46,  2.78s/it][2025-10-22 19:08:16,645] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [11:56<5:23:53,  3.38s/it]                                                    {'loss': 2.7918, 'grad_norm': 0.8392895460128784, 'learning_rate': 4.872033898305085e-05, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:56<5:23:53,  3.38s/it]  4%|â–         | 252/6000 [11:59<5:10:19,  3.24s/it]                                                    {'loss': 2.8045, 'grad_norm': 1.665186882019043, 'learning_rate': 4.8711864406779663e-05, 'epoch': 0.04}
  4%|â–         | 252/6000 [11:59<5:10:19,  3.24s/it]  4%|â–         | 253/6000 [12:02<4:55:36,  3.09s/it]                                                    {'loss': 2.8011, 'grad_norm': 1.4238917827606201, 'learning_rate': 4.8703389830508475e-05, 'epoch': 0.04}
  4%|â–         | 253/6000 [12:02<4:55:36,  3.09s/it]  4%|â–         | 254/6000 [12:04<4:47:26,  3.00s/it]                                                    {'loss': 2.7703, 'grad_norm': 1.6068507432937622, 'learning_rate': 4.8694915254237286e-05, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:04<4:47:26,  3.00s/it]  4%|â–         | 255/6000 [12:07<4:37:38,  2.90s/it]                                                    {'loss': 2.7767, 'grad_norm': 1.1572598218917847, 'learning_rate': 4.8686440677966104e-05, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:07<4:37:38,  2.90s/it]  4%|â–         | 256/6000 [12:10<4:45:56,  2.99s/it]                                                    {'loss': 2.7929, 'grad_norm': 1.077364206314087, 'learning_rate': 4.8677966101694915e-05, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:10<4:45:56,  2.99s/it]  4%|â–         | 257/6000 [12:13<4:37:13,  2.90s/it]                                                    {'loss': 2.7675, 'grad_norm': 0.9662802219390869, 'learning_rate': 4.8669491525423733e-05, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:13<4:37:13,  2.90s/it]  4%|â–         | 258/6000 [12:16<4:33:30,  2.86s/it]                                                    {'loss': 2.7638, 'grad_norm': 1.4871293306350708, 'learning_rate': 4.8661016949152545e-05, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:16<4:33:30,  2.86s/it]  4%|â–         | 259/6000 [12:18<4:26:53,  2.79s/it]                                                    {'loss': 2.7892, 'grad_norm': 1.6265480518341064, 'learning_rate': 4.865254237288136e-05, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:18<4:26:53,  2.79s/it]  4%|â–         | 260/6000 [12:21<4:24:50,  2.77s/it]                                                    {'loss': 2.8053, 'grad_norm': 1.8119369745254517, 'learning_rate': 4.8644067796610174e-05, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:21<4:24:50,  2.77s/it]  4%|â–         | 261/6000 [12:24<4:28:06,  2.80s/it]                                                    {'loss': 2.7811, 'grad_norm': 0.9740840196609497, 'learning_rate': 4.8635593220338985e-05, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:24<4:28:06,  2.80s/it]  4%|â–         | 262/6000 [12:27<4:27:33,  2.80s/it]                                                    {'loss': 2.7693, 'grad_norm': 1.3416855335235596, 'learning_rate': 4.86271186440678e-05, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:27<4:27:33,  2.80s/it]  4%|â–         | 263/6000 [12:30<4:36:10,  2.89s/it]                                                    {'loss': 2.8446, 'grad_norm': 0.7369863986968994, 'learning_rate': 4.8618644067796615e-05, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:30<4:36:10,  2.89s/it]  4%|â–         | 264/6000 [12:32<4:31:16,  2.84s/it]                                                    {'loss': 2.7825, 'grad_norm': 2.2998719215393066, 'learning_rate': 4.8610169491525426e-05, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:32<4:31:16,  2.84s/it]  4%|â–         | 265/6000 [12:35<4:35:03,  2.88s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.8899153470993042, 'learning_rate': 4.8601694915254244e-05, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:35<4:35:03,  2.88s/it]  4%|â–         | 266/6000 [12:38<4:35:47,  2.89s/it]                                                    {'loss': 2.8032, 'grad_norm': 0.9800182580947876, 'learning_rate': 4.8593220338983055e-05, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:38<4:35:47,  2.89s/it]  4%|â–         | 267/6000 [12:41<4:30:20,  2.83s/it]                                                    {'loss': 2.7821, 'grad_norm': 1.1318697929382324, 'learning_rate': 4.858474576271187e-05, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:41<4:30:20,  2.83s/it]  4%|â–         | 268/6000 [12:44<4:27:55,  2.80s/it]                                                    {'loss': 2.8465, 'grad_norm': 0.9368306994438171, 'learning_rate': 4.857627118644068e-05, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:44<4:27:55,  2.80s/it]  4%|â–         | 269/6000 [12:47<4:31:24,  2.84s/it]                                                    {'loss': 2.7762, 'grad_norm': 1.228013515472412, 'learning_rate': 4.856779661016949e-05, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:47<4:31:24,  2.84s/it]  4%|â–         | 270/6000 [12:49<4:27:56,  2.81s/it]                                                    {'loss': 2.7859, 'grad_norm': 1.1343659162521362, 'learning_rate': 4.855932203389831e-05, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:49<4:27:56,  2.81s/it]  5%|â–         | 271/6000 [12:52<4:24:48,  2.77s/it]                                                    {'loss': 2.8054, 'grad_norm': 1.0710715055465698, 'learning_rate': 4.855084745762712e-05, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:52<4:24:48,  2.77s/it]  5%|â–         | 272/6000 [12:55<4:21:52,  2.74s/it]                                                    {'loss': 2.7792, 'grad_norm': 1.5512140989303589, 'learning_rate': 4.8542372881355937e-05, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:55<4:21:52,  2.74s/it]  5%|â–         | 273/6000 [12:58<4:21:03,  2.73s/it]                                                    {'loss': 2.8017, 'grad_norm': 1.0353902578353882, 'learning_rate': 4.853389830508475e-05, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:58<4:21:03,  2.73s/it]  5%|â–         | 274/6000 [13:01<4:32:50,  2.86s/it]                                                    {'loss': 3.2309, 'grad_norm': 871.2932739257812, 'learning_rate': 4.8525423728813566e-05, 'epoch': 0.05}
  5%|â–         | 274/6000 [13:01<4:32:50,  2.86s/it]  5%|â–         | 275/6000 [13:03<4:30:09,  2.83s/it]                                                    {'loss': 2.7855, 'grad_norm': 1.37944757938385, 'learning_rate': 4.851694915254237e-05, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:03<4:30:09,  2.83s/it]  5%|â–         | 276/6000 [13:06<4:29:37,  2.83s/it]                                                    {'loss': 2.8425, 'grad_norm': 0.9606418013572693, 'learning_rate': 4.850847457627119e-05, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:06<4:29:37,  2.83s/it]  5%|â–         | 277/6000 [13:09<4:24:00,  2.77s/it]                                                    {'loss': 2.7788, 'grad_norm': 0.8292312026023865, 'learning_rate': 4.85e-05, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:09<4:24:00,  2.77s/it]  5%|â–         | 278/6000 [13:12<4:22:46,  2.76s/it]                                                    {'loss': 2.7695, 'grad_norm': 1.0517146587371826, 'learning_rate': 4.849152542372882e-05, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:12<4:22:46,  2.76s/it]  5%|â–         | 279/6000 [13:14<4:24:21,  2.77s/it]                                                    {'loss': 2.7964, 'grad_norm': 1.0305081605911255, 'learning_rate': 4.848305084745763e-05, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:14<4:24:21,  2.77s/it]  5%|â–         | 280/6000 [13:17<4:22:42,  2.76s/it]                                                    {'loss': 2.7722, 'grad_norm': 1.098232388496399, 'learning_rate': 4.847457627118645e-05, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:17<4:22:42,  2.76s/it]  5%|â–         | 281/6000 [13:20<4:23:26,  2.76s/it]                                                    {'loss': 2.7811, 'grad_norm': 1.0133302211761475, 'learning_rate': 4.846610169491526e-05, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:20<4:23:26,  2.76s/it]  5%|â–         | 282/6000 [13:23<4:35:19,  2.89s/it]                                                    {'loss': 2.7813, 'grad_norm': 1.1997913122177124, 'learning_rate': 4.845762711864407e-05, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:23<4:35:19,  2.89s/it]  5%|â–         | 283/6000 [13:26<4:39:40,  2.94s/it]                                                    {'loss': 2.773, 'grad_norm': 0.9541418552398682, 'learning_rate': 4.844915254237288e-05, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:26<4:39:40,  2.94s/it]  5%|â–         | 284/6000 [13:30<4:53:37,  3.08s/it]                                                    {'loss': 2.779, 'grad_norm': 1.1257200241088867, 'learning_rate': 4.84406779661017e-05, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:30<4:53:37,  3.08s/it]  5%|â–         | 285/6000 [13:33<4:49:42,  3.04s/it]                                                    {'loss': 2.7777, 'grad_norm': 1.550330638885498, 'learning_rate': 4.843220338983051e-05, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:33<4:49:42,  3.04s/it]  5%|â–         | 286/6000 [13:35<4:38:51,  2.93s/it]                                                    {'loss': 2.7843, 'grad_norm': 1.3143881559371948, 'learning_rate': 4.842372881355933e-05, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:35<4:38:51,  2.93s/it]  5%|â–         | 287/6000 [13:38<4:34:34,  2.88s/it]                                                    {'loss': 2.8014, 'grad_norm': 1.0058320760726929, 'learning_rate': 4.841525423728814e-05, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:38<4:34:34,  2.88s/it]  5%|â–         | 288/6000 [13:41<4:30:09,  2.84s/it]                                                    {'loss': 2.7816, 'grad_norm': 1.5520246028900146, 'learning_rate': 4.840677966101695e-05, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:41<4:30:09,  2.84s/it]  5%|â–         | 289/6000 [13:44<4:32:05,  2.86s/it]                                                    {'loss': 2.7691, 'grad_norm': 1.0808210372924805, 'learning_rate': 4.839830508474576e-05, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:44<4:32:05,  2.86s/it]  5%|â–         | 290/6000 [13:46<4:29:05,  2.83s/it]                                                    {'loss': 2.8155, 'grad_norm': 1.0137803554534912, 'learning_rate': 4.8389830508474574e-05, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:46<4:29:05,  2.83s/it]  5%|â–         | 291/6000 [13:49<4:24:33,  2.78s/it]                                                    {'loss': 2.781, 'grad_norm': 1.111617922782898, 'learning_rate': 4.838135593220339e-05, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:49<4:24:33,  2.78s/it]  5%|â–         | 292/6000 [13:52<4:21:48,  2.75s/it]                                                    {'loss': 2.7692, 'grad_norm': 0.9250470399856567, 'learning_rate': 4.83728813559322e-05, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:52<4:21:48,  2.75s/it]  5%|â–         | 293/6000 [13:54<4:19:29,  2.73s/it]                                                    {'loss': 2.8384, 'grad_norm': 1.316334843635559, 'learning_rate': 4.836440677966102e-05, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:54<4:19:29,  2.73s/it]  5%|â–         | 294/6000 [13:57<4:18:58,  2.72s/it]                                                    {'loss': 2.7779, 'grad_norm': 1.385183334350586, 'learning_rate': 4.835593220338983e-05, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:57<4:18:58,  2.72s/it]  5%|â–         | 295/6000 [14:00<4:18:33,  2.72s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.943327009677887, 'learning_rate': 4.834745762711865e-05, 'epoch': 0.05}
  5%|â–         | 295/6000 [14:00<4:18:33,  2.72s/it]  5%|â–         | 296/6000 [14:03<4:23:22,  2.77s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.541396141052246, 'learning_rate': 4.833898305084746e-05, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:03<4:23:22,  2.77s/it]  5%|â–         | 297/6000 [14:05<4:23:02,  2.77s/it]                                                    {'loss': 2.7958, 'grad_norm': 1.3036457300186157, 'learning_rate': 4.833050847457627e-05, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:05<4:23:02,  2.77s/it]  5%|â–         | 298/6000 [14:08<4:27:47,  2.82s/it]                                                    {'loss': 2.7813, 'grad_norm': 0.8210105895996094, 'learning_rate': 4.8322033898305084e-05, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:08<4:27:47,  2.82s/it]  5%|â–         | 299/6000 [14:12<4:50:31,  3.06s/it]                                                    {'loss': 2.7854, 'grad_norm': 1.3393940925598145, 'learning_rate': 4.83135593220339e-05, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:12<4:50:31,  3.06s/it]  5%|â–Œ         | 300/6000 [14:15<4:40:45,  2.96s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.9612180590629578, 'learning_rate': 4.8305084745762714e-05, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:15<4:40:45,  2.96s/it][2025-10-22 19:10:40,260] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [14:20<5:39:09,  3.57s/it]                                                    {'loss': 2.8115, 'grad_norm': 2.0308847427368164, 'learning_rate': 4.829661016949153e-05, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:20<5:39:09,  3.57s/it]  5%|â–Œ         | 302/6000 [14:22<5:15:13,  3.32s/it]                                                    {'loss': 2.7726, 'grad_norm': 1.634096384048462, 'learning_rate': 4.828813559322034e-05, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:23<5:15:13,  3.32s/it]  5%|â–Œ         | 303/6000 [14:25<4:58:50,  3.15s/it]                                                    {'loss': 2.7702, 'grad_norm': 4.019636154174805, 'learning_rate': 4.8279661016949154e-05, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:25<4:58:50,  3.15s/it]  5%|â–Œ         | 304/6000 [14:28<4:47:35,  3.03s/it]                                                    {'loss': 2.7949, 'grad_norm': 2.134908676147461, 'learning_rate': 4.8271186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:28<4:47:35,  3.03s/it]  5%|â–Œ         | 305/6000 [14:31<4:36:16,  2.91s/it]                                                    {'loss': 2.7889, 'grad_norm': 3.7292933464050293, 'learning_rate': 4.8262711864406784e-05, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:31<4:36:16,  2.91s/it]  5%|â–Œ         | 306/6000 [14:33<4:31:33,  2.86s/it]                                                    {'loss': 2.7677, 'grad_norm': 1.3235185146331787, 'learning_rate': 4.8254237288135595e-05, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:33<4:31:33,  2.86s/it]  5%|â–Œ         | 307/6000 [14:36<4:26:45,  2.81s/it]                                                    {'loss': 2.7954, 'grad_norm': 2.2316949367523193, 'learning_rate': 4.824576271186441e-05, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:36<4:26:45,  2.81s/it]  5%|â–Œ         | 308/6000 [14:39<4:28:33,  2.83s/it]                                                    {'loss': 2.7893, 'grad_norm': 1.6258431673049927, 'learning_rate': 4.8237288135593224e-05, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:39<4:28:33,  2.83s/it]  5%|â–Œ         | 309/6000 [14:42<4:28:33,  2.83s/it]                                                    {'loss': 2.7792, 'grad_norm': 3.176619529724121, 'learning_rate': 4.8228813559322036e-05, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:42<4:28:33,  2.83s/it]  5%|â–Œ         | 310/6000 [14:45<4:26:13,  2.81s/it]                                                    {'loss': 2.8167, 'grad_norm': 3.3163745403289795, 'learning_rate': 4.822033898305085e-05, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:45<4:26:13,  2.81s/it]  5%|â–Œ         | 311/6000 [14:47<4:24:56,  2.79s/it]                                                    {'loss': 2.7739, 'grad_norm': 0.8914790749549866, 'learning_rate': 4.821186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:47<4:24:56,  2.79s/it]  5%|â–Œ         | 312/6000 [14:50<4:22:32,  2.77s/it]                                                    {'loss': 2.8, 'grad_norm': 1.8468360900878906, 'learning_rate': 4.8203389830508476e-05, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:50<4:22:32,  2.77s/it]  5%|â–Œ         | 313/6000 [14:53<4:22:55,  2.77s/it]                                                    {'loss': 2.8556, 'grad_norm': 1.6417148113250732, 'learning_rate': 4.819491525423729e-05, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:53<4:22:55,  2.77s/it]  5%|â–Œ         | 314/6000 [14:56<4:21:15,  2.76s/it]                                                    {'loss': 2.7884, 'grad_norm': 1.820271611213684, 'learning_rate': 4.8186440677966105e-05, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:56<4:21:15,  2.76s/it]  5%|â–Œ         | 315/6000 [14:58<4:17:49,  2.72s/it]                                                    {'loss': 2.7822, 'grad_norm': 0.793633759021759, 'learning_rate': 4.817796610169492e-05, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:58<4:17:49,  2.72s/it]  5%|â–Œ         | 316/6000 [15:01<4:16:20,  2.71s/it]                                                    {'loss': 2.7841, 'grad_norm': 1.6087710857391357, 'learning_rate': 4.8169491525423735e-05, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [15:01<4:16:20,  2.71s/it]  5%|â–Œ         | 317/6000 [15:04<4:22:46,  2.77s/it]                                                    {'loss': 2.8403, 'grad_norm': 0.8885043263435364, 'learning_rate': 4.8161016949152546e-05, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [15:04<4:22:46,  2.77s/it]  5%|â–Œ         | 318/6000 [15:07<4:30:54,  2.86s/it]                                                    {'loss': 2.786, 'grad_norm': 1.3269869089126587, 'learning_rate': 4.815254237288136e-05, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [15:07<4:30:54,  2.86s/it]  5%|â–Œ         | 319/6000 [15:09<4:25:50,  2.81s/it]                                                    {'loss': 2.7721, 'grad_norm': 0.8335872292518616, 'learning_rate': 4.814406779661017e-05, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [15:09<4:25:50,  2.81s/it]  5%|â–Œ         | 320/6000 [15:12<4:25:41,  2.81s/it]                                                    {'loss': 2.7675, 'grad_norm': 1.6013484001159668, 'learning_rate': 4.813559322033899e-05, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:12<4:25:41,  2.81s/it]  5%|â–Œ         | 321/6000 [15:15<4:35:05,  2.91s/it]                                                    {'loss': 2.7862, 'grad_norm': 0.9758970141410828, 'learning_rate': 4.81271186440678e-05, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:15<4:35:05,  2.91s/it]  5%|â–Œ         | 322/6000 [15:18<4:30:02,  2.85s/it]                                                    {'loss': 2.7727, 'grad_norm': 1.0510646104812622, 'learning_rate': 4.8118644067796616e-05, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:18<4:30:02,  2.85s/it]  5%|â–Œ         | 323/6000 [15:21<4:43:17,  2.99s/it]                                                    {'loss': 2.7792, 'grad_norm': 0.6769871711730957, 'learning_rate': 4.811016949152543e-05, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:21<4:43:17,  2.99s/it]  5%|â–Œ         | 324/6000 [15:24<4:33:50,  2.89s/it]                                                    {'loss': 2.7992, 'grad_norm': 0.6834317445755005, 'learning_rate': 4.810169491525424e-05, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:24<4:33:50,  2.89s/it]  5%|â–Œ         | 325/6000 [15:27<4:27:25,  2.83s/it]                                                    {'loss': 2.7763, 'grad_norm': 1.0388734340667725, 'learning_rate': 4.809322033898305e-05, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:27<4:27:25,  2.83s/it]  5%|â–Œ         | 326/6000 [15:30<4:25:15,  2.80s/it]                                                    {'loss': 2.7801, 'grad_norm': 1.0917766094207764, 'learning_rate': 4.808474576271187e-05, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:30<4:25:15,  2.80s/it]  5%|â–Œ         | 327/6000 [15:32<4:21:02,  2.76s/it]                                                    {'loss': 2.7743, 'grad_norm': 0.5914376974105835, 'learning_rate': 4.807627118644068e-05, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:32<4:21:02,  2.76s/it]  5%|â–Œ         | 328/6000 [15:35<4:19:48,  2.75s/it]                                                    {'loss': 2.7804, 'grad_norm': 0.8662992715835571, 'learning_rate': 4.80677966101695e-05, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:35<4:19:48,  2.75s/it]  5%|â–Œ         | 329/6000 [15:38<4:19:17,  2.74s/it]                                                    {'loss': 2.7912, 'grad_norm': 0.7983425259590149, 'learning_rate': 4.805932203389831e-05, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:38<4:19:17,  2.74s/it]  6%|â–Œ         | 330/6000 [15:40<4:19:51,  2.75s/it]                                                    {'loss': 2.7966, 'grad_norm': 1.26876962184906, 'learning_rate': 4.805084745762712e-05, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:40<4:19:51,  2.75s/it]  6%|â–Œ         | 331/6000 [15:43<4:19:17,  2.74s/it]                                                    {'loss': 2.7771, 'grad_norm': 1.4404889345169067, 'learning_rate': 4.804237288135594e-05, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:43<4:19:17,  2.74s/it]  6%|â–Œ         | 332/6000 [15:46<4:16:44,  2.72s/it]                                                    {'loss': 2.7831, 'grad_norm': 1.0193284749984741, 'learning_rate': 4.803389830508474e-05, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:46<4:16:44,  2.72s/it]  6%|â–Œ         | 333/6000 [15:49<4:22:09,  2.78s/it]                                                    {'loss': 2.7804, 'grad_norm': 1.161956787109375, 'learning_rate': 4.802542372881356e-05, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:49<4:22:09,  2.78s/it]  6%|â–Œ         | 334/6000 [15:51<4:20:31,  2.76s/it]                                                    {'loss': 2.7787, 'grad_norm': 0.9779345989227295, 'learning_rate': 4.801694915254237e-05, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:51<4:20:31,  2.76s/it]  6%|â–Œ         | 335/6000 [15:54<4:22:54,  2.78s/it]                                                    {'loss': 2.7954, 'grad_norm': 1.0792573690414429, 'learning_rate': 4.800847457627119e-05, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [15:54<4:22:54,  2.78s/it]  6%|â–Œ         | 336/6000 [15:57<4:18:46,  2.74s/it]                                                    {'loss': 2.776, 'grad_norm': 0.9639716744422913, 'learning_rate': 4.8e-05, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [15:57<4:18:46,  2.74s/it]  6%|â–Œ         | 337/6000 [16:00<4:21:15,  2.77s/it]                                                    {'loss': 2.7835, 'grad_norm': 0.6570354700088501, 'learning_rate': 4.799152542372882e-05, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [16:00<4:21:15,  2.77s/it]  6%|â–Œ         | 338/6000 [16:02<4:18:45,  2.74s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.9353364706039429, 'learning_rate': 4.798305084745763e-05, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [16:02<4:18:45,  2.74s/it]  6%|â–Œ         | 339/6000 [16:05<4:19:31,  2.75s/it]                                                    {'loss': 2.7724, 'grad_norm': 0.6838298439979553, 'learning_rate': 4.797457627118644e-05, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [16:05<4:19:31,  2.75s/it]  6%|â–Œ         | 340/6000 [16:08<4:26:08,  2.82s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.6611545085906982, 'learning_rate': 4.796610169491525e-05, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [16:08<4:26:08,  2.82s/it]  6%|â–Œ         | 341/6000 [16:11<4:22:37,  2.78s/it]                                                    {'loss': 2.7771, 'grad_norm': 1.1082711219787598, 'learning_rate': 4.795762711864407e-05, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [16:11<4:22:37,  2.78s/it]  6%|â–Œ         | 342/6000 [16:14<4:19:45,  2.75s/it]                                                    {'loss': 2.8051, 'grad_norm': 0.8535230159759521, 'learning_rate': 4.794915254237288e-05, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [16:14<4:19:45,  2.75s/it]  6%|â–Œ         | 343/6000 [16:17<4:35:57,  2.93s/it]                                                    {'loss': 2.8287, 'grad_norm': 1.0429328680038452, 'learning_rate': 4.79406779661017e-05, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [16:17<4:35:57,  2.93s/it]  6%|â–Œ         | 344/6000 [16:20<4:28:59,  2.85s/it]                                                    {'loss': 2.7919, 'grad_norm': 0.7648784518241882, 'learning_rate': 4.793220338983051e-05, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [16:20<4:28:59,  2.85s/it]  6%|â–Œ         | 345/6000 [16:22<4:23:24,  2.79s/it]                                                    {'loss': 2.8252, 'grad_norm': 0.54165118932724, 'learning_rate': 4.792372881355933e-05, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [16:22<4:23:24,  2.79s/it]  6%|â–Œ         | 346/6000 [16:25<4:21:51,  2.78s/it]                                                    {'loss': 2.8213, 'grad_norm': 0.4815484285354614, 'learning_rate': 4.7915254237288134e-05, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:25<4:21:51,  2.78s/it]  6%|â–Œ         | 347/6000 [16:28<4:19:13,  2.75s/it]                                                    {'loss': 2.7899, 'grad_norm': 0.6882910132408142, 'learning_rate': 4.790677966101695e-05, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:28<4:19:13,  2.75s/it]  6%|â–Œ         | 348/6000 [16:31<4:20:49,  2.77s/it]                                                    {'loss': 2.7941, 'grad_norm': 0.6998135447502136, 'learning_rate': 4.7898305084745764e-05, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:31<4:20:49,  2.77s/it]  6%|â–Œ         | 349/6000 [16:33<4:19:20,  2.75s/it]                                                    {'loss': 2.7933, 'grad_norm': 0.9416264891624451, 'learning_rate': 4.788983050847458e-05, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:33<4:19:20,  2.75s/it]  6%|â–Œ         | 350/6000 [16:36<4:18:21,  2.74s/it]                                                    {'loss': 2.7804, 'grad_norm': 0.5103703737258911, 'learning_rate': 4.788135593220339e-05, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:36<4:18:21,  2.74s/it][2025-10-22 19:13:01,461] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [16:41<5:32:46,  3.53s/it]                                                    {'loss': 2.7736, 'grad_norm': 0.5936635136604309, 'learning_rate': 4.7872881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:41<5:32:46,  3.53s/it]  6%|â–Œ         | 352/6000 [16:44<5:07:01,  3.26s/it]                                                    {'loss': 2.7768, 'grad_norm': 0.5633448362350464, 'learning_rate': 4.786440677966102e-05, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:44<5:07:01,  3.26s/it]  6%|â–Œ         | 353/6000 [16:47<4:48:58,  3.07s/it]                                                    {'loss': 2.8227, 'grad_norm': 0.42175301909446716, 'learning_rate': 4.7855932203389834e-05, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:47<4:48:58,  3.07s/it]  6%|â–Œ         | 354/6000 [16:49<4:38:32,  2.96s/it]                                                    {'loss': 2.7719, 'grad_norm': 0.505303144454956, 'learning_rate': 4.7847457627118645e-05, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:49<4:38:32,  2.96s/it]  6%|â–Œ         | 355/6000 [16:52<4:30:52,  2.88s/it]                                                    {'loss': 2.8062, 'grad_norm': 0.45602500438690186, 'learning_rate': 4.7838983050847456e-05, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:52<4:30:52,  2.88s/it]  6%|â–Œ         | 356/6000 [16:55<4:34:02,  2.91s/it]                                                    {'loss': 2.7897, 'grad_norm': 0.448150098323822, 'learning_rate': 4.7830508474576274e-05, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [16:55<4:34:02,  2.91s/it]  6%|â–Œ         | 357/6000 [16:58<4:31:02,  2.88s/it]                                                    {'loss': 2.8234, 'grad_norm': 0.5548089146614075, 'learning_rate': 4.7822033898305086e-05, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [16:58<4:31:02,  2.88s/it]  6%|â–Œ         | 358/6000 [17:00<4:25:28,  2.82s/it]                                                    {'loss': 2.7737, 'grad_norm': 0.42224013805389404, 'learning_rate': 4.7813559322033904e-05, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [17:00<4:25:28,  2.82s/it]  6%|â–Œ         | 359/6000 [17:03<4:23:02,  2.80s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.47470709681510925, 'learning_rate': 4.7805084745762715e-05, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [17:03<4:23:02,  2.80s/it]  6%|â–Œ         | 360/6000 [17:06<4:21:44,  2.78s/it]                                                    {'loss': 2.7906, 'grad_norm': 0.5083480477333069, 'learning_rate': 4.7796610169491526e-05, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [17:06<4:21:44,  2.78s/it]  6%|â–Œ         | 361/6000 [17:09<4:18:13,  2.75s/it]                                                    {'loss': 2.7728, 'grad_norm': 0.44580110907554626, 'learning_rate': 4.778813559322034e-05, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [17:09<4:18:13,  2.75s/it]  6%|â–Œ         | 362/6000 [17:11<4:15:47,  2.72s/it]                                                    {'loss': 2.7692, 'grad_norm': 0.5196375250816345, 'learning_rate': 4.7779661016949156e-05, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [17:11<4:15:47,  2.72s/it]  6%|â–Œ         | 363/6000 [17:14<4:14:58,  2.71s/it]                                                    {'loss': 2.8021, 'grad_norm': 0.42523977160453796, 'learning_rate': 4.777118644067797e-05, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [17:14<4:14:58,  2.71s/it]  6%|â–Œ         | 364/6000 [17:17<4:19:27,  2.76s/it]                                                    {'loss': 2.7741, 'grad_norm': 0.4138975143432617, 'learning_rate': 4.7762711864406785e-05, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [17:17<4:19:27,  2.76s/it]  6%|â–Œ         | 365/6000 [17:20<4:16:36,  2.73s/it]                                                    {'loss': 2.7822, 'grad_norm': 0.6200409531593323, 'learning_rate': 4.7754237288135596e-05, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [17:20<4:16:36,  2.73s/it]  6%|â–Œ         | 366/6000 [17:22<4:15:19,  2.72s/it]                                                    {'loss': 2.781, 'grad_norm': 0.4896470606327057, 'learning_rate': 4.7745762711864414e-05, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [17:22<4:15:19,  2.72s/it]  6%|â–Œ         | 367/6000 [17:25<4:14:29,  2.71s/it]                                                    {'loss': 2.7789, 'grad_norm': 0.44233787059783936, 'learning_rate': 4.773728813559322e-05, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [17:25<4:14:29,  2.71s/it]  6%|â–Œ         | 368/6000 [17:28<4:28:11,  2.86s/it]                                                    {'loss': 2.78, 'grad_norm': 0.4550652801990509, 'learning_rate': 4.772881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [17:28<4:28:11,  2.86s/it]  6%|â–Œ         | 369/6000 [17:31<4:23:55,  2.81s/it]                                                    {'loss': 2.7911, 'grad_norm': 0.46773096919059753, 'learning_rate': 4.772033898305085e-05, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [17:31<4:23:55,  2.81s/it]  6%|â–Œ         | 370/6000 [17:34<4:21:28,  2.79s/it]                                                    {'loss': 2.7691, 'grad_norm': 0.4900642931461334, 'learning_rate': 4.7711864406779666e-05, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [17:34<4:21:28,  2.79s/it]  6%|â–Œ         | 371/6000 [17:36<4:19:07,  2.76s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.48903152346611023, 'learning_rate': 4.770338983050848e-05, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [17:36<4:19:07,  2.76s/it]  6%|â–Œ         | 372/6000 [17:39<4:19:58,  2.77s/it]                                                    {'loss': 2.7899, 'grad_norm': 0.5388422012329102, 'learning_rate': 4.769491525423729e-05, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [17:39<4:19:58,  2.77s/it]  6%|â–Œ         | 373/6000 [17:42<4:17:30,  2.75s/it]                                                    {'loss': 2.9052, 'grad_norm': 0.4930418133735657, 'learning_rate': 4.768644067796611e-05, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [17:42<4:17:30,  2.75s/it]  6%|â–Œ         | 374/6000 [17:44<4:17:30,  2.75s/it]                                                    {'loss': 2.7757, 'grad_norm': 0.6085723042488098, 'learning_rate': 4.767796610169492e-05, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [17:44<4:17:30,  2.75s/it]  6%|â–‹         | 375/6000 [17:47<4:15:39,  2.73s/it]                                                    {'loss': 2.8045, 'grad_norm': 0.5399082899093628, 'learning_rate': 4.766949152542373e-05, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [17:47<4:15:39,  2.73s/it]  6%|â–‹         | 376/6000 [17:50<4:14:50,  2.72s/it]                                                    {'loss': 2.7798, 'grad_norm': 0.36250951886177063, 'learning_rate': 4.766101694915254e-05, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [17:50<4:14:50,  2.72s/it]  6%|â–‹         | 377/6000 [17:53<4:14:07,  2.71s/it]                                                    {'loss': 2.8046, 'grad_norm': 0.5121911764144897, 'learning_rate': 4.765254237288136e-05, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [17:53<4:14:07,  2.71s/it]  6%|â–‹         | 378/6000 [17:55<4:18:03,  2.75s/it]                                                    {'loss': 2.7812, 'grad_norm': 0.4597512483596802, 'learning_rate': 4.764406779661017e-05, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [17:55<4:18:03,  2.75s/it]  6%|â–‹         | 379/6000 [17:58<4:18:25,  2.76s/it]                                                    {'loss': 2.7803, 'grad_norm': 0.6345716118812561, 'learning_rate': 4.763559322033899e-05, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [17:58<4:18:25,  2.76s/it]  6%|â–‹         | 380/6000 [18:01<4:26:36,  2.85s/it]                                                    {'loss': 2.7882, 'grad_norm': 0.4815528988838196, 'learning_rate': 4.76271186440678e-05, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [18:01<4:26:36,  2.85s/it]  6%|â–‹         | 381/6000 [18:04<4:22:45,  2.81s/it]                                                    {'loss': 2.7822, 'grad_norm': 0.6077999472618103, 'learning_rate': 4.761864406779661e-05, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [18:04<4:22:45,  2.81s/it]  6%|â–‹         | 382/6000 [18:07<4:20:43,  2.78s/it]                                                    {'loss': 2.7742, 'grad_norm': 0.43319612741470337, 'learning_rate': 4.761016949152542e-05, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [18:07<4:20:43,  2.78s/it]  6%|â–‹         | 383/6000 [18:09<4:18:48,  2.76s/it]                                                    {'loss': 2.8048, 'grad_norm': 0.5511143803596497, 'learning_rate': 4.760169491525424e-05, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [18:09<4:18:48,  2.76s/it]  6%|â–‹         | 384/6000 [18:12<4:17:16,  2.75s/it]                                                    {'loss': 2.7705, 'grad_norm': 0.5679190754890442, 'learning_rate': 4.759322033898305e-05, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [18:12<4:17:16,  2.75s/it]  6%|â–‹         | 385/6000 [18:15<4:27:45,  2.86s/it]                                                    {'loss': 2.7737, 'grad_norm': 0.5832809209823608, 'learning_rate': 4.758474576271187e-05, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [18:15<4:27:45,  2.86s/it]  6%|â–‹         | 386/6000 [18:18<4:25:15,  2.83s/it]                                                    {'loss': 2.7904, 'grad_norm': 0.6812322735786438, 'learning_rate': 4.757627118644068e-05, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [18:18<4:25:15,  2.83s/it]  6%|â–‹         | 387/6000 [18:21<4:35:11,  2.94s/it]                                                    {'loss': 2.7686, 'grad_norm': 0.610093355178833, 'learning_rate': 4.75677966101695e-05, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [18:21<4:35:11,  2.94s/it]  6%|â–‹         | 388/6000 [18:24<4:38:45,  2.98s/it]                                                    {'loss': 2.8226, 'grad_norm': 0.6338546276092529, 'learning_rate': 4.755932203389831e-05, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [18:24<4:38:45,  2.98s/it]  6%|â–‹         | 389/6000 [18:27<4:35:05,  2.94s/it]                                                    {'loss': 2.7845, 'grad_norm': 0.9130272269248962, 'learning_rate': 4.755084745762712e-05, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [18:27<4:35:05,  2.94s/it]  6%|â–‹         | 390/6000 [18:30<4:29:57,  2.89s/it]                                                    {'loss': 2.7728, 'grad_norm': 0.5424142479896545, 'learning_rate': 4.754237288135593e-05, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [18:30<4:29:57,  2.89s/it]  7%|â–‹         | 391/6000 [18:33<4:24:38,  2.83s/it]                                                    {'loss': 2.7748, 'grad_norm': 0.5860805511474609, 'learning_rate': 4.7533898305084744e-05, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [18:33<4:24:38,  2.83s/it]  7%|â–‹         | 392/6000 [18:35<4:23:48,  2.82s/it]                                                    {'loss': 2.7753, 'grad_norm': 0.5593711733818054, 'learning_rate': 4.752542372881356e-05, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [18:35<4:23:48,  2.82s/it]  7%|â–‹         | 393/6000 [18:38<4:22:03,  2.80s/it]                                                    {'loss': 2.7816, 'grad_norm': 0.4993354082107544, 'learning_rate': 4.751694915254237e-05, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [18:38<4:22:03,  2.80s/it]  7%|â–‹         | 394/6000 [18:41<4:22:07,  2.81s/it]                                                    {'loss': 2.791, 'grad_norm': 0.5262820720672607, 'learning_rate': 4.750847457627119e-05, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [18:41<4:22:07,  2.81s/it]  7%|â–‹         | 395/6000 [18:44<4:19:58,  2.78s/it]                                                    {'loss': 2.7913, 'grad_norm': 0.4826474189758301, 'learning_rate': 4.75e-05, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [18:44<4:19:58,  2.78s/it]  7%|â–‹         | 396/6000 [18:46<4:19:01,  2.77s/it]                                                    {'loss': 2.7945, 'grad_norm': 0.5831425786018372, 'learning_rate': 4.7491525423728814e-05, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [18:46<4:19:01,  2.77s/it]  7%|â–‹         | 397/6000 [18:50<4:28:25,  2.87s/it]                                                    {'loss': 2.7803, 'grad_norm': 0.6966367959976196, 'learning_rate': 4.7483050847457625e-05, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [18:50<4:28:25,  2.87s/it]  7%|â–‹         | 398/6000 [18:52<4:24:21,  2.83s/it]                                                    {'loss': 2.7833, 'grad_norm': 0.5378730893135071, 'learning_rate': 4.747457627118644e-05, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [18:52<4:24:21,  2.83s/it]  7%|â–‹         | 399/6000 [18:55<4:23:07,  2.82s/it]                                                    {'loss': 2.7892, 'grad_norm': 0.5545874834060669, 'learning_rate': 4.7466101694915255e-05, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [18:55<4:23:07,  2.82s/it]  7%|â–‹         | 400/6000 [18:58<4:20:50,  2.79s/it]                                                    {'loss': 2.7706, 'grad_norm': 0.6300573348999023, 'learning_rate': 4.745762711864407e-05, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [18:58<4:20:50,  2.79s/it][2025-10-22 19:15:23,277] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [19:03<5:18:31,  3.41s/it]                                                    {'loss': 2.7736, 'grad_norm': 0.6291854977607727, 'learning_rate': 4.7449152542372884e-05, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [19:03<5:18:31,  3.41s/it]  7%|â–‹         | 402/6000 [19:05<4:57:39,  3.19s/it]                                                    {'loss': 2.7808, 'grad_norm': 0.6326956152915955, 'learning_rate': 4.74406779661017e-05, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [19:05<4:57:39,  3.19s/it]  7%|â–‹         | 403/6000 [19:08<4:44:47,  3.05s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.5243048667907715, 'learning_rate': 4.7432203389830506e-05, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [19:08<4:44:47,  3.05s/it]  7%|â–‹         | 404/6000 [19:11<4:38:15,  2.98s/it]                                                    {'loss': 2.7824, 'grad_norm': 0.37767013907432556, 'learning_rate': 4.7423728813559325e-05, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [19:11<4:38:15,  2.98s/it]  7%|â–‹         | 405/6000 [19:14<4:31:30,  2.91s/it]                                                    {'loss': 2.7745, 'grad_norm': 0.4863675534725189, 'learning_rate': 4.7415254237288136e-05, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [19:14<4:31:30,  2.91s/it]  7%|â–‹         | 406/6000 [19:16<4:26:57,  2.86s/it]                                                    {'loss': 2.7838, 'grad_norm': 0.5174292325973511, 'learning_rate': 4.7406779661016954e-05, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [19:16<4:26:57,  2.86s/it]  7%|â–‹         | 407/6000 [19:19<4:25:47,  2.85s/it]                                                    {'loss': 2.7821, 'grad_norm': 0.5123596787452698, 'learning_rate': 4.7398305084745765e-05, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [19:19<4:25:47,  2.85s/it]  7%|â–‹         | 408/6000 [19:23<4:42:51,  3.04s/it]                                                    {'loss': 2.8015, 'grad_norm': 0.484718918800354, 'learning_rate': 4.738983050847458e-05, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [19:23<4:42:51,  3.04s/it]  7%|â–‹         | 409/6000 [19:25<4:35:15,  2.95s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.43392789363861084, 'learning_rate': 4.7381355932203395e-05, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [19:25<4:35:15,  2.95s/it]  7%|â–‹         | 410/6000 [19:28<4:27:34,  2.87s/it]                                                    {'loss': 2.773, 'grad_norm': 0.3561064600944519, 'learning_rate': 4.7372881355932206e-05, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [19:28<4:27:34,  2.87s/it]  7%|â–‹         | 411/6000 [19:32<4:43:58,  3.05s/it]                                                    {'loss': 2.7739, 'grad_norm': 0.6005209684371948, 'learning_rate': 4.736440677966102e-05, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [19:32<4:43:58,  3.05s/it]  7%|â–‹         | 412/6000 [19:34<4:32:47,  2.93s/it]                                                    {'loss': 2.8213, 'grad_norm': 0.3164457082748413, 'learning_rate': 4.735593220338983e-05, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [19:34<4:32:47,  2.93s/it]  7%|â–‹         | 413/6000 [19:37<4:31:32,  2.92s/it]                                                    {'loss': 2.7915, 'grad_norm': 0.42856165766716003, 'learning_rate': 4.7347457627118646e-05, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [19:37<4:31:32,  2.92s/it]  7%|â–‹         | 414/6000 [19:40<4:26:26,  2.86s/it]                                                    {'loss': 2.7921, 'grad_norm': 0.47879356145858765, 'learning_rate': 4.733898305084746e-05, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [19:40<4:26:26,  2.86s/it]  7%|â–‹         | 415/6000 [19:43<4:27:56,  2.88s/it]                                                    {'loss': 2.774, 'grad_norm': 0.42523810267448425, 'learning_rate': 4.7330508474576276e-05, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [19:43<4:27:56,  2.88s/it]  7%|â–‹         | 416/6000 [19:45<4:24:45,  2.84s/it]                                                    {'loss': 2.8042, 'grad_norm': 0.49372589588165283, 'learning_rate': 4.732203389830509e-05, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [19:46<4:24:45,  2.84s/it]  7%|â–‹         | 417/6000 [19:48<4:25:32,  2.85s/it]                                                    {'loss': 2.846, 'grad_norm': 0.44726526737213135, 'learning_rate': 4.73135593220339e-05, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [19:48<4:25:32,  2.85s/it]  7%|â–‹         | 418/6000 [19:51<4:21:56,  2.82s/it]                                                    {'loss': 2.7773, 'grad_norm': 0.4139588177204132, 'learning_rate': 4.730508474576271e-05, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [19:51<4:21:56,  2.82s/it]  7%|â–‹         | 419/6000 [19:54<4:33:00,  2.94s/it]                                                    {'loss': 2.7922, 'grad_norm': 0.45942628383636475, 'learning_rate': 4.729661016949153e-05, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [19:54<4:33:00,  2.94s/it]  7%|â–‹         | 420/6000 [19:57<4:27:42,  2.88s/it]                                                    {'loss': 2.8034, 'grad_norm': 0.4007694721221924, 'learning_rate': 4.728813559322034e-05, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [19:57<4:27:42,  2.88s/it]  7%|â–‹         | 421/6000 [20:00<4:23:38,  2.84s/it]                                                    {'loss': 2.782, 'grad_norm': 0.42265811562538147, 'learning_rate': 4.727966101694916e-05, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [20:00<4:23:38,  2.84s/it]  7%|â–‹         | 422/6000 [20:03<4:20:12,  2.80s/it]                                                    {'loss': 2.7971, 'grad_norm': 0.4353998005390167, 'learning_rate': 4.727118644067797e-05, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [20:03<4:20:12,  2.80s/it]  7%|â–‹         | 423/6000 [20:05<4:17:27,  2.77s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.39514538645744324, 'learning_rate': 4.7262711864406786e-05, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [20:05<4:17:27,  2.77s/it]  7%|â–‹         | 424/6000 [20:08<4:15:45,  2.75s/it]                                                    {'loss': 2.7734, 'grad_norm': 0.4011397659778595, 'learning_rate': 4.72542372881356e-05, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [20:08<4:15:45,  2.75s/it]  7%|â–‹         | 425/6000 [20:11<4:14:41,  2.74s/it]                                                    {'loss': 2.7864, 'grad_norm': 0.5176583528518677, 'learning_rate': 4.724576271186441e-05, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [20:11<4:14:41,  2.74s/it]  7%|â–‹         | 426/6000 [20:13<4:14:56,  2.74s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.37302955985069275, 'learning_rate': 4.723728813559322e-05, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [20:13<4:14:56,  2.74s/it]  7%|â–‹         | 427/6000 [20:16<4:15:18,  2.75s/it]                                                    {'loss': 2.7919, 'grad_norm': 0.4118068516254425, 'learning_rate': 4.722881355932204e-05, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [20:16<4:15:18,  2.75s/it]  7%|â–‹         | 428/6000 [20:19<4:14:46,  2.74s/it]                                                    {'loss': 2.7879, 'grad_norm': 0.3876892626285553, 'learning_rate': 4.722033898305085e-05, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [20:19<4:14:46,  2.74s/it]  7%|â–‹         | 429/6000 [20:22<4:13:04,  2.73s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.3654484748840332, 'learning_rate': 4.721186440677967e-05, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [20:22<4:13:04,  2.73s/it]  7%|â–‹         | 430/6000 [20:24<4:13:17,  2.73s/it]                                                    {'loss': 2.823, 'grad_norm': 0.3743434250354767, 'learning_rate': 4.720338983050848e-05, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [20:24<4:13:17,  2.73s/it]  7%|â–‹         | 431/6000 [20:27<4:12:20,  2.72s/it]                                                    {'loss': 2.7772, 'grad_norm': 0.40227600932121277, 'learning_rate': 4.719491525423729e-05, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [20:27<4:12:20,  2.72s/it]  7%|â–‹         | 432/6000 [20:30<4:10:29,  2.70s/it]                                                    {'loss': 2.789, 'grad_norm': 0.42285147309303284, 'learning_rate': 4.71864406779661e-05, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [20:30<4:10:29,  2.70s/it]  7%|â–‹         | 433/6000 [20:32<4:09:26,  2.69s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.37541210651397705, 'learning_rate': 4.717796610169491e-05, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [20:32<4:09:26,  2.69s/it]  7%|â–‹         | 434/6000 [20:35<4:10:31,  2.70s/it]                                                    {'loss': 2.7669, 'grad_norm': 0.429097443819046, 'learning_rate': 4.716949152542373e-05, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [20:35<4:10:31,  2.70s/it]  7%|â–‹         | 435/6000 [20:38<4:14:28,  2.74s/it]                                                    {'loss': 2.7721, 'grad_norm': 0.41607654094696045, 'learning_rate': 4.716101694915254e-05, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [20:38<4:14:28,  2.74s/it]  7%|â–‹         | 436/6000 [20:41<4:15:24,  2.75s/it]                                                    {'loss': 2.8249, 'grad_norm': 0.44552716612815857, 'learning_rate': 4.715254237288136e-05, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [20:41<4:15:24,  2.75s/it]  7%|â–‹         | 437/6000 [20:43<4:17:19,  2.78s/it]                                                    {'loss': 2.7953, 'grad_norm': 0.4953634440898895, 'learning_rate': 4.714406779661017e-05, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [20:43<4:17:19,  2.78s/it]  7%|â–‹         | 438/6000 [20:46<4:19:22,  2.80s/it]                                                    {'loss': 2.7799, 'grad_norm': 0.4661438763141632, 'learning_rate': 4.713559322033898e-05, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [20:46<4:19:22,  2.80s/it]  7%|â–‹         | 439/6000 [20:49<4:16:17,  2.77s/it]                                                    {'loss': 2.8064, 'grad_norm': 0.34013718366622925, 'learning_rate': 4.7127118644067794e-05, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [20:49<4:16:17,  2.77s/it]  7%|â–‹         | 440/6000 [20:52<4:15:40,  2.76s/it]                                                    {'loss': 2.781, 'grad_norm': 0.4663093090057373, 'learning_rate': 4.711864406779661e-05, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [20:52<4:15:40,  2.76s/it]  7%|â–‹         | 441/6000 [20:55<4:23:00,  2.84s/it]                                                    {'loss': 2.7854, 'grad_norm': 0.40910741686820984, 'learning_rate': 4.7110169491525423e-05, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [20:55<4:23:00,  2.84s/it]  7%|â–‹         | 442/6000 [20:58<4:29:33,  2.91s/it]                                                    {'loss': 2.7767, 'grad_norm': 0.81597501039505, 'learning_rate': 4.710169491525424e-05, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [20:58<4:29:33,  2.91s/it]  7%|â–‹         | 443/6000 [21:01<4:26:15,  2.87s/it]                                                    {'loss': 2.8218, 'grad_norm': 0.5172448754310608, 'learning_rate': 4.709322033898305e-05, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [21:01<4:26:15,  2.87s/it]  7%|â–‹         | 444/6000 [21:03<4:21:23,  2.82s/it]                                                    {'loss': 2.8231, 'grad_norm': 0.37608179450035095, 'learning_rate': 4.708474576271187e-05, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [21:03<4:21:23,  2.82s/it]  7%|â–‹         | 445/6000 [21:06<4:21:52,  2.83s/it]                                                    {'loss': 2.7759, 'grad_norm': 0.5067852735519409, 'learning_rate': 4.707627118644068e-05, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [21:06<4:21:52,  2.83s/it]  7%|â–‹         | 446/6000 [21:09<4:19:09,  2.80s/it]                                                    {'loss': 2.7785, 'grad_norm': 0.4326438903808594, 'learning_rate': 4.7067796610169493e-05, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [21:09<4:19:09,  2.80s/it]  7%|â–‹         | 447/6000 [21:12<4:17:59,  2.79s/it]                                                    {'loss': 2.7754, 'grad_norm': 0.4490906298160553, 'learning_rate': 4.7059322033898305e-05, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [21:12<4:17:59,  2.79s/it]  7%|â–‹         | 448/6000 [21:14<4:16:09,  2.77s/it]                                                    {'loss': 2.7795, 'grad_norm': 0.3386135995388031, 'learning_rate': 4.705084745762712e-05, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [21:14<4:16:09,  2.77s/it]  7%|â–‹         | 449/6000 [21:17<4:14:07,  2.75s/it]                                                    {'loss': 2.7745, 'grad_norm': 0.36062684655189514, 'learning_rate': 4.7042372881355934e-05, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [21:17<4:14:07,  2.75s/it]  8%|â–Š         | 450/6000 [21:20<4:14:05,  2.75s/it]                                                    {'loss': 2.7736, 'grad_norm': 0.42345213890075684, 'learning_rate': 4.703389830508475e-05, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [21:20<4:14:05,  2.75s/it][2025-10-22 19:17:45,376] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 451/6000 [21:25<5:13:47,  3.39s/it]                                                    {'loss': 2.7811, 'grad_norm': 0.6984336376190186, 'learning_rate': 4.702542372881356e-05, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [21:25<5:13:47,  3.39s/it]  8%|â–Š         | 452/6000 [21:28<5:04:30,  3.29s/it]                                                    {'loss': 2.7935, 'grad_norm': 0.4349793493747711, 'learning_rate': 4.7016949152542375e-05, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [21:28<5:04:30,  3.29s/it]  8%|â–Š         | 453/6000 [21:30<4:47:02,  3.10s/it]                                                    {'loss': 2.7745, 'grad_norm': 0.4269687235355377, 'learning_rate': 4.7008474576271186e-05, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [21:30<4:47:02,  3.10s/it]  8%|â–Š         | 454/6000 [21:33<4:36:35,  2.99s/it]                                                    {'loss': 2.8411, 'grad_norm': 0.5574510097503662, 'learning_rate': 4.7e-05, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [21:33<4:36:35,  2.99s/it]  8%|â–Š         | 455/6000 [21:36<4:27:27,  2.89s/it]                                                    {'loss': 2.7735, 'grad_norm': 0.5440147519111633, 'learning_rate': 4.6991525423728815e-05, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [21:36<4:27:27,  2.89s/it]  8%|â–Š         | 456/6000 [21:39<4:21:46,  2.83s/it]                                                    {'loss': 2.7738, 'grad_norm': 0.3474463224411011, 'learning_rate': 4.6983050847457627e-05, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [21:39<4:21:46,  2.83s/it]  8%|â–Š         | 457/6000 [21:41<4:16:45,  2.78s/it]                                                    {'loss': 2.805, 'grad_norm': 0.31145140528678894, 'learning_rate': 4.6974576271186445e-05, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [21:41<4:16:45,  2.78s/it]  8%|â–Š         | 458/6000 [21:44<4:16:07,  2.77s/it]                                                    {'loss': 2.8108, 'grad_norm': 0.5167335867881775, 'learning_rate': 4.6966101694915256e-05, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [21:44<4:16:07,  2.77s/it]  8%|â–Š         | 459/6000 [21:47<4:15:56,  2.77s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.48452121019363403, 'learning_rate': 4.6957627118644074e-05, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [21:47<4:15:56,  2.77s/it]  8%|â–Š         | 460/6000 [21:49<4:14:08,  2.75s/it]                                                    {'loss': 2.783, 'grad_norm': 0.4884202778339386, 'learning_rate': 4.694915254237288e-05, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [21:49<4:14:08,  2.75s/it]  8%|â–Š         | 461/6000 [21:52<4:13:46,  2.75s/it]                                                    {'loss': 2.8002, 'grad_norm': 0.42541494965553284, 'learning_rate': 4.6940677966101697e-05, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [21:52<4:13:46,  2.75s/it]  8%|â–Š         | 462/6000 [21:55<4:10:34,  2.71s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.42010608315467834, 'learning_rate': 4.693220338983051e-05, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [21:55<4:10:34,  2.71s/it]  8%|â–Š         | 463/6000 [21:58<4:13:17,  2.74s/it]                                                    {'loss': 2.7756, 'grad_norm': 0.3455643355846405, 'learning_rate': 4.6923728813559326e-05, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [21:58<4:13:17,  2.74s/it]  8%|â–Š         | 464/6000 [22:00<4:12:25,  2.74s/it]                                                    {'loss': 2.7776, 'grad_norm': 0.42812275886535645, 'learning_rate': 4.691525423728814e-05, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [22:00<4:12:25,  2.74s/it]  8%|â–Š         | 465/6000 [22:03<4:16:32,  2.78s/it]                                                    {'loss': 2.7708, 'grad_norm': 0.5325865149497986, 'learning_rate': 4.6906779661016955e-05, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [22:03<4:16:32,  2.78s/it]  8%|â–Š         | 466/6000 [22:06<4:16:07,  2.78s/it]                                                    {'loss': 2.7814, 'grad_norm': 0.5482026934623718, 'learning_rate': 4.6898305084745767e-05, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [22:06<4:16:07,  2.78s/it]  8%|â–Š         | 467/6000 [22:09<4:15:37,  2.77s/it]                                                    {'loss': 2.7926, 'grad_norm': 0.36093568801879883, 'learning_rate': 4.688983050847458e-05, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [22:09<4:15:37,  2.77s/it]  8%|â–Š         | 468/6000 [22:12<4:16:15,  2.78s/it]                                                    {'loss': 2.7741, 'grad_norm': 0.4724234342575073, 'learning_rate': 4.688135593220339e-05, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [22:12<4:16:15,  2.78s/it]  8%|â–Š         | 469/6000 [22:14<4:12:32,  2.74s/it]                                                    {'loss': 2.7738, 'grad_norm': 0.49850642681121826, 'learning_rate': 4.687288135593221e-05, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [22:14<4:12:32,  2.74s/it]  8%|â–Š         | 470/6000 [22:17<4:15:59,  2.78s/it]                                                    {'loss': 2.772, 'grad_norm': 0.47064661979675293, 'learning_rate': 4.686440677966102e-05, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [22:17<4:15:59,  2.78s/it]  8%|â–Š         | 471/6000 [22:20<4:11:00,  2.72s/it]                                                    {'loss': 2.8516, 'grad_norm': 8.947369575500488, 'learning_rate': 4.6855932203389837e-05, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [22:20<4:11:00,  2.72s/it]  8%|â–Š         | 472/6000 [22:22<4:12:09,  2.74s/it]                                                    {'loss': 2.8062, 'grad_norm': 0.5311599373817444, 'learning_rate': 4.684745762711865e-05, 'epoch': 0.08}
  8%|â–Š         | 472/6000 [22:22<4:12:09,  2.74s/it]  8%|â–Š         | 473/6000 [22:25<4:11:41,  2.73s/it]                                                    {'loss': 2.7791, 'grad_norm': 0.481962651014328, 'learning_rate': 4.6838983050847466e-05, 'epoch': 0.08}
  8%|â–Š         | 473/6000 [22:25<4:11:41,  2.73s/it]  8%|â–Š         | 474/6000 [22:28<4:10:28,  2.72s/it]                                                    {'loss': 2.7797, 'grad_norm': 0.4071873426437378, 'learning_rate': 4.683050847457627e-05, 'epoch': 0.08}
  8%|â–Š         | 474/6000 [22:28<4:10:28,  2.72s/it]  8%|â–Š         | 475/6000 [22:31<4:11:29,  2.73s/it]                                                    {'loss': 2.7795, 'grad_norm': 0.4452604353427887, 'learning_rate': 4.682203389830508e-05, 'epoch': 0.08}
  8%|â–Š         | 475/6000 [22:31<4:11:29,  2.73s/it]  8%|â–Š         | 476/6000 [22:33<4:09:46,  2.71s/it]                                                    {'loss': 2.7933, 'grad_norm': 0.4214404821395874, 'learning_rate': 4.68135593220339e-05, 'epoch': 0.08}
  8%|â–Š         | 476/6000 [22:33<4:09:46,  2.71s/it]  8%|â–Š         | 477/6000 [22:36<4:08:50,  2.70s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.3948533833026886, 'learning_rate': 4.680508474576271e-05, 'epoch': 0.08}
  8%|â–Š         | 477/6000 [22:36<4:08:50,  2.70s/it]  8%|â–Š         | 478/6000 [22:39<4:09:08,  2.71s/it]                                                    {'loss': 2.777, 'grad_norm': 0.3903367817401886, 'learning_rate': 4.679661016949153e-05, 'epoch': 0.08}
  8%|â–Š         | 478/6000 [22:39<4:09:08,  2.71s/it]  8%|â–Š         | 479/6000 [22:41<4:10:02,  2.72s/it]                                                    {'loss': 2.7935, 'grad_norm': 0.4482525885105133, 'learning_rate': 4.678813559322034e-05, 'epoch': 0.08}
  8%|â–Š         | 479/6000 [22:41<4:10:02,  2.72s/it]  8%|â–Š         | 480/6000 [22:44<4:09:12,  2.71s/it]                                                    {'loss': 2.7756, 'grad_norm': 0.3740628659725189, 'learning_rate': 4.677966101694916e-05, 'epoch': 0.08}
  8%|â–Š         | 480/6000 [22:44<4:09:12,  2.71s/it]  8%|â–Š         | 481/6000 [22:47<4:12:48,  2.75s/it]                                                    {'loss': 2.7688, 'grad_norm': 0.3864583373069763, 'learning_rate': 4.677118644067797e-05, 'epoch': 0.08}
  8%|â–Š         | 481/6000 [22:47<4:12:48,  2.75s/it]  8%|â–Š         | 482/6000 [22:50<4:12:53,  2.75s/it]                                                    {'loss': 2.7744, 'grad_norm': 0.42805731296539307, 'learning_rate': 4.676271186440678e-05, 'epoch': 0.08}
  8%|â–Š         | 482/6000 [22:50<4:12:53,  2.75s/it]  8%|â–Š         | 483/6000 [22:53<4:14:06,  2.76s/it]                                                    {'loss': 2.7891, 'grad_norm': 0.4765404760837555, 'learning_rate': 4.675423728813559e-05, 'epoch': 0.08}
  8%|â–Š         | 483/6000 [22:53<4:14:06,  2.76s/it]  8%|â–Š         | 484/6000 [22:55<4:17:48,  2.80s/it]                                                    {'loss': 2.7811, 'grad_norm': 0.3668734133243561, 'learning_rate': 4.674576271186441e-05, 'epoch': 0.08}
  8%|â–Š         | 484/6000 [22:55<4:17:48,  2.80s/it]  8%|â–Š         | 485/6000 [22:59<4:28:50,  2.92s/it]                                                    {'loss': 2.7673, 'grad_norm': 0.5029850006103516, 'learning_rate': 4.673728813559322e-05, 'epoch': 0.08}
  8%|â–Š         | 485/6000 [22:59<4:28:50,  2.92s/it]  8%|â–Š         | 486/6000 [23:01<4:22:17,  2.85s/it]                                                    {'loss': 2.7949, 'grad_norm': 0.428072452545166, 'learning_rate': 4.672881355932204e-05, 'epoch': 0.08}
  8%|â–Š         | 486/6000 [23:01<4:22:17,  2.85s/it]  8%|â–Š         | 487/6000 [23:04<4:22:18,  2.85s/it]                                                    {'loss': 2.7941, 'grad_norm': 0.4133588969707489, 'learning_rate': 4.672033898305085e-05, 'epoch': 0.08}
  8%|â–Š         | 487/6000 [23:04<4:22:18,  2.85s/it]  8%|â–Š         | 488/6000 [23:07<4:23:50,  2.87s/it]                                                    {'loss': 2.7756, 'grad_norm': 0.5938869118690491, 'learning_rate': 4.671186440677966e-05, 'epoch': 0.08}
  8%|â–Š         | 488/6000 [23:07<4:23:50,  2.87s/it]  8%|â–Š         | 489/6000 [23:10<4:22:25,  2.86s/it]                                                    {'loss': 2.7718, 'grad_norm': 0.47910910844802856, 'learning_rate': 4.6703389830508474e-05, 'epoch': 0.08}
  8%|â–Š         | 489/6000 [23:10<4:22:25,  2.86s/it]  8%|â–Š         | 490/6000 [23:13<4:20:05,  2.83s/it]                                                    {'loss': 2.8198, 'grad_norm': 0.3784016966819763, 'learning_rate': 4.669491525423729e-05, 'epoch': 0.08}
  8%|â–Š         | 490/6000 [23:13<4:20:05,  2.83s/it]  8%|â–Š         | 491/6000 [23:16<4:20:31,  2.84s/it]                                                    {'loss': 2.7755, 'grad_norm': 0.49614447355270386, 'learning_rate': 4.66864406779661e-05, 'epoch': 0.08}
  8%|â–Š         | 491/6000 [23:16<4:20:31,  2.84s/it]  8%|â–Š         | 492/6000 [23:18<4:16:56,  2.80s/it]                                                    {'loss': 2.8029, 'grad_norm': 0.44979265332221985, 'learning_rate': 4.667796610169492e-05, 'epoch': 0.08}
  8%|â–Š         | 492/6000 [23:18<4:16:56,  2.80s/it]  8%|â–Š         | 493/6000 [23:21<4:18:56,  2.82s/it]                                                    {'loss': 2.7992, 'grad_norm': 0.5706362128257751, 'learning_rate': 4.666949152542373e-05, 'epoch': 0.08}
  8%|â–Š         | 493/6000 [23:21<4:18:56,  2.82s/it]  8%|â–Š         | 494/6000 [23:24<4:15:33,  2.78s/it]                                                    {'loss': 2.7768, 'grad_norm': 0.4394153952598572, 'learning_rate': 4.666101694915255e-05, 'epoch': 0.08}
  8%|â–Š         | 494/6000 [23:24<4:15:33,  2.78s/it]  8%|â–Š         | 495/6000 [23:27<4:16:58,  2.80s/it]                                                    {'loss': 2.7814, 'grad_norm': 0.37627649307250977, 'learning_rate': 4.6652542372881355e-05, 'epoch': 0.08}
  8%|â–Š         | 495/6000 [23:27<4:16:58,  2.80s/it]  8%|â–Š         | 496/6000 [23:29<4:15:03,  2.78s/it]                                                    {'loss': 2.7763, 'grad_norm': 0.36507943272590637, 'learning_rate': 4.6644067796610166e-05, 'epoch': 0.08}
  8%|â–Š         | 496/6000 [23:29<4:15:03,  2.78s/it]  8%|â–Š         | 497/6000 [23:32<4:11:54,  2.75s/it]                                                    {'loss': 2.7819, 'grad_norm': 0.3845108151435852, 'learning_rate': 4.6635593220338984e-05, 'epoch': 0.08}
  8%|â–Š         | 497/6000 [23:32<4:11:54,  2.75s/it]  8%|â–Š         | 498/6000 [23:35<4:10:53,  2.74s/it]                                                    {'loss': 2.7715, 'grad_norm': 0.44920462369918823, 'learning_rate': 4.6627118644067795e-05, 'epoch': 0.08}
  8%|â–Š         | 498/6000 [23:35<4:10:53,  2.74s/it]  8%|â–Š         | 499/6000 [23:37<4:10:33,  2.73s/it]                                                    {'loss': 2.7745, 'grad_norm': 0.4504326283931732, 'learning_rate': 4.6618644067796614e-05, 'epoch': 0.08}
  8%|â–Š         | 499/6000 [23:37<4:10:33,  2.73s/it]  8%|â–Š         | 500/6000 [23:40<4:11:36,  2.74s/it]                                                    {'loss': 2.7712, 'grad_norm': 0.3703559339046478, 'learning_rate': 4.6610169491525425e-05, 'epoch': 0.08}
  8%|â–Š         | 500/6000 [23:40<4:11:36,  2.74s/it][2025-10-22 19:20:05,760] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 501/6000 [23:45<5:12:10,  3.41s/it]                                                    {'loss': 2.8033, 'grad_norm': 0.3444420099258423, 'learning_rate': 4.660169491525424e-05, 'epoch': 0.08}
  8%|â–Š         | 501/6000 [23:45<5:12:10,  3.41s/it]  8%|â–Š         | 502/6000 [23:48<4:59:50,  3.27s/it]                                                    {'loss': 2.7743, 'grad_norm': 0.3338588774204254, 'learning_rate': 4.6593220338983054e-05, 'epoch': 0.08}
  8%|â–Š         | 502/6000 [23:48<4:59:50,  3.27s/it]  8%|â–Š         | 503/6000 [23:51<4:44:33,  3.11s/it]                                                    {'loss': 2.8216, 'grad_norm': 0.39156731963157654, 'learning_rate': 4.6584745762711865e-05, 'epoch': 0.08}
  8%|â–Š         | 503/6000 [23:51<4:44:33,  3.11s/it]  8%|â–Š         | 504/6000 [23:54<4:32:27,  2.97s/it]                                                    {'loss': 2.7757, 'grad_norm': 0.4143983721733093, 'learning_rate': 4.657627118644068e-05, 'epoch': 0.08}
  8%|â–Š         | 504/6000 [23:54<4:32:27,  2.97s/it]  8%|â–Š         | 505/6000 [23:56<4:25:49,  2.90s/it]                                                    {'loss': 2.8223, 'grad_norm': 0.37956464290618896, 'learning_rate': 4.6567796610169495e-05, 'epoch': 0.08}
  8%|â–Š         | 505/6000 [23:56<4:25:49,  2.90s/it]  8%|â–Š         | 506/6000 [23:59<4:31:47,  2.97s/it]                                                    {'loss': 2.7752, 'grad_norm': 0.392984539270401, 'learning_rate': 4.6559322033898306e-05, 'epoch': 0.08}
  8%|â–Š         | 506/6000 [23:59<4:31:47,  2.97s/it]  8%|â–Š         | 507/6000 [24:02<4:27:01,  2.92s/it]                                                    {'loss': 2.8048, 'grad_norm': 0.34062308073043823, 'learning_rate': 4.6550847457627124e-05, 'epoch': 0.08}
  8%|â–Š         | 507/6000 [24:02<4:27:01,  2.92s/it]  8%|â–Š         | 508/6000 [24:05<4:21:29,  2.86s/it]                                                    {'loss': 2.7923, 'grad_norm': 0.3791210651397705, 'learning_rate': 4.6542372881355935e-05, 'epoch': 0.08}
  8%|â–Š         | 508/6000 [24:05<4:21:29,  2.86s/it]  8%|â–Š         | 509/6000 [24:08<4:16:00,  2.80s/it]                                                    {'loss': 2.7911, 'grad_norm': 0.3630746304988861, 'learning_rate': 4.653389830508475e-05, 'epoch': 0.08}
  8%|â–Š         | 509/6000 [24:08<4:16:00,  2.80s/it]  8%|â–Š         | 510/6000 [24:10<4:15:05,  2.79s/it]                                                    {'loss': 2.7735, 'grad_norm': 0.5004041790962219, 'learning_rate': 4.652542372881356e-05, 'epoch': 0.09}
  8%|â–Š         | 510/6000 [24:10<4:15:05,  2.79s/it]  9%|â–Š         | 511/6000 [24:13<4:14:38,  2.78s/it]                                                    {'loss': 2.7719, 'grad_norm': 0.39435675740242004, 'learning_rate': 4.6516949152542376e-05, 'epoch': 0.09}
  9%|â–Š         | 511/6000 [24:13<4:14:38,  2.78s/it]  9%|â–Š         | 512/6000 [24:16<4:13:02,  2.77s/it]                                                    {'loss': 2.7775, 'grad_norm': 0.5233387351036072, 'learning_rate': 4.650847457627119e-05, 'epoch': 0.09}
  9%|â–Š         | 512/6000 [24:16<4:13:02,  2.77s/it]  9%|â–Š         | 513/6000 [24:19<4:18:14,  2.82s/it]                                                    {'loss': 2.7744, 'grad_norm': 0.5052180290222168, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.09}
  9%|â–Š         | 513/6000 [24:19<4:18:14,  2.82s/it]  9%|â–Š         | 514/6000 [24:22<4:17:17,  2.81s/it]                                                    {'loss': 2.7931, 'grad_norm': 0.34757381677627563, 'learning_rate': 4.649152542372882e-05, 'epoch': 0.09}
  9%|â–Š         | 514/6000 [24:22<4:17:17,  2.81s/it]  9%|â–Š         | 515/6000 [24:24<4:15:12,  2.79s/it]                                                    {'loss': 2.7705, 'grad_norm': 0.3090505301952362, 'learning_rate': 4.6483050847457635e-05, 'epoch': 0.09}
  9%|â–Š         | 515/6000 [24:24<4:15:12,  2.79s/it]  9%|â–Š         | 516/6000 [24:27<4:21:27,  2.86s/it]                                                    {'loss': 2.7726, 'grad_norm': 0.3605658710002899, 'learning_rate': 4.6474576271186446e-05, 'epoch': 0.09}
  9%|â–Š         | 516/6000 [24:27<4:21:27,  2.86s/it]  9%|â–Š         | 517/6000 [24:30<4:19:20,  2.84s/it]                                                    {'loss': 2.7814, 'grad_norm': 0.3376607596874237, 'learning_rate': 4.646610169491525e-05, 'epoch': 0.09}
  9%|â–Š         | 517/6000 [24:30<4:19:20,  2.84s/it]  9%|â–Š         | 518/6000 [24:33<4:15:47,  2.80s/it]                                                    {'loss': 2.775, 'grad_norm': 0.39021390676498413, 'learning_rate': 4.645762711864407e-05, 'epoch': 0.09}
  9%|â–Š         | 518/6000 [24:33<4:15:47,  2.80s/it]  9%|â–Š         | 519/6000 [24:36<4:13:27,  2.77s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.33096688985824585, 'learning_rate': 4.644915254237288e-05, 'epoch': 0.09}
  9%|â–Š         | 519/6000 [24:36<4:13:27,  2.77s/it]  9%|â–Š         | 520/6000 [24:38<4:17:27,  2.82s/it]                                                    {'loss': 2.7742, 'grad_norm': 0.3523556888103485, 'learning_rate': 4.64406779661017e-05, 'epoch': 0.09}
  9%|â–Š         | 520/6000 [24:38<4:17:27,  2.82s/it]  9%|â–Š         | 521/6000 [24:41<4:13:20,  2.77s/it]                                                    {'loss': 2.7773, 'grad_norm': 0.31923922896385193, 'learning_rate': 4.643220338983051e-05, 'epoch': 0.09}
  9%|â–Š         | 521/6000 [24:41<4:13:20,  2.77s/it]  9%|â–Š         | 522/6000 [24:44<4:12:56,  2.77s/it]                                                    {'loss': 2.7769, 'grad_norm': 0.4388749897480011, 'learning_rate': 4.642372881355933e-05, 'epoch': 0.09}
  9%|â–Š         | 522/6000 [24:44<4:12:56,  2.77s/it]  9%|â–Š         | 523/6000 [24:47<4:13:32,  2.78s/it]                                                    {'loss': 2.7903, 'grad_norm': 0.3426324725151062, 'learning_rate': 4.641525423728814e-05, 'epoch': 0.09}
  9%|â–Š         | 523/6000 [24:47<4:13:32,  2.78s/it]  9%|â–Š         | 524/6000 [24:49<4:09:40,  2.74s/it]                                                    {'loss': 2.8454, 'grad_norm': 0.40942853689193726, 'learning_rate': 4.640677966101695e-05, 'epoch': 0.09}
  9%|â–Š         | 524/6000 [24:49<4:09:40,  2.74s/it]  9%|â–‰         | 525/6000 [24:52<4:08:51,  2.73s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.2731854021549225, 'learning_rate': 4.639830508474576e-05, 'epoch': 0.09}
  9%|â–‰         | 525/6000 [24:52<4:08:51,  2.73s/it]  9%|â–‰         | 526/6000 [24:55<4:08:48,  2.73s/it]                                                    {'loss': 2.8192, 'grad_norm': 0.4118046462535858, 'learning_rate': 4.638983050847458e-05, 'epoch': 0.09}
  9%|â–‰         | 526/6000 [24:55<4:08:48,  2.73s/it]  9%|â–‰         | 527/6000 [24:58<4:10:35,  2.75s/it]                                                    {'loss': 2.7818, 'grad_norm': 0.3053891360759735, 'learning_rate': 4.638135593220339e-05, 'epoch': 0.09}
  9%|â–‰         | 527/6000 [24:58<4:10:35,  2.75s/it]  9%|â–‰         | 528/6000 [25:00<4:13:38,  2.78s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.3302074372768402, 'learning_rate': 4.637288135593221e-05, 'epoch': 0.09}
  9%|â–‰         | 528/6000 [25:00<4:13:38,  2.78s/it]  9%|â–‰         | 529/6000 [25:03<4:11:52,  2.76s/it]                                                    {'loss': 2.7712, 'grad_norm': 0.3295086622238159, 'learning_rate': 4.636440677966102e-05, 'epoch': 0.09}
  9%|â–‰         | 529/6000 [25:03<4:11:52,  2.76s/it]  9%|â–‰         | 530/6000 [25:06<4:12:33,  2.77s/it]                                                    {'loss': 2.7937, 'grad_norm': 0.43462660908699036, 'learning_rate': 4.635593220338984e-05, 'epoch': 0.09}
  9%|â–‰         | 530/6000 [25:06<4:12:33,  2.77s/it]  9%|â–‰         | 531/6000 [25:09<4:10:42,  2.75s/it]                                                    {'loss': 2.7771, 'grad_norm': 0.32165461778640747, 'learning_rate': 4.634745762711864e-05, 'epoch': 0.09}
  9%|â–‰         | 531/6000 [25:09<4:10:42,  2.75s/it]  9%|â–‰         | 532/6000 [25:11<4:10:04,  2.74s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.40236377716064453, 'learning_rate': 4.633898305084746e-05, 'epoch': 0.09}
  9%|â–‰         | 532/6000 [25:11<4:10:04,  2.74s/it]  9%|â–‰         | 533/6000 [25:14<4:08:24,  2.73s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.3753599524497986, 'learning_rate': 4.633050847457627e-05, 'epoch': 0.09}
  9%|â–‰         | 533/6000 [25:14<4:08:24,  2.73s/it]  9%|â–‰         | 534/6000 [25:17<4:09:31,  2.74s/it]                                                    {'loss': 2.7912, 'grad_norm': 0.30084314942359924, 'learning_rate': 4.632203389830509e-05, 'epoch': 0.09}
  9%|â–‰         | 534/6000 [25:17<4:09:31,  2.74s/it]  9%|â–‰         | 535/6000 [25:20<4:09:44,  2.74s/it]                                                    {'loss': 2.8197, 'grad_norm': 0.3250683546066284, 'learning_rate': 4.63135593220339e-05, 'epoch': 0.09}
  9%|â–‰         | 535/6000 [25:20<4:09:44,  2.74s/it]  9%|â–‰         | 536/6000 [25:22<4:10:01,  2.75s/it]                                                    {'loss': 2.7743, 'grad_norm': 0.36726248264312744, 'learning_rate': 4.630508474576272e-05, 'epoch': 0.09}
  9%|â–‰         | 536/6000 [25:22<4:10:01,  2.75s/it]  9%|â–‰         | 537/6000 [25:25<4:07:57,  2.72s/it]                                                    {'loss': 2.7889, 'grad_norm': 0.4174099266529083, 'learning_rate': 4.629661016949153e-05, 'epoch': 0.09}
  9%|â–‰         | 537/6000 [25:25<4:07:57,  2.72s/it]  9%|â–‰         | 538/6000 [25:28<4:07:14,  2.72s/it]                                                    {'loss': 2.7888, 'grad_norm': 0.2948492169380188, 'learning_rate': 4.628813559322034e-05, 'epoch': 0.09}
  9%|â–‰         | 538/6000 [25:28<4:07:14,  2.72s/it]  9%|â–‰         | 539/6000 [25:30<4:05:39,  2.70s/it]                                                    {'loss': 2.7715, 'grad_norm': 0.2785615622997284, 'learning_rate': 4.627966101694915e-05, 'epoch': 0.09}
  9%|â–‰         | 539/6000 [25:30<4:05:39,  2.70s/it]  9%|â–‰         | 540/6000 [25:33<4:04:24,  2.69s/it]                                                    {'loss': 2.7735, 'grad_norm': 0.297164648771286, 'learning_rate': 4.6271186440677964e-05, 'epoch': 0.09}
  9%|â–‰         | 540/6000 [25:33<4:04:24,  2.69s/it]  9%|â–‰         | 541/6000 [25:36<4:08:18,  2.73s/it]                                                    {'loss': 2.7726, 'grad_norm': 0.3713444769382477, 'learning_rate': 4.626271186440678e-05, 'epoch': 0.09}
  9%|â–‰         | 541/6000 [25:36<4:08:18,  2.73s/it]  9%|â–‰         | 542/6000 [25:39<4:08:23,  2.73s/it]                                                    {'loss': 2.7763, 'grad_norm': 0.34174609184265137, 'learning_rate': 4.6254237288135594e-05, 'epoch': 0.09}
  9%|â–‰         | 542/6000 [25:39<4:08:23,  2.73s/it]  9%|â–‰         | 543/6000 [25:41<4:07:03,  2.72s/it]                                                    {'loss': 2.7668, 'grad_norm': 0.41069257259368896, 'learning_rate': 4.624576271186441e-05, 'epoch': 0.09}
  9%|â–‰         | 543/6000 [25:41<4:07:03,  2.72s/it]  9%|â–‰         | 544/6000 [25:44<4:09:08,  2.74s/it]                                                    {'loss': 2.7893, 'grad_norm': 0.4271131455898285, 'learning_rate': 4.623728813559322e-05, 'epoch': 0.09}
  9%|â–‰         | 544/6000 [25:44<4:09:08,  2.74s/it]  9%|â–‰         | 545/6000 [25:47<4:12:24,  2.78s/it]                                                    {'loss': 2.7826, 'grad_norm': 0.5153446197509766, 'learning_rate': 4.6228813559322034e-05, 'epoch': 0.09}
  9%|â–‰         | 545/6000 [25:47<4:12:24,  2.78s/it]  9%|â–‰         | 546/6000 [25:50<4:12:31,  2.78s/it]                                                    {'loss': 2.7743, 'grad_norm': 0.34754350781440735, 'learning_rate': 4.6220338983050846e-05, 'epoch': 0.09}
  9%|â–‰         | 546/6000 [25:50<4:12:31,  2.78s/it]  9%|â–‰         | 547/6000 [25:53<4:15:27,  2.81s/it]                                                    {'loss': 2.8036, 'grad_norm': 0.5133436918258667, 'learning_rate': 4.6211864406779664e-05, 'epoch': 0.09}
  9%|â–‰         | 547/6000 [25:53<4:15:27,  2.81s/it]  9%|â–‰         | 548/6000 [25:55<4:13:25,  2.79s/it]                                                    {'loss': 2.7753, 'grad_norm': 0.3327963054180145, 'learning_rate': 4.6203389830508475e-05, 'epoch': 0.09}
  9%|â–‰         | 548/6000 [25:55<4:13:25,  2.79s/it]  9%|â–‰         | 549/6000 [25:58<4:09:25,  2.75s/it]                                                    {'loss': 2.7823, 'grad_norm': 0.3001070022583008, 'learning_rate': 4.619491525423729e-05, 'epoch': 0.09}
  9%|â–‰         | 549/6000 [25:58<4:09:25,  2.75s/it]  9%|â–‰         | 550/6000 [26:01<4:13:48,  2.79s/it]                                                    {'loss': 2.7762, 'grad_norm': 0.4170343279838562, 'learning_rate': 4.6186440677966104e-05, 'epoch': 0.09}
  9%|â–‰         | 550/6000 [26:01<4:13:48,  2.79s/it][2025-10-22 19:22:26,389] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 551/6000 [26:07<5:35:33,  3.69s/it]                                                    {'loss': 2.7839, 'grad_norm': 0.39179232716560364, 'learning_rate': 4.617796610169492e-05, 'epoch': 0.09}
  9%|â–‰         | 551/6000 [26:07<5:35:33,  3.69s/it]  9%|â–‰         | 552/6000 [26:09<5:08:42,  3.40s/it]                                                    {'loss': 2.7693, 'grad_norm': 0.3210676610469818, 'learning_rate': 4.6169491525423734e-05, 'epoch': 0.09}
  9%|â–‰         | 552/6000 [26:09<5:08:42,  3.40s/it]  9%|â–‰         | 553/6000 [26:12<4:54:01,  3.24s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.32642245292663574, 'learning_rate': 4.6161016949152545e-05, 'epoch': 0.09}
  9%|â–‰         | 553/6000 [26:12<4:54:01,  3.24s/it]  9%|â–‰         | 554/6000 [26:15<4:40:03,  3.09s/it]                                                    {'loss': 2.779, 'grad_norm': 0.396417498588562, 'learning_rate': 4.6152542372881356e-05, 'epoch': 0.09}
  9%|â–‰         | 554/6000 [26:15<4:40:03,  3.09s/it]  9%|â–‰         | 555/6000 [26:18<4:28:37,  2.96s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.3531123697757721, 'learning_rate': 4.6144067796610174e-05, 'epoch': 0.09}
  9%|â–‰         | 555/6000 [26:18<4:28:37,  2.96s/it]  9%|â–‰         | 556/6000 [26:20<4:20:18,  2.87s/it]                                                    {'loss': 2.7897, 'grad_norm': 0.40508151054382324, 'learning_rate': 4.6135593220338986e-05, 'epoch': 0.09}
  9%|â–‰         | 556/6000 [26:20<4:20:18,  2.87s/it]  9%|â–‰         | 557/6000 [26:23<4:17:16,  2.84s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.36302056908607483, 'learning_rate': 4.6127118644067804e-05, 'epoch': 0.09}
  9%|â–‰         | 557/6000 [26:23<4:17:16,  2.84s/it]  9%|â–‰         | 558/6000 [26:26<4:18:11,  2.85s/it]                                                    {'loss': 2.7761, 'grad_norm': 0.3823605179786682, 'learning_rate': 4.6118644067796615e-05, 'epoch': 0.09}
  9%|â–‰         | 558/6000 [26:26<4:18:11,  2.85s/it]  9%|â–‰         | 559/6000 [26:29<4:18:55,  2.86s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.3641565144062042, 'learning_rate': 4.6110169491525426e-05, 'epoch': 0.09}
  9%|â–‰         | 559/6000 [26:29<4:18:55,  2.86s/it]  9%|â–‰         | 560/6000 [26:31<4:14:02,  2.80s/it]                                                    {'loss': 2.7774, 'grad_norm': 0.4066115617752075, 'learning_rate': 4.610169491525424e-05, 'epoch': 0.09}
  9%|â–‰         | 560/6000 [26:31<4:14:02,  2.80s/it]  9%|â–‰         | 561/6000 [26:35<4:25:22,  2.93s/it]                                                    {'loss': 2.7731, 'grad_norm': 0.39243122935295105, 'learning_rate': 4.609322033898305e-05, 'epoch': 0.09}
  9%|â–‰         | 561/6000 [26:35<4:25:22,  2.93s/it]  9%|â–‰         | 562/6000 [26:37<4:17:29,  2.84s/it]                                                    {'loss': 2.8671, 'grad_norm': 0.44944044947624207, 'learning_rate': 4.608474576271187e-05, 'epoch': 0.09}
  9%|â–‰         | 562/6000 [26:37<4:17:29,  2.84s/it]  9%|â–‰         | 563/6000 [26:40<4:13:16,  2.80s/it]                                                    {'loss': 2.7722, 'grad_norm': 0.4377264082431793, 'learning_rate': 4.607627118644068e-05, 'epoch': 0.09}
  9%|â–‰         | 563/6000 [26:40<4:13:16,  2.80s/it]  9%|â–‰         | 564/6000 [26:43<4:12:53,  2.79s/it]                                                    {'loss': 2.818, 'grad_norm': 0.41504672169685364, 'learning_rate': 4.6067796610169496e-05, 'epoch': 0.09}
  9%|â–‰         | 564/6000 [26:43<4:12:53,  2.79s/it]  9%|â–‰         | 565/6000 [26:46<4:17:00,  2.84s/it]                                                    {'loss': 2.7709, 'grad_norm': 0.4114246070384979, 'learning_rate': 4.605932203389831e-05, 'epoch': 0.09}
  9%|â–‰         | 565/6000 [26:46<4:17:00,  2.84s/it]  9%|â–‰         | 566/6000 [26:48<4:12:27,  2.79s/it]                                                    {'loss': 2.7697, 'grad_norm': 0.3524303436279297, 'learning_rate': 4.605084745762712e-05, 'epoch': 0.09}
  9%|â–‰         | 566/6000 [26:48<4:12:27,  2.79s/it]  9%|â–‰         | 567/6000 [26:52<4:21:42,  2.89s/it]                                                    {'loss': 2.7885, 'grad_norm': 0.42143768072128296, 'learning_rate': 4.604237288135593e-05, 'epoch': 0.09}
  9%|â–‰         | 567/6000 [26:52<4:21:42,  2.89s/it]  9%|â–‰         | 568/6000 [26:54<4:18:36,  2.86s/it]                                                    {'loss': 2.8217, 'grad_norm': 0.34319397807121277, 'learning_rate': 4.603389830508475e-05, 'epoch': 0.09}
  9%|â–‰         | 568/6000 [26:54<4:18:36,  2.86s/it]  9%|â–‰         | 569/6000 [26:57<4:16:41,  2.84s/it]                                                    {'loss': 2.7897, 'grad_norm': 0.3076348304748535, 'learning_rate': 4.602542372881356e-05, 'epoch': 0.09}
  9%|â–‰         | 569/6000 [26:57<4:16:41,  2.84s/it] 10%|â–‰         | 570/6000 [27:00<4:15:09,  2.82s/it]                                                    {'loss': 2.7759, 'grad_norm': 0.3687182664871216, 'learning_rate': 4.601694915254238e-05, 'epoch': 0.1}
 10%|â–‰         | 570/6000 [27:00<4:15:09,  2.82s/it] 10%|â–‰         | 571/6000 [27:04<4:49:32,  3.20s/it]                                                    {'loss': 2.7817, 'grad_norm': 0.4578700363636017, 'learning_rate': 4.600847457627119e-05, 'epoch': 0.1}
 10%|â–‰         | 571/6000 [27:04<4:49:32,  3.20s/it] 10%|â–‰         | 572/6000 [27:07<4:37:32,  3.07s/it]                                                    {'loss': 2.824, 'grad_norm': 0.4479356110095978, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.1}
 10%|â–‰         | 572/6000 [27:07<4:37:32,  3.07s/it] 10%|â–‰         | 573/6000 [27:09<4:27:06,  2.95s/it]                                                    {'loss': 2.7849, 'grad_norm': 0.624589741230011, 'learning_rate': 4.599152542372882e-05, 'epoch': 0.1}
 10%|â–‰         | 573/6000 [27:09<4:27:06,  2.95s/it] 10%|â–‰         | 574/6000 [27:12<4:22:23,  2.90s/it]                                                    {'loss': 2.7816, 'grad_norm': 0.5056571960449219, 'learning_rate': 4.598305084745763e-05, 'epoch': 0.1}
 10%|â–‰         | 574/6000 [27:12<4:22:23,  2.90s/it] 10%|â–‰         | 575/6000 [27:15<4:16:51,  2.84s/it]                                                    {'loss': 2.782, 'grad_norm': 0.3916482925415039, 'learning_rate': 4.597457627118644e-05, 'epoch': 0.1}
 10%|â–‰         | 575/6000 [27:15<4:16:51,  2.84s/it] 10%|â–‰         | 576/6000 [27:18<4:18:38,  2.86s/it]                                                    {'loss': 2.7788, 'grad_norm': 0.523789644241333, 'learning_rate': 4.596610169491526e-05, 'epoch': 0.1}
 10%|â–‰         | 576/6000 [27:18<4:18:38,  2.86s/it] 10%|â–‰         | 577/6000 [27:21<4:21:47,  2.90s/it]                                                    {'loss': 2.7694, 'grad_norm': 0.45315220952033997, 'learning_rate': 4.595762711864407e-05, 'epoch': 0.1}
 10%|â–‰         | 577/6000 [27:21<4:21:47,  2.90s/it] 10%|â–‰         | 578/6000 [27:24<4:34:38,  3.04s/it]                                                    {'loss': 2.7736, 'grad_norm': 0.4591827392578125, 'learning_rate': 4.594915254237288e-05, 'epoch': 0.1}
 10%|â–‰         | 578/6000 [27:24<4:34:38,  3.04s/it] 10%|â–‰         | 579/6000 [27:27<4:38:44,  3.09s/it]                                                    {'loss': 2.7761, 'grad_norm': 0.4827750027179718, 'learning_rate': 4.59406779661017e-05, 'epoch': 0.1}
 10%|â–‰         | 579/6000 [27:27<4:38:44,  3.09s/it] 10%|â–‰         | 580/6000 [27:30<4:29:57,  2.99s/it]                                                    {'loss': 2.8093, 'grad_norm': 0.45595234632492065, 'learning_rate': 4.593220338983051e-05, 'epoch': 0.1}
 10%|â–‰         | 580/6000 [27:30<4:29:57,  2.99s/it] 10%|â–‰         | 581/6000 [27:34<4:46:02,  3.17s/it]                                                    {'loss': 2.7801, 'grad_norm': 0.9508959650993347, 'learning_rate': 4.592372881355932e-05, 'epoch': 0.1}
 10%|â–‰         | 581/6000 [27:34<4:46:02,  3.17s/it] 10%|â–‰         | 582/6000 [27:37<5:00:01,  3.32s/it]                                                    {'loss': 2.7792, 'grad_norm': 0.3433251678943634, 'learning_rate': 4.591525423728813e-05, 'epoch': 0.1}
 10%|â–‰         | 582/6000 [27:37<5:00:01,  3.32s/it] 10%|â–‰         | 583/6000 [27:40<4:46:30,  3.17s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.3787812292575836, 'learning_rate': 4.590677966101695e-05, 'epoch': 0.1}
 10%|â–‰         | 583/6000 [27:40<4:46:30,  3.17s/it] 10%|â–‰         | 584/6000 [27:43<4:34:30,  3.04s/it]                                                    {'loss': 2.8103, 'grad_norm': 0.889771044254303, 'learning_rate': 4.589830508474576e-05, 'epoch': 0.1}
 10%|â–‰         | 584/6000 [27:43<4:34:30,  3.04s/it] 10%|â–‰         | 585/6000 [27:46<4:39:34,  3.10s/it]                                                    {'loss': 2.7786, 'grad_norm': 0.6017855405807495, 'learning_rate': 4.588983050847458e-05, 'epoch': 0.1}
 10%|â–‰         | 585/6000 [27:46<4:39:34,  3.10s/it] 10%|â–‰         | 586/6000 [27:49<4:30:01,  2.99s/it]                                                    {'loss': 2.7687, 'grad_norm': 0.5013502240180969, 'learning_rate': 4.588135593220339e-05, 'epoch': 0.1}
 10%|â–‰         | 586/6000 [27:49<4:30:01,  2.99s/it] 10%|â–‰         | 587/6000 [27:52<4:25:57,  2.95s/it]                                                    {'loss': 2.8053, 'grad_norm': 0.30612730979919434, 'learning_rate': 4.587288135593221e-05, 'epoch': 0.1}
 10%|â–‰         | 587/6000 [27:52<4:25:57,  2.95s/it] 10%|â–‰         | 588/6000 [27:55<4:19:58,  2.88s/it]                                                    {'loss': 2.7675, 'grad_norm': 0.573765218257904, 'learning_rate': 4.5864406779661014e-05, 'epoch': 0.1}
 10%|â–‰         | 588/6000 [27:55<4:19:58,  2.88s/it] 10%|â–‰         | 589/6000 [27:57<4:17:25,  2.85s/it]                                                    {'loss': 2.7907, 'grad_norm': 0.5043594241142273, 'learning_rate': 4.585593220338983e-05, 'epoch': 0.1}
 10%|â–‰         | 589/6000 [27:57<4:17:25,  2.85s/it] 10%|â–‰         | 590/6000 [28:00<4:18:18,  2.86s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.5560078024864197, 'learning_rate': 4.5847457627118644e-05, 'epoch': 0.1}
 10%|â–‰         | 590/6000 [28:00<4:18:18,  2.86s/it] 10%|â–‰         | 591/6000 [28:03<4:14:18,  2.82s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.9148135781288147, 'learning_rate': 4.583898305084746e-05, 'epoch': 0.1}
 10%|â–‰         | 591/6000 [28:03<4:14:18,  2.82s/it] 10%|â–‰         | 592/6000 [28:06<4:12:14,  2.80s/it]                                                    {'loss': 2.7778, 'grad_norm': 0.4139603078365326, 'learning_rate': 4.583050847457627e-05, 'epoch': 0.1}
 10%|â–‰         | 592/6000 [28:06<4:12:14,  2.80s/it] 10%|â–‰         | 593/6000 [28:08<4:10:36,  2.78s/it]                                                    {'loss': 2.779, 'grad_norm': 0.6902550458908081, 'learning_rate': 4.582203389830509e-05, 'epoch': 0.1}
 10%|â–‰         | 593/6000 [28:08<4:10:36,  2.78s/it] 10%|â–‰         | 594/6000 [28:11<4:11:39,  2.79s/it]                                                    {'loss': 2.7755, 'grad_norm': 0.6885732412338257, 'learning_rate': 4.58135593220339e-05, 'epoch': 0.1}
 10%|â–‰         | 594/6000 [28:11<4:11:39,  2.79s/it] 10%|â–‰         | 595/6000 [28:14<4:19:39,  2.88s/it]                                                    {'loss': 2.782, 'grad_norm': 1.0974359512329102, 'learning_rate': 4.5805084745762714e-05, 'epoch': 0.1}
 10%|â–‰         | 595/6000 [28:14<4:19:39,  2.88s/it] 10%|â–‰         | 596/6000 [28:18<4:41:16,  3.12s/it]                                                    {'loss': 2.7825, 'grad_norm': 1.2519444227218628, 'learning_rate': 4.5796610169491525e-05, 'epoch': 0.1}
 10%|â–‰         | 596/6000 [28:18<4:41:16,  3.12s/it] 10%|â–‰         | 597/6000 [28:21<4:28:43,  2.98s/it]                                                    {'loss': 2.7889, 'grad_norm': 1.6562834978103638, 'learning_rate': 4.578813559322034e-05, 'epoch': 0.1}
 10%|â–‰         | 597/6000 [28:21<4:28:43,  2.98s/it] 10%|â–‰         | 598/6000 [28:23<4:23:11,  2.92s/it]                                                    {'loss': 2.7912, 'grad_norm': 1.3782862424850464, 'learning_rate': 4.5779661016949154e-05, 'epoch': 0.1}
 10%|â–‰         | 598/6000 [28:23<4:23:11,  2.92s/it] 10%|â–‰         | 599/6000 [28:26<4:15:48,  2.84s/it]                                                    {'loss': 2.8089, 'grad_norm': 0.6833428740501404, 'learning_rate': 4.5771186440677966e-05, 'epoch': 0.1}
 10%|â–‰         | 599/6000 [28:26<4:15:48,  2.84s/it] 10%|â–ˆ         | 600/6000 [28:29<4:13:12,  2.81s/it]                                                    {'loss': 2.8219, 'grad_norm': 0.6892455220222473, 'learning_rate': 4.5762711864406784e-05, 'epoch': 0.1}
 10%|â–ˆ         | 600/6000 [28:29<4:13:12,  2.81s/it][2025-10-22 19:24:54,344] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 601/6000 [28:34<5:08:41,  3.43s/it]                                                    {'loss': 2.7807, 'grad_norm': 0.9396779537200928, 'learning_rate': 4.5754237288135595e-05, 'epoch': 0.1}
 10%|â–ˆ         | 601/6000 [28:34<5:08:41,  3.43s/it] 10%|â–ˆ         | 602/6000 [28:36<4:49:27,  3.22s/it]                                                    {'loss': 2.7794, 'grad_norm': 0.9357848763465881, 'learning_rate': 4.5745762711864406e-05, 'epoch': 0.1}
 10%|â–ˆ         | 602/6000 [28:36<4:49:27,  3.22s/it] 10%|â–ˆ         | 603/6000 [28:39<4:36:06,  3.07s/it]                                                    {'loss': 2.8299, 'grad_norm': 1.3734769821166992, 'learning_rate': 4.573728813559322e-05, 'epoch': 0.1}
 10%|â–ˆ         | 603/6000 [28:39<4:36:06,  3.07s/it] 10%|â–ˆ         | 604/6000 [28:43<4:44:28,  3.16s/it]                                                    {'loss': 2.778, 'grad_norm': 0.6562738418579102, 'learning_rate': 4.5728813559322036e-05, 'epoch': 0.1}
 10%|â–ˆ         | 604/6000 [28:43<4:44:28,  3.16s/it] 10%|â–ˆ         | 605/6000 [28:45<4:31:09,  3.02s/it]                                                    {'loss': 2.81, 'grad_norm': 0.9820795059204102, 'learning_rate': 4.572033898305085e-05, 'epoch': 0.1}
 10%|â–ˆ         | 605/6000 [28:45<4:31:09,  3.02s/it] 10%|â–ˆ         | 606/6000 [28:48<4:29:05,  2.99s/it]                                                    {'loss': 2.7877, 'grad_norm': 0.7611591219902039, 'learning_rate': 4.5711864406779665e-05, 'epoch': 0.1}
 10%|â–ˆ         | 606/6000 [28:48<4:29:05,  2.99s/it] 10%|â–ˆ         | 607/6000 [28:51<4:25:36,  2.95s/it]                                                    {'loss': 2.7776, 'grad_norm': 0.8168428540229797, 'learning_rate': 4.5703389830508476e-05, 'epoch': 0.1}
 10%|â–ˆ         | 607/6000 [28:51<4:25:36,  2.95s/it] 10%|â–ˆ         | 608/6000 [28:54<4:24:33,  2.94s/it]                                                    {'loss': 2.7805, 'grad_norm': 0.6693205237388611, 'learning_rate': 4.5694915254237294e-05, 'epoch': 0.1}
 10%|â–ˆ         | 608/6000 [28:54<4:24:33,  2.94s/it] 10%|â–ˆ         | 609/6000 [28:57<4:17:40,  2.87s/it]                                                    {'loss': 2.7741, 'grad_norm': 0.5322916507720947, 'learning_rate': 4.5686440677966106e-05, 'epoch': 0.1}
 10%|â–ˆ         | 609/6000 [28:57<4:17:40,  2.87s/it] 10%|â–ˆ         | 610/6000 [28:59<4:13:30,  2.82s/it]                                                    {'loss': 2.7712, 'grad_norm': 0.6235384345054626, 'learning_rate': 4.567796610169492e-05, 'epoch': 0.1}
 10%|â–ˆ         | 610/6000 [28:59<4:13:30,  2.82s/it] 10%|â–ˆ         | 611/6000 [29:02<4:10:07,  2.78s/it]                                                    {'loss': 2.8213, 'grad_norm': 0.6700077056884766, 'learning_rate': 4.566949152542373e-05, 'epoch': 0.1}
 10%|â–ˆ         | 611/6000 [29:02<4:10:07,  2.78s/it] 10%|â–ˆ         | 612/6000 [29:05<4:11:42,  2.80s/it]                                                    {'loss': 2.7708, 'grad_norm': 0.4903571903705597, 'learning_rate': 4.5661016949152546e-05, 'epoch': 0.1}
 10%|â–ˆ         | 612/6000 [29:05<4:11:42,  2.80s/it] 10%|â–ˆ         | 613/6000 [29:08<4:10:55,  2.79s/it]                                                    {'loss': 2.8011, 'grad_norm': 0.6232836246490479, 'learning_rate': 4.565254237288136e-05, 'epoch': 0.1}
 10%|â–ˆ         | 613/6000 [29:08<4:10:55,  2.79s/it] 10%|â–ˆ         | 614/6000 [29:10<4:06:59,  2.75s/it]                                                    {'loss': 2.7767, 'grad_norm': 0.5260719060897827, 'learning_rate': 4.5644067796610176e-05, 'epoch': 0.1}
 10%|â–ˆ         | 614/6000 [29:10<4:06:59,  2.75s/it] 10%|â–ˆ         | 615/6000 [29:13<4:07:30,  2.76s/it]                                                    {'loss': 2.7973, 'grad_norm': 0.630706250667572, 'learning_rate': 4.563559322033899e-05, 'epoch': 0.1}
 10%|â–ˆ         | 615/6000 [29:13<4:07:30,  2.76s/it] 10%|â–ˆ         | 616/6000 [29:16<4:06:22,  2.75s/it]                                                    {'loss': 2.7823, 'grad_norm': 0.5971598625183105, 'learning_rate': 4.56271186440678e-05, 'epoch': 0.1}
 10%|â–ˆ         | 616/6000 [29:16<4:06:22,  2.75s/it] 10%|â–ˆ         | 617/6000 [29:19<4:06:30,  2.75s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.7554448843002319, 'learning_rate': 4.561864406779661e-05, 'epoch': 0.1}
 10%|â–ˆ         | 617/6000 [29:19<4:06:30,  2.75s/it] 10%|â–ˆ         | 618/6000 [29:21<4:05:20,  2.74s/it]                                                    {'loss': 2.7787, 'grad_norm': 0.6983531713485718, 'learning_rate': 4.561016949152543e-05, 'epoch': 0.1}
 10%|â–ˆ         | 618/6000 [29:21<4:05:20,  2.74s/it] 10%|â–ˆ         | 619/6000 [29:24<4:04:57,  2.73s/it]                                                    {'loss': 2.7768, 'grad_norm': 0.41870278120040894, 'learning_rate': 4.560169491525424e-05, 'epoch': 0.1}
 10%|â–ˆ         | 619/6000 [29:24<4:04:57,  2.73s/it] 10%|â–ˆ         | 620/6000 [29:27<4:02:16,  2.70s/it]                                                    {'loss': 2.7786, 'grad_norm': 0.6035856008529663, 'learning_rate': 4.559322033898305e-05, 'epoch': 0.1}
 10%|â–ˆ         | 620/6000 [29:27<4:02:16,  2.70s/it] 10%|â–ˆ         | 621/6000 [29:29<4:02:20,  2.70s/it]                                                    {'loss': 2.8417, 'grad_norm': 0.43307575583457947, 'learning_rate': 4.558474576271187e-05, 'epoch': 0.1}
 10%|â–ˆ         | 621/6000 [29:29<4:02:20,  2.70s/it] 10%|â–ˆ         | 622/6000 [29:32<4:02:16,  2.70s/it]                                                    {'loss': 2.7747, 'grad_norm': 0.5084816217422485, 'learning_rate': 4.557627118644068e-05, 'epoch': 0.1}
 10%|â–ˆ         | 622/6000 [29:32<4:02:16,  2.70s/it] 10%|â–ˆ         | 623/6000 [29:35<4:02:05,  2.70s/it]                                                    {'loss': 2.8013, 'grad_norm': 0.47992751002311707, 'learning_rate': 4.556779661016949e-05, 'epoch': 0.1}
 10%|â–ˆ         | 623/6000 [29:35<4:02:05,  2.70s/it] 10%|â–ˆ         | 624/6000 [29:37<4:03:48,  2.72s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.776703417301178, 'learning_rate': 4.55593220338983e-05, 'epoch': 0.1}
 10%|â–ˆ         | 624/6000 [29:37<4:03:48,  2.72s/it] 10%|â–ˆ         | 625/6000 [29:40<4:09:30,  2.79s/it]                                                    {'loss': 2.878, 'grad_norm': 121.71109008789062, 'learning_rate': 4.555084745762712e-05, 'epoch': 0.1}
 10%|â–ˆ         | 625/6000 [29:40<4:09:30,  2.79s/it] 10%|â–ˆ         | 626/6000 [29:43<4:08:34,  2.78s/it]                                                    {'loss': 2.7731, 'grad_norm': 0.992926836013794, 'learning_rate': 4.554237288135593e-05, 'epoch': 0.1}
 10%|â–ˆ         | 626/6000 [29:43<4:08:34,  2.78s/it] 10%|â–ˆ         | 627/6000 [29:47<4:32:35,  3.04s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.4329570233821869, 'learning_rate': 4.553389830508475e-05, 'epoch': 0.1}
 10%|â–ˆ         | 627/6000 [29:47<4:32:35,  3.04s/it] 10%|â–ˆ         | 628/6000 [29:50<4:26:08,  2.97s/it]                                                    {'loss': 2.7916, 'grad_norm': 0.5787168145179749, 'learning_rate': 4.552542372881356e-05, 'epoch': 0.1}
 10%|â–ˆ         | 628/6000 [29:50<4:26:08,  2.97s/it] 10%|â–ˆ         | 629/6000 [29:52<4:19:10,  2.90s/it]                                                    {'loss': 2.7725, 'grad_norm': 0.5012324452400208, 'learning_rate': 4.551694915254238e-05, 'epoch': 0.1}
 10%|â–ˆ         | 629/6000 [29:52<4:19:10,  2.90s/it] 10%|â–ˆ         | 630/6000 [29:55<4:14:37,  2.84s/it]                                                    {'loss': 2.7704, 'grad_norm': 0.8268994092941284, 'learning_rate': 4.550847457627119e-05, 'epoch': 0.1}
 10%|â–ˆ         | 630/6000 [29:55<4:14:37,  2.84s/it] 11%|â–ˆ         | 631/6000 [29:58<4:13:03,  2.83s/it]                                                    {'loss': 2.7808, 'grad_norm': 0.46148210763931274, 'learning_rate': 4.55e-05, 'epoch': 0.11}
 11%|â–ˆ         | 631/6000 [29:58<4:13:03,  2.83s/it] 11%|â–ˆ         | 632/6000 [30:01<4:12:32,  2.82s/it]                                                    {'loss': 2.8694, 'grad_norm': 0.5003032684326172, 'learning_rate': 4.549152542372881e-05, 'epoch': 0.11}
 11%|â–ˆ         | 632/6000 [30:01<4:12:32,  2.82s/it] 11%|â–ˆ         | 633/6000 [30:03<4:11:36,  2.81s/it]                                                    {'loss': 2.7766, 'grad_norm': 0.4688802659511566, 'learning_rate': 4.548305084745763e-05, 'epoch': 0.11}
 11%|â–ˆ         | 633/6000 [30:03<4:11:36,  2.81s/it] 11%|â–ˆ         | 634/6000 [30:07<4:19:46,  2.90s/it]                                                    {'loss': 2.7708, 'grad_norm': 0.951741099357605, 'learning_rate': 4.547457627118644e-05, 'epoch': 0.11}
 11%|â–ˆ         | 634/6000 [30:07<4:19:46,  2.90s/it] 11%|â–ˆ         | 635/6000 [30:09<4:19:26,  2.90s/it]                                                    {'loss': 2.7746, 'grad_norm': 0.5781002640724182, 'learning_rate': 4.546610169491526e-05, 'epoch': 0.11}
 11%|â–ˆ         | 635/6000 [30:09<4:19:26,  2.90s/it] 11%|â–ˆ         | 636/6000 [30:12<4:15:03,  2.85s/it]                                                    {'loss': 2.779, 'grad_norm': 0.4866264760494232, 'learning_rate': 4.545762711864407e-05, 'epoch': 0.11}
 11%|â–ˆ         | 636/6000 [30:12<4:15:03,  2.85s/it] 11%|â–ˆ         | 637/6000 [30:15<4:11:51,  2.82s/it]                                                    {'loss': 2.7921, 'grad_norm': 0.6620981097221375, 'learning_rate': 4.544915254237288e-05, 'epoch': 0.11}
 11%|â–ˆ         | 637/6000 [30:15<4:11:51,  2.82s/it] 11%|â–ˆ         | 638/6000 [30:18<4:20:47,  2.92s/it]                                                    {'loss': 2.7753, 'grad_norm': 0.5160518288612366, 'learning_rate': 4.5440677966101694e-05, 'epoch': 0.11}
 11%|â–ˆ         | 638/6000 [30:18<4:20:47,  2.92s/it] 11%|â–ˆ         | 639/6000 [30:21<4:15:41,  2.86s/it]                                                    {'loss': 2.8056, 'grad_norm': 0.48428237438201904, 'learning_rate': 4.543220338983051e-05, 'epoch': 0.11}
 11%|â–ˆ         | 639/6000 [30:21<4:15:41,  2.86s/it] 11%|â–ˆ         | 640/6000 [30:24<4:22:42,  2.94s/it]                                                    {'loss': 2.8231, 'grad_norm': 0.6160967946052551, 'learning_rate': 4.542372881355932e-05, 'epoch': 0.11}
 11%|â–ˆ         | 640/6000 [30:24<4:22:42,  2.94s/it] 11%|â–ˆ         | 641/6000 [30:27<4:30:07,  3.02s/it]                                                    {'loss': 2.7684, 'grad_norm': 0.499281644821167, 'learning_rate': 4.5415254237288135e-05, 'epoch': 0.11}
 11%|â–ˆ         | 641/6000 [30:27<4:30:07,  3.02s/it] 11%|â–ˆ         | 642/6000 [30:30<4:21:38,  2.93s/it]                                                    {'loss': 2.78, 'grad_norm': 0.42488977313041687, 'learning_rate': 4.540677966101695e-05, 'epoch': 0.11}
 11%|â–ˆ         | 642/6000 [30:30<4:21:38,  2.93s/it] 11%|â–ˆ         | 643/6000 [30:33<4:17:03,  2.88s/it]                                                    {'loss': 2.7913, 'grad_norm': 0.4520612955093384, 'learning_rate': 4.5398305084745764e-05, 'epoch': 0.11}
 11%|â–ˆ         | 643/6000 [30:33<4:17:03,  2.88s/it] 11%|â–ˆ         | 644/6000 [30:35<4:13:36,  2.84s/it]                                                    {'loss': 2.7929, 'grad_norm': 0.4712953567504883, 'learning_rate': 4.538983050847458e-05, 'epoch': 0.11}
 11%|â–ˆ         | 644/6000 [30:35<4:13:36,  2.84s/it] 11%|â–ˆ         | 645/6000 [30:38<4:10:00,  2.80s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.5219127535820007, 'learning_rate': 4.5381355932203387e-05, 'epoch': 0.11}
 11%|â–ˆ         | 645/6000 [30:38<4:10:00,  2.80s/it] 11%|â–ˆ         | 646/6000 [30:41<4:17:47,  2.89s/it]                                                    {'loss': 2.7751, 'grad_norm': 0.39625003933906555, 'learning_rate': 4.5372881355932205e-05, 'epoch': 0.11}
 11%|â–ˆ         | 646/6000 [30:41<4:17:47,  2.89s/it] 11%|â–ˆ         | 647/6000 [30:44<4:11:15,  2.82s/it]                                                    {'loss': 2.8725, 'grad_norm': 0.5448843240737915, 'learning_rate': 4.5364406779661016e-05, 'epoch': 0.11}
 11%|â–ˆ         | 647/6000 [30:44<4:11:15,  2.82s/it] 11%|â–ˆ         | 648/6000 [30:47<4:18:26,  2.90s/it]                                                    {'loss': 2.773, 'grad_norm': 0.4330444931983948, 'learning_rate': 4.5355932203389834e-05, 'epoch': 0.11}
 11%|â–ˆ         | 648/6000 [30:47<4:18:26,  2.90s/it] 11%|â–ˆ         | 649/6000 [30:50<4:13:58,  2.85s/it]                                                    {'loss': 2.774, 'grad_norm': 0.40715163946151733, 'learning_rate': 4.5347457627118645e-05, 'epoch': 0.11}
 11%|â–ˆ         | 649/6000 [30:50<4:13:58,  2.85s/it] 11%|â–ˆ         | 650/6000 [30:52<4:09:12,  2.79s/it]                                                    {'loss': 2.8039, 'grad_norm': 0.5366482138633728, 'learning_rate': 4.533898305084746e-05, 'epoch': 0.11}
 11%|â–ˆ         | 650/6000 [30:52<4:09:12,  2.79s/it][2025-10-22 19:27:17,868] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 11%|â–ˆ         | 651/6000 [30:57<5:03:51,  3.41s/it]                                                    {'loss': 2.8048, 'grad_norm': 0.5223296880722046, 'learning_rate': 4.5330508474576275e-05, 'epoch': 0.11}
 11%|â–ˆ         | 651/6000 [30:57<5:03:51,  3.41s/it] 11%|â–ˆ         | 652/6000 [31:00<4:54:54,  3.31s/it]                                                    {'loss': 2.7764, 'grad_norm': 0.553244411945343, 'learning_rate': 4.5322033898305086e-05, 'epoch': 0.11}
 11%|â–ˆ         | 652/6000 [31:00<4:54:54,  3.31s/it] 11%|â–ˆ         | 653/6000 [31:03<4:42:07,  3.17s/it]                                                    {'loss': 2.8231, 'grad_norm': 0.45857787132263184, 'learning_rate': 4.53135593220339e-05, 'epoch': 0.11}
 11%|â–ˆ         | 653/6000 [31:03<4:42:07,  3.17s/it] 11%|â–ˆ         | 654/6000 [31:06<4:30:38,  3.04s/it]                                                    {'loss': 2.7653, 'grad_norm': 0.5888499617576599, 'learning_rate': 4.5305084745762715e-05, 'epoch': 0.11}
 11%|â–ˆ         | 654/6000 [31:06<4:30:38,  3.04s/it] 11%|â–ˆ         | 655/6000 [31:09<4:21:52,  2.94s/it]                                                    {'loss': 2.8014, 'grad_norm': 0.507203996181488, 'learning_rate': 4.5296610169491527e-05, 'epoch': 0.11}
 11%|â–ˆ         | 655/6000 [31:09<4:21:52,  2.94s/it] 11%|â–ˆ         | 656/6000 [31:11<4:15:08,  2.86s/it]                                                    {'loss': 2.7829, 'grad_norm': 0.7149774432182312, 'learning_rate': 4.5288135593220345e-05, 'epoch': 0.11}
 11%|â–ˆ         | 656/6000 [31:11<4:15:08,  2.86s/it] 11%|â–ˆ         | 657/6000 [31:14<4:16:06,  2.88s/it]                                                    {'loss': 2.8053, 'grad_norm': 0.8297963738441467, 'learning_rate': 4.5279661016949156e-05, 'epoch': 0.11}
 11%|â–ˆ         | 657/6000 [31:14<4:16:06,  2.88s/it] 11%|â–ˆ         | 658/6000 [31:17<4:24:00,  2.97s/it]                                                    {'loss': 2.7959, 'grad_norm': 3.4502081871032715, 'learning_rate': 4.5271186440677974e-05, 'epoch': 0.11}
 11%|â–ˆ         | 658/6000 [31:17<4:24:00,  2.97s/it] 11%|â–ˆ         | 659/6000 [31:20<4:16:47,  2.88s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.685433566570282, 'learning_rate': 4.526271186440678e-05, 'epoch': 0.11}
 11%|â–ˆ         | 659/6000 [31:20<4:16:47,  2.88s/it] 11%|â–ˆ         | 660/6000 [31:23<4:12:43,  2.84s/it]                                                    {'loss': 2.7715, 'grad_norm': 0.6642649173736572, 'learning_rate': 4.5254237288135596e-05, 'epoch': 0.11}
 11%|â–ˆ         | 660/6000 [31:23<4:12:43,  2.84s/it] 11%|â–ˆ         | 661/6000 [31:25<4:09:08,  2.80s/it]                                                    {'loss': 2.7673, 'grad_norm': 1.1831936836242676, 'learning_rate': 4.524576271186441e-05, 'epoch': 0.11}
 11%|â–ˆ         | 661/6000 [31:25<4:09:08,  2.80s/it] 11%|â–ˆ         | 662/6000 [31:28<4:07:06,  2.78s/it]                                                    {'loss': 2.7786, 'grad_norm': 0.7802046537399292, 'learning_rate': 4.523728813559322e-05, 'epoch': 0.11}
 11%|â–ˆ         | 662/6000 [31:28<4:07:06,  2.78s/it] 11%|â–ˆ         | 663/6000 [31:31<4:07:34,  2.78s/it]                                                    {'loss': 2.7681, 'grad_norm': 1.0695971250534058, 'learning_rate': 4.522881355932204e-05, 'epoch': 0.11}
 11%|â–ˆ         | 663/6000 [31:31<4:07:34,  2.78s/it] 11%|â–ˆ         | 664/6000 [31:34<4:10:10,  2.81s/it]                                                    {'loss': 2.8188, 'grad_norm': 0.6511762142181396, 'learning_rate': 4.522033898305085e-05, 'epoch': 0.11}
 11%|â–ˆ         | 664/6000 [31:34<4:10:10,  2.81s/it] 11%|â–ˆ         | 665/6000 [31:37<4:09:32,  2.81s/it]                                                    {'loss': 2.7921, 'grad_norm': 1.3874757289886475, 'learning_rate': 4.5211864406779666e-05, 'epoch': 0.11}
 11%|â–ˆ         | 665/6000 [31:37<4:09:32,  2.81s/it] 11%|â–ˆ         | 666/6000 [31:39<4:08:47,  2.80s/it]                                                    {'loss': 2.7827, 'grad_norm': 1.9068797826766968, 'learning_rate': 4.520338983050848e-05, 'epoch': 0.11}
 11%|â–ˆ         | 666/6000 [31:39<4:08:47,  2.80s/it] 11%|â–ˆ         | 667/6000 [31:42<4:09:21,  2.81s/it]                                                    {'loss': 2.8268, 'grad_norm': 1.1787077188491821, 'learning_rate': 4.519491525423729e-05, 'epoch': 0.11}
 11%|â–ˆ         | 667/6000 [31:42<4:09:21,  2.81s/it] 11%|â–ˆ         | 668/6000 [31:45<4:08:16,  2.79s/it]                                                    {'loss': 2.7815, 'grad_norm': 1.4732751846313477, 'learning_rate': 4.51864406779661e-05, 'epoch': 0.11}
 11%|â–ˆ         | 668/6000 [31:45<4:08:16,  2.79s/it] 11%|â–ˆ         | 669/6000 [31:48<4:06:20,  2.77s/it]                                                    {'loss': 2.7842, 'grad_norm': 1.7995715141296387, 'learning_rate': 4.517796610169492e-05, 'epoch': 0.11}
 11%|â–ˆ         | 669/6000 [31:48<4:06:20,  2.77s/it] 11%|â–ˆ         | 670/6000 [31:51<4:10:28,  2.82s/it]                                                    {'loss': 2.7776, 'grad_norm': 2.301429033279419, 'learning_rate': 4.516949152542373e-05, 'epoch': 0.11}
 11%|â–ˆ         | 670/6000 [31:51<4:10:28,  2.82s/it] 11%|â–ˆ         | 671/6000 [31:53<4:08:15,  2.80s/it]                                                    {'loss': 2.7849, 'grad_norm': 1.0448485612869263, 'learning_rate': 4.516101694915255e-05, 'epoch': 0.11}
 11%|â–ˆ         | 671/6000 [31:53<4:08:15,  2.80s/it] 11%|â–ˆ         | 672/6000 [31:56<4:08:59,  2.80s/it]                                                    {'loss': 2.7866, 'grad_norm': 1.1422468423843384, 'learning_rate': 4.515254237288136e-05, 'epoch': 0.11}
 11%|â–ˆ         | 672/6000 [31:56<4:08:59,  2.80s/it] 11%|â–ˆ         | 673/6000 [31:59<4:08:02,  2.79s/it]                                                    {'loss': 2.8241, 'grad_norm': 1.8481990098953247, 'learning_rate': 4.514406779661017e-05, 'epoch': 0.11}
 11%|â–ˆ         | 673/6000 [31:59<4:08:02,  2.79s/it] 11%|â–ˆ         | 674/6000 [32:02<4:05:29,  2.77s/it]                                                    {'loss': 2.7824, 'grad_norm': 1.470126986503601, 'learning_rate': 4.513559322033898e-05, 'epoch': 0.11}
 11%|â–ˆ         | 674/6000 [32:02<4:05:29,  2.77s/it] 11%|â–ˆâ–        | 675/6000 [32:04<4:03:34,  2.74s/it]                                                    {'loss': 2.7797, 'grad_norm': 1.4173686504364014, 'learning_rate': 4.51271186440678e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 675/6000 [32:04<4:03:34,  2.74s/it] 11%|â–ˆâ–        | 676/6000 [32:07<4:02:14,  2.73s/it]                                                    {'loss': 2.8238, 'grad_norm': 1.1843267679214478, 'learning_rate': 4.511864406779661e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 676/6000 [32:07<4:02:14,  2.73s/it] 11%|â–ˆâ–        | 677/6000 [32:10<3:59:28,  2.70s/it]                                                    {'loss': 2.7747, 'grad_norm': 1.5643004179000854, 'learning_rate': 4.511016949152543e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 677/6000 [32:10<3:59:28,  2.70s/it] 11%|â–ˆâ–        | 678/6000 [32:12<4:00:59,  2.72s/it]                                                    {'loss': 2.7882, 'grad_norm': 1.5188034772872925, 'learning_rate': 4.510169491525424e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 678/6000 [32:12<4:00:59,  2.72s/it] 11%|â–ˆâ–        | 679/6000 [32:15<3:59:58,  2.71s/it]                                                    {'loss': 2.7753, 'grad_norm': 1.539125680923462, 'learning_rate': 4.509322033898306e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 679/6000 [32:15<3:59:58,  2.71s/it] 11%|â–ˆâ–        | 680/6000 [32:18<4:02:23,  2.73s/it]                                                    {'loss': 2.8062, 'grad_norm': 1.5108295679092407, 'learning_rate': 4.508474576271187e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 680/6000 [32:18<4:02:23,  2.73s/it] 11%|â–ˆâ–        | 681/6000 [32:21<4:13:24,  2.86s/it]                                                    {'loss': 2.7797, 'grad_norm': 2.9214980602264404, 'learning_rate': 4.507627118644068e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 681/6000 [32:21<4:13:24,  2.86s/it] 11%|â–ˆâ–        | 682/6000 [32:24<4:12:21,  2.85s/it]                                                    {'loss': 2.7685, 'grad_norm': 1.4077130556106567, 'learning_rate': 4.506779661016949e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 682/6000 [32:24<4:12:21,  2.85s/it] 11%|â–ˆâ–        | 683/6000 [32:27<4:09:21,  2.81s/it]                                                    {'loss': 2.7741, 'grad_norm': 1.7287429571151733, 'learning_rate': 4.5059322033898304e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 683/6000 [32:27<4:09:21,  2.81s/it] 11%|â–ˆâ–        | 684/6000 [32:29<4:06:29,  2.78s/it]                                                    {'loss': 2.7886, 'grad_norm': 2.443401575088501, 'learning_rate': 4.505084745762712e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 684/6000 [32:29<4:06:29,  2.78s/it] 11%|â–ˆâ–        | 685/6000 [32:32<4:04:32,  2.76s/it]                                                    {'loss': 2.7731, 'grad_norm': 1.6802408695220947, 'learning_rate': 4.504237288135593e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 685/6000 [32:32<4:04:32,  2.76s/it] 11%|â–ˆâ–        | 686/6000 [32:35<4:18:30,  2.92s/it]                                                    {'loss': 2.7789, 'grad_norm': 2.391423463821411, 'learning_rate': 4.503389830508475e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 686/6000 [32:35<4:18:30,  2.92s/it] 11%|â–ˆâ–        | 687/6000 [32:38<4:13:32,  2.86s/it]                                                    {'loss': 2.8045, 'grad_norm': 0.9661009907722473, 'learning_rate': 4.502542372881356e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 687/6000 [32:38<4:13:32,  2.86s/it] 11%|â–ˆâ–        | 688/6000 [32:41<4:10:22,  2.83s/it]                                                    {'loss': 2.8212, 'grad_norm': 1.1382551193237305, 'learning_rate': 4.5016949152542373e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 688/6000 [32:41<4:10:22,  2.83s/it] 11%|â–ˆâ–        | 689/6000 [32:44<4:06:52,  2.79s/it]                                                    {'loss': 2.8038, 'grad_norm': 1.192612886428833, 'learning_rate': 4.5008474576271185e-05, 'epoch': 0.11}
 11%|â–ˆâ–        | 689/6000 [32:44<4:06:52,  2.79s/it] 12%|â–ˆâ–        | 690/6000 [32:46<4:06:44,  2.79s/it]                                                    {'loss': 2.7812, 'grad_norm': 1.3558496236801147, 'learning_rate': 4.5e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 690/6000 [32:46<4:06:44,  2.79s/it] 12%|â–ˆâ–        | 691/6000 [32:49<4:07:34,  2.80s/it]                                                    {'loss': 2.7826, 'grad_norm': 2.006197690963745, 'learning_rate': 4.4991525423728814e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 691/6000 [32:49<4:07:34,  2.80s/it] 12%|â–ˆâ–        | 692/6000 [32:52<4:05:47,  2.78s/it]                                                    {'loss': 2.7507, 'grad_norm': 2.9422504901885986, 'learning_rate': 4.498305084745763e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 692/6000 [32:52<4:05:47,  2.78s/it] 12%|â–ˆâ–        | 693/6000 [32:55<4:02:53,  2.75s/it]                                                    {'loss': 2.7883, 'grad_norm': 1.578662633895874, 'learning_rate': 4.4974576271186443e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 693/6000 [32:55<4:02:53,  2.75s/it] 12%|â–ˆâ–        | 694/6000 [32:57<4:04:20,  2.76s/it]                                                    {'loss': 2.7682, 'grad_norm': 1.9279201030731201, 'learning_rate': 4.4966101694915255e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 694/6000 [32:57<4:04:20,  2.76s/it] 12%|â–ˆâ–        | 695/6000 [33:00<4:03:58,  2.76s/it]                                                    {'loss': 2.7699, 'grad_norm': 1.4509153366088867, 'learning_rate': 4.4957627118644066e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 695/6000 [33:00<4:03:58,  2.76s/it] 12%|â–ˆâ–        | 696/6000 [33:03<4:13:44,  2.87s/it]                                                    {'loss': 2.7738, 'grad_norm': 2.349628448486328, 'learning_rate': 4.4949152542372884e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 696/6000 [33:03<4:13:44,  2.87s/it] 12%|â–ˆâ–        | 697/6000 [33:06<4:08:11,  2.81s/it]                                                    {'loss': 2.7858, 'grad_norm': 1.4654532670974731, 'learning_rate': 4.4940677966101695e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 697/6000 [33:06<4:08:11,  2.81s/it] 12%|â–ˆâ–        | 698/6000 [33:09<4:04:44,  2.77s/it]                                                    {'loss': 2.8496, 'grad_norm': 2.5367417335510254, 'learning_rate': 4.4932203389830513e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 698/6000 [33:09<4:04:44,  2.77s/it] 12%|â–ˆâ–        | 699/6000 [33:11<4:04:37,  2.77s/it]                                                    {'loss': 2.7564, 'grad_norm': 2.992419481277466, 'learning_rate': 4.4923728813559325e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 699/6000 [33:11<4:04:37,  2.77s/it] 12%|â–ˆâ–        | 700/6000 [33:14<4:06:01,  2.79s/it]                                                    {'loss': 2.7772, 'grad_norm': 2.317992687225342, 'learning_rate': 4.491525423728814e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 700/6000 [33:14<4:06:01,  2.79s/it][2025-10-22 19:29:39,684] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 12%|â–ˆâ–        | 701/6000 [33:19<5:04:36,  3.45s/it]                                                    {'loss': 2.8466, 'grad_norm': 4.945860862731934, 'learning_rate': 4.4906779661016954e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 701/6000 [33:19<5:04:36,  3.45s/it] 12%|â–ˆâ–        | 702/6000 [33:22<4:57:29,  3.37s/it]                                                    {'loss': 2.8029, 'grad_norm': 3.0987930297851562, 'learning_rate': 4.4898305084745765e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 702/6000 [33:22<4:57:29,  3.37s/it] 12%|â–ˆâ–        | 703/6000 [33:25<4:41:29,  3.19s/it]                                                    {'loss': 2.7795, 'grad_norm': 2.0925562381744385, 'learning_rate': 4.488983050847458e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 703/6000 [33:25<4:41:29,  3.19s/it] 12%|â–ˆâ–        | 704/6000 [33:28<4:38:54,  3.16s/it]                                                    {'loss': 2.745, 'grad_norm': 2.4750115871429443, 'learning_rate': 4.488135593220339e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 704/6000 [33:28<4:38:54,  3.16s/it] 12%|â–ˆâ–        | 705/6000 [33:31<4:30:07,  3.06s/it]                                                    {'loss': 2.8064, 'grad_norm': 3.954632043838501, 'learning_rate': 4.4872881355932206e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 705/6000 [33:31<4:30:07,  3.06s/it] 12%|â–ˆâ–        | 706/6000 [33:34<4:20:38,  2.95s/it]                                                    {'loss': 2.7899, 'grad_norm': 1.3053886890411377, 'learning_rate': 4.486440677966102e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 706/6000 [33:34<4:20:38,  2.95s/it] 12%|â–ˆâ–        | 707/6000 [33:37<4:15:28,  2.90s/it]                                                    {'loss': 2.7814, 'grad_norm': 2.7834746837615967, 'learning_rate': 4.4855932203389835e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 707/6000 [33:37<4:15:28,  2.90s/it] 12%|â–ˆâ–        | 708/6000 [33:39<4:17:10,  2.92s/it]                                                    {'loss': 2.7663, 'grad_norm': 1.7216362953186035, 'learning_rate': 4.484745762711865e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 708/6000 [33:39<4:17:10,  2.92s/it] 12%|â–ˆâ–        | 709/6000 [33:42<4:12:34,  2.86s/it]                                                    {'loss': 2.7763, 'grad_norm': 1.3119449615478516, 'learning_rate': 4.483898305084746e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 709/6000 [33:42<4:12:34,  2.86s/it] 12%|â–ˆâ–        | 710/6000 [33:45<4:17:54,  2.93s/it]                                                    {'loss': 2.774, 'grad_norm': 0.8602496981620789, 'learning_rate': 4.483050847457627e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 710/6000 [33:45<4:17:54,  2.93s/it] 12%|â–ˆâ–        | 711/6000 [33:48<4:11:41,  2.86s/it]                                                    {'loss': 2.7733, 'grad_norm': 0.9553055763244629, 'learning_rate': 4.482203389830509e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 711/6000 [33:48<4:11:41,  2.86s/it] 12%|â–ˆâ–        | 712/6000 [33:51<4:10:00,  2.84s/it]                                                    {'loss': 2.7705, 'grad_norm': 1.3563083410263062, 'learning_rate': 4.48135593220339e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 712/6000 [33:51<4:10:00,  2.84s/it] 12%|â–ˆâ–        | 713/6000 [33:54<4:07:20,  2.81s/it]                                                    {'loss': 2.7678, 'grad_norm': 1.233359932899475, 'learning_rate': 4.480508474576272e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 713/6000 [33:54<4:07:20,  2.81s/it] 12%|â–ˆâ–        | 714/6000 [33:56<4:06:42,  2.80s/it]                                                    {'loss': 2.7725, 'grad_norm': 1.0399521589279175, 'learning_rate': 4.479661016949153e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 714/6000 [33:56<4:06:42,  2.80s/it] 12%|â–ˆâ–        | 715/6000 [33:59<4:04:14,  2.77s/it]                                                    {'loss': 2.7735, 'grad_norm': 1.103852391242981, 'learning_rate': 4.4788135593220346e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 715/6000 [33:59<4:04:14,  2.77s/it] 12%|â–ˆâ–        | 716/6000 [34:02<4:04:32,  2.78s/it]                                                    {'loss': 2.8469, 'grad_norm': 0.9337189197540283, 'learning_rate': 4.477966101694915e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 716/6000 [34:02<4:04:32,  2.78s/it] 12%|â–ˆâ–        | 717/6000 [34:05<4:13:13,  2.88s/it]                                                    {'loss': 2.7819, 'grad_norm': 1.1142172813415527, 'learning_rate': 4.477118644067797e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 717/6000 [34:05<4:13:13,  2.88s/it] 12%|â–ˆâ–        | 718/6000 [34:08<4:08:45,  2.83s/it]                                                    {'loss': 2.7773, 'grad_norm': 0.8882904052734375, 'learning_rate': 4.476271186440678e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 718/6000 [34:08<4:08:45,  2.83s/it] 12%|â–ˆâ–        | 719/6000 [34:10<4:09:23,  2.83s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.5396963357925415, 'learning_rate': 4.47542372881356e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 719/6000 [34:10<4:09:23,  2.83s/it] 12%|â–ˆâ–        | 720/6000 [34:13<4:05:18,  2.79s/it]                                                    {'loss': 2.7766, 'grad_norm': 0.864270031452179, 'learning_rate': 4.474576271186441e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 720/6000 [34:13<4:05:18,  2.79s/it] 12%|â–ˆâ–        | 721/6000 [34:16<4:04:54,  2.78s/it]                                                    {'loss': 2.7733, 'grad_norm': 1.6248233318328857, 'learning_rate': 4.473728813559323e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 721/6000 [34:16<4:04:54,  2.78s/it] 12%|â–ˆâ–        | 722/6000 [34:19<4:05:20,  2.79s/it]                                                    {'loss': 2.7823, 'grad_norm': 1.2281684875488281, 'learning_rate': 4.472881355932204e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 722/6000 [34:19<4:05:20,  2.79s/it] 12%|â–ˆâ–        | 723/6000 [34:22<4:15:36,  2.91s/it]                                                    {'loss': 2.8013, 'grad_norm': 1.6127036809921265, 'learning_rate': 4.472033898305085e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 723/6000 [34:22<4:15:36,  2.91s/it] 12%|â–ˆâ–        | 724/6000 [34:25<4:08:39,  2.83s/it]                                                    {'loss': 2.8008, 'grad_norm': 0.9295997619628906, 'learning_rate': 4.471186440677966e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 724/6000 [34:25<4:08:39,  2.83s/it] 12%|â–ˆâ–        | 725/6000 [34:27<4:05:00,  2.79s/it]                                                    {'loss': 2.7845, 'grad_norm': 1.4614777565002441, 'learning_rate': 4.470338983050847e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 725/6000 [34:27<4:05:00,  2.79s/it] 12%|â–ˆâ–        | 726/6000 [34:30<4:01:47,  2.75s/it]                                                    {'loss': 2.773, 'grad_norm': 1.0876315832138062, 'learning_rate': 4.469491525423729e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 726/6000 [34:30<4:01:47,  2.75s/it] 12%|â–ˆâ–        | 727/6000 [34:33<3:59:58,  2.73s/it]                                                    {'loss': 2.8063, 'grad_norm': 1.4327486753463745, 'learning_rate': 4.46864406779661e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 727/6000 [34:33<3:59:58,  2.73s/it] 12%|â–ˆâ–        | 728/6000 [34:35<3:58:51,  2.72s/it]                                                    {'loss': 2.7711, 'grad_norm': 0.9280591607093811, 'learning_rate': 4.467796610169492e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 728/6000 [34:35<3:58:51,  2.72s/it] 12%|â–ˆâ–        | 729/6000 [34:38<3:58:40,  2.72s/it]                                                    {'loss': 2.7689, 'grad_norm': 1.1034237146377563, 'learning_rate': 4.466949152542373e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 729/6000 [34:38<3:58:40,  2.72s/it] 12%|â–ˆâ–        | 730/6000 [34:41<3:57:14,  2.70s/it]                                                    {'loss': 2.778, 'grad_norm': 1.9498769044876099, 'learning_rate': 4.466101694915254e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 730/6000 [34:41<3:57:14,  2.70s/it] 12%|â–ˆâ–        | 731/6000 [34:43<3:57:00,  2.70s/it]                                                    {'loss': 2.7678, 'grad_norm': 1.275612235069275, 'learning_rate': 4.4652542372881354e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 731/6000 [34:43<3:57:00,  2.70s/it] 12%|â–ˆâ–        | 732/6000 [34:46<4:02:34,  2.76s/it]                                                    {'loss': 2.7621, 'grad_norm': 2.8115360736846924, 'learning_rate': 4.464406779661017e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 732/6000 [34:46<4:02:34,  2.76s/it] 12%|â–ˆâ–        | 733/6000 [34:49<4:04:30,  2.79s/it]                                                    {'loss': 2.8077, 'grad_norm': 1.1624200344085693, 'learning_rate': 4.463559322033898e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 733/6000 [34:49<4:04:30,  2.79s/it] 12%|â–ˆâ–        | 734/6000 [34:52<4:14:51,  2.90s/it]                                                    {'loss': 2.7666, 'grad_norm': 4.037003993988037, 'learning_rate': 4.46271186440678e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 734/6000 [34:52<4:14:51,  2.90s/it] 12%|â–ˆâ–        | 735/6000 [34:55<4:10:07,  2.85s/it]                                                    {'loss': 2.7871, 'grad_norm': 2.9068195819854736, 'learning_rate': 4.461864406779661e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 735/6000 [34:55<4:10:07,  2.85s/it] 12%|â–ˆâ–        | 736/6000 [34:58<4:05:42,  2.80s/it]                                                    {'loss': 2.8241, 'grad_norm': 9.17998218536377, 'learning_rate': 4.461016949152543e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 736/6000 [34:58<4:05:42,  2.80s/it] 12%|â–ˆâ–        | 737/6000 [35:00<4:02:56,  2.77s/it]                                                    {'loss': 2.8319, 'grad_norm': 5.606501579284668, 'learning_rate': 4.460169491525424e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 737/6000 [35:00<4:02:56,  2.77s/it] 12%|â–ˆâ–        | 738/6000 [35:03<3:59:03,  2.73s/it]                                                    {'loss': 2.8594, 'grad_norm': 9.718056678771973, 'learning_rate': 4.459322033898305e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 738/6000 [35:03<3:59:03,  2.73s/it] 12%|â–ˆâ–        | 739/6000 [35:06<3:57:10,  2.70s/it]                                                    {'loss': 2.7734, 'grad_norm': 3.84944486618042, 'learning_rate': 4.4584745762711864e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 739/6000 [35:06<3:57:10,  2.70s/it] 12%|â–ˆâ–        | 740/6000 [35:08<4:00:48,  2.75s/it]                                                    {'loss': 2.7499, 'grad_norm': 6.06974983215332, 'learning_rate': 4.457627118644068e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 740/6000 [35:09<4:00:48,  2.75s/it] 12%|â–ˆâ–        | 741/6000 [35:11<4:00:09,  2.74s/it]                                                    {'loss': 2.7905, 'grad_norm': 3.5315606594085693, 'learning_rate': 4.4567796610169494e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 741/6000 [35:11<4:00:09,  2.74s/it] 12%|â–ˆâ–        | 742/6000 [35:14<4:00:11,  2.74s/it]                                                    {'loss': 2.7998, 'grad_norm': 6.120607852935791, 'learning_rate': 4.455932203389831e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 742/6000 [35:14<4:00:11,  2.74s/it] 12%|â–ˆâ–        | 743/6000 [35:17<3:58:13,  2.72s/it]                                                    {'loss': 2.8105, 'grad_norm': 4.402438640594482, 'learning_rate': 4.455084745762712e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 743/6000 [35:17<3:58:13,  2.72s/it] 12%|â–ˆâ–        | 744/6000 [35:19<3:56:52,  2.70s/it]                                                    {'loss': 2.7766, 'grad_norm': 4.328339099884033, 'learning_rate': 4.4542372881355934e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 744/6000 [35:19<3:56:52,  2.70s/it] 12%|â–ˆâ–        | 745/6000 [35:22<3:56:48,  2.70s/it]                                                    {'loss': 2.8082, 'grad_norm': 2.583282947540283, 'learning_rate': 4.4533898305084746e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 745/6000 [35:22<3:56:48,  2.70s/it] 12%|â–ˆâ–        | 746/6000 [35:25<3:58:02,  2.72s/it]                                                    {'loss': 2.7901, 'grad_norm': 2.7687578201293945, 'learning_rate': 4.452542372881356e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 746/6000 [35:25<3:58:02,  2.72s/it] 12%|â–ˆâ–        | 747/6000 [35:28<3:59:07,  2.73s/it]                                                    {'loss': 2.7847, 'grad_norm': 1.6850309371948242, 'learning_rate': 4.4516949152542375e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 747/6000 [35:28<3:59:07,  2.73s/it] 12%|â–ˆâ–        | 748/6000 [35:30<4:02:35,  2.77s/it]                                                    {'loss': 2.7695, 'grad_norm': 2.061990976333618, 'learning_rate': 4.4508474576271186e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 748/6000 [35:30<4:02:35,  2.77s/it] 12%|â–ˆâ–        | 749/6000 [35:34<4:21:51,  2.99s/it]                                                    {'loss': 2.7694, 'grad_norm': 0.9974969625473022, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.12}
 12%|â–ˆâ–        | 749/6000 [35:34<4:21:51,  2.99s/it] 12%|â–ˆâ–Ž        | 750/6000 [35:37<4:23:35,  3.01s/it]                                                    {'loss': 2.7872, 'grad_norm': 1.143914818763733, 'learning_rate': 4.4491525423728816e-05, 'epoch': 0.12}
 12%|â–ˆâ–Ž        | 750/6000 [35:37<4:23:35,  3.01s/it][2025-10-22 19:32:02,462] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 751/6000 [35:42<5:15:03,  3.60s/it]                                                    {'loss': 2.789, 'grad_norm': 0.8142707943916321, 'learning_rate': 4.448305084745763e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 751/6000 [35:42<5:15:03,  3.60s/it] 13%|â–ˆâ–Ž        | 752/6000 [35:45<5:02:23,  3.46s/it]                                                    {'loss': 2.8061, 'grad_norm': 0.9177693724632263, 'learning_rate': 4.447457627118644e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 752/6000 [35:45<5:02:23,  3.46s/it] 13%|â–ˆâ–Ž        | 753/6000 [35:48<4:44:24,  3.25s/it]                                                    {'loss': 2.7801, 'grad_norm': 0.7816761136054993, 'learning_rate': 4.4466101694915256e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 753/6000 [35:48<4:44:24,  3.25s/it] 13%|â–ˆâ–Ž        | 754/6000 [35:51<4:32:22,  3.12s/it]                                                    {'loss': 2.791, 'grad_norm': 1.5246601104736328, 'learning_rate': 4.445762711864407e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 754/6000 [35:51<4:32:22,  3.12s/it] 13%|â–ˆâ–Ž        | 755/6000 [35:53<4:20:34,  2.98s/it]                                                    {'loss': 2.7741, 'grad_norm': 0.6527704000473022, 'learning_rate': 4.4449152542372886e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 755/6000 [35:53<4:20:34,  2.98s/it] 13%|â–ˆâ–Ž        | 756/6000 [35:56<4:15:44,  2.93s/it]                                                    {'loss': 2.7764, 'grad_norm': 0.6815054416656494, 'learning_rate': 4.44406779661017e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 756/6000 [35:56<4:15:44,  2.93s/it] 13%|â–ˆâ–Ž        | 757/6000 [35:59<4:15:50,  2.93s/it]                                                    {'loss': 2.7708, 'grad_norm': 0.6026679277420044, 'learning_rate': 4.4432203389830515e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 757/6000 [35:59<4:15:50,  2.93s/it] 13%|â–ˆâ–Ž        | 758/6000 [36:02<4:15:10,  2.92s/it]                                                    {'loss': 2.7812, 'grad_norm': 0.5780080556869507, 'learning_rate': 4.4423728813559326e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 758/6000 [36:02<4:15:10,  2.92s/it] 13%|â–ˆâ–Ž        | 759/6000 [36:05<4:10:24,  2.87s/it]                                                    {'loss': 2.7697, 'grad_norm': 0.5766662955284119, 'learning_rate': 4.441525423728814e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 759/6000 [36:05<4:10:24,  2.87s/it] 13%|â–ˆâ–Ž        | 760/6000 [36:07<4:06:43,  2.83s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.7449018359184265, 'learning_rate': 4.440677966101695e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 760/6000 [36:07<4:06:43,  2.83s/it] 13%|â–ˆâ–Ž        | 761/6000 [36:10<4:05:34,  2.81s/it]                                                    {'loss': 2.7828, 'grad_norm': 0.5937213897705078, 'learning_rate': 4.439830508474577e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 761/6000 [36:10<4:05:34,  2.81s/it] 13%|â–ˆâ–Ž        | 762/6000 [36:13<4:07:38,  2.84s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.5487657189369202, 'learning_rate': 4.438983050847458e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 762/6000 [36:13<4:07:38,  2.84s/it] 13%|â–ˆâ–Ž        | 763/6000 [36:16<4:04:36,  2.80s/it]                                                    {'loss': 2.7734, 'grad_norm': 0.46237364411354065, 'learning_rate': 4.4381355932203396e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 763/6000 [36:16<4:04:36,  2.80s/it] 13%|â–ˆâ–Ž        | 764/6000 [36:19<4:02:54,  2.78s/it]                                                    {'loss': 2.7724, 'grad_norm': 0.4736942648887634, 'learning_rate': 4.437288135593221e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 764/6000 [36:19<4:02:54,  2.78s/it] 13%|â–ˆâ–Ž        | 765/6000 [36:21<4:01:27,  2.77s/it]                                                    {'loss': 2.8184, 'grad_norm': 0.894777238368988, 'learning_rate': 4.436440677966102e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 765/6000 [36:21<4:01:27,  2.77s/it] 13%|â–ˆâ–Ž        | 766/6000 [36:24<4:02:13,  2.78s/it]                                                    {'loss': 2.7757, 'grad_norm': 0.6537627577781677, 'learning_rate': 4.435593220338983e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 766/6000 [36:24<4:02:13,  2.78s/it] 13%|â–ˆâ–Ž        | 767/6000 [36:27<4:10:58,  2.88s/it]                                                    {'loss': 2.776, 'grad_norm': 0.5496526956558228, 'learning_rate': 4.434745762711864e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 767/6000 [36:27<4:10:58,  2.88s/it] 13%|â–ˆâ–Ž        | 768/6000 [36:30<4:05:43,  2.82s/it]                                                    {'loss': 2.7756, 'grad_norm': 0.8080326318740845, 'learning_rate': 4.433898305084746e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 768/6000 [36:30<4:05:43,  2.82s/it] 13%|â–ˆâ–Ž        | 769/6000 [36:33<4:08:09,  2.85s/it]                                                    {'loss': 2.7806, 'grad_norm': 0.6489886045455933, 'learning_rate': 4.433050847457627e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 769/6000 [36:33<4:08:09,  2.85s/it] 13%|â–ˆâ–Ž        | 770/6000 [36:35<4:03:58,  2.80s/it]                                                    {'loss': 2.8053, 'grad_norm': 0.6613184809684753, 'learning_rate': 4.432203389830509e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 770/6000 [36:35<4:03:58,  2.80s/it] 13%|â–ˆâ–Ž        | 771/6000 [36:38<4:01:15,  2.77s/it]                                                    {'loss': 2.7706, 'grad_norm': 0.4901895821094513, 'learning_rate': 4.43135593220339e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 771/6000 [36:38<4:01:15,  2.77s/it] 13%|â–ˆâ–Ž        | 772/6000 [36:41<3:57:29,  2.73s/it]                                                    {'loss': 2.774, 'grad_norm': 0.5357352495193481, 'learning_rate': 4.430508474576272e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 772/6000 [36:41<3:57:29,  2.73s/it] 13%|â–ˆâ–Ž        | 773/6000 [36:43<3:55:52,  2.71s/it]                                                    {'loss': 2.7719, 'grad_norm': 0.5053325295448303, 'learning_rate': 4.429661016949152e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 773/6000 [36:43<3:55:52,  2.71s/it] 13%|â–ˆâ–Ž        | 774/6000 [36:46<3:55:29,  2.70s/it]                                                    {'loss': 2.7765, 'grad_norm': 0.6480419635772705, 'learning_rate': 4.428813559322034e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 774/6000 [36:46<3:55:29,  2.70s/it] 13%|â–ˆâ–Ž        | 775/6000 [36:49<3:58:58,  2.74s/it]                                                    {'loss': 2.7697, 'grad_norm': 0.5663089752197266, 'learning_rate': 4.427966101694915e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 775/6000 [36:49<3:58:58,  2.74s/it] 13%|â–ˆâ–Ž        | 776/6000 [36:52<3:59:33,  2.75s/it]                                                    {'loss': 2.8209, 'grad_norm': 0.684650719165802, 'learning_rate': 4.427118644067797e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 776/6000 [36:52<3:59:33,  2.75s/it] 13%|â–ˆâ–Ž        | 777/6000 [36:55<4:02:51,  2.79s/it]                                                    {'loss': 2.7725, 'grad_norm': 0.7326390147209167, 'learning_rate': 4.426271186440678e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 777/6000 [36:55<4:02:51,  2.79s/it] 13%|â–ˆâ–Ž        | 778/6000 [36:58<4:10:11,  2.87s/it]                                                    {'loss': 2.7723, 'grad_norm': 0.7929724454879761, 'learning_rate': 4.42542372881356e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 778/6000 [36:58<4:10:11,  2.87s/it] 13%|â–ˆâ–Ž        | 779/6000 [37:00<4:06:00,  2.83s/it]                                                    {'loss': 2.7767, 'grad_norm': 0.6054328680038452, 'learning_rate': 4.424576271186441e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 779/6000 [37:00<4:06:00,  2.83s/it] 13%|â–ˆâ–Ž        | 780/6000 [37:03<4:03:39,  2.80s/it]                                                    {'loss': 2.7752, 'grad_norm': 0.7785767912864685, 'learning_rate': 4.423728813559322e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 780/6000 [37:03<4:03:39,  2.80s/it] 13%|â–ˆâ–Ž        | 781/6000 [37:06<4:03:05,  2.79s/it]                                                    {'loss': 2.8022, 'grad_norm': 1.0438846349716187, 'learning_rate': 4.422881355932203e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 781/6000 [37:06<4:03:05,  2.79s/it] 13%|â–ˆâ–Ž        | 782/6000 [37:09<4:01:01,  2.77s/it]                                                    {'loss': 2.7763, 'grad_norm': 0.9842380285263062, 'learning_rate': 4.422033898305085e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 782/6000 [37:09<4:01:01,  2.77s/it] 13%|â–ˆâ–Ž        | 783/6000 [37:11<3:58:50,  2.75s/it]                                                    {'loss': 2.7705, 'grad_norm': 1.2524912357330322, 'learning_rate': 4.421186440677966e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 783/6000 [37:11<3:58:50,  2.75s/it] 13%|â–ˆâ–Ž        | 784/6000 [37:14<3:56:20,  2.72s/it]                                                    {'loss': 2.7692, 'grad_norm': 1.3126026391983032, 'learning_rate': 4.420338983050848e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 784/6000 [37:14<3:56:20,  2.72s/it] 13%|â–ˆâ–Ž        | 785/6000 [37:17<3:55:58,  2.72s/it]                                                    {'loss': 2.7943, 'grad_norm': 0.9505349397659302, 'learning_rate': 4.419491525423729e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 785/6000 [37:17<3:55:58,  2.72s/it] 13%|â–ˆâ–Ž        | 786/6000 [37:19<3:55:09,  2.71s/it]                                                    {'loss': 2.7789, 'grad_norm': 1.3640130758285522, 'learning_rate': 4.41864406779661e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 786/6000 [37:19<3:55:09,  2.71s/it] 13%|â–ˆâ–Ž        | 787/6000 [37:22<4:00:16,  2.77s/it]                                                    {'loss': 2.8243, 'grad_norm': 1.6774613857269287, 'learning_rate': 4.4177966101694914e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 787/6000 [37:22<4:00:16,  2.77s/it] 13%|â–ˆâ–Ž        | 788/6000 [37:25<3:58:43,  2.75s/it]                                                    {'loss': 2.7714, 'grad_norm': 1.0957818031311035, 'learning_rate': 4.4169491525423726e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 788/6000 [37:25<3:58:43,  2.75s/it] 13%|â–ˆâ–Ž        | 789/6000 [37:28<4:02:06,  2.79s/it]                                                    {'loss': 2.7774, 'grad_norm': 1.5895596742630005, 'learning_rate': 4.4161016949152544e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 789/6000 [37:28<4:02:06,  2.79s/it] 13%|â–ˆâ–Ž        | 790/6000 [37:31<3:59:51,  2.76s/it]                                                    {'loss': 2.7952, 'grad_norm': 1.857201099395752, 'learning_rate': 4.4152542372881355e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 790/6000 [37:31<3:59:51,  2.76s/it] 13%|â–ˆâ–Ž        | 791/6000 [37:33<3:57:29,  2.74s/it]                                                    {'loss': 2.8155, 'grad_norm': 2.121608018875122, 'learning_rate': 4.414406779661017e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 791/6000 [37:33<3:57:29,  2.74s/it] 13%|â–ˆâ–Ž        | 792/6000 [37:36<3:58:53,  2.75s/it]                                                    {'loss': 2.7706, 'grad_norm': 1.1404449939727783, 'learning_rate': 4.4135593220338984e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 792/6000 [37:36<3:58:53,  2.75s/it] 13%|â–ˆâ–Ž        | 793/6000 [37:39<3:58:03,  2.74s/it]                                                    {'loss': 2.811, 'grad_norm': 1.2035444974899292, 'learning_rate': 4.41271186440678e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 793/6000 [37:39<3:58:03,  2.74s/it] 13%|â–ˆâ–Ž        | 794/6000 [37:42<4:00:07,  2.77s/it]                                                    {'loss': 2.7751, 'grad_norm': 3.72403621673584, 'learning_rate': 4.4118644067796614e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 794/6000 [37:42<4:00:07,  2.77s/it] 13%|â–ˆâ–Ž        | 795/6000 [37:45<4:08:13,  2.86s/it]                                                    {'loss': 2.755, 'grad_norm': 2.8628103733062744, 'learning_rate': 4.4110169491525425e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 795/6000 [37:45<4:08:13,  2.86s/it] 13%|â–ˆâ–Ž        | 796/6000 [37:47<4:05:01,  2.82s/it]                                                    {'loss': 2.7572, 'grad_norm': 2.814894437789917, 'learning_rate': 4.4101694915254236e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 796/6000 [37:47<4:05:01,  2.82s/it] 13%|â–ˆâ–Ž        | 797/6000 [37:50<4:03:01,  2.80s/it]                                                    {'loss': 2.8096, 'grad_norm': 7.475245952606201, 'learning_rate': 4.4093220338983054e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 797/6000 [37:50<4:03:01,  2.80s/it] 13%|â–ˆâ–Ž        | 798/6000 [37:53<3:59:26,  2.76s/it]                                                    {'loss': 2.7796, 'grad_norm': 3.581587553024292, 'learning_rate': 4.4084745762711866e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 798/6000 [37:53<3:59:26,  2.76s/it] 13%|â–ˆâ–Ž        | 799/6000 [37:56<3:58:54,  2.76s/it]                                                    {'loss': 2.832, 'grad_norm': 8.846111297607422, 'learning_rate': 4.4076271186440684e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 799/6000 [37:56<3:58:54,  2.76s/it] 13%|â–ˆâ–Ž        | 800/6000 [37:58<4:01:06,  2.78s/it]                                                    {'loss': 2.8024, 'grad_norm': 6.717667102813721, 'learning_rate': 4.4067796610169495e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 800/6000 [37:58<4:01:06,  2.78s/it][2025-10-22 19:34:23,921] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 801/6000 [38:03<4:58:36,  3.45s/it]                                                    {'loss': 2.8234, 'grad_norm': 14.09085750579834, 'learning_rate': 4.4059322033898306e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 801/6000 [38:03<4:58:36,  3.45s/it] 13%|â–ˆâ–Ž        | 802/6000 [38:06<4:41:33,  3.25s/it]                                                    {'loss': 2.7997, 'grad_norm': 7.590798377990723, 'learning_rate': 4.405084745762712e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 802/6000 [38:06<4:41:33,  3.25s/it] 13%|â–ˆâ–Ž        | 803/6000 [38:09<4:26:45,  3.08s/it]                                                    {'loss': 2.7684, 'grad_norm': 6.4392852783203125, 'learning_rate': 4.4042372881355936e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 803/6000 [38:09<4:26:45,  3.08s/it] 13%|â–ˆâ–Ž        | 804/6000 [38:12<4:16:57,  2.97s/it]                                                    {'loss': 2.7545, 'grad_norm': 5.856830596923828, 'learning_rate': 4.403389830508475e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 804/6000 [38:12<4:16:57,  2.97s/it] 13%|â–ˆâ–Ž        | 805/6000 [38:14<4:10:25,  2.89s/it]                                                    {'loss': 2.8506, 'grad_norm': 18.64965057373047, 'learning_rate': 4.4025423728813565e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 805/6000 [38:14<4:10:25,  2.89s/it] 13%|â–ˆâ–Ž        | 806/6000 [38:17<4:08:13,  2.87s/it]                                                    {'loss': 2.8402, 'grad_norm': 7.427092552185059, 'learning_rate': 4.4016949152542376e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 806/6000 [38:17<4:08:13,  2.87s/it] 13%|â–ˆâ–Ž        | 807/6000 [38:20<4:04:39,  2.83s/it]                                                    {'loss': 2.7418, 'grad_norm': 5.991029262542725, 'learning_rate': 4.400847457627119e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 807/6000 [38:20<4:04:39,  2.83s/it] 13%|â–ˆâ–Ž        | 808/6000 [38:23<4:03:02,  2.81s/it]                                                    {'loss': 2.8139, 'grad_norm': 5.72882604598999, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 808/6000 [38:23<4:03:02,  2.81s/it] 13%|â–ˆâ–Ž        | 809/6000 [38:25<4:01:54,  2.80s/it]                                                    {'loss': 2.7674, 'grad_norm': 6.978237152099609, 'learning_rate': 4.399152542372881e-05, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 809/6000 [38:25<4:01:54,  2.80s/it] 14%|â–ˆâ–Ž        | 810/6000 [38:28<4:04:35,  2.83s/it]                                                    {'loss': 2.7541, 'grad_norm': 4.333649158477783, 'learning_rate': 4.398305084745763e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 810/6000 [38:28<4:04:35,  2.83s/it] 14%|â–ˆâ–Ž        | 811/6000 [38:31<4:01:09,  2.79s/it]                                                    {'loss': 2.7548, 'grad_norm': 4.878421306610107, 'learning_rate': 4.397457627118644e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 811/6000 [38:31<4:01:09,  2.79s/it] 14%|â–ˆâ–Ž        | 812/6000 [38:34<3:57:38,  2.75s/it]                                                    {'loss': 2.7855, 'grad_norm': 3.807011365890503, 'learning_rate': 4.396610169491526e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 812/6000 [38:34<3:57:38,  2.75s/it] 14%|â–ˆâ–Ž        | 813/6000 [38:37<4:16:06,  2.96s/it]                                                    {'loss': 2.7669, 'grad_norm': 5.417588710784912, 'learning_rate': 4.395762711864407e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 813/6000 [38:37<4:16:06,  2.96s/it] 14%|â–ˆâ–Ž        | 814/6000 [38:40<4:09:56,  2.89s/it]                                                    {'loss': 2.8115, 'grad_norm': 4.936895370483398, 'learning_rate': 4.394915254237289e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 814/6000 [38:40<4:09:56,  2.89s/it] 14%|â–ˆâ–Ž        | 815/6000 [38:42<4:04:01,  2.82s/it]                                                    {'loss': 2.8114, 'grad_norm': 2.9570794105529785, 'learning_rate': 4.39406779661017e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 815/6000 [38:42<4:04:01,  2.82s/it] 14%|â–ˆâ–Ž        | 816/6000 [38:45<4:01:12,  2.79s/it]                                                    {'loss': 2.7912, 'grad_norm': 2.893953561782837, 'learning_rate': 4.393220338983051e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 816/6000 [38:45<4:01:12,  2.79s/it] 14%|â–ˆâ–Ž        | 817/6000 [38:49<4:18:31,  2.99s/it]                                                    {'loss': 2.7849, 'grad_norm': 2.7015628814697266, 'learning_rate': 4.392372881355932e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 817/6000 [38:49<4:18:31,  2.99s/it] 14%|â–ˆâ–Ž        | 818/6000 [38:51<4:10:03,  2.90s/it]                                                    {'loss': 2.7562, 'grad_norm': 2.43432879447937, 'learning_rate': 4.391525423728814e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 818/6000 [38:51<4:10:03,  2.90s/it] 14%|â–ˆâ–Ž        | 819/6000 [38:54<4:05:06,  2.84s/it]                                                    {'loss': 2.7422, 'grad_norm': 3.252760887145996, 'learning_rate': 4.390677966101695e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 819/6000 [38:54<4:05:06,  2.84s/it] 14%|â–ˆâ–Ž        | 820/6000 [38:57<4:03:13,  2.82s/it]                                                    {'loss': 2.7596, 'grad_norm': 2.080934524536133, 'learning_rate': 4.389830508474577e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 820/6000 [38:57<4:03:13,  2.82s/it] 14%|â–ˆâ–Ž        | 821/6000 [39:00<4:02:41,  2.81s/it]                                                    {'loss': 2.8043, 'grad_norm': 4.067592144012451, 'learning_rate': 4.388983050847458e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 821/6000 [39:00<4:02:41,  2.81s/it] 14%|â–ˆâ–Ž        | 822/6000 [39:02<3:59:30,  2.78s/it]                                                    {'loss': 2.7962, 'grad_norm': 2.468500852584839, 'learning_rate': 4.388135593220339e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 822/6000 [39:02<3:59:30,  2.78s/it] 14%|â–ˆâ–Ž        | 823/6000 [39:05<3:57:20,  2.75s/it]                                                    {'loss': 2.771, 'grad_norm': 2.3759195804595947, 'learning_rate': 4.38728813559322e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 823/6000 [39:05<3:57:20,  2.75s/it] 14%|â–ˆâ–Ž        | 824/6000 [39:08<3:56:01,  2.74s/it]                                                    {'loss': 2.7739, 'grad_norm': 2.2417380809783936, 'learning_rate': 4.386440677966102e-05, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 824/6000 [39:08<3:56:01,  2.74s/it] 14%|â–ˆâ–        | 825/6000 [39:11<4:03:46,  2.83s/it]                                                    {'loss': 2.8438, 'grad_norm': 3.0739543437957764, 'learning_rate': 4.385593220338983e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 825/6000 [39:11<4:03:46,  2.83s/it] 14%|â–ˆâ–        | 826/6000 [39:13<4:01:47,  2.80s/it]                                                    {'loss': 2.7758, 'grad_norm': 4.630797863006592, 'learning_rate': 4.384745762711865e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 826/6000 [39:13<4:01:47,  2.80s/it] 14%|â–ˆâ–        | 827/6000 [39:16<3:59:31,  2.78s/it]                                                    {'loss': 2.8317, 'grad_norm': 3.577498435974121, 'learning_rate': 4.383898305084746e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 827/6000 [39:16<3:59:31,  2.78s/it] 14%|â–ˆâ–        | 828/6000 [39:19<3:58:50,  2.77s/it]                                                    {'loss': 2.8451, 'grad_norm': 4.304657936096191, 'learning_rate': 4.383050847457627e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 828/6000 [39:19<3:58:50,  2.77s/it] 14%|â–ˆâ–        | 829/6000 [39:22<3:58:20,  2.77s/it]                                                    {'loss': 2.8022, 'grad_norm': 7.985410213470459, 'learning_rate': 4.382203389830509e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 829/6000 [39:22<3:58:20,  2.77s/it] 14%|â–ˆâ–        | 830/6000 [39:25<4:20:08,  3.02s/it]                                                    {'loss': 2.7784, 'grad_norm': 2.8594374656677246, 'learning_rate': 4.38135593220339e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 830/6000 [39:25<4:20:08,  3.02s/it] 14%|â–ˆâ–        | 831/6000 [39:28<4:15:49,  2.97s/it]                                                    {'loss': 2.8278, 'grad_norm': 1.959252953529358, 'learning_rate': 4.380508474576271e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 831/6000 [39:28<4:15:49,  2.97s/it] 14%|â–ˆâ–        | 832/6000 [39:31<4:09:21,  2.90s/it]                                                    {'loss': 2.7773, 'grad_norm': 1.7393765449523926, 'learning_rate': 4.3796610169491524e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 832/6000 [39:31<4:09:21,  2.90s/it] 14%|â–ˆâ–        | 833/6000 [39:34<4:03:38,  2.83s/it]                                                    {'loss': 2.7596, 'grad_norm': 1.4164557456970215, 'learning_rate': 4.378813559322034e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 833/6000 [39:34<4:03:38,  2.83s/it] 14%|â–ˆâ–        | 834/6000 [39:36<4:00:10,  2.79s/it]                                                    {'loss': 2.785, 'grad_norm': 1.241059422492981, 'learning_rate': 4.377966101694915e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 834/6000 [39:36<4:00:10,  2.79s/it] 14%|â–ˆâ–        | 835/6000 [39:39<4:09:58,  2.90s/it]                                                    {'loss': 2.7945, 'grad_norm': 2.2043776512145996, 'learning_rate': 4.377118644067797e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 835/6000 [39:39<4:09:58,  2.90s/it] 14%|â–ˆâ–        | 836/6000 [39:42<4:03:07,  2.82s/it]                                                    {'loss': 2.7589, 'grad_norm': 2.2101144790649414, 'learning_rate': 4.376271186440678e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 836/6000 [39:42<4:03:07,  2.82s/it] 14%|â–ˆâ–        | 837/6000 [39:46<4:22:51,  3.05s/it]                                                    {'loss': 2.765, 'grad_norm': 1.1103307008743286, 'learning_rate': 4.3754237288135594e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 837/6000 [39:46<4:22:51,  3.05s/it] 14%|â–ˆâ–        | 838/6000 [39:49<4:25:43,  3.09s/it]                                                    {'loss': 2.7861, 'grad_norm': 1.3679602146148682, 'learning_rate': 4.3745762711864405e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 838/6000 [39:49<4:25:43,  3.09s/it] 14%|â–ˆâ–        | 839/6000 [39:52<4:17:10,  2.99s/it]                                                    {'loss': 2.8038, 'grad_norm': 1.0620417594909668, 'learning_rate': 4.373728813559322e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 839/6000 [39:52<4:17:10,  2.99s/it] 14%|â–ˆâ–        | 840/6000 [39:54<4:11:15,  2.92s/it]                                                    {'loss': 2.7908, 'grad_norm': 2.5079777240753174, 'learning_rate': 4.3728813559322035e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 840/6000 [39:54<4:11:15,  2.92s/it] 14%|â–ˆâ–        | 841/6000 [39:58<4:21:19,  3.04s/it]                                                    {'loss': 2.7964, 'grad_norm': 2.621875286102295, 'learning_rate': 4.372033898305085e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 841/6000 [39:58<4:21:19,  3.04s/it] 14%|â–ˆâ–        | 842/6000 [40:00<4:15:04,  2.97s/it]                                                    {'loss': 2.7828, 'grad_norm': 1.2319799661636353, 'learning_rate': 4.3711864406779664e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 842/6000 [40:00<4:15:04,  2.97s/it] 14%|â–ˆâ–        | 843/6000 [40:03<4:07:39,  2.88s/it]                                                    {'loss': 2.7874, 'grad_norm': 1.049943208694458, 'learning_rate': 4.370338983050848e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 843/6000 [40:03<4:07:39,  2.88s/it] 14%|â–ˆâ–        | 844/6000 [40:06<4:04:51,  2.85s/it]                                                    {'loss': 2.7868, 'grad_norm': 0.6451480984687805, 'learning_rate': 4.3694915254237286e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 844/6000 [40:06<4:04:51,  2.85s/it] 14%|â–ˆâ–        | 845/6000 [40:09<4:00:17,  2.80s/it]                                                    {'loss': 2.8499, 'grad_norm': 0.7938222289085388, 'learning_rate': 4.3686440677966105e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 845/6000 [40:09<4:00:17,  2.80s/it] 14%|â–ˆâ–        | 846/6000 [40:11<4:02:23,  2.82s/it]                                                    {'loss': 2.769, 'grad_norm': 0.581238865852356, 'learning_rate': 4.3677966101694916e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 846/6000 [40:11<4:02:23,  2.82s/it] 14%|â–ˆâ–        | 847/6000 [40:14<4:01:31,  2.81s/it]                                                    {'loss': 2.7759, 'grad_norm': 0.7883492112159729, 'learning_rate': 4.3669491525423734e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 847/6000 [40:14<4:01:31,  2.81s/it] 14%|â–ˆâ–        | 848/6000 [40:17<4:09:17,  2.90s/it]                                                    {'loss': 2.785, 'grad_norm': 0.7178796529769897, 'learning_rate': 4.3661016949152545e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 848/6000 [40:17<4:09:17,  2.90s/it] 14%|â–ˆâ–        | 849/6000 [40:20<4:08:21,  2.89s/it]                                                    {'loss': 2.7807, 'grad_norm': 0.5848310589790344, 'learning_rate': 4.3652542372881356e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 849/6000 [40:20<4:08:21,  2.89s/it] 14%|â–ˆâ–        | 850/6000 [40:23<4:04:03,  2.84s/it]                                                    {'loss': 2.7741, 'grad_norm': 0.4120008945465088, 'learning_rate': 4.3644067796610175e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 850/6000 [40:23<4:04:03,  2.84s/it][2025-10-22 19:36:48,489] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 14%|â–ˆâ–        | 851/6000 [40:28<4:54:32,  3.43s/it]                                                    {'loss': 2.7925, 'grad_norm': 0.3855905532836914, 'learning_rate': 4.3635593220338986e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 851/6000 [40:28<4:54:32,  3.43s/it] 14%|â–ˆâ–        | 852/6000 [40:30<4:35:48,  3.21s/it]                                                    {'loss': 2.7737, 'grad_norm': 0.5615700483322144, 'learning_rate': 4.36271186440678e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 852/6000 [40:30<4:35:48,  3.21s/it] 14%|â–ˆâ–        | 853/6000 [40:33<4:22:13,  3.06s/it]                                                    {'loss': 2.845, 'grad_norm': 0.3892543315887451, 'learning_rate': 4.361864406779661e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 853/6000 [40:33<4:22:13,  3.06s/it] 14%|â–ˆâ–        | 854/6000 [40:36<4:14:42,  2.97s/it]                                                    {'loss': 2.7932, 'grad_norm': 0.7900910377502441, 'learning_rate': 4.3610169491525426e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 854/6000 [40:36<4:14:42,  2.97s/it] 14%|â–ˆâ–        | 855/6000 [40:39<4:10:39,  2.92s/it]                                                    {'loss': 2.8454, 'grad_norm': 0.4872000515460968, 'learning_rate': 4.360169491525424e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 855/6000 [40:39<4:10:39,  2.92s/it] 14%|â–ˆâ–        | 856/6000 [40:42<4:07:20,  2.89s/it]                                                    {'loss': 2.8439, 'grad_norm': 0.7115940451622009, 'learning_rate': 4.3593220338983056e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 856/6000 [40:42<4:07:20,  2.89s/it] 14%|â–ˆâ–        | 857/6000 [40:44<4:01:41,  2.82s/it]                                                    {'loss': 2.8038, 'grad_norm': 0.36208054423332214, 'learning_rate': 4.358474576271187e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 857/6000 [40:44<4:01:41,  2.82s/it] 14%|â–ˆâ–        | 858/6000 [40:47<3:59:28,  2.79s/it]                                                    {'loss': 2.7719, 'grad_norm': 0.49831974506378174, 'learning_rate': 4.357627118644068e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 858/6000 [40:47<3:59:28,  2.79s/it] 14%|â–ˆâ–        | 859/6000 [40:50<3:58:14,  2.78s/it]                                                    {'loss': 2.7698, 'grad_norm': 0.6889443397521973, 'learning_rate': 4.356779661016949e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 859/6000 [40:50<3:58:14,  2.78s/it] 14%|â–ˆâ–        | 860/6000 [40:53<4:08:34,  2.90s/it]                                                    {'loss': 2.7757, 'grad_norm': 0.47363921999931335, 'learning_rate': 4.355932203389831e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 860/6000 [40:53<4:08:34,  2.90s/it] 14%|â–ˆâ–        | 861/6000 [40:56<4:03:09,  2.84s/it]                                                    {'loss': 2.7815, 'grad_norm': 0.8480150103569031, 'learning_rate': 4.355084745762712e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 861/6000 [40:56<4:03:09,  2.84s/it] 14%|â–ˆâ–        | 862/6000 [40:58<3:59:38,  2.80s/it]                                                    {'loss': 2.7749, 'grad_norm': 0.580554187297821, 'learning_rate': 4.354237288135594e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 862/6000 [40:58<3:59:38,  2.80s/it] 14%|â–ˆâ–        | 863/6000 [41:01<3:59:35,  2.80s/it]                                                    {'loss': 2.7875, 'grad_norm': 0.8158549070358276, 'learning_rate': 4.353389830508475e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 863/6000 [41:01<3:59:35,  2.80s/it] 14%|â–ˆâ–        | 864/6000 [41:04<3:56:35,  2.76s/it]                                                    {'loss': 2.774, 'grad_norm': 0.5393339991569519, 'learning_rate': 4.3525423728813566e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 864/6000 [41:04<3:56:35,  2.76s/it] 14%|â–ˆâ–        | 865/6000 [41:07<4:05:02,  2.86s/it]                                                    {'loss': 2.7775, 'grad_norm': 0.6617993116378784, 'learning_rate': 4.351694915254238e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 865/6000 [41:07<4:05:02,  2.86s/it] 14%|â–ˆâ–        | 866/6000 [41:10<4:01:29,  2.82s/it]                                                    {'loss': 2.7751, 'grad_norm': 1.9270800352096558, 'learning_rate': 4.350847457627119e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 866/6000 [41:10<4:01:29,  2.82s/it] 14%|â–ˆâ–        | 867/6000 [41:13<4:04:54,  2.86s/it]                                                    {'loss': 2.7875, 'grad_norm': 0.5322328805923462, 'learning_rate': 4.35e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 867/6000 [41:13<4:04:54,  2.86s/it] 14%|â–ˆâ–        | 868/6000 [41:15<3:58:57,  2.79s/it]                                                    {'loss': 2.7785, 'grad_norm': 0.423764705657959, 'learning_rate': 4.349152542372882e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 868/6000 [41:15<3:58:57,  2.79s/it] 14%|â–ˆâ–        | 869/6000 [41:18<3:58:24,  2.79s/it]                                                    {'loss': 2.7736, 'grad_norm': 0.5518534779548645, 'learning_rate': 4.348305084745763e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 869/6000 [41:18<3:58:24,  2.79s/it] 14%|â–ˆâ–        | 870/6000 [41:21<3:56:11,  2.76s/it]                                                    {'loss': 2.768, 'grad_norm': 0.44001173973083496, 'learning_rate': 4.347457627118644e-05, 'epoch': 0.14}
 14%|â–ˆâ–        | 870/6000 [41:21<3:56:11,  2.76s/it] 15%|â–ˆâ–        | 871/6000 [41:23<3:56:09,  2.76s/it]                                                    {'loss': 2.7945, 'grad_norm': 0.44808119535446167, 'learning_rate': 4.346610169491526e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 871/6000 [41:23<3:56:09,  2.76s/it] 15%|â–ˆâ–        | 872/6000 [41:26<3:55:14,  2.75s/it]                                                    {'loss': 2.7739, 'grad_norm': 0.5002517700195312, 'learning_rate': 4.345762711864407e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 872/6000 [41:26<3:55:14,  2.75s/it] 15%|â–ˆâ–        | 873/6000 [41:29<3:58:27,  2.79s/it]                                                    {'loss': 2.7808, 'grad_norm': 0.5358446836471558, 'learning_rate': 4.344915254237288e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 873/6000 [41:29<3:58:27,  2.79s/it] 15%|â–ˆâ–        | 874/6000 [41:32<3:57:34,  2.78s/it]                                                    {'loss': 2.9047, 'grad_norm': 0.5062493681907654, 'learning_rate': 4.344067796610169e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 874/6000 [41:32<3:57:34,  2.78s/it] 15%|â–ˆâ–        | 875/6000 [41:35<4:03:19,  2.85s/it]                                                    {'loss': 2.8041, 'grad_norm': 0.6931248307228088, 'learning_rate': 4.343220338983051e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 875/6000 [41:35<4:03:19,  2.85s/it] 15%|â–ˆâ–        | 876/6000 [41:38<4:00:26,  2.82s/it]                                                    {'loss': 2.7806, 'grad_norm': 0.5714884996414185, 'learning_rate': 4.342372881355932e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 876/6000 [41:38<4:00:26,  2.82s/it] 15%|â–ˆâ–        | 877/6000 [41:40<4:00:23,  2.82s/it]                                                    {'loss': 2.7727, 'grad_norm': 0.41538339853286743, 'learning_rate': 4.341525423728814e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 877/6000 [41:40<4:00:23,  2.82s/it] 15%|â–ˆâ–        | 878/6000 [41:43<4:00:45,  2.82s/it]                                                    {'loss': 2.7787, 'grad_norm': 0.701339840888977, 'learning_rate': 4.340677966101695e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 878/6000 [41:43<4:00:45,  2.82s/it] 15%|â–ˆâ–        | 879/6000 [41:46<4:13:36,  2.97s/it]                                                    {'loss': 2.7781, 'grad_norm': 0.8757963180541992, 'learning_rate': 4.339830508474576e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 879/6000 [41:47<4:13:36,  2.97s/it] 15%|â–ˆâ–        | 880/6000 [41:49<4:06:40,  2.89s/it]                                                    {'loss': 2.77, 'grad_norm': 0.6790735125541687, 'learning_rate': 4.3389830508474574e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 880/6000 [41:49<4:06:40,  2.89s/it] 15%|â–ˆâ–        | 881/6000 [41:52<4:01:14,  2.83s/it]                                                    {'loss': 2.7774, 'grad_norm': 0.7321061491966248, 'learning_rate': 4.338135593220339e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 881/6000 [41:52<4:01:14,  2.83s/it] 15%|â–ˆâ–        | 882/6000 [41:55<4:01:28,  2.83s/it]                                                    {'loss': 2.774, 'grad_norm': 0.48506593704223633, 'learning_rate': 4.3372881355932203e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 882/6000 [41:55<4:01:28,  2.83s/it] 15%|â–ˆâ–        | 883/6000 [41:57<3:59:13,  2.80s/it]                                                    {'loss': 2.7675, 'grad_norm': 1.0276246070861816, 'learning_rate': 4.336440677966102e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 883/6000 [41:57<3:59:13,  2.80s/it] 15%|â–ˆâ–        | 884/6000 [42:00<4:02:58,  2.85s/it]                                                    {'loss': 2.7764, 'grad_norm': 0.9656988382339478, 'learning_rate': 4.335593220338983e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 884/6000 [42:00<4:02:58,  2.85s/it] 15%|â–ˆâ–        | 885/6000 [42:03<4:01:30,  2.83s/it]                                                    {'loss': 2.776, 'grad_norm': 1.269247055053711, 'learning_rate': 4.334745762711865e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 885/6000 [42:03<4:01:30,  2.83s/it] 15%|â–ˆâ–        | 886/6000 [42:06<3:59:46,  2.81s/it]                                                    {'loss': 2.7661, 'grad_norm': 1.362943172454834, 'learning_rate': 4.333898305084746e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 886/6000 [42:06<3:59:46,  2.81s/it] 15%|â–ˆâ–        | 887/6000 [42:09<4:00:59,  2.83s/it]                                                    {'loss': 2.7645, 'grad_norm': 1.7971199750900269, 'learning_rate': 4.3330508474576273e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 887/6000 [42:09<4:00:59,  2.83s/it] 15%|â–ˆâ–        | 888/6000 [42:12<4:01:35,  2.84s/it]                                                    {'loss': 2.7961, 'grad_norm': 2.3896806240081787, 'learning_rate': 4.3322033898305085e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 888/6000 [42:12<4:01:35,  2.84s/it] 15%|â–ˆâ–        | 889/6000 [42:14<3:58:31,  2.80s/it]                                                    {'loss': 2.7697, 'grad_norm': 1.408945083618164, 'learning_rate': 4.33135593220339e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 889/6000 [42:14<3:58:31,  2.80s/it] 15%|â–ˆâ–        | 890/6000 [42:17<3:57:06,  2.78s/it]                                                    {'loss': 2.8158, 'grad_norm': 1.2130979299545288, 'learning_rate': 4.3305084745762714e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 890/6000 [42:17<3:57:06,  2.78s/it] 15%|â–ˆâ–        | 891/6000 [42:20<3:53:52,  2.75s/it]                                                    {'loss': 2.7694, 'grad_norm': 1.181631326675415, 'learning_rate': 4.3296610169491525e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 891/6000 [42:20<3:53:52,  2.75s/it] 15%|â–ˆâ–        | 892/6000 [42:23<3:53:04,  2.74s/it]                                                    {'loss': 2.7738, 'grad_norm': 1.5453345775604248, 'learning_rate': 4.3288135593220343e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 892/6000 [42:23<3:53:04,  2.74s/it] 15%|â–ˆâ–        | 893/6000 [42:25<3:51:50,  2.72s/it]                                                    {'loss': 2.7749, 'grad_norm': 1.8993741273880005, 'learning_rate': 4.3279661016949155e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 893/6000 [42:25<3:51:50,  2.72s/it] 15%|â–ˆâ–        | 894/6000 [42:28<3:53:12,  2.74s/it]                                                    {'loss': 2.772, 'grad_norm': 6.567131519317627, 'learning_rate': 4.3271186440677966e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 894/6000 [42:28<3:53:12,  2.74s/it] 15%|â–ˆâ–        | 895/6000 [42:31<3:51:06,  2.72s/it]                                                    {'loss': 2.7846, 'grad_norm': 2.8942458629608154, 'learning_rate': 4.326271186440678e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 895/6000 [42:31<3:51:06,  2.72s/it] 15%|â–ˆâ–        | 896/6000 [42:33<3:51:08,  2.72s/it]                                                    {'loss': 2.7472, 'grad_norm': 17.628305435180664, 'learning_rate': 4.3254237288135595e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 896/6000 [42:33<3:51:08,  2.72s/it] 15%|â–ˆâ–        | 897/6000 [42:36<3:51:23,  2.72s/it]                                                    {'loss': 2.7573, 'grad_norm': 7.243478775024414, 'learning_rate': 4.3245762711864407e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 897/6000 [42:36<3:51:23,  2.72s/it] 15%|â–ˆâ–        | 898/6000 [42:39<3:52:05,  2.73s/it]                                                    {'loss': 2.7953, 'grad_norm': 18.87813377380371, 'learning_rate': 4.3237288135593225e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 898/6000 [42:39<3:52:05,  2.73s/it] 15%|â–ˆâ–        | 899/6000 [42:42<4:14:10,  2.99s/it]                                                    {'loss': 2.7739, 'grad_norm': 3.9740793704986572, 'learning_rate': 4.3228813559322036e-05, 'epoch': 0.15}
 15%|â–ˆâ–        | 899/6000 [42:42<4:14:10,  2.99s/it] 15%|â–ˆâ–Œ        | 900/6000 [42:45<4:08:13,  2.92s/it]                                                    {'loss': 2.8275, 'grad_norm': 6.843653678894043, 'learning_rate': 4.3220338983050854e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 900/6000 [42:45<4:08:13,  2.92s/it][2025-10-22 19:39:10,730] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-900
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 15%|â–ˆâ–Œ        | 901/6000 [42:50<5:07:43,  3.62s/it]                                                    {'loss': 2.7832, 'grad_norm': 14.24360466003418, 'learning_rate': 4.321186440677966e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 901/6000 [42:50<5:07:43,  3.62s/it] 15%|â–ˆâ–Œ        | 902/6000 [42:53<4:42:31,  3.33s/it]                                                    {'loss': 2.766, 'grad_norm': 9.053647994995117, 'learning_rate': 4.3203389830508477e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 902/6000 [42:53<4:42:31,  3.33s/it] 15%|â–ˆâ–Œ        | 903/6000 [42:56<4:26:34,  3.14s/it]                                                    {'loss': 2.7605, 'grad_norm': 8.373892784118652, 'learning_rate': 4.319491525423729e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 903/6000 [42:56<4:26:34,  3.14s/it] 15%|â–ˆâ–Œ        | 904/6000 [42:58<4:14:49,  3.00s/it]                                                    {'loss': 2.8611, 'grad_norm': 15.01661205291748, 'learning_rate': 4.3186440677966106e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 904/6000 [42:58<4:14:49,  3.00s/it] 15%|â–ˆâ–Œ        | 905/6000 [43:02<4:22:45,  3.09s/it]                                                    {'loss': 2.7619, 'grad_norm': 7.239819049835205, 'learning_rate': 4.317796610169492e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 905/6000 [43:02<4:22:45,  3.09s/it] 15%|â–ˆâ–Œ        | 906/6000 [43:05<4:18:36,  3.05s/it]                                                    {'loss': 2.7748, 'grad_norm': 6.942611217498779, 'learning_rate': 4.3169491525423735e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 906/6000 [43:05<4:18:36,  3.05s/it] 15%|â–ˆâ–Œ        | 907/6000 [43:08<4:20:39,  3.07s/it]                                                    {'loss': 2.8634, 'grad_norm': 3.4169273376464844, 'learning_rate': 4.3161016949152547e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 907/6000 [43:08<4:20:39,  3.07s/it] 15%|â–ˆâ–Œ        | 908/6000 [43:11<4:21:04,  3.08s/it]                                                    {'loss': 2.7736, 'grad_norm': 4.858124732971191, 'learning_rate': 4.315254237288136e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 908/6000 [43:11<4:21:04,  3.08s/it] 15%|â–ˆâ–Œ        | 909/6000 [43:14<4:11:15,  2.96s/it]                                                    {'loss': 2.7699, 'grad_norm': 3.8420748710632324, 'learning_rate': 4.314406779661017e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 909/6000 [43:14<4:11:15,  2.96s/it] 15%|â–ˆâ–Œ        | 910/6000 [43:16<4:04:42,  2.88s/it]                                                    {'loss': 2.7589, 'grad_norm': 15.419034957885742, 'learning_rate': 4.313559322033899e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 910/6000 [43:16<4:04:42,  2.88s/it] 15%|â–ˆâ–Œ        | 911/6000 [43:19<4:02:05,  2.85s/it]                                                    {'loss': 2.831, 'grad_norm': 8.729878425598145, 'learning_rate': 4.31271186440678e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 911/6000 [43:19<4:02:05,  2.85s/it] 15%|â–ˆâ–Œ        | 912/6000 [43:22<3:57:59,  2.81s/it]                                                    {'loss': 2.7483, 'grad_norm': 5.177042007446289, 'learning_rate': 4.311864406779661e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 912/6000 [43:22<3:57:59,  2.81s/it] 15%|â–ˆâ–Œ        | 913/6000 [43:25<3:54:54,  2.77s/it]                                                    {'loss': 2.827, 'grad_norm': 16.341123580932617, 'learning_rate': 4.311016949152543e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 913/6000 [43:25<3:54:54,  2.77s/it] 15%|â–ˆâ–Œ        | 914/6000 [43:28<4:04:12,  2.88s/it]                                                    {'loss': 2.8622, 'grad_norm': 11.00308609008789, 'learning_rate': 4.310169491525424e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 914/6000 [43:28<4:04:12,  2.88s/it] 15%|â–ˆâ–Œ        | 915/6000 [43:31<4:24:22,  3.12s/it]                                                    {'loss': 2.7476, 'grad_norm': 8.57918930053711, 'learning_rate': 4.309322033898305e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 915/6000 [43:31<4:24:22,  3.12s/it] 15%|â–ˆâ–Œ        | 916/6000 [43:34<4:15:16,  3.01s/it]                                                    {'loss': 2.7892, 'grad_norm': 4.849067687988281, 'learning_rate': 4.308474576271186e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 916/6000 [43:34<4:15:16,  3.01s/it] 15%|â–ˆâ–Œ        | 917/6000 [43:37<4:06:17,  2.91s/it]                                                    {'loss': 2.7722, 'grad_norm': 4.179154872894287, 'learning_rate': 4.307627118644068e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 917/6000 [43:37<4:06:17,  2.91s/it] 15%|â–ˆâ–Œ        | 918/6000 [43:39<4:00:38,  2.84s/it]                                                    {'loss': 2.7458, 'grad_norm': 5.931394577026367, 'learning_rate': 4.306779661016949e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 918/6000 [43:39<4:00:38,  2.84s/it] 15%|â–ˆâ–Œ        | 919/6000 [43:42<3:59:14,  2.83s/it]                                                    {'loss': 2.82, 'grad_norm': 3.9600892066955566, 'learning_rate': 4.305932203389831e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 919/6000 [43:42<3:59:14,  2.83s/it] 15%|â–ˆâ–Œ        | 920/6000 [43:45<3:57:15,  2.80s/it]                                                    {'loss': 2.7923, 'grad_norm': 2.6892242431640625, 'learning_rate': 4.305084745762712e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 920/6000 [43:45<3:57:15,  2.80s/it] 15%|â–ˆâ–Œ        | 921/6000 [43:48<3:54:52,  2.77s/it]                                                    {'loss': 2.7702, 'grad_norm': 3.9151248931884766, 'learning_rate': 4.304237288135594e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 921/6000 [43:48<3:54:52,  2.77s/it] 15%|â–ˆâ–Œ        | 922/6000 [43:50<3:53:35,  2.76s/it]                                                    {'loss': 2.7413, 'grad_norm': 4.1102399826049805, 'learning_rate': 4.303389830508475e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 922/6000 [43:50<3:53:35,  2.76s/it] 15%|â–ˆâ–Œ        | 923/6000 [43:53<3:53:12,  2.76s/it]                                                    {'loss': 2.776, 'grad_norm': 2.36365008354187, 'learning_rate': 4.302542372881356e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 923/6000 [43:53<3:53:12,  2.76s/it] 15%|â–ˆâ–Œ        | 924/6000 [43:56<3:50:36,  2.73s/it]                                                    {'loss': 2.8095, 'grad_norm': 2.9266357421875, 'learning_rate': 4.301694915254237e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 924/6000 [43:56<3:50:36,  2.73s/it] 15%|â–ˆâ–Œ        | 925/6000 [43:59<3:51:01,  2.73s/it]                                                    {'loss': 2.7744, 'grad_norm': 3.6555235385894775, 'learning_rate': 4.300847457627119e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 925/6000 [43:59<3:51:01,  2.73s/it] 15%|â–ˆâ–Œ        | 926/6000 [44:02<3:58:45,  2.82s/it]                                                    {'loss': 2.8518, 'grad_norm': 1.7475512027740479, 'learning_rate': 4.3e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 926/6000 [44:02<3:58:45,  2.82s/it] 15%|â–ˆâ–Œ        | 927/6000 [44:04<3:55:57,  2.79s/it]                                                    {'loss': 2.7466, 'grad_norm': 3.0587337017059326, 'learning_rate': 4.299152542372882e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 927/6000 [44:04<3:55:57,  2.79s/it] 15%|â–ˆâ–Œ        | 928/6000 [44:07<3:55:26,  2.79s/it]                                                    {'loss': 2.7978, 'grad_norm': 3.395824670791626, 'learning_rate': 4.298305084745763e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 928/6000 [44:07<3:55:26,  2.79s/it] 15%|â–ˆâ–Œ        | 929/6000 [44:10<3:54:16,  2.77s/it]                                                    {'loss': 2.7841, 'grad_norm': 3.2814793586730957, 'learning_rate': 4.297457627118644e-05, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 929/6000 [44:10<3:54:16,  2.77s/it] 16%|â–ˆâ–Œ        | 930/6000 [44:13<3:52:06,  2.75s/it]                                                    {'loss': 2.7764, 'grad_norm': 2.018752336502075, 'learning_rate': 4.2966101694915254e-05, 'epoch': 0.15}
 16%|â–ˆâ–Œ        | 930/6000 [44:13<3:52:06,  2.75s/it] 16%|â–ˆâ–Œ        | 931/6000 [44:15<3:50:54,  2.73s/it]                                                    {'loss': 2.7866, 'grad_norm': 1.4486808776855469, 'learning_rate': 4.2957627118644065e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 931/6000 [44:15<3:50:54,  2.73s/it] 16%|â–ˆâ–Œ        | 932/6000 [44:18<3:49:13,  2.71s/it]                                                    {'loss': 2.7732, 'grad_norm': 1.9616535902023315, 'learning_rate': 4.294915254237288e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 932/6000 [44:18<3:49:13,  2.71s/it] 16%|â–ˆâ–Œ        | 933/6000 [44:21<3:50:11,  2.73s/it]                                                    {'loss': 2.7735, 'grad_norm': 2.0125441551208496, 'learning_rate': 4.2940677966101694e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 933/6000 [44:21<3:50:11,  2.73s/it] 16%|â–ˆâ–Œ        | 934/6000 [44:24<4:00:33,  2.85s/it]                                                    {'loss': 2.7696, 'grad_norm': 1.6225025653839111, 'learning_rate': 4.293220338983051e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 934/6000 [44:24<4:00:33,  2.85s/it] 16%|â–ˆâ–Œ        | 935/6000 [44:26<3:55:57,  2.80s/it]                                                    {'loss': 2.7926, 'grad_norm': 2.352883815765381, 'learning_rate': 4.2923728813559324e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 935/6000 [44:26<3:55:57,  2.80s/it] 16%|â–ˆâ–Œ        | 936/6000 [44:29<3:53:32,  2.77s/it]                                                    {'loss': 2.8222, 'grad_norm': 2.457383871078491, 'learning_rate': 4.291525423728814e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 936/6000 [44:29<3:53:32,  2.77s/it] 16%|â–ˆâ–Œ        | 937/6000 [44:32<3:52:16,  2.75s/it]                                                    {'loss': 2.8176, 'grad_norm': 2.8434698581695557, 'learning_rate': 4.2906779661016946e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 937/6000 [44:32<3:52:16,  2.75s/it] 16%|â–ˆâ–Œ        | 938/6000 [44:35<3:52:46,  2.76s/it]                                                    {'loss': 2.756, 'grad_norm': 3.5735936164855957, 'learning_rate': 4.2898305084745764e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 938/6000 [44:35<3:52:46,  2.76s/it] 16%|â–ˆâ–Œ        | 939/6000 [44:37<3:52:55,  2.76s/it]                                                    {'loss': 2.7698, 'grad_norm': 1.5086408853530884, 'learning_rate': 4.2889830508474575e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 939/6000 [44:37<3:52:55,  2.76s/it] 16%|â–ˆâ–Œ        | 940/6000 [44:40<3:54:00,  2.77s/it]                                                    {'loss': 2.7744, 'grad_norm': 4.0592851638793945, 'learning_rate': 4.2881355932203394e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 940/6000 [44:40<3:54:00,  2.77s/it] 16%|â–ˆâ–Œ        | 941/6000 [44:44<4:08:49,  2.95s/it]                                                    {'loss': 2.8, 'grad_norm': 3.014739513397217, 'learning_rate': 4.2872881355932205e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 941/6000 [44:44<4:08:49,  2.95s/it] 16%|â–ˆâ–Œ        | 942/6000 [44:46<4:02:41,  2.88s/it]                                                    {'loss': 2.7704, 'grad_norm': 2.2290544509887695, 'learning_rate': 4.286440677966102e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 942/6000 [44:46<4:02:41,  2.88s/it] 16%|â–ˆâ–Œ        | 943/6000 [44:49<3:57:20,  2.82s/it]                                                    {'loss': 2.7612, 'grad_norm': 1.8940653800964355, 'learning_rate': 4.2855932203389834e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 943/6000 [44:49<3:57:20,  2.82s/it] 16%|â–ˆâ–Œ        | 944/6000 [44:52<3:54:52,  2.79s/it]                                                    {'loss': 2.7619, 'grad_norm': 1.975832223892212, 'learning_rate': 4.2847457627118645e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 944/6000 [44:52<3:54:52,  2.79s/it] 16%|â–ˆâ–Œ        | 945/6000 [44:55<4:02:20,  2.88s/it]                                                    {'loss': 2.8097, 'grad_norm': 1.9625910520553589, 'learning_rate': 4.283898305084746e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 945/6000 [44:55<4:02:20,  2.88s/it] 16%|â–ˆâ–Œ        | 946/6000 [44:58<3:59:04,  2.84s/it]                                                    {'loss': 2.7704, 'grad_norm': 1.4904652833938599, 'learning_rate': 4.2830508474576275e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 946/6000 [44:58<3:59:04,  2.84s/it] 16%|â–ˆâ–Œ        | 947/6000 [45:00<3:57:17,  2.82s/it]                                                    {'loss': 2.7544, 'grad_norm': 2.611795663833618, 'learning_rate': 4.2822033898305086e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 947/6000 [45:00<3:57:17,  2.82s/it] 16%|â–ˆâ–Œ        | 948/6000 [45:03<3:59:18,  2.84s/it]                                                    {'loss': 2.7756, 'grad_norm': 1.3450777530670166, 'learning_rate': 4.2813559322033904e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 948/6000 [45:03<3:59:18,  2.84s/it] 16%|â–ˆâ–Œ        | 949/6000 [45:06<4:06:14,  2.92s/it]                                                    {'loss': 2.7899, 'grad_norm': 3.3174021244049072, 'learning_rate': 4.2805084745762715e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 949/6000 [45:06<4:06:14,  2.92s/it] 16%|â–ˆâ–Œ        | 950/6000 [45:09<4:05:24,  2.92s/it]                                                    {'loss': 2.754, 'grad_norm': 2.2508792877197266, 'learning_rate': 4.279661016949153e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 950/6000 [45:09<4:05:24,  2.92s/it][2025-10-22 19:41:34,703] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-950
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 16%|â–ˆâ–Œ        | 951/6000 [45:14<4:55:27,  3.51s/it]                                                    {'loss': 2.7778, 'grad_norm': 2.033514976501465, 'learning_rate': 4.278813559322034e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 951/6000 [45:14<4:55:27,  3.51s/it] 16%|â–ˆâ–Œ        | 952/6000 [45:17<4:34:51,  3.27s/it]                                                    {'loss': 2.7623, 'grad_norm': 1.8538340330123901, 'learning_rate': 4.277966101694915e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 952/6000 [45:17<4:34:51,  3.27s/it] 16%|â–ˆâ–Œ        | 953/6000 [45:20<4:24:52,  3.15s/it]                                                    {'loss': 2.7783, 'grad_norm': 4.1152191162109375, 'learning_rate': 4.277118644067797e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 953/6000 [45:20<4:24:52,  3.15s/it] 16%|â–ˆâ–Œ        | 954/6000 [45:22<4:14:19,  3.02s/it]                                                    {'loss': 2.7944, 'grad_norm': 3.31799054145813, 'learning_rate': 4.276271186440678e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 954/6000 [45:22<4:14:19,  3.02s/it] 16%|â–ˆâ–Œ        | 955/6000 [45:25<4:06:24,  2.93s/it]                                                    {'loss': 2.8151, 'grad_norm': 12.729077339172363, 'learning_rate': 4.27542372881356e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 955/6000 [45:25<4:06:24,  2.93s/it] 16%|â–ˆâ–Œ        | 956/6000 [45:28<4:05:24,  2.92s/it]                                                    {'loss': 2.7737, 'grad_norm': 19.67742347717285, 'learning_rate': 4.274576271186441e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 956/6000 [45:28<4:05:24,  2.92s/it] 16%|â–ˆâ–Œ        | 957/6000 [45:31<3:58:14,  2.83s/it]                                                    {'loss': 2.7789, 'grad_norm': 5.556328296661377, 'learning_rate': 4.2737288135593226e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 957/6000 [45:31<3:58:14,  2.83s/it] 16%|â–ˆâ–Œ        | 958/6000 [45:33<3:58:26,  2.84s/it]                                                    {'loss': 2.8172, 'grad_norm': 2.527367115020752, 'learning_rate': 4.272881355932204e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 958/6000 [45:33<3:58:26,  2.84s/it] 16%|â–ˆâ–Œ        | 959/6000 [45:36<3:53:27,  2.78s/it]                                                    {'loss': 2.7792, 'grad_norm': 3.841798782348633, 'learning_rate': 4.272033898305085e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 959/6000 [45:36<3:53:27,  2.78s/it] 16%|â–ˆâ–Œ        | 960/6000 [45:39<3:51:45,  2.76s/it]                                                    {'loss': 2.7847, 'grad_norm': 2.041396379470825, 'learning_rate': 4.271186440677966e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 960/6000 [45:39<3:51:45,  2.76s/it] 16%|â–ˆâ–Œ        | 961/6000 [45:42<3:53:32,  2.78s/it]                                                    {'loss': 2.9436, 'grad_norm': 2.953502655029297, 'learning_rate': 4.270338983050848e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 961/6000 [45:42<3:53:32,  2.78s/it] 16%|â–ˆâ–Œ        | 962/6000 [45:45<4:01:36,  2.88s/it]                                                    {'loss': 2.7672, 'grad_norm': 4.042131423950195, 'learning_rate': 4.269491525423729e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 962/6000 [45:45<4:01:36,  2.88s/it] 16%|â–ˆâ–Œ        | 963/6000 [45:48<3:59:22,  2.85s/it]                                                    {'loss': 2.8411, 'grad_norm': 6.624642848968506, 'learning_rate': 4.268644067796611e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 963/6000 [45:48<3:59:22,  2.85s/it] 16%|â–ˆâ–Œ        | 964/6000 [45:50<3:56:45,  2.82s/it]                                                    {'loss': 2.797, 'grad_norm': 2.3311755657196045, 'learning_rate': 4.267796610169492e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 964/6000 [45:50<3:56:45,  2.82s/it] 16%|â–ˆâ–Œ        | 965/6000 [45:53<3:58:39,  2.84s/it]                                                    {'loss': 2.7693, 'grad_norm': 4.044867038726807, 'learning_rate': 4.266949152542373e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 965/6000 [45:53<3:58:39,  2.84s/it] 16%|â–ˆâ–Œ        | 966/6000 [45:56<3:57:55,  2.84s/it]                                                    {'loss': 2.77, 'grad_norm': 2.3518574237823486, 'learning_rate': 4.266101694915254e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 966/6000 [45:56<3:57:55,  2.84s/it] 16%|â–ˆâ–Œ        | 967/6000 [45:59<3:57:25,  2.83s/it]                                                    {'loss': 2.7617, 'grad_norm': 3.195091962814331, 'learning_rate': 4.265254237288136e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 967/6000 [45:59<3:57:25,  2.83s/it] 16%|â–ˆâ–Œ        | 968/6000 [46:02<4:03:32,  2.90s/it]                                                    {'loss': 2.8015, 'grad_norm': 2.764594078063965, 'learning_rate': 4.264406779661017e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 968/6000 [46:02<4:03:32,  2.90s/it] 16%|â–ˆâ–Œ        | 969/6000 [46:05<3:59:04,  2.85s/it]                                                    {'loss': 2.7583, 'grad_norm': 6.591379642486572, 'learning_rate': 4.263559322033899e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 969/6000 [46:05<3:59:04,  2.85s/it] 16%|â–ˆâ–Œ        | 970/6000 [46:08<3:59:26,  2.86s/it]                                                    {'loss': 2.7897, 'grad_norm': 7.729696273803711, 'learning_rate': 4.26271186440678e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 970/6000 [46:08<3:59:26,  2.86s/it] 16%|â–ˆâ–Œ        | 971/6000 [46:10<3:54:28,  2.80s/it]                                                    {'loss': 2.7834, 'grad_norm': 5.792545795440674, 'learning_rate': 4.261864406779662e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 971/6000 [46:10<3:54:28,  2.80s/it] 16%|â–ˆâ–Œ        | 972/6000 [46:13<3:52:50,  2.78s/it]                                                    {'loss': 2.7509, 'grad_norm': 4.240123748779297, 'learning_rate': 4.261016949152542e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 972/6000 [46:13<3:52:50,  2.78s/it] 16%|â–ˆâ–Œ        | 973/6000 [46:16<3:52:19,  2.77s/it]                                                    {'loss': 2.7371, 'grad_norm': 10.891974449157715, 'learning_rate': 4.2601694915254234e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 973/6000 [46:16<3:52:19,  2.77s/it] 16%|â–ˆâ–Œ        | 974/6000 [46:18<3:53:18,  2.79s/it]                                                    {'loss': 2.9395, 'grad_norm': 7.300453186035156, 'learning_rate': 4.259322033898305e-05, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 974/6000 [46:18<3:53:18,  2.79s/it] 16%|â–ˆâ–‹        | 975/6000 [46:21<3:51:07,  2.76s/it]                                                    {'loss': 2.8095, 'grad_norm': 10.419678688049316, 'learning_rate': 4.258474576271186e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 975/6000 [46:21<3:51:07,  2.76s/it] 16%|â–ˆâ–‹        | 976/6000 [46:24<3:52:18,  2.77s/it]                                                    {'loss': 2.784, 'grad_norm': 15.872644424438477, 'learning_rate': 4.257627118644068e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 976/6000 [46:24<3:52:18,  2.77s/it] 16%|â–ˆâ–‹        | 977/6000 [46:27<3:49:11,  2.74s/it]                                                    {'loss': 2.8702, 'grad_norm': 37.756248474121094, 'learning_rate': 4.256779661016949e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 977/6000 [46:27<3:49:11,  2.74s/it] 16%|â–ˆâ–‹        | 978/6000 [46:29<3:46:34,  2.71s/it]                                                    {'loss': 2.7772, 'grad_norm': 9.979737281799316, 'learning_rate': 4.255932203389831e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 978/6000 [46:29<3:46:34,  2.71s/it] 16%|â–ˆâ–‹        | 979/6000 [46:32<3:47:41,  2.72s/it]                                                    {'loss': 2.7581, 'grad_norm': 4.223824501037598, 'learning_rate': 4.255084745762712e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 979/6000 [46:32<3:47:41,  2.72s/it] 16%|â–ˆâ–‹        | 980/6000 [46:35<3:50:26,  2.75s/it]                                                    {'loss': 2.7848, 'grad_norm': 17.57581329345703, 'learning_rate': 4.254237288135593e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 980/6000 [46:35<3:50:26,  2.75s/it] 16%|â–ˆâ–‹        | 981/6000 [46:38<3:49:18,  2.74s/it]                                                    {'loss': 2.8135, 'grad_norm': 18.86954116821289, 'learning_rate': 4.2533898305084744e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 981/6000 [46:38<3:49:18,  2.74s/it] 16%|â–ˆâ–‹        | 982/6000 [46:40<3:50:07,  2.75s/it]                                                    {'loss': 2.7753, 'grad_norm': 4.7915778160095215, 'learning_rate': 4.252542372881356e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 982/6000 [46:40<3:50:07,  2.75s/it] 16%|â–ˆâ–‹        | 983/6000 [46:43<3:50:43,  2.76s/it]                                                    {'loss': 2.7827, 'grad_norm': 3.794843912124634, 'learning_rate': 4.2516949152542374e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 983/6000 [46:43<3:50:43,  2.76s/it] 16%|â–ˆâ–‹        | 984/6000 [46:46<3:50:56,  2.76s/it]                                                    {'loss': 2.7867, 'grad_norm': 3.2971811294555664, 'learning_rate': 4.250847457627119e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 984/6000 [46:46<3:50:56,  2.76s/it] 16%|â–ˆâ–‹        | 985/6000 [46:49<3:54:29,  2.81s/it]                                                    {'loss': 2.7764, 'grad_norm': 2.6286585330963135, 'learning_rate': 4.25e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 985/6000 [46:49<3:54:29,  2.81s/it] 16%|â–ˆâ–‹        | 986/6000 [46:51<3:50:30,  2.76s/it]                                                    {'loss': 2.7571, 'grad_norm': 4.595006942749023, 'learning_rate': 4.2491525423728814e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 986/6000 [46:51<3:50:30,  2.76s/it] 16%|â–ˆâ–‹        | 987/6000 [46:54<3:53:46,  2.80s/it]                                                    {'loss': 2.7529, 'grad_norm': 3.4533684253692627, 'learning_rate': 4.2483050847457626e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 987/6000 [46:54<3:53:46,  2.80s/it] 16%|â–ˆâ–‹        | 988/6000 [46:57<3:50:23,  2.76s/it]                                                    {'loss': 2.8079, 'grad_norm': 6.65673303604126, 'learning_rate': 4.2474576271186444e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 988/6000 [46:57<3:50:23,  2.76s/it] 16%|â–ˆâ–‹        | 989/6000 [47:00<3:48:37,  2.74s/it]                                                    {'loss': 2.7679, 'grad_norm': 5.24522590637207, 'learning_rate': 4.2466101694915255e-05, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 989/6000 [47:00<3:48:37,  2.74s/it] 16%|â–ˆâ–‹        | 990/6000 [47:02<3:47:50,  2.73s/it]                                                    {'loss': 2.7969, 'grad_norm': 11.040472030639648, 'learning_rate': 4.245762711864407e-05, 'epoch': 0.17}
 16%|â–ˆâ–‹        | 990/6000 [47:02<3:47:50,  2.73s/it] 17%|â–ˆâ–‹        | 991/6000 [47:06<4:00:21,  2.88s/it]                                                    {'loss': 2.7978, 'grad_norm': 6.245473384857178, 'learning_rate': 4.2449152542372884e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 991/6000 [47:06<4:00:21,  2.88s/it] 17%|â–ˆâ–‹        | 992/6000 [47:08<3:55:35,  2.82s/it]                                                    {'loss': 2.749, 'grad_norm': 8.11839485168457, 'learning_rate': 4.24406779661017e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 992/6000 [47:08<3:55:35,  2.82s/it] 17%|â–ˆâ–‹        | 993/6000 [47:11<3:56:28,  2.83s/it]                                                    {'loss': 2.8199, 'grad_norm': 8.507365226745605, 'learning_rate': 4.2432203389830514e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 993/6000 [47:11<3:56:28,  2.83s/it] 17%|â–ˆâ–‹        | 994/6000 [47:14<3:56:08,  2.83s/it]                                                    {'loss': 2.8087, 'grad_norm': 11.028989791870117, 'learning_rate': 4.242372881355932e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 994/6000 [47:14<3:56:08,  2.83s/it] 17%|â–ˆâ–‹        | 995/6000 [47:17<3:53:05,  2.79s/it]                                                    {'loss': 2.7426, 'grad_norm': 5.384196758270264, 'learning_rate': 4.2415254237288136e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 995/6000 [47:17<3:53:05,  2.79s/it] 17%|â–ˆâ–‹        | 996/6000 [47:19<3:51:22,  2.77s/it]                                                    {'loss': 2.8091, 'grad_norm': 8.393349647521973, 'learning_rate': 4.240677966101695e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 996/6000 [47:19<3:51:22,  2.77s/it] 17%|â–ˆâ–‹        | 997/6000 [47:22<3:53:05,  2.80s/it]                                                    {'loss': 2.766, 'grad_norm': 5.797824382781982, 'learning_rate': 4.2398305084745766e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 997/6000 [47:22<3:53:05,  2.80s/it] 17%|â–ˆâ–‹        | 998/6000 [47:25<3:49:54,  2.76s/it]                                                    {'loss': 2.7725, 'grad_norm': 5.93826150894165, 'learning_rate': 4.238983050847458e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 998/6000 [47:25<3:49:54,  2.76s/it] 17%|â–ˆâ–‹        | 999/6000 [47:28<3:48:52,  2.75s/it]                                                    {'loss': 2.7477, 'grad_norm': 4.974730968475342, 'learning_rate': 4.2381355932203395e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 999/6000 [47:28<3:48:52,  2.75s/it] 17%|â–ˆâ–‹        | 1000/6000 [47:30<3:49:29,  2.75s/it]                                                     {'loss': 2.8107, 'grad_norm': 22.5493106842041, 'learning_rate': 4.2372881355932206e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1000/6000 [47:30<3:49:29,  2.75s/it][2025-10-22 19:43:55,971] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1000
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 17%|â–ˆâ–‹        | 1001/6000 [47:35<4:43:15,  3.40s/it]                                                     {'loss': 2.7795, 'grad_norm': 22.551536560058594, 'learning_rate': 4.236440677966102e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1001/6000 [47:35<4:43:15,  3.40s/it] 17%|â–ˆâ–‹        | 1002/6000 [47:38<4:27:10,  3.21s/it]                                                     {'loss': 2.8006, 'grad_norm': 8.009180068969727, 'learning_rate': 4.235593220338983e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1002/6000 [47:38<4:27:10,  3.21s/it] 17%|â–ˆâ–‹        | 1003/6000 [47:41<4:13:10,  3.04s/it]                                                     {'loss': 2.7594, 'grad_norm': 9.162373542785645, 'learning_rate': 4.234745762711865e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1003/6000 [47:41<4:13:10,  3.04s/it] 17%|â–ˆâ–‹        | 1004/6000 [47:43<4:04:11,  2.93s/it]                                                     {'loss': 2.7799, 'grad_norm': 4.61358642578125, 'learning_rate': 4.233898305084746e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1004/6000 [47:43<4:04:11,  2.93s/it] 17%|â–ˆâ–‹        | 1005/6000 [47:46<4:00:58,  2.89s/it]                                                     {'loss': 2.7589, 'grad_norm': 3.1130197048187256, 'learning_rate': 4.2330508474576276e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1005/6000 [47:46<4:00:58,  2.89s/it] 17%|â–ˆâ–‹        | 1006/6000 [47:49<3:56:26,  2.84s/it]                                                     {'loss': 2.7366, 'grad_norm': 4.91188383102417, 'learning_rate': 4.232203389830509e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1006/6000 [47:49<3:56:26,  2.84s/it] 17%|â–ˆâ–‹        | 1007/6000 [47:52<3:52:23,  2.79s/it]                                                     {'loss': 2.7507, 'grad_norm': 2.86206316947937, 'learning_rate': 4.23135593220339e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1007/6000 [47:52<3:52:23,  2.79s/it] 17%|â–ˆâ–‹        | 1008/6000 [47:54<3:50:44,  2.77s/it]                                                     {'loss': 2.7808, 'grad_norm': 2.8281657695770264, 'learning_rate': 4.230508474576271e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1008/6000 [47:54<3:50:44,  2.77s/it] 17%|â–ˆâ–‹        | 1009/6000 [47:57<3:50:34,  2.77s/it]                                                     {'loss': 2.7865, 'grad_norm': 3.5244507789611816, 'learning_rate': 4.229661016949153e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1009/6000 [47:57<3:50:34,  2.77s/it] 17%|â–ˆâ–‹        | 1010/6000 [48:00<3:52:38,  2.80s/it]                                                     {'loss': 2.7835, 'grad_norm': 3.1273653507232666, 'learning_rate': 4.228813559322034e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1010/6000 [48:00<3:52:38,  2.80s/it] 17%|â–ˆâ–‹        | 1011/6000 [48:03<3:51:47,  2.79s/it]                                                     {'loss': 2.7625, 'grad_norm': 3.271198272705078, 'learning_rate': 4.227966101694916e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1011/6000 [48:03<3:51:47,  2.79s/it] 17%|â–ˆâ–‹        | 1012/6000 [48:06<3:53:30,  2.81s/it]                                                     {'loss': 2.8406, 'grad_norm': 4.080060005187988, 'learning_rate': 4.227118644067797e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1012/6000 [48:06<3:53:30,  2.81s/it] 17%|â–ˆâ–‹        | 1013/6000 [48:08<3:51:25,  2.78s/it]                                                     {'loss': 2.7456, 'grad_norm': 3.5419564247131348, 'learning_rate': 4.226271186440679e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1013/6000 [48:08<3:51:25,  2.78s/it] 17%|â–ˆâ–‹        | 1014/6000 [48:11<3:49:37,  2.76s/it]                                                     {'loss': 2.7323, 'grad_norm': 6.912781715393066, 'learning_rate': 4.22542372881356e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1014/6000 [48:11<3:49:37,  2.76s/it] 17%|â–ˆâ–‹        | 1015/6000 [48:14<3:49:31,  2.76s/it]                                                     {'loss': 2.7762, 'grad_norm': 3.9497251510620117, 'learning_rate': 4.224576271186441e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1015/6000 [48:14<3:49:31,  2.76s/it] 17%|â–ˆâ–‹        | 1016/6000 [48:17<3:48:58,  2.76s/it]                                                     {'loss': 2.8625, 'grad_norm': 5.944988250732422, 'learning_rate': 4.223728813559322e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1016/6000 [48:17<3:48:58,  2.76s/it] 17%|â–ˆâ–‹        | 1017/6000 [48:19<3:50:49,  2.78s/it]                                                     {'loss': 2.7999, 'grad_norm': 6.3420186042785645, 'learning_rate': 4.222881355932203e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1017/6000 [48:19<3:50:49,  2.78s/it] 17%|â–ˆâ–‹        | 1018/6000 [48:22<3:50:18,  2.77s/it]                                                     {'loss': 2.73, 'grad_norm': 4.905149936676025, 'learning_rate': 4.222033898305085e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1018/6000 [48:22<3:50:18,  2.77s/it] 17%|â–ˆâ–‹        | 1019/6000 [48:25<3:48:15,  2.75s/it]                                                     {'loss': 2.7938, 'grad_norm': 5.771580696105957, 'learning_rate': 4.221186440677966e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1019/6000 [48:25<3:48:15,  2.75s/it] 17%|â–ˆâ–‹        | 1020/6000 [48:28<3:47:20,  2.74s/it]                                                     {'loss': 2.7573, 'grad_norm': 4.056385040283203, 'learning_rate': 4.220338983050848e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1020/6000 [48:28<3:47:20,  2.74s/it] 17%|â–ˆâ–‹        | 1021/6000 [48:30<3:46:54,  2.73s/it]                                                     {'loss': 2.7895, 'grad_norm': 3.393866777420044, 'learning_rate': 4.219491525423729e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1021/6000 [48:30<3:46:54,  2.73s/it] 17%|â–ˆâ–‹        | 1022/6000 [48:33<3:51:09,  2.79s/it]                                                     {'loss': 2.7546, 'grad_norm': 3.2528765201568604, 'learning_rate': 4.21864406779661e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1022/6000 [48:33<3:51:09,  2.79s/it] 17%|â–ˆâ–‹        | 1023/6000 [48:36<3:48:48,  2.76s/it]                                                     {'loss': 2.7944, 'grad_norm': 4.139530181884766, 'learning_rate': 4.217796610169491e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1023/6000 [48:36<3:48:48,  2.76s/it] 17%|â–ˆâ–‹        | 1024/6000 [48:39<3:52:26,  2.80s/it]                                                     {'loss': 2.7411, 'grad_norm': 4.214812755584717, 'learning_rate': 4.216949152542373e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1024/6000 [48:39<3:52:26,  2.80s/it] 17%|â–ˆâ–‹        | 1025/6000 [48:41<3:48:43,  2.76s/it]                                                     {'loss': 2.7796, 'grad_norm': 4.896056175231934, 'learning_rate': 4.216101694915254e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1025/6000 [48:41<3:48:43,  2.76s/it] 17%|â–ˆâ–‹        | 1026/6000 [48:44<3:47:44,  2.75s/it]                                                     {'loss': 2.7743, 'grad_norm': 2.720118761062622, 'learning_rate': 4.215254237288136e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1026/6000 [48:44<3:47:44,  2.75s/it] 17%|â–ˆâ–‹        | 1027/6000 [48:47<3:46:25,  2.73s/it]                                                     {'loss': 2.7367, 'grad_norm': 3.4485530853271484, 'learning_rate': 4.214406779661017e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1027/6000 [48:47<3:46:25,  2.73s/it] 17%|â–ˆâ–‹        | 1028/6000 [48:50<3:44:41,  2.71s/it]                                                     {'loss': 2.7904, 'grad_norm': 3.6375627517700195, 'learning_rate': 4.213559322033899e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1028/6000 [48:50<3:44:41,  2.71s/it] 17%|â–ˆâ–‹        | 1029/6000 [48:52<3:43:58,  2.70s/it]                                                     {'loss': 2.7572, 'grad_norm': 5.72133207321167, 'learning_rate': 4.2127118644067795e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1029/6000 [48:52<3:43:58,  2.70s/it] 17%|â–ˆâ–‹        | 1030/6000 [48:55<3:43:34,  2.70s/it]                                                     {'loss': 2.7821, 'grad_norm': 6.726931571960449, 'learning_rate': 4.211864406779661e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1030/6000 [48:55<3:43:34,  2.70s/it] 17%|â–ˆâ–‹        | 1031/6000 [48:58<3:43:49,  2.70s/it]                                                     {'loss': 2.9639, 'grad_norm': 15.621520042419434, 'learning_rate': 4.2110169491525424e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1031/6000 [48:58<3:43:49,  2.70s/it] 17%|â–ˆâ–‹        | 1032/6000 [49:00<3:45:02,  2.72s/it]                                                     {'loss': 2.7006, 'grad_norm': 11.810895919799805, 'learning_rate': 4.210169491525424e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1032/6000 [49:00<3:45:02,  2.72s/it] 17%|â–ˆâ–‹        | 1033/6000 [49:03<3:43:40,  2.70s/it]                                                     {'loss': 2.712, 'grad_norm': 38.49690628051758, 'learning_rate': 4.209322033898305e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1033/6000 [49:03<3:43:40,  2.70s/it] 17%|â–ˆâ–‹        | 1034/6000 [49:06<3:43:46,  2.70s/it]                                                     {'loss': 2.7535, 'grad_norm': 6.504467010498047, 'learning_rate': 4.208474576271187e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1034/6000 [49:06<3:43:46,  2.70s/it] 17%|â–ˆâ–‹        | 1035/6000 [49:09<3:45:53,  2.73s/it]                                                     {'loss': 2.7692, 'grad_norm': 3.772935628890991, 'learning_rate': 4.207627118644068e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1035/6000 [49:09<3:45:53,  2.73s/it] 17%|â–ˆâ–‹        | 1036/6000 [49:11<3:46:17,  2.74s/it]                                                     {'loss': 2.8269, 'grad_norm': 8.071627616882324, 'learning_rate': 4.2067796610169494e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1036/6000 [49:11<3:46:17,  2.74s/it] 17%|â–ˆâ–‹        | 1037/6000 [49:14<3:46:40,  2.74s/it]                                                     {'loss': 2.8573, 'grad_norm': 4.702587604522705, 'learning_rate': 4.2059322033898305e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1037/6000 [49:14<3:46:40,  2.74s/it] 17%|â–ˆâ–‹        | 1038/6000 [49:17<3:55:29,  2.85s/it]                                                     {'loss': 2.8054, 'grad_norm': 4.664961338043213, 'learning_rate': 4.2050847457627116e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1038/6000 [49:17<3:55:29,  2.85s/it] 17%|â–ˆâ–‹        | 1039/6000 [49:20<3:54:43,  2.84s/it]                                                     {'loss': 2.8494, 'grad_norm': 4.5483479499816895, 'learning_rate': 4.2042372881355934e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1039/6000 [49:20<3:54:43,  2.84s/it] 17%|â–ˆâ–‹        | 1040/6000 [49:23<3:54:56,  2.84s/it]                                                     {'loss': 2.7726, 'grad_norm': 4.045321464538574, 'learning_rate': 4.2033898305084746e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1040/6000 [49:23<3:54:56,  2.84s/it] 17%|â–ˆâ–‹        | 1041/6000 [49:25<3:51:03,  2.80s/it]                                                     {'loss': 2.7472, 'grad_norm': 3.8421123027801514, 'learning_rate': 4.2025423728813564e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1041/6000 [49:26<3:51:03,  2.80s/it] 17%|â–ˆâ–‹        | 1042/6000 [49:28<3:49:33,  2.78s/it]                                                     {'loss': 2.7747, 'grad_norm': 2.6892054080963135, 'learning_rate': 4.2016949152542375e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1042/6000 [49:28<3:49:33,  2.78s/it] 17%|â–ˆâ–‹        | 1043/6000 [49:31<3:47:32,  2.75s/it]                                                     {'loss': 2.7934, 'grad_norm': 2.3224527835845947, 'learning_rate': 4.2008474576271186e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1043/6000 [49:31<3:47:32,  2.75s/it] 17%|â–ˆâ–‹        | 1044/6000 [49:34<3:46:31,  2.74s/it]                                                     {'loss': 2.8159, 'grad_norm': 11.002859115600586, 'learning_rate': 4.2e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1044/6000 [49:34<3:46:31,  2.74s/it] 17%|â–ˆâ–‹        | 1045/6000 [49:36<3:48:54,  2.77s/it]                                                     {'loss': 2.8197, 'grad_norm': 7.866257667541504, 'learning_rate': 4.1991525423728816e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1045/6000 [49:36<3:48:54,  2.77s/it] 17%|â–ˆâ–‹        | 1046/6000 [49:39<3:48:53,  2.77s/it]                                                     {'loss': 2.7864, 'grad_norm': 5.192139625549316, 'learning_rate': 4.198305084745763e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1046/6000 [49:39<3:48:53,  2.77s/it] 17%|â–ˆâ–‹        | 1047/6000 [49:42<3:57:21,  2.88s/it]                                                     {'loss': 2.839, 'grad_norm': 4.266461372375488, 'learning_rate': 4.1974576271186445e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1047/6000 [49:42<3:57:21,  2.88s/it] 17%|â–ˆâ–‹        | 1048/6000 [49:45<3:53:20,  2.83s/it]                                                     {'loss': 2.7903, 'grad_norm': 2.865675449371338, 'learning_rate': 4.1966101694915256e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1048/6000 [49:45<3:53:20,  2.83s/it] 17%|â–ˆâ–‹        | 1049/6000 [49:48<3:51:02,  2.80s/it]                                                     {'loss': 2.8238, 'grad_norm': 4.304464340209961, 'learning_rate': 4.1957627118644074e-05, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1049/6000 [49:48<3:51:02,  2.80s/it] 18%|â–ˆâ–Š        | 1050/6000 [49:51<4:06:51,  2.99s/it]                                                     {'loss': 2.767, 'grad_norm': 2.7494122982025146, 'learning_rate': 4.1949152542372886e-05, 'epoch': 0.17}
 18%|â–ˆâ–Š        | 1050/6000 [49:51<4:06:51,  2.99s/it][2025-10-22 19:46:16,778] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1050
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1051/6000 [49:56<4:55:02,  3.58s/it]                                                     {'loss': 2.7978, 'grad_norm': 2.463688611984253, 'learning_rate': 4.19406779661017e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1051/6000 [49:56<4:55:02,  3.58s/it] 18%|â–ˆâ–Š        | 1052/6000 [49:59<4:34:49,  3.33s/it]                                                     {'loss': 2.7682, 'grad_norm': 3.187601089477539, 'learning_rate': 4.193220338983051e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1052/6000 [49:59<4:34:49,  3.33s/it] 18%|â–ˆâ–Š        | 1053/6000 [50:02<4:27:20,  3.24s/it]                                                     {'loss': 2.7902, 'grad_norm': 2.607553005218506, 'learning_rate': 4.1923728813559326e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1053/6000 [50:02<4:27:20,  3.24s/it] 18%|â–ˆâ–Š        | 1054/6000 [50:05<4:16:46,  3.11s/it]                                                     {'loss': 2.7711, 'grad_norm': 2.0515384674072266, 'learning_rate': 4.191525423728814e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1054/6000 [50:05<4:16:46,  3.11s/it] 18%|â–ˆâ–Š        | 1055/6000 [50:08<4:06:18,  2.99s/it]                                                     {'loss': 2.7863, 'grad_norm': 2.1403331756591797, 'learning_rate': 4.1906779661016956e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1055/6000 [50:08<4:06:18,  2.99s/it] 18%|â–ˆâ–Š        | 1056/6000 [50:10<4:00:22,  2.92s/it]                                                     {'loss': 2.7759, 'grad_norm': 1.329533576965332, 'learning_rate': 4.189830508474577e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1056/6000 [50:10<4:00:22,  2.92s/it] 18%|â–ˆâ–Š        | 1057/6000 [50:13<3:54:42,  2.85s/it]                                                     {'loss': 2.7758, 'grad_norm': 1.6179840564727783, 'learning_rate': 4.188983050847458e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1057/6000 [50:13<3:54:42,  2.85s/it] 18%|â–ˆâ–Š        | 1058/6000 [50:16<3:50:23,  2.80s/it]                                                     {'loss': 2.7764, 'grad_norm': 2.445544481277466, 'learning_rate': 4.188135593220339e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1058/6000 [50:16<3:50:23,  2.80s/it] 18%|â–ˆâ–Š        | 1059/6000 [50:18<3:48:39,  2.78s/it]                                                     {'loss': 2.8051, 'grad_norm': 3.250060558319092, 'learning_rate': 4.18728813559322e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1059/6000 [50:18<3:48:39,  2.78s/it] 18%|â–ˆâ–Š        | 1060/6000 [50:21<3:47:01,  2.76s/it]                                                     {'loss': 2.7717, 'grad_norm': 3.2190675735473633, 'learning_rate': 4.186440677966102e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1060/6000 [50:21<3:47:01,  2.76s/it] 18%|â–ˆâ–Š        | 1061/6000 [50:24<3:56:13,  2.87s/it]                                                     {'loss': 2.7857, 'grad_norm': 5.38831901550293, 'learning_rate': 4.185593220338983e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1061/6000 [50:24<3:56:13,  2.87s/it] 18%|â–ˆâ–Š        | 1062/6000 [50:27<3:54:10,  2.85s/it]                                                     {'loss': 2.8051, 'grad_norm': 2.5945041179656982, 'learning_rate': 4.184745762711865e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1062/6000 [50:27<3:54:10,  2.85s/it] 18%|â–ˆâ–Š        | 1063/6000 [50:30<3:51:02,  2.81s/it]                                                     {'loss': 2.78, 'grad_norm': 2.4934966564178467, 'learning_rate': 4.183898305084746e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1063/6000 [50:30<3:51:02,  2.81s/it] 18%|â–ˆâ–Š        | 1064/6000 [50:33<3:50:49,  2.81s/it]                                                     {'loss': 2.8013, 'grad_norm': 1.8942357301712036, 'learning_rate': 4.183050847457628e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1064/6000 [50:33<3:50:49,  2.81s/it] 18%|â–ˆâ–Š        | 1065/6000 [50:35<3:47:39,  2.77s/it]                                                     {'loss': 2.777, 'grad_norm': 2.636502265930176, 'learning_rate': 4.182203389830508e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1065/6000 [50:35<3:47:39,  2.77s/it] 18%|â–ˆâ–Š        | 1066/6000 [50:38<3:48:36,  2.78s/it]                                                     {'loss': 2.7998, 'grad_norm': 2.9380059242248535, 'learning_rate': 4.18135593220339e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1066/6000 [50:38<3:48:36,  2.78s/it] 18%|â–ˆâ–Š        | 1067/6000 [50:41<3:48:18,  2.78s/it]                                                     {'loss': 2.8059, 'grad_norm': 4.264843463897705, 'learning_rate': 4.180508474576271e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1067/6000 [50:41<3:48:18,  2.78s/it] 18%|â–ˆâ–Š        | 1068/6000 [50:43<3:46:34,  2.76s/it]                                                     {'loss': 2.7802, 'grad_norm': 2.156928777694702, 'learning_rate': 4.179661016949153e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1068/6000 [50:43<3:46:34,  2.76s/it] 18%|â–ˆâ–Š        | 1069/6000 [50:46<3:44:32,  2.73s/it]                                                     {'loss': 2.7633, 'grad_norm': 2.9504523277282715, 'learning_rate': 4.178813559322034e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1069/6000 [50:46<3:44:32,  2.73s/it] 18%|â–ˆâ–Š        | 1070/6000 [50:49<3:43:53,  2.72s/it]                                                     {'loss': 2.764, 'grad_norm': 3.8650918006896973, 'learning_rate': 4.177966101694916e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1070/6000 [50:49<3:43:53,  2.72s/it] 18%|â–ˆâ–Š        | 1071/6000 [50:52<3:42:57,  2.71s/it]                                                     {'loss': 2.7638, 'grad_norm': 2.649182081222534, 'learning_rate': 4.177118644067797e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1071/6000 [50:52<3:42:57,  2.71s/it] 18%|â–ˆâ–Š        | 1072/6000 [50:54<3:41:58,  2.70s/it]                                                     {'loss': 2.7702, 'grad_norm': 2.4529640674591064, 'learning_rate': 4.176271186440678e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1072/6000 [50:54<3:41:58,  2.70s/it] 18%|â–ˆâ–Š        | 1073/6000 [50:57<3:42:57,  2.72s/it]                                                     {'loss': 2.8116, 'grad_norm': 5.0024614334106445, 'learning_rate': 4.175423728813559e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1073/6000 [50:57<3:42:57,  2.72s/it] 18%|â–ˆâ–Š        | 1074/6000 [51:00<3:43:38,  2.72s/it]                                                     {'loss': 2.7665, 'grad_norm': 2.2615857124328613, 'learning_rate': 4.174576271186441e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1074/6000 [51:00<3:43:38,  2.72s/it] 18%|â–ˆâ–Š        | 1075/6000 [51:02<3:43:15,  2.72s/it]                                                     {'loss': 2.7818, 'grad_norm': 2.740281105041504, 'learning_rate': 4.173728813559322e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1075/6000 [51:02<3:43:15,  2.72s/it] 18%|â–ˆâ–Š        | 1076/6000 [51:05<3:42:56,  2.72s/it]                                                     {'loss': 2.7817, 'grad_norm': 3.5011894702911377, 'learning_rate': 4.172881355932204e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1076/6000 [51:05<3:42:56,  2.72s/it] 18%|â–ˆâ–Š        | 1077/6000 [51:08<3:43:12,  2.72s/it]                                                     {'loss': 2.7733, 'grad_norm': 1.6744189262390137, 'learning_rate': 4.172033898305085e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1077/6000 [51:08<3:43:12,  2.72s/it] 18%|â–ˆâ–Š        | 1078/6000 [51:11<3:42:54,  2.72s/it]                                                     {'loss': 2.7632, 'grad_norm': 5.5305256843566895, 'learning_rate': 4.171186440677966e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1078/6000 [51:11<3:42:54,  2.72s/it] 18%|â–ˆâ–Š        | 1079/6000 [51:14<3:53:08,  2.84s/it]                                                     {'loss': 2.7586, 'grad_norm': 4.905357360839844, 'learning_rate': 4.1703389830508474e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1079/6000 [51:14<3:53:08,  2.84s/it] 18%|â–ˆâ–Š        | 1080/6000 [51:16<3:49:42,  2.80s/it]                                                     {'loss': 2.7893, 'grad_norm': 2.9935858249664307, 'learning_rate': 4.1694915254237285e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1080/6000 [51:16<3:49:42,  2.80s/it] 18%|â–ˆâ–Š        | 1081/6000 [51:19<3:46:38,  2.76s/it]                                                     {'loss': 2.7881, 'grad_norm': 6.60617733001709, 'learning_rate': 4.16864406779661e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1081/6000 [51:19<3:46:38,  2.76s/it] 18%|â–ˆâ–Š        | 1082/6000 [51:22<3:46:10,  2.76s/it]                                                     {'loss': 2.8379, 'grad_norm': 2.7626750469207764, 'learning_rate': 4.1677966101694915e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1082/6000 [51:22<3:46:10,  2.76s/it] 18%|â–ˆâ–Š        | 1083/6000 [51:25<3:44:04,  2.73s/it]                                                     {'loss': 2.7706, 'grad_norm': 4.115180015563965, 'learning_rate': 4.166949152542373e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1083/6000 [51:25<3:44:04,  2.73s/it] 18%|â–ˆâ–Š        | 1084/6000 [51:27<3:45:37,  2.75s/it]                                                     {'loss': 2.7989, 'grad_norm': 4.363701343536377, 'learning_rate': 4.1661016949152544e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1084/6000 [51:27<3:45:37,  2.75s/it] 18%|â–ˆâ–Š        | 1085/6000 [51:30<3:44:12,  2.74s/it]                                                     {'loss': 2.7831, 'grad_norm': 4.491684436798096, 'learning_rate': 4.165254237288136e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1085/6000 [51:30<3:44:12,  2.74s/it] 18%|â–ˆâ–Š        | 1086/6000 [51:33<3:50:59,  2.82s/it]                                                     {'loss': 2.7706, 'grad_norm': 2.2263307571411133, 'learning_rate': 4.164406779661017e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1086/6000 [51:33<3:50:59,  2.82s/it] 18%|â–ˆâ–Š        | 1087/6000 [51:36<3:49:24,  2.80s/it]                                                     {'loss': 2.7649, 'grad_norm': 2.644846200942993, 'learning_rate': 4.1635593220338985e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1087/6000 [51:36<3:49:24,  2.80s/it] 18%|â–ˆâ–Š        | 1088/6000 [51:39<3:48:07,  2.79s/it]                                                     {'loss': 2.7904, 'grad_norm': 1.9874612092971802, 'learning_rate': 4.1627118644067796e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1088/6000 [51:39<3:48:07,  2.79s/it] 18%|â–ˆâ–Š        | 1089/6000 [51:42<4:08:02,  3.03s/it]                                                     {'loss': 2.8076, 'grad_norm': 6.149919033050537, 'learning_rate': 4.1618644067796614e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1089/6000 [51:42<4:08:02,  3.03s/it] 18%|â–ˆâ–Š        | 1090/6000 [51:45<3:59:56,  2.93s/it]                                                     {'loss': 2.777, 'grad_norm': 4.433347702026367, 'learning_rate': 4.1610169491525425e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1090/6000 [51:45<3:59:56,  2.93s/it] 18%|â–ˆâ–Š        | 1091/6000 [51:48<3:56:36,  2.89s/it]                                                     {'loss': 2.7743, 'grad_norm': 2.3060033321380615, 'learning_rate': 4.160169491525424e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1091/6000 [51:48<3:56:36,  2.89s/it] 18%|â–ˆâ–Š        | 1092/6000 [51:50<3:51:21,  2.83s/it]                                                     {'loss': 2.7929, 'grad_norm': 13.095213890075684, 'learning_rate': 4.1593220338983055e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1092/6000 [51:50<3:51:21,  2.83s/it] 18%|â–ˆâ–Š        | 1093/6000 [51:53<3:49:28,  2.81s/it]                                                     {'loss': 2.7798, 'grad_norm': 6.142838478088379, 'learning_rate': 4.1584745762711866e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1093/6000 [51:53<3:49:28,  2.81s/it] 18%|â–ˆâ–Š        | 1094/6000 [51:56<3:50:53,  2.82s/it]                                                     {'loss': 2.7769, 'grad_norm': 2.6408743858337402, 'learning_rate': 4.157627118644068e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1094/6000 [51:56<3:50:53,  2.82s/it] 18%|â–ˆâ–Š        | 1095/6000 [51:59<3:47:58,  2.79s/it]                                                     {'loss': 2.8033, 'grad_norm': 3.2231478691101074, 'learning_rate': 4.1567796610169495e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1095/6000 [51:59<3:47:58,  2.79s/it] 18%|â–ˆâ–Š        | 1096/6000 [52:02<3:50:23,  2.82s/it]                                                     {'loss': 2.7716, 'grad_norm': 10.618522644042969, 'learning_rate': 4.1559322033898307e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1096/6000 [52:02<3:50:23,  2.82s/it] 18%|â–ˆâ–Š        | 1097/6000 [52:04<3:48:20,  2.79s/it]                                                     {'loss': 2.7626, 'grad_norm': 8.1571683883667, 'learning_rate': 4.1550847457627125e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1097/6000 [52:04<3:48:20,  2.79s/it] 18%|â–ˆâ–Š        | 1098/6000 [52:08<4:00:11,  2.94s/it]                                                     {'loss': 2.7757, 'grad_norm': 4.2225494384765625, 'learning_rate': 4.1542372881355936e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1098/6000 [52:08<4:00:11,  2.94s/it] 18%|â–ˆâ–Š        | 1099/6000 [52:11<4:09:48,  3.06s/it]                                                     {'loss': 2.7579, 'grad_norm': 4.869174957275391, 'learning_rate': 4.153389830508475e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1099/6000 [52:11<4:09:48,  3.06s/it] 18%|â–ˆâ–Š        | 1100/6000 [52:14<4:00:50,  2.95s/it]                                                     {'loss': 2.7584, 'grad_norm': 4.754455089569092, 'learning_rate': 4.152542372881356e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1100/6000 [52:14<4:00:50,  2.95s/it][2025-10-22 19:48:39,090] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1101/6000 [52:18<4:47:35,  3.52s/it]                                                     {'loss': 2.7874, 'grad_norm': 3.2148540019989014, 'learning_rate': 4.151694915254237e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1101/6000 [52:18<4:47:35,  3.52s/it] 18%|â–ˆâ–Š        | 1102/6000 [52:21<4:26:12,  3.26s/it]                                                     {'loss': 2.8002, 'grad_norm': 2.2244813442230225, 'learning_rate': 4.150847457627119e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1102/6000 [52:21<4:26:12,  3.26s/it] 18%|â–ˆâ–Š        | 1103/6000 [52:24<4:13:04,  3.10s/it]                                                     {'loss': 2.7675, 'grad_norm': 3.7034130096435547, 'learning_rate': 4.15e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1103/6000 [52:24<4:13:04,  3.10s/it] 18%|â–ˆâ–Š        | 1104/6000 [52:27<4:02:59,  2.98s/it]                                                     {'loss': 2.7744, 'grad_norm': 2.092801570892334, 'learning_rate': 4.149152542372882e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1104/6000 [52:27<4:02:59,  2.98s/it] 18%|â–ˆâ–Š        | 1105/6000 [52:29<4:00:37,  2.95s/it]                                                     {'loss': 2.7878, 'grad_norm': 3.7714595794677734, 'learning_rate': 4.148305084745763e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1105/6000 [52:29<4:00:37,  2.95s/it] 18%|â–ˆâ–Š        | 1106/6000 [52:32<3:54:28,  2.87s/it]                                                     {'loss': 2.7758, 'grad_norm': 4.837888717651367, 'learning_rate': 4.1474576271186446e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1106/6000 [52:32<3:54:28,  2.87s/it] 18%|â–ˆâ–Š        | 1107/6000 [52:35<3:49:43,  2.82s/it]                                                     {'loss': 2.7497, 'grad_norm': 3.1597094535827637, 'learning_rate': 4.146610169491526e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1107/6000 [52:35<3:49:43,  2.82s/it] 18%|â–ˆâ–Š        | 1108/6000 [52:38<3:50:06,  2.82s/it]                                                     {'loss': 2.7817, 'grad_norm': 5.537050724029541, 'learning_rate': 4.145762711864407e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1108/6000 [52:38<3:50:06,  2.82s/it] 18%|â–ˆâ–Š        | 1109/6000 [52:40<3:49:34,  2.82s/it]                                                     {'loss': 2.7973, 'grad_norm': 2.914126396179199, 'learning_rate': 4.144915254237288e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1109/6000 [52:40<3:49:34,  2.82s/it] 18%|â–ˆâ–Š        | 1110/6000 [52:43<3:48:38,  2.81s/it]                                                     {'loss': 2.75, 'grad_norm': 3.5186829566955566, 'learning_rate': 4.14406779661017e-05, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1110/6000 [52:43<3:48:38,  2.81s/it] 19%|â–ˆâ–Š        | 1111/6000 [52:46<3:44:28,  2.75s/it]                                                     {'loss': 2.8512, 'grad_norm': 3.075345754623413, 'learning_rate': 4.143220338983051e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1111/6000 [52:46<3:44:28,  2.75s/it] 19%|â–ˆâ–Š        | 1112/6000 [52:49<3:43:16,  2.74s/it]                                                     {'loss': 2.7817, 'grad_norm': 4.566281795501709, 'learning_rate': 4.142372881355933e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1112/6000 [52:49<3:43:16,  2.74s/it] 19%|â–ˆâ–Š        | 1113/6000 [52:51<3:42:03,  2.73s/it]                                                     {'loss': 2.792, 'grad_norm': 3.769191026687622, 'learning_rate': 4.141525423728814e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1113/6000 [52:51<3:42:03,  2.73s/it] 19%|â–ˆâ–Š        | 1114/6000 [52:54<3:42:10,  2.73s/it]                                                     {'loss': 2.7665, 'grad_norm': 3.757683753967285, 'learning_rate': 4.140677966101695e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1114/6000 [52:54<3:42:10,  2.73s/it] 19%|â–ˆâ–Š        | 1115/6000 [52:57<3:42:41,  2.74s/it]                                                     {'loss': 2.7237, 'grad_norm': 9.68908405303955, 'learning_rate': 4.139830508474576e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1115/6000 [52:57<3:42:41,  2.74s/it] 19%|â–ˆâ–Š        | 1116/6000 [52:59<3:42:43,  2.74s/it]                                                     {'loss': 2.7675, 'grad_norm': 6.074821472167969, 'learning_rate': 4.138983050847458e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1116/6000 [52:59<3:42:43,  2.74s/it] 19%|â–ˆâ–Š        | 1117/6000 [53:02<3:45:38,  2.77s/it]                                                     {'loss': 2.8231, 'grad_norm': 6.343675136566162, 'learning_rate': 4.138135593220339e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1117/6000 [53:02<3:45:38,  2.77s/it] 19%|â–ˆâ–Š        | 1118/6000 [53:05<3:45:18,  2.77s/it]                                                     {'loss': 2.7892, 'grad_norm': 7.528463363647461, 'learning_rate': 4.13728813559322e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1118/6000 [53:05<3:45:18,  2.77s/it] 19%|â–ˆâ–Š        | 1119/6000 [53:08<3:47:34,  2.80s/it]                                                     {'loss': 2.7889, 'grad_norm': 12.866310119628906, 'learning_rate': 4.136440677966102e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1119/6000 [53:08<3:47:34,  2.80s/it] 19%|â–ˆâ–Š        | 1120/6000 [53:11<3:45:35,  2.77s/it]                                                     {'loss': 2.817, 'grad_norm': 4.201909065246582, 'learning_rate': 4.135593220338983e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1120/6000 [53:11<3:45:35,  2.77s/it] 19%|â–ˆâ–Š        | 1121/6000 [53:13<3:44:17,  2.76s/it]                                                     {'loss': 2.7781, 'grad_norm': 4.469670295715332, 'learning_rate': 4.134745762711865e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1121/6000 [53:13<3:44:17,  2.76s/it] 19%|â–ˆâ–Š        | 1122/6000 [53:16<3:42:47,  2.74s/it]                                                     {'loss': 2.7861, 'grad_norm': 2.472304105758667, 'learning_rate': 4.1338983050847454e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1122/6000 [53:16<3:42:47,  2.74s/it] 19%|â–ˆâ–Š        | 1123/6000 [53:19<3:42:22,  2.74s/it]                                                     {'loss': 2.7352, 'grad_norm': 3.015043020248413, 'learning_rate': 4.133050847457627e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1123/6000 [53:19<3:42:22,  2.74s/it] 19%|â–ˆâ–Š        | 1124/6000 [53:22<3:51:14,  2.85s/it]                                                     {'loss': 2.7957, 'grad_norm': 4.101809978485107, 'learning_rate': 4.1322033898305084e-05, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1124/6000 [53:22<3:51:14,  2.85s/it] 19%|â–ˆâ–‰        | 1125/6000 [53:25<3:50:51,  2.84s/it]                                                     {'loss': 2.8071, 'grad_norm': 3.901271343231201, 'learning_rate': 4.13135593220339e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1125/6000 [53:25<3:50:51,  2.84s/it] 19%|â–ˆâ–‰        | 1126/6000 [53:27<3:48:29,  2.81s/it]                                                     {'loss': 2.8371, 'grad_norm': 3.5469937324523926, 'learning_rate': 4.130508474576271e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1126/6000 [53:27<3:48:29,  2.81s/it] 19%|â–ˆâ–‰        | 1127/6000 [53:30<3:48:46,  2.82s/it]                                                     {'loss': 2.774, 'grad_norm': 4.99418830871582, 'learning_rate': 4.129661016949153e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1127/6000 [53:30<3:48:46,  2.82s/it] 19%|â–ˆâ–‰        | 1128/6000 [53:33<3:45:38,  2.78s/it]                                                     {'loss': 2.7644, 'grad_norm': 1.9299778938293457, 'learning_rate': 4.128813559322034e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1128/6000 [53:33<3:45:38,  2.78s/it] 19%|â–ˆâ–‰        | 1129/6000 [53:36<3:43:27,  2.75s/it]                                                     {'loss': 2.7542, 'grad_norm': 5.7104573249816895, 'learning_rate': 4.1279661016949153e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1129/6000 [53:36<3:43:27,  2.75s/it] 19%|â–ˆâ–‰        | 1130/6000 [53:38<3:44:29,  2.77s/it]                                                     {'loss': 2.7839, 'grad_norm': 5.156084060668945, 'learning_rate': 4.1271186440677965e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1130/6000 [53:38<3:44:29,  2.77s/it] 19%|â–ˆâ–‰        | 1131/6000 [53:41<3:42:53,  2.75s/it]                                                     {'loss': 2.7876, 'grad_norm': 4.224920272827148, 'learning_rate': 4.126271186440678e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1131/6000 [53:41<3:42:53,  2.75s/it] 19%|â–ˆâ–‰        | 1132/6000 [53:44<3:41:44,  2.73s/it]                                                     {'loss': 2.7809, 'grad_norm': 4.0658793449401855, 'learning_rate': 4.1254237288135594e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1132/6000 [53:44<3:41:44,  2.73s/it] 19%|â–ˆâ–‰        | 1133/6000 [53:47<3:50:57,  2.85s/it]                                                     {'loss': 2.7708, 'grad_norm': 5.662169456481934, 'learning_rate': 4.124576271186441e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1133/6000 [53:47<3:50:57,  2.85s/it] 19%|â–ˆâ–‰        | 1134/6000 [53:50<3:59:19,  2.95s/it]                                                     {'loss': 2.7664, 'grad_norm': 7.623584747314453, 'learning_rate': 4.1237288135593223e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1134/6000 [53:50<3:59:19,  2.95s/it] 19%|â–ˆâ–‰        | 1135/6000 [53:53<3:53:44,  2.88s/it]                                                     {'loss': 2.807, 'grad_norm': 4.171708583831787, 'learning_rate': 4.1228813559322035e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1135/6000 [53:53<3:53:44,  2.88s/it] 19%|â–ˆâ–‰        | 1136/6000 [53:56<3:50:07,  2.84s/it]                                                     {'loss': 2.7591, 'grad_norm': 2.186673879623413, 'learning_rate': 4.1220338983050846e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1136/6000 [53:56<3:50:07,  2.84s/it] 19%|â–ˆâ–‰        | 1137/6000 [53:58<3:48:06,  2.81s/it]                                                     {'loss': 2.8169, 'grad_norm': 2.0910394191741943, 'learning_rate': 4.1211864406779664e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1137/6000 [53:58<3:48:06,  2.81s/it] 19%|â–ˆâ–‰        | 1138/6000 [54:01<3:44:54,  2.78s/it]                                                     {'loss': 2.8104, 'grad_norm': 2.642930507659912, 'learning_rate': 4.1203389830508475e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1138/6000 [54:01<3:44:54,  2.78s/it] 19%|â–ˆâ–‰        | 1139/6000 [54:04<3:46:10,  2.79s/it]                                                     {'loss': 2.7811, 'grad_norm': 4.509045124053955, 'learning_rate': 4.119491525423729e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1139/6000 [54:04<3:46:10,  2.79s/it] 19%|â–ˆâ–‰        | 1140/6000 [54:07<3:47:35,  2.81s/it]                                                     {'loss': 2.7753, 'grad_norm': 1.5350984334945679, 'learning_rate': 4.1186440677966105e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1140/6000 [54:07<3:47:35,  2.81s/it] 19%|â–ˆâ–‰        | 1141/6000 [54:10<3:46:04,  2.79s/it]                                                     {'loss': 2.7727, 'grad_norm': 1.6939811706542969, 'learning_rate': 4.1177966101694916e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1141/6000 [54:10<3:46:04,  2.79s/it] 19%|â–ˆâ–‰        | 1142/6000 [54:12<3:45:31,  2.79s/it]                                                     {'loss': 2.7982, 'grad_norm': 1.682100534439087, 'learning_rate': 4.1169491525423734e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1142/6000 [54:12<3:45:31,  2.79s/it] 19%|â–ˆâ–‰        | 1143/6000 [54:15<3:43:01,  2.76s/it]                                                     {'loss': 2.8136, 'grad_norm': 2.9975318908691406, 'learning_rate': 4.1161016949152545e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1143/6000 [54:15<3:43:01,  2.76s/it] 19%|â–ˆâ–‰        | 1144/6000 [54:18<3:41:21,  2.74s/it]                                                     {'loss': 2.8283, 'grad_norm': 1.364524006843567, 'learning_rate': 4.115254237288136e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1144/6000 [54:18<3:41:21,  2.74s/it] 19%|â–ˆâ–‰        | 1145/6000 [54:20<3:42:05,  2.74s/it]                                                     {'loss': 2.8448, 'grad_norm': 2.4142303466796875, 'learning_rate': 4.114406779661017e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1145/6000 [54:20<3:42:05,  2.74s/it] 19%|â–ˆâ–‰        | 1146/6000 [54:23<3:43:40,  2.76s/it]                                                     {'loss': 2.7575, 'grad_norm': 3.0624442100524902, 'learning_rate': 4.1135593220338986e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1146/6000 [54:23<3:43:40,  2.76s/it] 19%|â–ˆâ–‰        | 1147/6000 [54:26<3:42:30,  2.75s/it]                                                     {'loss': 2.7873, 'grad_norm': 1.817663550376892, 'learning_rate': 4.11271186440678e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1147/6000 [54:26<3:42:30,  2.75s/it] 19%|â–ˆâ–‰        | 1148/6000 [54:29<3:42:00,  2.75s/it]                                                     {'loss': 2.782, 'grad_norm': 1.2638849020004272, 'learning_rate': 4.1118644067796615e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1148/6000 [54:29<3:42:00,  2.75s/it] 19%|â–ˆâ–‰        | 1149/6000 [54:32<3:50:28,  2.85s/it]                                                     {'loss': 2.773, 'grad_norm': 1.917723298072815, 'learning_rate': 4.111016949152543e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1149/6000 [54:32<3:50:28,  2.85s/it] 19%|â–ˆâ–‰        | 1150/6000 [54:35<3:49:02,  2.83s/it]                                                     {'loss': 2.7874, 'grad_norm': 1.2988845109939575, 'learning_rate': 4.110169491525424e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1150/6000 [54:35<3:49:02,  2.83s/it][2025-10-22 19:51:00,121] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 19%|â–ˆâ–‰        | 1151/6000 [54:40<4:43:01,  3.50s/it]                                                     {'loss': 2.7638, 'grad_norm': 2.6649765968322754, 'learning_rate': 4.109322033898305e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1151/6000 [54:40<4:43:01,  3.50s/it] 19%|â–ˆâ–‰        | 1152/6000 [54:42<4:23:22,  3.26s/it]                                                     {'loss': 2.7697, 'grad_norm': 1.816880464553833, 'learning_rate': 4.108474576271187e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1152/6000 [54:42<4:23:22,  3.26s/it] 19%|â–ˆâ–‰        | 1153/6000 [54:45<4:11:56,  3.12s/it]                                                     {'loss': 2.7912, 'grad_norm': 2.6020302772521973, 'learning_rate': 4.107627118644068e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1153/6000 [54:45<4:11:56,  3.12s/it] 19%|â–ˆâ–‰        | 1154/6000 [54:48<4:03:37,  3.02s/it]                                                     {'loss': 2.7643, 'grad_norm': 1.8996949195861816, 'learning_rate': 4.10677966101695e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1154/6000 [54:48<4:03:37,  3.02s/it] 19%|â–ˆâ–‰        | 1155/6000 [54:51<3:57:29,  2.94s/it]                                                     {'loss': 2.7653, 'grad_norm': 1.6284903287887573, 'learning_rate': 4.105932203389831e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1155/6000 [54:51<3:57:29,  2.94s/it] 19%|â–ˆâ–‰        | 1156/6000 [54:53<3:52:20,  2.88s/it]                                                     {'loss': 2.7618, 'grad_norm': 2.8578381538391113, 'learning_rate': 4.1050847457627126e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1156/6000 [54:53<3:52:20,  2.88s/it] 19%|â–ˆâ–‰        | 1157/6000 [54:56<3:50:11,  2.85s/it]                                                     {'loss': 2.7697, 'grad_norm': 3.028729200363159, 'learning_rate': 4.104237288135593e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1157/6000 [54:56<3:50:11,  2.85s/it] 19%|â–ˆâ–‰        | 1158/6000 [54:59<3:47:07,  2.81s/it]                                                     {'loss': 2.7474, 'grad_norm': 2.4878976345062256, 'learning_rate': 4.103389830508475e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1158/6000 [54:59<3:47:07,  2.81s/it] 19%|â–ˆâ–‰        | 1159/6000 [55:02<3:45:31,  2.80s/it]                                                     {'loss': 2.9186, 'grad_norm': 2.0893290042877197, 'learning_rate': 4.102542372881356e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1159/6000 [55:02<3:45:31,  2.80s/it] 19%|â–ˆâ–‰        | 1160/6000 [55:04<3:43:43,  2.77s/it]                                                     {'loss': 2.8131, 'grad_norm': 4.096654891967773, 'learning_rate': 4.101694915254237e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1160/6000 [55:04<3:43:43,  2.77s/it] 19%|â–ˆâ–‰        | 1161/6000 [55:07<3:40:17,  2.73s/it]                                                     {'loss': 2.7578, 'grad_norm': 1.9639531373977661, 'learning_rate': 4.100847457627119e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1161/6000 [55:07<3:40:17,  2.73s/it] 19%|â–ˆâ–‰        | 1162/6000 [55:10<3:37:52,  2.70s/it]                                                     {'loss': 2.813, 'grad_norm': 2.684204578399658, 'learning_rate': 4.1e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1162/6000 [55:10<3:37:52,  2.70s/it] 19%|â–ˆâ–‰        | 1163/6000 [55:12<3:37:19,  2.70s/it]                                                     {'loss': 2.7412, 'grad_norm': 3.1218159198760986, 'learning_rate': 4.099152542372882e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1163/6000 [55:12<3:37:19,  2.70s/it] 19%|â–ˆâ–‰        | 1164/6000 [55:15<3:38:32,  2.71s/it]                                                     {'loss': 2.7698, 'grad_norm': 1.8478938341140747, 'learning_rate': 4.098305084745763e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1164/6000 [55:15<3:38:32,  2.71s/it] 19%|â–ˆâ–‰        | 1165/6000 [55:18<3:40:27,  2.74s/it]                                                     {'loss': 2.8065, 'grad_norm': 2.7962682247161865, 'learning_rate': 4.097457627118644e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1165/6000 [55:18<3:40:27,  2.74s/it] 19%|â–ˆâ–‰        | 1166/6000 [55:21<3:37:53,  2.70s/it]                                                     {'loss': 2.9785, 'grad_norm': 2.920912742614746, 'learning_rate': 4.096610169491525e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1166/6000 [55:21<3:37:53,  2.70s/it] 19%|â–ˆâ–‰        | 1167/6000 [55:24<3:50:58,  2.87s/it]                                                     {'loss': 2.7604, 'grad_norm': 1.8278837203979492, 'learning_rate': 4.095762711864407e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1167/6000 [55:24<3:50:58,  2.87s/it] 19%|â–ˆâ–‰        | 1168/6000 [55:27<3:47:56,  2.83s/it]                                                     {'loss': 2.7763, 'grad_norm': 1.6729854345321655, 'learning_rate': 4.094915254237288e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1168/6000 [55:27<3:47:56,  2.83s/it] 19%|â–ˆâ–‰        | 1169/6000 [55:29<3:45:20,  2.80s/it]                                                     {'loss': 2.7741, 'grad_norm': 3.9201083183288574, 'learning_rate': 4.09406779661017e-05, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1169/6000 [55:29<3:45:20,  2.80s/it] 20%|â–ˆâ–‰        | 1170/6000 [55:32<3:43:40,  2.78s/it]                                                     {'loss': 2.7933, 'grad_norm': 2.0984129905700684, 'learning_rate': 4.093220338983051e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1170/6000 [55:32<3:43:40,  2.78s/it] 20%|â–ˆâ–‰        | 1171/6000 [55:35<3:45:47,  2.81s/it]                                                     {'loss': 2.764, 'grad_norm': 2.1564371585845947, 'learning_rate': 4.092372881355932e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1171/6000 [55:35<3:45:47,  2.81s/it] 20%|â–ˆâ–‰        | 1172/6000 [55:38<3:44:08,  2.79s/it]                                                     {'loss': 2.7478, 'grad_norm': 1.5609464645385742, 'learning_rate': 4.0915254237288134e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1172/6000 [55:38<3:44:08,  2.79s/it] 20%|â–ˆâ–‰        | 1173/6000 [55:40<3:41:56,  2.76s/it]                                                     {'loss': 2.7442, 'grad_norm': 1.7171692848205566, 'learning_rate': 4.090677966101695e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1173/6000 [55:40<3:41:56,  2.76s/it] 20%|â–ˆâ–‰        | 1174/6000 [55:43<3:41:03,  2.75s/it]                                                     {'loss': 2.7374, 'grad_norm': 2.147272825241089, 'learning_rate': 4.089830508474576e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1174/6000 [55:43<3:41:03,  2.75s/it] 20%|â–ˆâ–‰        | 1175/6000 [55:46<3:42:04,  2.76s/it]                                                     {'loss': 2.7914, 'grad_norm': 3.5237255096435547, 'learning_rate': 4.088983050847458e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1175/6000 [55:46<3:42:04,  2.76s/it] 20%|â–ˆâ–‰        | 1176/6000 [55:48<3:39:19,  2.73s/it]                                                     {'loss': 2.8056, 'grad_norm': 2.5886666774749756, 'learning_rate': 4.088135593220339e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1176/6000 [55:48<3:39:19,  2.73s/it] 20%|â–ˆâ–‰        | 1177/6000 [55:51<3:39:30,  2.73s/it]                                                     {'loss': 2.7005, 'grad_norm': 3.5695228576660156, 'learning_rate': 4.087288135593221e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1177/6000 [55:51<3:39:30,  2.73s/it] 20%|â–ˆâ–‰        | 1178/6000 [55:54<3:49:47,  2.86s/it]                                                     {'loss': 2.7496, 'grad_norm': 3.206326961517334, 'learning_rate': 4.086440677966102e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1178/6000 [55:54<3:49:47,  2.86s/it] 20%|â–ˆâ–‰        | 1179/6000 [55:57<3:56:11,  2.94s/it]                                                     {'loss': 2.7502, 'grad_norm': 2.4372758865356445, 'learning_rate': 4.085593220338983e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1179/6000 [55:57<3:56:11,  2.94s/it] 20%|â–ˆâ–‰        | 1180/6000 [56:01<4:00:38,  3.00s/it]                                                     {'loss': 3.0603, 'grad_norm': 7.706171035766602, 'learning_rate': 4.0847457627118644e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1180/6000 [56:01<4:00:38,  3.00s/it] 20%|â–ˆâ–‰        | 1181/6000 [56:03<3:54:12,  2.92s/it]                                                     {'loss': 2.7445, 'grad_norm': 6.236010551452637, 'learning_rate': 4.0838983050847456e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1181/6000 [56:03<3:54:12,  2.92s/it] 20%|â–ˆâ–‰        | 1182/6000 [56:07<4:11:24,  3.13s/it]                                                     {'loss': 2.7718, 'grad_norm': 3.029278039932251, 'learning_rate': 4.0830508474576274e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1182/6000 [56:07<4:11:24,  3.13s/it] 20%|â–ˆâ–‰        | 1183/6000 [56:10<4:02:36,  3.02s/it]                                                     {'loss': 2.8498, 'grad_norm': 4.798056602478027, 'learning_rate': 4.0822033898305085e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1183/6000 [56:10<4:02:36,  3.02s/it] 20%|â–ˆâ–‰        | 1184/6000 [56:13<3:59:58,  2.99s/it]                                                     {'loss': 2.7836, 'grad_norm': 3.9015796184539795, 'learning_rate': 4.08135593220339e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1184/6000 [56:13<3:59:58,  2.99s/it] 20%|â–ˆâ–‰        | 1185/6000 [56:15<3:52:12,  2.89s/it]                                                     {'loss': 2.774, 'grad_norm': 3.9843857288360596, 'learning_rate': 4.0805084745762714e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1185/6000 [56:15<3:52:12,  2.89s/it] 20%|â–ˆâ–‰        | 1186/6000 [56:18<3:48:24,  2.85s/it]                                                     {'loss': 2.7492, 'grad_norm': 2.1658682823181152, 'learning_rate': 4.0796610169491526e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1186/6000 [56:18<3:48:24,  2.85s/it] 20%|â–ˆâ–‰        | 1187/6000 [56:21<3:44:32,  2.80s/it]                                                     {'loss': 2.841, 'grad_norm': 3.5588841438293457, 'learning_rate': 4.078813559322034e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1187/6000 [56:21<3:44:32,  2.80s/it] 20%|â–ˆâ–‰        | 1188/6000 [56:23<3:41:05,  2.76s/it]                                                     {'loss': 2.8672, 'grad_norm': 2.5892889499664307, 'learning_rate': 4.0779661016949155e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1188/6000 [56:23<3:41:05,  2.76s/it] 20%|â–ˆâ–‰        | 1189/6000 [56:26<3:39:18,  2.74s/it]                                                     {'loss': 2.7956, 'grad_norm': 1.792818307876587, 'learning_rate': 4.0771186440677966e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1189/6000 [56:26<3:39:18,  2.74s/it] 20%|â–ˆâ–‰        | 1190/6000 [56:29<3:42:11,  2.77s/it]                                                     {'loss': 2.7649, 'grad_norm': 3.257341146469116, 'learning_rate': 4.0762711864406784e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1190/6000 [56:29<3:42:11,  2.77s/it] 20%|â–ˆâ–‰        | 1191/6000 [56:32<3:42:05,  2.77s/it]                                                     {'loss': 2.722, 'grad_norm': 3.1335537433624268, 'learning_rate': 4.0754237288135596e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1191/6000 [56:32<3:42:05,  2.77s/it] 20%|â–ˆâ–‰        | 1192/6000 [56:34<3:42:37,  2.78s/it]                                                     {'loss': 2.7485, 'grad_norm': 3.8092830181121826, 'learning_rate': 4.0745762711864414e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1192/6000 [56:34<3:42:37,  2.78s/it] 20%|â–ˆâ–‰        | 1193/6000 [56:37<3:40:31,  2.75s/it]                                                     {'loss': 2.768, 'grad_norm': 2.619041681289673, 'learning_rate': 4.073728813559322e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1193/6000 [56:37<3:40:31,  2.75s/it] 20%|â–ˆâ–‰        | 1194/6000 [56:40<3:41:59,  2.77s/it]                                                     {'loss': 2.7303, 'grad_norm': 2.938206195831299, 'learning_rate': 4.0728813559322036e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1194/6000 [56:40<3:41:59,  2.77s/it] 20%|â–ˆâ–‰        | 1195/6000 [56:43<3:42:47,  2.78s/it]                                                     {'loss': 2.7637, 'grad_norm': 4.292956352233887, 'learning_rate': 4.072033898305085e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1195/6000 [56:43<3:42:47,  2.78s/it] 20%|â–ˆâ–‰        | 1196/6000 [56:46<3:42:21,  2.78s/it]                                                     {'loss': 2.7139, 'grad_norm': 3.7031850814819336, 'learning_rate': 4.0711864406779666e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1196/6000 [56:46<3:42:21,  2.78s/it] 20%|â–ˆâ–‰        | 1197/6000 [56:48<3:44:05,  2.80s/it]                                                     {'loss': 2.7808, 'grad_norm': 3.990037441253662, 'learning_rate': 4.070338983050848e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1197/6000 [56:48<3:44:05,  2.80s/it] 20%|â–ˆâ–‰        | 1198/6000 [56:51<3:44:18,  2.80s/it]                                                     {'loss': 2.7721, 'grad_norm': 5.8409810066223145, 'learning_rate': 4.0694915254237295e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1198/6000 [56:51<3:44:18,  2.80s/it] 20%|â–ˆâ–‰        | 1199/6000 [56:54<3:42:07,  2.78s/it]                                                     {'loss': 2.7708, 'grad_norm': 6.359512805938721, 'learning_rate': 4.0686440677966106e-05, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1199/6000 [56:54<3:42:07,  2.78s/it] 20%|â–ˆâ–ˆ        | 1200/6000 [56:57<3:39:31,  2.74s/it]                                                     {'loss': 2.7431, 'grad_norm': 7.367832183837891, 'learning_rate': 4.067796610169492e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1200/6000 [56:57<3:39:31,  2.74s/it][2025-10-22 19:53:22,125] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 20%|â–ˆâ–ˆ        | 1201/6000 [57:02<4:31:02,  3.39s/it]                                                     {'loss': 2.7819, 'grad_norm': 6.137892723083496, 'learning_rate': 4.066949152542373e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1201/6000 [57:02<4:31:02,  3.39s/it] 20%|â–ˆâ–ˆ        | 1202/6000 [57:04<4:15:58,  3.20s/it]                                                     {'loss': 2.7932, 'grad_norm': 6.366387844085693, 'learning_rate': 4.066101694915254e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1202/6000 [57:04<4:15:58,  3.20s/it] 20%|â–ˆâ–ˆ        | 1203/6000 [57:07<4:03:43,  3.05s/it]                                                     {'loss': 2.8007, 'grad_norm': 6.720641613006592, 'learning_rate': 4.065254237288136e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1203/6000 [57:07<4:03:43,  3.05s/it] 20%|â–ˆâ–ˆ        | 1204/6000 [57:10<4:07:41,  3.10s/it]                                                     {'loss': 2.772, 'grad_norm': 13.78768253326416, 'learning_rate': 4.064406779661017e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1204/6000 [57:10<4:07:41,  3.10s/it] 20%|â–ˆâ–ˆ        | 1205/6000 [57:13<3:59:20,  2.99s/it]                                                     {'loss': 2.792, 'grad_norm': 6.680281639099121, 'learning_rate': 4.063559322033899e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1205/6000 [57:13<3:59:20,  2.99s/it] 20%|â–ˆâ–ˆ        | 1206/6000 [57:16<3:52:03,  2.90s/it]                                                     {'loss': 2.8162, 'grad_norm': 5.62822151184082, 'learning_rate': 4.06271186440678e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1206/6000 [57:16<3:52:03,  2.90s/it] 20%|â–ˆâ–ˆ        | 1207/6000 [57:18<3:48:00,  2.85s/it]                                                     {'loss': 2.7083, 'grad_norm': 5.936334609985352, 'learning_rate': 4.061864406779661e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1207/6000 [57:18<3:48:00,  2.85s/it] 20%|â–ˆâ–ˆ        | 1208/6000 [57:21<3:45:34,  2.82s/it]                                                     {'loss': 2.7773, 'grad_norm': 3.3614652156829834, 'learning_rate': 4.061016949152542e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1208/6000 [57:21<3:45:34,  2.82s/it] 20%|â–ˆâ–ˆ        | 1209/6000 [57:24<3:42:12,  2.78s/it]                                                     {'loss': 2.7542, 'grad_norm': 3.2985377311706543, 'learning_rate': 4.060169491525424e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1209/6000 [57:24<3:42:12,  2.78s/it] 20%|â–ˆâ–ˆ        | 1210/6000 [57:27<3:42:33,  2.79s/it]                                                     {'loss': 2.7597, 'grad_norm': 2.9783127307891846, 'learning_rate': 4.059322033898305e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1210/6000 [57:27<3:42:33,  2.79s/it] 20%|â–ˆâ–ˆ        | 1211/6000 [57:29<3:41:54,  2.78s/it]                                                     {'loss': 2.7761, 'grad_norm': 6.025665283203125, 'learning_rate': 4.058474576271187e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1211/6000 [57:29<3:41:54,  2.78s/it] 20%|â–ˆâ–ˆ        | 1212/6000 [57:32<3:39:46,  2.75s/it]                                                     {'loss': 2.7809, 'grad_norm': 14.174931526184082, 'learning_rate': 4.057627118644068e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1212/6000 [57:32<3:39:46,  2.75s/it] 20%|â–ˆâ–ˆ        | 1213/6000 [57:35<3:38:31,  2.74s/it]                                                     {'loss': 2.7921, 'grad_norm': 7.360379695892334, 'learning_rate': 4.05677966101695e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1213/6000 [57:35<3:38:31,  2.74s/it] 20%|â–ˆâ–ˆ        | 1214/6000 [57:38<3:39:50,  2.76s/it]                                                     {'loss': 2.7686, 'grad_norm': 4.519753456115723, 'learning_rate': 4.055932203389831e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1214/6000 [57:38<3:39:50,  2.76s/it] 20%|â–ˆâ–ˆ        | 1215/6000 [57:40<3:39:19,  2.75s/it]                                                     {'loss': 2.7149, 'grad_norm': 7.177496433258057, 'learning_rate': 4.055084745762712e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1215/6000 [57:40<3:39:19,  2.75s/it] 20%|â–ˆâ–ˆ        | 1216/6000 [57:43<3:37:42,  2.73s/it]                                                     {'loss': 2.7629, 'grad_norm': 3.8391640186309814, 'learning_rate': 4.054237288135593e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1216/6000 [57:43<3:37:42,  2.73s/it] 20%|â–ˆâ–ˆ        | 1217/6000 [57:46<3:47:49,  2.86s/it]                                                     {'loss': 2.7537, 'grad_norm': 6.877823352813721, 'learning_rate': 4.053389830508475e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1217/6000 [57:46<3:47:49,  2.86s/it] 20%|â–ˆâ–ˆ        | 1218/6000 [57:49<3:44:04,  2.81s/it]                                                     {'loss': 2.7652, 'grad_norm': 6.14713716506958, 'learning_rate': 4.052542372881356e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1218/6000 [57:49<3:44:04,  2.81s/it] 20%|â–ˆâ–ˆ        | 1219/6000 [57:52<3:49:09,  2.88s/it]                                                     {'loss': 2.7947, 'grad_norm': 7.973835468292236, 'learning_rate': 4.051694915254238e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1219/6000 [57:52<3:49:09,  2.88s/it] 20%|â–ˆâ–ˆ        | 1220/6000 [57:55<3:45:00,  2.82s/it]                                                     {'loss': 2.7585, 'grad_norm': 2.7583022117614746, 'learning_rate': 4.050847457627119e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1220/6000 [57:55<3:45:00,  2.82s/it] 20%|â–ˆâ–ˆ        | 1221/6000 [57:57<3:44:45,  2.82s/it]                                                     {'loss': 2.7642, 'grad_norm': 5.735035419464111, 'learning_rate': 4.05e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1221/6000 [57:57<3:44:45,  2.82s/it] 20%|â–ˆâ–ˆ        | 1222/6000 [58:00<3:43:56,  2.81s/it]                                                     {'loss': 2.7828, 'grad_norm': 11.354888916015625, 'learning_rate': 4.049152542372881e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1222/6000 [58:00<3:43:56,  2.81s/it] 20%|â–ˆâ–ˆ        | 1223/6000 [58:03<3:40:57,  2.78s/it]                                                     {'loss': 2.7699, 'grad_norm': 7.040398597717285, 'learning_rate': 4.0483050847457624e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1223/6000 [58:03<3:40:57,  2.78s/it] 20%|â–ˆâ–ˆ        | 1224/6000 [58:06<3:40:04,  2.76s/it]                                                     {'loss': 2.8172, 'grad_norm': 3.8138601779937744, 'learning_rate': 4.047457627118644e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1224/6000 [58:06<3:40:04,  2.76s/it] 20%|â–ˆâ–ˆ        | 1225/6000 [58:09<3:44:30,  2.82s/it]                                                     {'loss': 2.7743, 'grad_norm': 3.946882486343384, 'learning_rate': 4.0466101694915254e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1225/6000 [58:09<3:44:30,  2.82s/it] 20%|â–ˆâ–ˆ        | 1226/6000 [58:12<3:49:24,  2.88s/it]                                                     {'loss': 2.7705, 'grad_norm': 5.950394630432129, 'learning_rate': 4.045762711864407e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1226/6000 [58:12<3:49:24,  2.88s/it] 20%|â–ˆâ–ˆ        | 1227/6000 [58:15<3:54:34,  2.95s/it]                                                     {'loss': 2.7707, 'grad_norm': 5.478006839752197, 'learning_rate': 4.044915254237288e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1227/6000 [58:15<3:54:34,  2.95s/it] 20%|â–ˆâ–ˆ        | 1228/6000 [58:17<3:49:34,  2.89s/it]                                                     {'loss': 2.8077, 'grad_norm': 5.502452373504639, 'learning_rate': 4.0440677966101694e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1228/6000 [58:17<3:49:34,  2.89s/it] 20%|â–ˆâ–ˆ        | 1229/6000 [58:20<3:44:29,  2.82s/it]                                                     {'loss': 2.7842, 'grad_norm': 4.293727874755859, 'learning_rate': 4.0432203389830506e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1229/6000 [58:20<3:44:29,  2.82s/it] 20%|â–ˆâ–ˆ        | 1230/6000 [58:23<3:40:29,  2.77s/it]                                                     {'loss': 2.7755, 'grad_norm': 8.658778190612793, 'learning_rate': 4.0423728813559324e-05, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1230/6000 [58:23<3:40:29,  2.77s/it] 21%|â–ˆâ–ˆ        | 1231/6000 [58:26<3:39:49,  2.77s/it]                                                     {'loss': 2.8373, 'grad_norm': 2.8079302310943604, 'learning_rate': 4.0415254237288135e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1231/6000 [58:26<3:39:49,  2.77s/it] 21%|â–ˆâ–ˆ        | 1232/6000 [58:28<3:39:11,  2.76s/it]                                                     {'loss': 2.7704, 'grad_norm': 2.0012168884277344, 'learning_rate': 4.040677966101695e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1232/6000 [58:28<3:39:11,  2.76s/it] 21%|â–ˆâ–ˆ        | 1233/6000 [58:31<3:44:19,  2.82s/it]                                                     {'loss': 2.7625, 'grad_norm': 2.767657995223999, 'learning_rate': 4.0398305084745764e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1233/6000 [58:31<3:44:19,  2.82s/it] 21%|â–ˆâ–ˆ        | 1234/6000 [58:34<3:51:15,  2.91s/it]                                                     {'loss': 2.7661, 'grad_norm': 1.7679387331008911, 'learning_rate': 4.038983050847458e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1234/6000 [58:34<3:51:15,  2.91s/it] 21%|â–ˆâ–ˆ        | 1235/6000 [58:37<3:46:49,  2.86s/it]                                                     {'loss': 2.7542, 'grad_norm': 2.849385976791382, 'learning_rate': 4.0381355932203394e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1235/6000 [58:37<3:46:49,  2.86s/it] 21%|â–ˆâ–ˆ        | 1236/6000 [58:40<3:43:06,  2.81s/it]                                                     {'loss': 2.8361, 'grad_norm': 3.0653069019317627, 'learning_rate': 4.0372881355932205e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1236/6000 [58:40<3:43:06,  2.81s/it] 21%|â–ˆâ–ˆ        | 1237/6000 [58:43<3:41:14,  2.79s/it]                                                     {'loss': 2.804, 'grad_norm': 2.3610892295837402, 'learning_rate': 4.0364406779661016e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1237/6000 [58:43<3:41:14,  2.79s/it] 21%|â–ˆâ–ˆ        | 1238/6000 [58:45<3:41:38,  2.79s/it]                                                     {'loss': 2.7558, 'grad_norm': 3.9313743114471436, 'learning_rate': 4.0355932203389834e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1238/6000 [58:45<3:41:38,  2.79s/it] 21%|â–ˆâ–ˆ        | 1239/6000 [58:48<3:38:51,  2.76s/it]                                                     {'loss': 2.7783, 'grad_norm': 2.425405979156494, 'learning_rate': 4.0347457627118646e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1239/6000 [58:48<3:38:51,  2.76s/it] 21%|â–ˆâ–ˆ        | 1240/6000 [58:51<3:39:36,  2.77s/it]                                                     {'loss': 2.7592, 'grad_norm': 1.4299858808517456, 'learning_rate': 4.0338983050847464e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1240/6000 [58:51<3:39:36,  2.77s/it] 21%|â–ˆâ–ˆ        | 1241/6000 [58:54<3:38:34,  2.76s/it]                                                     {'loss': 2.7399, 'grad_norm': 1.924368977546692, 'learning_rate': 4.0330508474576275e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1241/6000 [58:54<3:38:34,  2.76s/it] 21%|â–ˆâ–ˆ        | 1242/6000 [58:56<3:41:35,  2.79s/it]                                                     {'loss': 2.7845, 'grad_norm': 1.7269140481948853, 'learning_rate': 4.0322033898305086e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1242/6000 [58:56<3:41:35,  2.79s/it] 21%|â–ˆâ–ˆ        | 1243/6000 [58:59<3:40:50,  2.79s/it]                                                     {'loss': 2.758, 'grad_norm': 3.61983585357666, 'learning_rate': 4.03135593220339e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1243/6000 [58:59<3:40:50,  2.79s/it] 21%|â–ˆâ–ˆ        | 1244/6000 [59:02<3:39:02,  2.76s/it]                                                     {'loss': 2.7474, 'grad_norm': 7.216653823852539, 'learning_rate': 4.030508474576271e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1244/6000 [59:02<3:39:02,  2.76s/it] 21%|â–ˆâ–ˆ        | 1245/6000 [59:05<3:38:30,  2.76s/it]                                                     {'loss': 2.7319, 'grad_norm': 2.9234087467193604, 'learning_rate': 4.029661016949153e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1245/6000 [59:05<3:38:30,  2.76s/it] 21%|â–ˆâ–ˆ        | 1246/6000 [59:07<3:37:09,  2.74s/it]                                                     {'loss': 2.7923, 'grad_norm': 5.900745391845703, 'learning_rate': 4.028813559322034e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1246/6000 [59:07<3:37:09,  2.74s/it] 21%|â–ˆâ–ˆ        | 1247/6000 [59:10<3:39:30,  2.77s/it]                                                     {'loss': 2.8287, 'grad_norm': 7.788261413574219, 'learning_rate': 4.0279661016949156e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1247/6000 [59:10<3:39:30,  2.77s/it] 21%|â–ˆâ–ˆ        | 1248/6000 [59:13<3:40:02,  2.78s/it]                                                     {'loss': 2.6975, 'grad_norm': 7.096545219421387, 'learning_rate': 4.027118644067797e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1248/6000 [59:13<3:40:02,  2.78s/it] 21%|â–ˆâ–ˆ        | 1249/6000 [59:16<3:37:49,  2.75s/it]                                                     {'loss': 2.7634, 'grad_norm': 3.3543200492858887, 'learning_rate': 4.0262711864406786e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1249/6000 [59:16<3:37:49,  2.75s/it] 21%|â–ˆâ–ˆ        | 1250/6000 [59:18<3:35:24,  2.72s/it]                                                     {'loss': 2.8077, 'grad_norm': 11.001147270202637, 'learning_rate': 4.025423728813559e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1250/6000 [59:18<3:35:24,  2.72s/it][2025-10-22 19:55:43,795] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 21%|â–ˆâ–ˆ        | 1251/6000 [59:23<4:27:31,  3.38s/it]                                                     {'loss': 2.7464, 'grad_norm': 4.594544410705566, 'learning_rate': 4.024576271186441e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1251/6000 [59:23<4:27:31,  3.38s/it] 21%|â–ˆâ–ˆ        | 1252/6000 [59:26<4:09:38,  3.15s/it]                                                     {'loss': 2.792, 'grad_norm': 5.863936424255371, 'learning_rate': 4.023728813559322e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1252/6000 [59:26<4:09:38,  3.15s/it] 21%|â–ˆâ–ˆ        | 1253/6000 [59:29<4:13:21,  3.20s/it]                                                     {'loss': 2.8288, 'grad_norm': 9.871118545532227, 'learning_rate': 4.022881355932204e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1253/6000 [59:29<4:13:21,  3.20s/it] 21%|â–ˆâ–ˆ        | 1254/6000 [59:32<3:59:35,  3.03s/it]                                                     {'loss': 2.7458, 'grad_norm': 4.049808025360107, 'learning_rate': 4.022033898305085e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1254/6000 [59:32<3:59:35,  3.03s/it] 21%|â–ˆâ–ˆ        | 1255/6000 [59:34<3:52:25,  2.94s/it]                                                     {'loss': 2.6936, 'grad_norm': 4.95542573928833, 'learning_rate': 4.021186440677967e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1255/6000 [59:35<3:52:25,  2.94s/it] 21%|â–ˆâ–ˆ        | 1256/6000 [59:37<3:49:03,  2.90s/it]                                                     {'loss': 2.7775, 'grad_norm': 4.172902584075928, 'learning_rate': 4.020338983050848e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1256/6000 [59:37<3:49:03,  2.90s/it] 21%|â–ˆâ–ˆ        | 1257/6000 [59:40<3:43:53,  2.83s/it]                                                     {'loss': 2.802, 'grad_norm': 2.5974342823028564, 'learning_rate': 4.019491525423729e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1257/6000 [59:40<3:43:53,  2.83s/it] 21%|â–ˆâ–ˆ        | 1258/6000 [59:43<3:44:04,  2.84s/it]                                                     {'loss': 2.9327, 'grad_norm': 6.186738967895508, 'learning_rate': 4.01864406779661e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1258/6000 [59:43<3:44:04,  2.84s/it] 21%|â–ˆâ–ˆ        | 1259/6000 [59:46<3:41:41,  2.81s/it]                                                     {'loss': 2.8731, 'grad_norm': 4.285209655761719, 'learning_rate': 4.017796610169492e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1259/6000 [59:46<3:41:41,  2.81s/it] 21%|â–ˆâ–ˆ        | 1260/6000 [59:48<3:40:10,  2.79s/it]                                                     {'loss': 2.7958, 'grad_norm': 2.4572019577026367, 'learning_rate': 4.016949152542373e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1260/6000 [59:48<3:40:10,  2.79s/it] 21%|â–ˆâ–ˆ        | 1261/6000 [59:51<3:38:50,  2.77s/it]                                                     {'loss': 2.65, 'grad_norm': 7.520213603973389, 'learning_rate': 4.016101694915255e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1261/6000 [59:51<3:38:50,  2.77s/it] 21%|â–ˆâ–ˆ        | 1262/6000 [59:54<3:46:17,  2.87s/it]                                                     {'loss': 2.7733, 'grad_norm': 2.200711965560913, 'learning_rate': 4.015254237288136e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1262/6000 [59:54<3:46:17,  2.87s/it] 21%|â–ˆâ–ˆ        | 1263/6000 [59:57<3:47:45,  2.88s/it]                                                     {'loss': 2.8207, 'grad_norm': 3.857593297958374, 'learning_rate': 4.014406779661017e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1263/6000 [59:57<3:47:45,  2.88s/it] 21%|â–ˆâ–ˆ        | 1264/6000 [1:00:00<3:44:25,  2.84s/it]                                                       {'loss': 2.7739, 'grad_norm': 1.7583976984024048, 'learning_rate': 4.013559322033898e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1264/6000 [1:00:00<3:44:25,  2.84s/it] 21%|â–ˆâ–ˆ        | 1265/6000 [1:00:03<3:42:52,  2.82s/it]                                                       {'loss': 2.7791, 'grad_norm': 3.1176846027374268, 'learning_rate': 4.012711864406779e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1265/6000 [1:00:03<3:42:52,  2.82s/it] 21%|â–ˆâ–ˆ        | 1266/6000 [1:00:05<3:41:52,  2.81s/it]                                                       {'loss': 2.7231, 'grad_norm': 5.06994104385376, 'learning_rate': 4.011864406779661e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1266/6000 [1:00:05<3:41:52,  2.81s/it] 21%|â–ˆâ–ˆ        | 1267/6000 [1:00:08<3:47:48,  2.89s/it]                                                       {'loss': 2.7936, 'grad_norm': 5.809399127960205, 'learning_rate': 4.011016949152542e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1267/6000 [1:00:08<3:47:48,  2.89s/it] 21%|â–ˆâ–ˆ        | 1268/6000 [1:00:11<3:44:28,  2.85s/it]                                                       {'loss': 2.7394, 'grad_norm': 3.183377981185913, 'learning_rate': 4.010169491525424e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1268/6000 [1:00:11<3:44:28,  2.85s/it] 21%|â–ˆâ–ˆ        | 1269/6000 [1:00:14<3:43:15,  2.83s/it]                                                       {'loss': 2.7866, 'grad_norm': 2.943310499191284, 'learning_rate': 4.009322033898305e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1269/6000 [1:00:14<3:43:15,  2.83s/it] 21%|â–ˆâ–ˆ        | 1270/6000 [1:00:17<3:40:53,  2.80s/it]                                                       {'loss': 2.8247, 'grad_norm': 1.7689049243927002, 'learning_rate': 4.008474576271187e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1270/6000 [1:00:17<3:40:53,  2.80s/it] 21%|â–ˆâ–ˆ        | 1271/6000 [1:00:20<3:42:50,  2.83s/it]                                                       {'loss': 2.7305, 'grad_norm': 1.8855341672897339, 'learning_rate': 4.007627118644068e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1271/6000 [1:00:20<3:42:50,  2.83s/it] 21%|â–ˆâ–ˆ        | 1272/6000 [1:00:23<3:48:02,  2.89s/it]                                                       {'loss': 2.7796, 'grad_norm': 2.505004644393921, 'learning_rate': 4.006779661016949e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1272/6000 [1:00:23<3:48:02,  2.89s/it] 21%|â–ˆâ–ˆ        | 1273/6000 [1:00:25<3:43:45,  2.84s/it]                                                       {'loss': 2.7524, 'grad_norm': 2.2428534030914307, 'learning_rate': 4.0059322033898304e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1273/6000 [1:00:25<3:43:45,  2.84s/it] 21%|â–ˆâ–ˆ        | 1274/6000 [1:00:28<3:39:54,  2.79s/it]                                                       {'loss': 2.818, 'grad_norm': 1.7068753242492676, 'learning_rate': 4.005084745762712e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1274/6000 [1:00:28<3:39:54,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1275/6000 [1:00:31<3:39:43,  2.79s/it]                                                       {'loss': 2.6901, 'grad_norm': 2.7029545307159424, 'learning_rate': 4.004237288135593e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1275/6000 [1:00:31<3:39:43,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1276/6000 [1:00:34<3:40:50,  2.80s/it]                                                       {'loss': 2.7295, 'grad_norm': 2.2430312633514404, 'learning_rate': 4.003389830508475e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1276/6000 [1:00:34<3:40:50,  2.80s/it] 21%|â–ˆâ–ˆâ–       | 1277/6000 [1:00:36<3:39:35,  2.79s/it]                                                       {'loss': 2.7873, 'grad_norm': 3.3816261291503906, 'learning_rate': 4.002542372881356e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1277/6000 [1:00:36<3:39:35,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1278/6000 [1:00:39<3:40:46,  2.81s/it]                                                       {'loss': 2.7836, 'grad_norm': 5.204431533813477, 'learning_rate': 4.0016949152542374e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1278/6000 [1:00:39<3:40:46,  2.81s/it] 21%|â–ˆâ–ˆâ–       | 1279/6000 [1:00:42<3:39:20,  2.79s/it]                                                       {'loss': 2.7424, 'grad_norm': 3.7391762733459473, 'learning_rate': 4.0008474576271185e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1279/6000 [1:00:42<3:39:20,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1280/6000 [1:00:45<3:39:08,  2.79s/it]                                                       {'loss': 2.8656, 'grad_norm': 9.159523010253906, 'learning_rate': 4e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1280/6000 [1:00:45<3:39:08,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1281/6000 [1:00:48<3:39:19,  2.79s/it]                                                       {'loss': 2.6586, 'grad_norm': 6.714564323425293, 'learning_rate': 3.9991525423728815e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1281/6000 [1:00:48<3:39:19,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1282/6000 [1:00:50<3:36:52,  2.76s/it]                                                       {'loss': 2.8637, 'grad_norm': 8.51032829284668, 'learning_rate': 3.998305084745763e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1282/6000 [1:00:50<3:36:52,  2.76s/it] 21%|â–ˆâ–ˆâ–       | 1283/6000 [1:00:53<3:34:53,  2.73s/it]                                                       {'loss': 2.7709, 'grad_norm': 5.659139633178711, 'learning_rate': 3.9974576271186444e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1283/6000 [1:00:53<3:34:53,  2.73s/it] 21%|â–ˆâ–ˆâ–       | 1284/6000 [1:00:56<3:35:36,  2.74s/it]                                                       {'loss': 2.818, 'grad_norm': 5.482259273529053, 'learning_rate': 3.996610169491526e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1284/6000 [1:00:56<3:35:36,  2.74s/it] 21%|â–ˆâ–ˆâ–       | 1285/6000 [1:00:58<3:34:46,  2.73s/it]                                                       {'loss': 2.9099, 'grad_norm': 8.233738899230957, 'learning_rate': 3.9957627118644066e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1285/6000 [1:00:58<3:34:46,  2.73s/it] 21%|â–ˆâ–ˆâ–       | 1286/6000 [1:01:01<3:36:01,  2.75s/it]                                                       {'loss': 2.6926, 'grad_norm': 6.421464920043945, 'learning_rate': 3.994915254237288e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1286/6000 [1:01:01<3:36:01,  2.75s/it] 21%|â–ˆâ–ˆâ–       | 1287/6000 [1:01:04<3:39:14,  2.79s/it]                                                       {'loss': 2.7735, 'grad_norm': 3.0055956840515137, 'learning_rate': 3.9940677966101696e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1287/6000 [1:01:04<3:39:14,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1288/6000 [1:01:07<3:39:08,  2.79s/it]                                                       {'loss': 2.8098, 'grad_norm': 4.362351417541504, 'learning_rate': 3.993220338983051e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1288/6000 [1:01:07<3:39:08,  2.79s/it] 21%|â–ˆâ–ˆâ–       | 1289/6000 [1:01:10<3:35:30,  2.74s/it]                                                       {'loss': 2.7949, 'grad_norm': 3.408825635910034, 'learning_rate': 3.9923728813559325e-05, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1289/6000 [1:01:10<3:35:30,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1290/6000 [1:01:12<3:36:35,  2.76s/it]                                                       {'loss': 2.751, 'grad_norm': 6.598154544830322, 'learning_rate': 3.9915254237288136e-05, 'epoch': 0.21}
 22%|â–ˆâ–ˆâ–       | 1290/6000 [1:01:12<3:36:35,  2.76s/it] 22%|â–ˆâ–ˆâ–       | 1291/6000 [1:01:15<3:34:24,  2.73s/it]                                                       {'loss': 2.8094, 'grad_norm': 2.413074016571045, 'learning_rate': 3.9906779661016955e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1291/6000 [1:01:15<3:34:24,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1292/6000 [1:01:18<3:33:51,  2.73s/it]                                                       {'loss': 2.7755, 'grad_norm': 6.379819869995117, 'learning_rate': 3.9898305084745766e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1292/6000 [1:01:18<3:33:51,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1293/6000 [1:01:21<3:36:42,  2.76s/it]                                                       {'loss': 2.7478, 'grad_norm': 2.7603676319122314, 'learning_rate': 3.988983050847458e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1293/6000 [1:01:21<3:36:42,  2.76s/it] 22%|â–ˆâ–ˆâ–       | 1294/6000 [1:01:23<3:33:36,  2.72s/it]                                                       {'loss': 2.7901, 'grad_norm': 2.0356645584106445, 'learning_rate': 3.988135593220339e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1294/6000 [1:01:23<3:33:36,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1295/6000 [1:01:26<3:32:30,  2.71s/it]                                                       {'loss': 2.803, 'grad_norm': 3.4263062477111816, 'learning_rate': 3.9872881355932206e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1295/6000 [1:01:26<3:32:30,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1296/6000 [1:01:29<3:34:24,  2.73s/it]                                                       {'loss': 2.7993, 'grad_norm': 1.848043441772461, 'learning_rate': 3.986440677966102e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1296/6000 [1:01:29<3:34:24,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1297/6000 [1:01:32<3:41:30,  2.83s/it]                                                       {'loss': 2.748, 'grad_norm': 4.467857837677002, 'learning_rate': 3.9855932203389836e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1297/6000 [1:01:32<3:41:30,  2.83s/it] 22%|â–ˆâ–ˆâ–       | 1298/6000 [1:01:34<3:38:44,  2.79s/it]                                                       {'loss': 2.7619, 'grad_norm': 2.511059284210205, 'learning_rate': 3.984745762711865e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1298/6000 [1:01:34<3:38:44,  2.79s/it] 22%|â–ˆâ–ˆâ–       | 1299/6000 [1:01:37<3:39:45,  2.80s/it]                                                       {'loss': 2.7628, 'grad_norm': 2.4377448558807373, 'learning_rate': 3.983898305084746e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1299/6000 [1:01:37<3:39:45,  2.80s/it] 22%|â–ˆâ–ˆâ–       | 1300/6000 [1:01:40<3:38:10,  2.79s/it]                                                       {'loss': 2.7965, 'grad_norm': 5.372525691986084, 'learning_rate': 3.983050847457627e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1300/6000 [1:01:40<3:38:10,  2.79s/it][2025-10-22 19:58:05,486] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 22%|â–ˆâ–ˆâ–       | 1301/6000 [1:01:45<4:30:34,  3.45s/it]                                                       {'loss': 2.8102, 'grad_norm': 1.7553340196609497, 'learning_rate': 3.982203389830509e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1301/6000 [1:01:45<4:30:34,  3.45s/it] 22%|â–ˆâ–ˆâ–       | 1302/6000 [1:01:48<4:13:12,  3.23s/it]                                                       {'loss': 2.7717, 'grad_norm': 3.261842966079712, 'learning_rate': 3.98135593220339e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1302/6000 [1:01:48<4:13:12,  3.23s/it] 22%|â–ˆâ–ˆâ–       | 1303/6000 [1:01:50<4:00:07,  3.07s/it]                                                       {'loss': 2.7804, 'grad_norm': 2.002880334854126, 'learning_rate': 3.980508474576272e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1303/6000 [1:01:50<4:00:07,  3.07s/it] 22%|â–ˆâ–ˆâ–       | 1304/6000 [1:01:53<3:52:22,  2.97s/it]                                                       {'loss': 2.7606, 'grad_norm': 1.6432769298553467, 'learning_rate': 3.979661016949153e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1304/6000 [1:01:53<3:52:22,  2.97s/it] 22%|â–ˆâ–ˆâ–       | 1305/6000 [1:01:56<3:44:39,  2.87s/it]                                                       {'loss': 2.8171, 'grad_norm': 4.028879165649414, 'learning_rate': 3.978813559322034e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1305/6000 [1:01:56<3:44:39,  2.87s/it] 22%|â–ˆâ–ˆâ–       | 1306/6000 [1:01:59<3:43:36,  2.86s/it]                                                       {'loss': 2.7705, 'grad_norm': 3.0382840633392334, 'learning_rate': 3.977966101694916e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1306/6000 [1:01:59<3:43:36,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1307/6000 [1:02:01<3:39:13,  2.80s/it]                                                       {'loss': 2.7687, 'grad_norm': 2.2196714878082275, 'learning_rate': 3.977118644067796e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1307/6000 [1:02:01<3:39:13,  2.80s/it] 22%|â–ˆâ–ˆâ–       | 1308/6000 [1:02:04<3:39:27,  2.81s/it]                                                       {'loss': 2.7431, 'grad_norm': 1.7625428438186646, 'learning_rate': 3.976271186440678e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1308/6000 [1:02:04<3:39:27,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1309/6000 [1:02:07<3:36:48,  2.77s/it]                                                       {'loss': 2.778, 'grad_norm': 2.3751845359802246, 'learning_rate': 3.975423728813559e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1309/6000 [1:02:07<3:36:48,  2.77s/it] 22%|â–ˆâ–ˆâ–       | 1310/6000 [1:02:10<3:36:08,  2.77s/it]                                                       {'loss': 2.7552, 'grad_norm': 3.0068092346191406, 'learning_rate': 3.974576271186441e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1310/6000 [1:02:10<3:36:08,  2.77s/it] 22%|â–ˆâ–ˆâ–       | 1311/6000 [1:02:12<3:33:45,  2.74s/it]                                                       {'loss': 2.8058, 'grad_norm': 3.6288504600524902, 'learning_rate': 3.973728813559322e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1311/6000 [1:02:12<3:33:45,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1312/6000 [1:02:15<3:33:04,  2.73s/it]                                                       {'loss': 2.8156, 'grad_norm': 2.7054405212402344, 'learning_rate': 3.972881355932204e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1312/6000 [1:02:15<3:33:04,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1313/6000 [1:02:18<3:32:54,  2.73s/it]                                                       {'loss': 2.8408, 'grad_norm': 3.014341354370117, 'learning_rate': 3.972033898305085e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1313/6000 [1:02:18<3:32:54,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1314/6000 [1:02:20<3:31:39,  2.71s/it]                                                       {'loss': 2.7847, 'grad_norm': 1.8993182182312012, 'learning_rate': 3.971186440677966e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1314/6000 [1:02:20<3:31:39,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1315/6000 [1:02:23<3:30:18,  2.69s/it]                                                       {'loss': 2.7667, 'grad_norm': 2.3702189922332764, 'learning_rate': 3.970338983050847e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1315/6000 [1:02:23<3:30:18,  2.69s/it] 22%|â–ˆâ–ˆâ–       | 1316/6000 [1:02:26<3:32:29,  2.72s/it]                                                       {'loss': 2.7866, 'grad_norm': 1.394957184791565, 'learning_rate': 3.969491525423729e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1316/6000 [1:02:26<3:32:29,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1317/6000 [1:02:28<3:31:36,  2.71s/it]                                                       {'loss': 2.7372, 'grad_norm': 2.3850791454315186, 'learning_rate': 3.96864406779661e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1317/6000 [1:02:28<3:31:36,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1318/6000 [1:02:31<3:31:51,  2.72s/it]                                                       {'loss': 2.7543, 'grad_norm': 1.8249233961105347, 'learning_rate': 3.967796610169492e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1318/6000 [1:02:31<3:31:51,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1319/6000 [1:02:34<3:30:47,  2.70s/it]                                                       {'loss': 2.7663, 'grad_norm': 1.761951208114624, 'learning_rate': 3.966949152542373e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1319/6000 [1:02:34<3:30:47,  2.70s/it] 22%|â–ˆâ–ˆâ–       | 1320/6000 [1:02:37<3:31:19,  2.71s/it]                                                       {'loss': 2.7395, 'grad_norm': 1.5936243534088135, 'learning_rate': 3.966101694915255e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1320/6000 [1:02:37<3:31:19,  2.71s/it] 22%|â–ˆâ–ˆâ–       | 1321/6000 [1:02:39<3:32:33,  2.73s/it]                                                       {'loss': 2.768, 'grad_norm': 2.0170748233795166, 'learning_rate': 3.9652542372881354e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1321/6000 [1:02:39<3:32:33,  2.73s/it] 22%|â–ˆâ–ˆâ–       | 1322/6000 [1:02:43<3:43:27,  2.87s/it]                                                       {'loss': 2.7655, 'grad_norm': 2.393458127975464, 'learning_rate': 3.964406779661017e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1322/6000 [1:02:43<3:43:27,  2.87s/it] 22%|â–ˆâ–ˆâ–       | 1323/6000 [1:02:45<3:42:35,  2.86s/it]                                                       {'loss': 2.7796, 'grad_norm': 2.2733852863311768, 'learning_rate': 3.9635593220338983e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1323/6000 [1:02:45<3:42:35,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1324/6000 [1:02:48<3:39:21,  2.81s/it]                                                       {'loss': 2.8429, 'grad_norm': 2.6344642639160156, 'learning_rate': 3.96271186440678e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1324/6000 [1:02:48<3:39:21,  2.81s/it] 22%|â–ˆâ–ˆâ–       | 1325/6000 [1:02:51<3:38:29,  2.80s/it]                                                       {'loss': 2.7666, 'grad_norm': 2.232475519180298, 'learning_rate': 3.961864406779661e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1325/6000 [1:02:51<3:38:29,  2.80s/it] 22%|â–ˆâ–ˆâ–       | 1326/6000 [1:02:54<3:37:15,  2.79s/it]                                                       {'loss': 2.7384, 'grad_norm': 4.465761661529541, 'learning_rate': 3.9610169491525424e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1326/6000 [1:02:54<3:37:15,  2.79s/it] 22%|â–ˆâ–ˆâ–       | 1327/6000 [1:02:56<3:37:08,  2.79s/it]                                                       {'loss': 2.7361, 'grad_norm': 3.3001012802124023, 'learning_rate': 3.960169491525424e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1327/6000 [1:02:56<3:37:08,  2.79s/it] 22%|â–ˆâ–ˆâ–       | 1328/6000 [1:02:59<3:36:22,  2.78s/it]                                                       {'loss': 2.8604, 'grad_norm': 3.8656227588653564, 'learning_rate': 3.9593220338983053e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1328/6000 [1:02:59<3:36:22,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1329/6000 [1:03:02<3:45:29,  2.90s/it]                                                       {'loss': 2.9128, 'grad_norm': 8.403669357299805, 'learning_rate': 3.9584745762711865e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1329/6000 [1:03:02<3:45:29,  2.90s/it] 22%|â–ˆâ–ˆâ–       | 1330/6000 [1:03:05<3:41:36,  2.85s/it]                                                       {'loss': 2.8048, 'grad_norm': 3.1260080337524414, 'learning_rate': 3.9576271186440676e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1330/6000 [1:03:05<3:41:36,  2.85s/it] 22%|â–ˆâ–ˆâ–       | 1331/6000 [1:03:08<3:39:23,  2.82s/it]                                                       {'loss': 2.8796, 'grad_norm': 5.859095573425293, 'learning_rate': 3.9567796610169494e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1331/6000 [1:03:08<3:39:23,  2.82s/it] 22%|â–ˆâ–ˆâ–       | 1332/6000 [1:03:11<3:37:01,  2.79s/it]                                                       {'loss': 2.7518, 'grad_norm': 4.962040424346924, 'learning_rate': 3.9559322033898305e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1332/6000 [1:03:11<3:37:01,  2.79s/it] 22%|â–ˆâ–ˆâ–       | 1333/6000 [1:03:13<3:37:34,  2.80s/it]                                                       {'loss': 2.7465, 'grad_norm': 4.40953254699707, 'learning_rate': 3.9550847457627123e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1333/6000 [1:03:13<3:37:34,  2.80s/it] 22%|â–ˆâ–ˆâ–       | 1334/6000 [1:03:16<3:36:27,  2.78s/it]                                                       {'loss': 2.7524, 'grad_norm': 3.308321475982666, 'learning_rate': 3.9542372881355935e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1334/6000 [1:03:16<3:36:27,  2.78s/it] 22%|â–ˆâ–ˆâ–       | 1335/6000 [1:03:19<3:34:14,  2.76s/it]                                                       {'loss': 2.7151, 'grad_norm': 3.1728501319885254, 'learning_rate': 3.9533898305084746e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1335/6000 [1:03:19<3:34:14,  2.76s/it] 22%|â–ˆâ–ˆâ–       | 1336/6000 [1:03:21<3:33:14,  2.74s/it]                                                       {'loss': 2.7799, 'grad_norm': 3.5673537254333496, 'learning_rate': 3.952542372881356e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1336/6000 [1:03:21<3:33:14,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1337/6000 [1:03:24<3:31:17,  2.72s/it]                                                       {'loss': 2.8025, 'grad_norm': 5.489956855773926, 'learning_rate': 3.9516949152542375e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1337/6000 [1:03:24<3:31:17,  2.72s/it] 22%|â–ˆâ–ˆâ–       | 1338/6000 [1:03:27<3:32:31,  2.74s/it]                                                       {'loss': 2.7652, 'grad_norm': 6.081405162811279, 'learning_rate': 3.9508474576271187e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1338/6000 [1:03:27<3:32:31,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1339/6000 [1:03:30<3:32:28,  2.74s/it]                                                       {'loss': 2.8483, 'grad_norm': 4.309269905090332, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1339/6000 [1:03:30<3:32:28,  2.74s/it] 22%|â–ˆâ–ˆâ–       | 1340/6000 [1:03:33<3:35:28,  2.77s/it]                                                       {'loss': 2.7863, 'grad_norm': 5.2206807136535645, 'learning_rate': 3.9491525423728816e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1340/6000 [1:03:33<3:35:28,  2.77s/it] 22%|â–ˆâ–ˆâ–       | 1341/6000 [1:03:35<3:33:35,  2.75s/it]                                                       {'loss': 2.7785, 'grad_norm': 2.2165331840515137, 'learning_rate': 3.9483050847457634e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1341/6000 [1:03:35<3:33:35,  2.75s/it] 22%|â–ˆâ–ˆâ–       | 1342/6000 [1:03:38<3:41:40,  2.86s/it]                                                       {'loss': 2.8296, 'grad_norm': 2.0288541316986084, 'learning_rate': 3.9474576271186445e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1342/6000 [1:03:38<3:41:40,  2.86s/it] 22%|â–ˆâ–ˆâ–       | 1343/6000 [1:03:41<3:46:23,  2.92s/it]                                                       {'loss': 2.7331, 'grad_norm': 1.7145065069198608, 'learning_rate': 3.9466101694915257e-05, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1343/6000 [1:03:41<3:46:23,  2.92s/it]