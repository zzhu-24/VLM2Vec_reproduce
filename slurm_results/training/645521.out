==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test3-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/train.log
W1022 20:54:29.047000 135936044648256 torch/distributed/run.py:779] 
W1022 20:54:29.047000 135936044648256 torch/distributed/run.py:779] *****************************************
W1022 20:54:29.047000 135936044648256 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 20:54:29.047000 135936044648256 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-22 20:54:38,861] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.08it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.82it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251022_205439-t5kw2guy
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test3-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/t5kw2guy
[2025-10-22 20:54:40,393] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.19it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.05it/s]
[2025-10-22 20:54:41,019] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-10-22 20:54:49,939] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-22 20:54:51,162] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-22 20:54:51,162] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-22 20:54:55,460] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-22 20:54:55,460] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-22 20:54:56,338] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-22 20:54:56,339] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-22 20:54:56,339] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-22 20:54:56,341] INFO [src.utils:19] ==================================================
[2025-10-22 20:54:56,341] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-22 20:54:56,342] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 20:54:56,343] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 20:54:56,344] INFO [src.utils:19] ==================================================
[2025-10-22 20:54:58,097] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 20:54:58,097] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 20:54:58,098] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 20:54:58,098] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 20:54:58,098] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 20:54:58,098] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 20:54:58,098] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 20:54:58,098] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 20:54:58,098] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 20:54:58,099] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 20:54:58,099] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 20:54:58,099] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 20:54:58,100] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 20:54:58,100] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 20:54:58,106] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 20:54:58,108] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 20:54:58,113] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-10-22 20:54:58,116] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1022 20:55:01.832845685 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1022 20:55:01.866799138 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:48:13,  4.08s/it]                                                  {'loss': 11.3703, 'grad_norm': 2564.57275390625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:48:13,  4.08s/it]  0%|          | 2/6000 [00:06<5:21:58,  3.22s/it]                                                  {'loss': 8.8499, 'grad_norm': 1832.630615234375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:21:58,  3.22s/it]  0%|          | 3/6000 [00:09<4:59:36,  3.00s/it]                                                  {'loss': 8.7183, 'grad_norm': 2471.591796875, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:59:36,  3.00s/it]  0%|          | 4/6000 [00:12<4:48:27,  2.89s/it]                                                  {'loss': 8.7937, 'grad_norm': 2024.252197265625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:48:27,  2.89s/it]  0%|          | 5/6000 [00:14<4:42:54,  2.83s/it]                                                  {'loss': 8.9009, 'grad_norm': 1484.9364013671875, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:42:54,  2.83s/it]  0%|          | 6/6000 [00:17<4:37:22,  2.78s/it]                                                  {'loss': 9.3436, 'grad_norm': 1781.478271484375, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:37:22,  2.78s/it]  0%|          | 7/6000 [00:20<4:34:04,  2.74s/it]                                                  {'loss': 9.069, 'grad_norm': 1669.842041015625, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:34:04,  2.74s/it]  0%|          | 8/6000 [00:22<4:30:04,  2.70s/it]                                                  {'loss': 8.3644, 'grad_norm': 2083.67919921875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:30:04,  2.70s/it]  0%|          | 9/6000 [00:25<4:31:27,  2.72s/it]                                                  {'loss': 6.0931, 'grad_norm': 1466.28173828125, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:31:27,  2.72s/it]  0%|          | 10/6000 [00:28<4:30:46,  2.71s/it]                                                   {'loss': 7.1386, 'grad_norm': 1417.779296875, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:30:46,  2.71s/it]  0%|          | 11/6000 [00:31<4:41:24,  2.82s/it]                                                   {'loss': 8.8272, 'grad_norm': 2438.9287109375, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:41:24,  2.82s/it]  0%|          | 12/6000 [00:34<4:45:05,  2.86s/it]                                                   {'loss': 6.4694, 'grad_norm': 1591.749755859375, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:45:05,  2.86s/it]  0%|          | 13/6000 [00:37<4:42:01,  2.83s/it]                                                   {'loss': 6.5726, 'grad_norm': 1578.564453125, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:42:01,  2.83s/it]  0%|          | 14/6000 [00:39<4:41:07,  2.82s/it]                                                   {'loss': 5.3535, 'grad_norm': 2102.616943359375, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:39<4:41:07,  2.82s/it]  0%|          | 15/6000 [00:42<4:35:59,  2.77s/it]                                                   {'loss': 4.474, 'grad_norm': 1618.05224609375, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:35:59,  2.77s/it]  0%|          | 16/6000 [00:45<4:32:15,  2.73s/it]                                                   {'loss': 4.5538, 'grad_norm': 1283.921875, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:32:15,  2.73s/it]  0%|          | 17/6000 [00:47<4:32:38,  2.73s/it]                                                   {'loss': 3.6879, 'grad_norm': 798.8887329101562, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:47<4:32:38,  2.73s/it]  0%|          | 18/6000 [00:50<4:31:20,  2.72s/it]                                                   {'loss': 3.1625, 'grad_norm': 891.1436767578125, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:31:20,  2.72s/it]  0%|          | 19/6000 [00:53<4:28:52,  2.70s/it]                                                   {'loss': 3.2871, 'grad_norm': 476.94732666015625, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:28:52,  2.70s/it]  0%|          | 20/6000 [00:55<4:29:12,  2.70s/it]                                                   {'loss': 3.1705, 'grad_norm': 318.180419921875, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:55<4:29:12,  2.70s/it]  0%|          | 21/6000 [00:58<4:32:29,  2.73s/it]                                                   {'loss': 2.8508, 'grad_norm': 278.4761962890625, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:58<4:32:29,  2.73s/it]  0%|          | 22/6000 [01:01<4:33:38,  2.75s/it]                                                   {'loss': 3.0647, 'grad_norm': 413.88446044921875, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:33:38,  2.75s/it]  0%|          | 23/6000 [01:04<4:31:30,  2.73s/it]                                                   {'loss': 3.0271, 'grad_norm': 325.32293701171875, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:31:30,  2.73s/it]  0%|          | 24/6000 [01:07<4:33:53,  2.75s/it]                                                   {'loss': 2.9556, 'grad_norm': 74.67266082763672, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:33:53,  2.75s/it]  0%|          | 25/6000 [01:09<4:33:14,  2.74s/it]                                                   {'loss': 2.8749, 'grad_norm': 192.85647583007812, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:09<4:33:14,  2.74s/it]  0%|          | 26/6000 [01:12<4:33:16,  2.74s/it]                                                   {'loss': 2.9296, 'grad_norm': 122.01929473876953, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:12<4:33:16,  2.74s/it]  0%|          | 27/6000 [01:15<4:32:52,  2.74s/it]                                                   {'loss': 2.8537, 'grad_norm': 92.67526245117188, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:32:52,  2.74s/it]  0%|          | 28/6000 [01:18<4:59:25,  3.01s/it]                                                   {'loss': 2.8381, 'grad_norm': 83.26182556152344, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:18<4:59:25,  3.01s/it]  0%|          | 29/6000 [01:21<4:49:23,  2.91s/it]                                                   {'loss': 2.8083, 'grad_norm': 51.71352767944336, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:21<4:49:23,  2.91s/it]  0%|          | 30/6000 [01:24<4:43:37,  2.85s/it]                                                   {'loss': 2.823, 'grad_norm': 68.02983856201172, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:43:37,  2.85s/it]  1%|          | 31/6000 [01:26<4:38:59,  2.80s/it]                                                   {'loss': 2.8102, 'grad_norm': 34.570350646972656, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:26<4:38:59,  2.80s/it]  1%|          | 32/6000 [01:29<4:36:26,  2.78s/it]                                                   {'loss': 2.8021, 'grad_norm': 58.526039123535156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:29<4:36:26,  2.78s/it]  1%|          | 33/6000 [01:32<4:34:53,  2.76s/it]                                                   {'loss': 2.7876, 'grad_norm': 55.268314361572266, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:32<4:34:53,  2.76s/it]  1%|          | 34/6000 [01:35<4:33:14,  2.75s/it]                                                   {'loss': 2.8459, 'grad_norm': 77.07478332519531, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:33:14,  2.75s/it]  1%|          | 35/6000 [01:37<4:33:47,  2.75s/it]                                                   {'loss': 2.8076, 'grad_norm': 70.97207641601562, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:37<4:33:47,  2.75s/it]  1%|          | 36/6000 [01:40<4:31:34,  2.73s/it]                                                   {'loss': 2.8218, 'grad_norm': 73.27017211914062, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:40<4:31:34,  2.73s/it]  1%|          | 37/6000 [01:43<4:29:43,  2.71s/it]                                                   {'loss': 2.812, 'grad_norm': 73.1921157836914, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:29:43,  2.71s/it]  1%|          | 38/6000 [01:45<4:28:53,  2.71s/it]                                                   {'loss': 2.7823, 'grad_norm': 40.048770904541016, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:45<4:28:53,  2.71s/it]  1%|          | 39/6000 [01:48<4:27:50,  2.70s/it]                                                   {'loss': 2.8579, 'grad_norm': 58.18981170654297, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:48<4:27:50,  2.70s/it]  1%|          | 40/6000 [01:51<4:27:57,  2.70s/it]                                                   {'loss': 2.8391, 'grad_norm': 91.0898208618164, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:51<4:27:57,  2.70s/it]  1%|          | 41/6000 [01:53<4:28:31,  2.70s/it]                                                   {'loss': 2.7897, 'grad_norm': 24.551469802856445, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:53<4:28:31,  2.70s/it]  1%|          | 42/6000 [01:56<4:27:58,  2.70s/it]                                                   {'loss': 2.7813, 'grad_norm': 29.15278434753418, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:56<4:27:58,  2.70s/it]  1%|          | 43/6000 [02:00<4:59:02,  3.01s/it]                                                   {'loss': 2.8046, 'grad_norm': 38.87826156616211, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [02:00<4:59:02,  3.01s/it]  1%|          | 44/6000 [02:03<5:03:46,  3.06s/it]                                                   {'loss': 2.7807, 'grad_norm': 39.11713409423828, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:03<5:03:46,  3.06s/it]  1%|          | 45/6000 [02:06<4:56:24,  2.99s/it]                                                   {'loss': 2.7941, 'grad_norm': 11.38315200805664, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:06<4:56:24,  2.99s/it]  1%|          | 46/6000 [02:09<4:50:50,  2.93s/it]                                                   {'loss': 2.7749, 'grad_norm': 14.646467208862305, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:09<4:50:50,  2.93s/it]  1%|          | 47/6000 [02:11<4:43:46,  2.86s/it]                                                   {'loss': 2.786, 'grad_norm': 99.86282348632812, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:11<4:43:46,  2.86s/it]  1%|          | 48/6000 [02:14<4:41:50,  2.84s/it]                                                   {'loss': 2.7853, 'grad_norm': 75.44588470458984, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:14<4:41:50,  2.84s/it]  1%|          | 49/6000 [02:17<4:36:04,  2.78s/it]                                                   {'loss': 2.7824, 'grad_norm': 20.505802154541016, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:17<4:36:04,  2.78s/it]  1%|          | 50/6000 [02:20<4:38:43,  2.81s/it]                                                   {'loss': 2.7795, 'grad_norm': 14.355629920959473, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:20<4:38:43,  2.81s/it][2025-10-22 20:57:18,529] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-10-22 20:57:18,546] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 20:57:19,173] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:25<5:39:34,  3.42s/it]                                                   {'loss': 2.7963, 'grad_norm': 14.585464477539062, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:25<5:39:34,  3.42s/it]  1%|          | 52/6000 [02:27<5:18:18,  3.21s/it]                                                   {'loss': 2.7913, 'grad_norm': 15.076726913452148, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:27<5:18:18,  3.21s/it]  1%|          | 53/6000 [02:30<5:06:08,  3.09s/it]                                                   {'loss': 2.8757, 'grad_norm': 20.289079666137695, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:30<5:06:08,  3.09s/it]  1%|          | 54/6000 [02:33<4:53:55,  2.97s/it]                                                   {'loss': 2.7894, 'grad_norm': 20.374439239501953, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:33<4:53:55,  2.97s/it]  1%|          | 55/6000 [02:36<4:46:38,  2.89s/it]                                                   {'loss': 2.7897, 'grad_norm': 47.16457748413086, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:36<4:46:38,  2.89s/it]  1%|          | 56/6000 [02:38<4:42:24,  2.85s/it]                                                   {'loss': 2.7823, 'grad_norm': 15.233864784240723, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:38<4:42:24,  2.85s/it]  1%|          | 57/6000 [02:41<4:38:36,  2.81s/it]                                                   {'loss': 2.7837, 'grad_norm': 18.992582321166992, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:41<4:38:36,  2.81s/it]  1%|          | 58/6000 [02:44<4:35:42,  2.78s/it]                                                   {'loss': 2.7772, 'grad_norm': 19.019071578979492, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:44<4:35:42,  2.78s/it]  1%|          | 59/6000 [02:46<4:34:20,  2.77s/it]                                                   {'loss': 2.7875, 'grad_norm': 22.924795150756836, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:46<4:34:20,  2.77s/it]  1%|          | 60/6000 [02:49<4:31:02,  2.74s/it]                                                   {'loss': 2.7858, 'grad_norm': 15.80793285369873, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:49<4:31:02,  2.74s/it]  1%|          | 61/6000 [02:52<4:27:28,  2.70s/it]                                                   {'loss': 2.8038, 'grad_norm': 24.78727149963379, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:52<4:27:28,  2.70s/it]  1%|          | 62/6000 [02:54<4:28:03,  2.71s/it]                                                   {'loss': 2.8032, 'grad_norm': 15.488447189331055, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:54<4:28:03,  2.71s/it]  1%|          | 63/6000 [02:57<4:29:18,  2.72s/it]                                                   {'loss': 2.7832, 'grad_norm': 17.634904861450195, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:57<4:29:18,  2.72s/it]  1%|          | 64/6000 [03:00<4:27:40,  2.71s/it]                                                   {'loss': 2.7766, 'grad_norm': 11.969337463378906, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [03:00<4:27:40,  2.71s/it]  1%|          | 65/6000 [03:03<4:25:57,  2.69s/it]                                                   {'loss': 2.7912, 'grad_norm': 14.967514991760254, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [03:03<4:25:57,  2.69s/it]  1%|          | 66/6000 [03:06<4:39:18,  2.82s/it]                                                   {'loss': 2.7813, 'grad_norm': 11.028284072875977, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:06<4:39:18,  2.82s/it]  1%|          | 67/6000 [03:08<4:36:37,  2.80s/it]                                                   {'loss': 2.7905, 'grad_norm': 11.765602111816406, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:08<4:36:37,  2.80s/it]  1%|          | 68/6000 [03:11<4:32:49,  2.76s/it]                                                   {'loss': 2.7873, 'grad_norm': 29.524024963378906, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:11<4:32:49,  2.76s/it]  1%|          | 69/6000 [03:14<4:31:49,  2.75s/it]                                                   {'loss': 2.7897, 'grad_norm': 7.535256862640381, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:14<4:31:49,  2.75s/it]  1%|          | 70/6000 [03:17<4:32:11,  2.75s/it]                                                   {'loss': 2.7925, 'grad_norm': 11.30912971496582, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:17<4:32:11,  2.75s/it]  1%|          | 71/6000 [03:19<4:31:14,  2.74s/it]                                                   {'loss': 2.7807, 'grad_norm': 6.8473100662231445, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:19<4:31:14,  2.74s/it]  1%|          | 72/6000 [03:23<4:49:18,  2.93s/it]                                                   {'loss': 2.7737, 'grad_norm': 8.829439163208008, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:23<4:49:18,  2.93s/it]  1%|          | 73/6000 [03:25<4:45:02,  2.89s/it]                                                   {'loss': 2.8255, 'grad_norm': 11.13896656036377, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:25<4:45:02,  2.89s/it]  1%|          | 74/6000 [03:28<4:38:54,  2.82s/it]                                                   {'loss': 2.7773, 'grad_norm': 24.93313980102539, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:28<4:38:54,  2.82s/it]  1%|â–         | 75/6000 [03:31<4:34:53,  2.78s/it]                                                   {'loss': 2.7911, 'grad_norm': 13.830080032348633, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:31<4:34:53,  2.78s/it]  1%|â–         | 76/6000 [03:34<4:36:26,  2.80s/it]                                                   {'loss': 2.808, 'grad_norm': 24.09355354309082, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:34<4:36:26,  2.80s/it]  1%|â–         | 77/6000 [03:36<4:36:17,  2.80s/it]                                                   {'loss': 2.8433, 'grad_norm': 20.745298385620117, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:36<4:36:17,  2.80s/it]  1%|â–         | 78/6000 [03:40<4:45:31,  2.89s/it]                                                   {'loss': 2.7712, 'grad_norm': 19.16971778869629, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:40<4:45:31,  2.89s/it]  1%|â–         | 79/6000 [03:42<4:39:44,  2.83s/it]                                                   {'loss': 2.7815, 'grad_norm': 10.678010940551758, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:42<4:39:44,  2.83s/it]  1%|â–         | 80/6000 [03:45<4:42:39,  2.86s/it]                                                   {'loss': 2.7723, 'grad_norm': 25.512224197387695, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:45<4:42:39,  2.86s/it]  1%|â–         | 81/6000 [03:48<4:35:59,  2.80s/it]                                                   {'loss': 2.7872, 'grad_norm': 26.927112579345703, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:48<4:35:59,  2.80s/it]  1%|â–         | 82/6000 [03:51<4:33:38,  2.77s/it]                                                   {'loss': 2.7672, 'grad_norm': 33.925636291503906, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:51<4:33:38,  2.77s/it]  1%|â–         | 83/6000 [03:53<4:31:02,  2.75s/it]                                                   {'loss': 2.7936, 'grad_norm': 11.971101760864258, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:53<4:31:02,  2.75s/it]  1%|â–         | 84/6000 [03:56<4:32:48,  2.77s/it]                                                   {'loss': 2.779, 'grad_norm': 15.59002685546875, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:56<4:32:48,  2.77s/it]  1%|â–         | 85/6000 [03:59<4:43:04,  2.87s/it]                                                   {'loss': 2.7788, 'grad_norm': 20.047313690185547, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:59<4:43:04,  2.87s/it]  1%|â–         | 86/6000 [04:02<4:38:50,  2.83s/it]                                                   {'loss': 2.7928, 'grad_norm': 6.856967449188232, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:02<4:38:50,  2.83s/it]  1%|â–         | 87/6000 [04:05<4:33:22,  2.77s/it]                                                   {'loss': 2.7853, 'grad_norm': 7.142881870269775, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:05<4:33:22,  2.77s/it]  1%|â–         | 88/6000 [04:07<4:30:11,  2.74s/it]                                                   {'loss': 2.7747, 'grad_norm': 13.81177806854248, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:07<4:30:11,  2.74s/it]  1%|â–         | 89/6000 [04:10<4:26:59,  2.71s/it]                                                   {'loss': 2.7769, 'grad_norm': 11.413623809814453, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:10<4:26:59,  2.71s/it]  2%|â–         | 90/6000 [04:13<4:29:17,  2.73s/it]                                                   {'loss': 2.8303, 'grad_norm': 29.423940658569336, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:13<4:29:17,  2.73s/it]  2%|â–         | 91/6000 [04:15<4:29:57,  2.74s/it]                                                   {'loss': 2.7858, 'grad_norm': 37.02952194213867, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:15<4:29:57,  2.74s/it]  2%|â–         | 92/6000 [04:18<4:40:38,  2.85s/it]                                                   {'loss': 2.7757, 'grad_norm': 40.284400939941406, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:18<4:40:38,  2.85s/it]  2%|â–         | 93/6000 [04:21<4:40:11,  2.85s/it]                                                   {'loss': 2.8045, 'grad_norm': 25.84905433654785, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:21<4:40:11,  2.85s/it]  2%|â–         | 94/6000 [04:24<4:39:04,  2.84s/it]                                                   {'loss': 2.7973, 'grad_norm': 13.079703330993652, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:24<4:39:04,  2.84s/it]  2%|â–         | 95/6000 [04:27<4:35:00,  2.79s/it]                                                   {'loss': 2.7756, 'grad_norm': 22.641103744506836, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:27<4:35:00,  2.79s/it]  2%|â–         | 96/6000 [04:30<4:33:42,  2.78s/it]                                                   {'loss': 2.79, 'grad_norm': 41.85572052001953, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:30<4:33:42,  2.78s/it]  2%|â–         | 97/6000 [04:32<4:31:23,  2.76s/it]                                                   {'loss': 2.7933, 'grad_norm': 34.060298919677734, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:32<4:31:23,  2.76s/it]  2%|â–         | 98/6000 [04:35<4:28:08,  2.73s/it]                                                   {'loss': 2.7942, 'grad_norm': 22.120159149169922, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:35<4:28:08,  2.73s/it]  2%|â–         | 99/6000 [04:38<4:24:40,  2.69s/it]                                                   {'loss': 2.7909, 'grad_norm': 14.099600791931152, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:38<4:24:40,  2.69s/it]  2%|â–         | 100/6000 [04:40<4:24:20,  2.69s/it]                                                    {'loss': 2.7772, 'grad_norm': 19.253705978393555, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:40<4:24:20,  2.69s/it][2025-10-22 20:59:39,012] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-10-22 20:59:39,022] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 20:59:39,786] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:46<6:02:22,  3.69s/it]                                                    {'loss': 2.7907, 'grad_norm': 14.09744930267334, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:46<6:02:22,  3.69s/it]  2%|â–         | 102/6000 [04:49<5:34:06,  3.40s/it]                                                    {'loss': 2.7962, 'grad_norm': 11.922965049743652, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:49<5:34:06,  3.40s/it]  2%|â–         | 103/6000 [04:52<5:15:32,  3.21s/it]                                                    {'loss': 2.7818, 'grad_norm': 6.456881046295166, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:52<5:15:32,  3.21s/it]  2%|â–         | 104/6000 [04:55<5:08:05,  3.14s/it]                                                    {'loss': 2.7751, 'grad_norm': 8.463698387145996, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:55<5:08:05,  3.14s/it]  2%|â–         | 105/6000 [04:57<4:58:09,  3.03s/it]                                                    {'loss': 2.7831, 'grad_norm': 9.914146423339844, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:57<4:58:09,  3.03s/it]  2%|â–         | 106/6000 [05:00<4:52:24,  2.98s/it]                                                    {'loss': 2.7742, 'grad_norm': 5.224260330200195, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:00<4:52:24,  2.98s/it]  2%|â–         | 107/6000 [05:03<4:44:28,  2.90s/it]                                                    {'loss': 2.7914, 'grad_norm': 5.207769393920898, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:03<4:44:28,  2.90s/it]  2%|â–         | 108/6000 [05:06<4:42:31,  2.88s/it]                                                    {'loss': 2.7784, 'grad_norm': 3.703000545501709, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:06<4:42:31,  2.88s/it]  2%|â–         | 109/6000 [05:09<4:51:21,  2.97s/it]                                                    {'loss': 2.8184, 'grad_norm': 3.0262365341186523, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:09<4:51:21,  2.97s/it]  2%|â–         | 110/6000 [05:12<4:44:06,  2.89s/it]                                                    {'loss': 2.854, 'grad_norm': 8.196920394897461, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:12<4:44:06,  2.89s/it]  2%|â–         | 111/6000 [05:15<4:41:46,  2.87s/it]                                                    {'loss': 2.7857, 'grad_norm': 8.615843772888184, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:15<4:41:46,  2.87s/it]  2%|â–         | 112/6000 [05:18<4:54:46,  3.00s/it]                                                    {'loss': 2.7818, 'grad_norm': 10.273004531860352, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:18<4:54:46,  3.00s/it]  2%|â–         | 113/6000 [05:21<4:47:21,  2.93s/it]                                                    {'loss': 2.7679, 'grad_norm': 16.666994094848633, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:21<4:47:21,  2.93s/it]  2%|â–         | 114/6000 [05:23<4:43:03,  2.89s/it]                                                    {'loss': 2.7736, 'grad_norm': 3.1714096069335938, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:23<4:43:03,  2.89s/it]  2%|â–         | 115/6000 [05:26<4:37:56,  2.83s/it]                                                    {'loss': 2.8485, 'grad_norm': 10.330784797668457, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:26<4:37:56,  2.83s/it]  2%|â–         | 116/6000 [05:29<4:34:04,  2.79s/it]                                                    {'loss': 2.7935, 'grad_norm': 6.428518772125244, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:29<4:34:04,  2.79s/it]  2%|â–         | 117/6000 [05:32<4:32:38,  2.78s/it]                                                    {'loss': 2.9045, 'grad_norm': 3.2347571849823, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:32<4:32:38,  2.78s/it]  2%|â–         | 118/6000 [05:35<4:48:51,  2.95s/it]                                                    {'loss': 2.7801, 'grad_norm': 4.0160040855407715, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:35<4:48:51,  2.95s/it]  2%|â–         | 119/6000 [05:38<4:44:12,  2.90s/it]                                                    {'loss': 2.7773, 'grad_norm': 6.941405773162842, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:38<4:44:12,  2.90s/it]  2%|â–         | 120/6000 [05:40<4:39:43,  2.85s/it]                                                    {'loss': 2.7739, 'grad_norm': 7.112756729125977, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:40<4:39:43,  2.85s/it]  2%|â–         | 121/6000 [05:43<4:36:48,  2.82s/it]                                                    {'loss': 2.7805, 'grad_norm': 4.784026622772217, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:43<4:36:48,  2.82s/it]  2%|â–         | 122/6000 [05:46<4:37:01,  2.83s/it]                                                    {'loss': 2.7761, 'grad_norm': 7.271839141845703, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:46<4:37:01,  2.83s/it]  2%|â–         | 123/6000 [05:49<4:32:40,  2.78s/it]                                                    {'loss': 2.7914, 'grad_norm': 5.203428745269775, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:49<4:32:40,  2.78s/it]  2%|â–         | 124/6000 [05:51<4:28:26,  2.74s/it]                                                    {'loss': 2.8272, 'grad_norm': 6.907011985778809, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:51<4:28:26,  2.74s/it]  2%|â–         | 125/6000 [05:54<4:34:17,  2.80s/it]                                                    {'loss': 2.9425, 'grad_norm': 14.62547492980957, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:54<4:34:17,  2.80s/it]  2%|â–         | 126/6000 [05:57<4:34:19,  2.80s/it]                                                    {'loss': 2.7753, 'grad_norm': 5.7844929695129395, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:57<4:34:19,  2.80s/it]  2%|â–         | 127/6000 [06:00<4:35:49,  2.82s/it]                                                    {'loss': 2.7775, 'grad_norm': 4.034266948699951, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:00<4:35:49,  2.82s/it]  2%|â–         | 128/6000 [06:03<4:33:27,  2.79s/it]                                                    {'loss': 2.7911, 'grad_norm': 7.82472038269043, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:03<4:33:27,  2.79s/it]  2%|â–         | 129/6000 [06:05<4:31:53,  2.78s/it]                                                    {'loss': 2.8528, 'grad_norm': 6.272214412689209, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:05<4:31:53,  2.78s/it]  2%|â–         | 130/6000 [06:08<4:30:02,  2.76s/it]                                                    {'loss': 2.7736, 'grad_norm': 2.9645044803619385, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:08<4:30:02,  2.76s/it]  2%|â–         | 131/6000 [06:11<4:28:05,  2.74s/it]                                                    {'loss': 2.7876, 'grad_norm': 5.512325286865234, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:11<4:28:05,  2.74s/it]  2%|â–         | 132/6000 [06:14<4:26:30,  2.73s/it]                                                    {'loss': 2.7832, 'grad_norm': 9.287848472595215, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:14<4:26:30,  2.73s/it]  2%|â–         | 133/6000 [06:16<4:26:34,  2.73s/it]                                                    {'loss': 2.7764, 'grad_norm': 1.9526009559631348, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:16<4:26:34,  2.73s/it]  2%|â–         | 134/6000 [06:19<4:32:49,  2.79s/it]                                                    {'loss': 2.777, 'grad_norm': 2.4043755531311035, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:19<4:32:49,  2.79s/it]  2%|â–         | 135/6000 [06:22<4:30:43,  2.77s/it]                                                    {'loss': 2.7936, 'grad_norm': 4.10207986831665, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:22<4:30:43,  2.77s/it]  2%|â–         | 136/6000 [06:25<4:27:49,  2.74s/it]                                                    {'loss': 2.7808, 'grad_norm': 5.016515254974365, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:25<4:27:49,  2.74s/it]  2%|â–         | 137/6000 [06:28<4:43:53,  2.91s/it]                                                    {'loss': 2.7741, 'grad_norm': 5.31311559677124, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:28<4:43:53,  2.91s/it]  2%|â–         | 138/6000 [06:31<4:38:00,  2.85s/it]                                                    {'loss': 2.778, 'grad_norm': 2.588085889816284, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:31<4:38:00,  2.85s/it]  2%|â–         | 139/6000 [06:33<4:37:36,  2.84s/it]                                                    {'loss': 2.8274, 'grad_norm': 4.043485641479492, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:33<4:37:36,  2.84s/it]  2%|â–         | 140/6000 [06:37<4:45:09,  2.92s/it]                                                    {'loss': 2.7752, 'grad_norm': 9.259252548217773, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:37<4:45:09,  2.92s/it]  2%|â–         | 141/6000 [06:39<4:39:55,  2.87s/it]                                                    {'loss': 2.781, 'grad_norm': 2.0781161785125732, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:39<4:39:55,  2.87s/it]  2%|â–         | 142/6000 [06:42<4:33:17,  2.80s/it]                                                    {'loss': 2.7758, 'grad_norm': 1.7590765953063965, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:42<4:33:17,  2.80s/it]  2%|â–         | 143/6000 [06:45<4:30:55,  2.78s/it]                                                    {'loss': 2.7729, 'grad_norm': 2.074389934539795, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:45<4:30:55,  2.78s/it]  2%|â–         | 144/6000 [06:47<4:30:04,  2.77s/it]                                                    {'loss': 2.788, 'grad_norm': 1.7636046409606934, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:47<4:30:04,  2.77s/it]  2%|â–         | 145/6000 [06:50<4:26:53,  2.73s/it]                                                    {'loss': 2.8033, 'grad_norm': 4.3752312660217285, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:50<4:26:53,  2.73s/it]  2%|â–         | 146/6000 [06:53<4:28:41,  2.75s/it]                                                    {'loss': 2.8052, 'grad_norm': 4.758136749267578, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:53<4:28:41,  2.75s/it]  2%|â–         | 147/6000 [06:56<4:25:55,  2.73s/it]                                                    {'loss': 2.7782, 'grad_norm': 4.9658522605896, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:56<4:25:55,  2.73s/it]  2%|â–         | 148/6000 [06:58<4:26:33,  2.73s/it]                                                    {'loss': 2.7891, 'grad_norm': 4.427802562713623, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:58<4:26:33,  2.73s/it]  2%|â–         | 149/6000 [07:01<4:23:41,  2.70s/it]                                                    {'loss': 2.7748, 'grad_norm': 1.8167635202407837, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:01<4:23:41,  2.70s/it]  2%|â–Ž         | 150/6000 [07:04<4:23:00,  2.70s/it]                                                    {'loss': 2.7875, 'grad_norm': 3.7181618213653564, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:04<4:23:00,  2.70s/it][2025-10-22 21:02:02,404] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-10-22 21:02:02,419] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:02:03,251] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:09<5:34:38,  3.43s/it]                                                    {'loss': 2.7769, 'grad_norm': 2.1936821937561035, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:09<5:34:38,  3.43s/it]  3%|â–Ž         | 152/6000 [07:11<5:13:13,  3.21s/it]                                                    {'loss': 2.7897, 'grad_norm': 8.555490493774414, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:11<5:13:13,  3.21s/it]  3%|â–Ž         | 153/6000 [07:14<4:58:04,  3.06s/it]                                                    {'loss': 2.782, 'grad_norm': 2.881035327911377, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:14<4:58:04,  3.06s/it]  3%|â–Ž         | 154/6000 [07:17<4:51:42,  2.99s/it]                                                    {'loss': 2.7824, 'grad_norm': 1.5971521139144897, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:17<4:51:42,  2.99s/it]  3%|â–Ž         | 155/6000 [07:20<4:45:41,  2.93s/it]                                                    {'loss': 2.8019, 'grad_norm': 3.6654038429260254, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:20<4:45:41,  2.93s/it]  3%|â–Ž         | 156/6000 [07:23<4:42:56,  2.90s/it]                                                    {'loss': 2.8441, 'grad_norm': 2.2359113693237305, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:23<4:42:56,  2.90s/it]  3%|â–Ž         | 157/6000 [07:26<4:52:25,  3.00s/it]                                                    {'loss': 2.7777, 'grad_norm': 4.163595676422119, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:26<4:52:25,  3.00s/it]  3%|â–Ž         | 158/6000 [07:29<4:44:31,  2.92s/it]                                                    {'loss': 2.7799, 'grad_norm': 5.991441249847412, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:29<4:44:31,  2.92s/it]  3%|â–Ž         | 159/6000 [07:31<4:38:38,  2.86s/it]                                                    {'loss': 2.7901, 'grad_norm': 1.4059404134750366, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:31<4:38:38,  2.86s/it]  3%|â–Ž         | 160/6000 [07:35<4:51:47,  3.00s/it]                                                    {'loss': 2.804, 'grad_norm': 2.0864615440368652, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:35<4:51:47,  3.00s/it]  3%|â–Ž         | 161/6000 [07:37<4:47:24,  2.95s/it]                                                    {'loss': 2.7729, 'grad_norm': 1.3443090915679932, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:37<4:47:24,  2.95s/it]  3%|â–Ž         | 162/6000 [07:40<4:40:02,  2.88s/it]                                                    {'loss': 2.7859, 'grad_norm': 3.312559127807617, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:40<4:40:02,  2.88s/it]  3%|â–Ž         | 163/6000 [07:43<4:51:53,  3.00s/it]                                                    {'loss': 2.7809, 'grad_norm': 2.1959609985351562, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:43<4:51:53,  3.00s/it]  3%|â–Ž         | 164/6000 [07:46<4:41:52,  2.90s/it]                                                    {'loss': 2.9005, 'grad_norm': 4.228507995605469, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:46<4:41:52,  2.90s/it]  3%|â–Ž         | 165/6000 [07:49<4:39:18,  2.87s/it]                                                    {'loss': 2.7797, 'grad_norm': 2.9253785610198975, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:49<4:39:18,  2.87s/it]  3%|â–Ž         | 166/6000 [07:52<4:34:10,  2.82s/it]                                                    {'loss': 2.7753, 'grad_norm': 5.030244827270508, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:52<4:34:10,  2.82s/it]  3%|â–Ž         | 167/6000 [07:54<4:32:45,  2.81s/it]                                                    {'loss': 2.7841, 'grad_norm': 5.786203384399414, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:54<4:32:45,  2.81s/it]  3%|â–Ž         | 168/6000 [07:57<4:30:57,  2.79s/it]                                                    {'loss': 2.8286, 'grad_norm': 2.687683343887329, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:57<4:30:57,  2.79s/it]  3%|â–Ž         | 169/6000 [08:00<4:42:41,  2.91s/it]                                                    {'loss': 2.8158, 'grad_norm': 5.592570781707764, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:00<4:42:41,  2.91s/it]  3%|â–Ž         | 170/6000 [08:03<4:38:01,  2.86s/it]                                                    {'loss': 2.79, 'grad_norm': 7.822580337524414, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:03<4:38:01,  2.86s/it]  3%|â–Ž         | 171/6000 [08:06<4:32:57,  2.81s/it]                                                    {'loss': 2.7809, 'grad_norm': 5.697071075439453, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:06<4:32:57,  2.81s/it]  3%|â–Ž         | 172/6000 [08:09<4:33:59,  2.82s/it]                                                    {'loss': 2.7934, 'grad_norm': 3.13065767288208, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:09<4:33:59,  2.82s/it]  3%|â–Ž         | 173/6000 [08:11<4:32:01,  2.80s/it]                                                    {'loss': 2.7808, 'grad_norm': 6.135506629943848, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:11<4:32:01,  2.80s/it]  3%|â–Ž         | 174/6000 [08:15<4:50:40,  2.99s/it]                                                    {'loss': 2.7715, 'grad_norm': 3.5234947204589844, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:15<4:50:40,  2.99s/it]  3%|â–Ž         | 175/6000 [08:18<4:44:59,  2.94s/it]                                                    {'loss': 2.7967, 'grad_norm': 1.9778733253479004, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:18<4:44:59,  2.94s/it]  3%|â–Ž         | 176/6000 [08:21<4:47:11,  2.96s/it]                                                    {'loss': 2.7787, 'grad_norm': 2.921734571456909, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:21<4:47:11,  2.96s/it]  3%|â–Ž         | 177/6000 [08:23<4:41:03,  2.90s/it]                                                    {'loss': 2.7945, 'grad_norm': 1.920506477355957, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:23<4:41:03,  2.90s/it]  3%|â–Ž         | 178/6000 [08:26<4:37:29,  2.86s/it]                                                    {'loss': 2.7903, 'grad_norm': 2.975353717803955, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:26<4:37:29,  2.86s/it]  3%|â–Ž         | 179/6000 [08:29<4:33:49,  2.82s/it]                                                    {'loss': 2.7907, 'grad_norm': 1.0046353340148926, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:29<4:33:49,  2.82s/it]  3%|â–Ž         | 180/6000 [08:32<4:34:46,  2.83s/it]                                                    {'loss': 2.7874, 'grad_norm': 1.825985074043274, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:32<4:34:46,  2.83s/it]  3%|â–Ž         | 181/6000 [08:35<4:32:18,  2.81s/it]                                                    {'loss': 2.8039, 'grad_norm': 3.4991040229797363, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:35<4:32:18,  2.81s/it]  3%|â–Ž         | 182/6000 [08:37<4:30:15,  2.79s/it]                                                    {'loss': 2.825, 'grad_norm': 2.858266592025757, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:37<4:30:15,  2.79s/it]  3%|â–Ž         | 183/6000 [08:40<4:28:14,  2.77s/it]                                                    {'loss': 2.7743, 'grad_norm': 1.3257932662963867, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:40<4:28:14,  2.77s/it]  3%|â–Ž         | 184/6000 [08:43<4:27:07,  2.76s/it]                                                    {'loss': 2.7788, 'grad_norm': 4.817991733551025, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:43<4:27:07,  2.76s/it]  3%|â–Ž         | 185/6000 [08:45<4:27:19,  2.76s/it]                                                    {'loss': 2.7905, 'grad_norm': 2.2706637382507324, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:45<4:27:19,  2.76s/it]  3%|â–Ž         | 186/6000 [08:48<4:27:52,  2.76s/it]                                                    {'loss': 2.7792, 'grad_norm': 4.009786605834961, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:48<4:27:52,  2.76s/it]  3%|â–Ž         | 187/6000 [08:51<4:27:03,  2.76s/it]                                                    {'loss': 2.777, 'grad_norm': 7.783341407775879, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:51<4:27:03,  2.76s/it]  3%|â–Ž         | 188/6000 [08:54<4:26:49,  2.75s/it]                                                    {'loss': 2.7833, 'grad_norm': 4.142425060272217, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:54<4:26:49,  2.75s/it]  3%|â–Ž         | 189/6000 [08:56<4:25:58,  2.75s/it]                                                    {'loss': 2.7857, 'grad_norm': 4.931325912475586, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:56<4:25:58,  2.75s/it]  3%|â–Ž         | 190/6000 [08:59<4:23:08,  2.72s/it]                                                    {'loss': 2.7963, 'grad_norm': 1.8644193410873413, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:59<4:23:08,  2.72s/it]  3%|â–Ž         | 191/6000 [09:02<4:24:57,  2.74s/it]                                                    {'loss': 2.7796, 'grad_norm': 2.5135490894317627, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:02<4:24:57,  2.74s/it]  3%|â–Ž         | 192/6000 [09:05<4:42:33,  2.92s/it]                                                    {'loss': 2.8709, 'grad_norm': 2.8106014728546143, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:05<4:42:33,  2.92s/it]  3%|â–Ž         | 193/6000 [09:08<4:49:38,  2.99s/it]                                                    {'loss': 2.7743, 'grad_norm': 3.077146053314209, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:08<4:49:38,  2.99s/it]  3%|â–Ž         | 194/6000 [09:11<4:41:31,  2.91s/it]                                                    {'loss': 2.7818, 'grad_norm': 6.823005199432373, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:11<4:41:31,  2.91s/it]  3%|â–Ž         | 195/6000 [09:14<4:38:13,  2.88s/it]                                                    {'loss': 2.8061, 'grad_norm': 3.7825605869293213, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:14<4:38:13,  2.88s/it]  3%|â–Ž         | 196/6000 [09:17<4:36:44,  2.86s/it]                                                    {'loss': 2.7915, 'grad_norm': 2.0477144718170166, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:17<4:36:44,  2.86s/it]  3%|â–Ž         | 197/6000 [09:20<4:38:54,  2.88s/it]                                                    {'loss': 2.7906, 'grad_norm': 1.8073190450668335, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:20<4:38:54,  2.88s/it]  3%|â–Ž         | 198/6000 [09:23<4:39:15,  2.89s/it]                                                    {'loss': 2.7837, 'grad_norm': 1.614707112312317, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:23<4:39:15,  2.89s/it]  3%|â–Ž         | 199/6000 [09:25<4:33:53,  2.83s/it]                                                    {'loss': 2.774, 'grad_norm': 1.637625813484192, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:25<4:33:53,  2.83s/it]  3%|â–Ž         | 200/6000 [09:28<4:44:44,  2.95s/it]                                                    {'loss': 2.7799, 'grad_norm': 1.0380749702453613, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:28<4:44:44,  2.95s/it][2025-10-22 21:04:27,286] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-10-22 21:04:27,296] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:04:27,888] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:33<5:42:05,  3.54s/it]                                                    {'loss': 2.8076, 'grad_norm': 1.4320980310440063, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:33<5:42:05,  3.54s/it]  3%|â–Ž         | 202/6000 [09:36<5:17:15,  3.28s/it]                                                    {'loss': 2.7793, 'grad_norm': 3.021366834640503, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:36<5:17:15,  3.28s/it]  3%|â–Ž         | 203/6000 [09:39<5:01:57,  3.13s/it]                                                    {'loss': 2.7879, 'grad_norm': 4.869484901428223, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:39<5:01:57,  3.13s/it]  3%|â–Ž         | 204/6000 [09:42<4:54:54,  3.05s/it]                                                    {'loss': 2.7804, 'grad_norm': 2.306185245513916, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:42<4:54:54,  3.05s/it]  3%|â–Ž         | 205/6000 [09:44<4:44:58,  2.95s/it]                                                    {'loss': 2.7764, 'grad_norm': 4.528505325317383, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:44<4:44:58,  2.95s/it]  3%|â–Ž         | 206/6000 [09:47<4:36:56,  2.87s/it]                                                    {'loss': 2.8215, 'grad_norm': 1.0838080644607544, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:47<4:36:56,  2.87s/it]  3%|â–Ž         | 207/6000 [09:50<4:31:49,  2.82s/it]                                                    {'loss': 2.8725, 'grad_norm': 1.384211778640747, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:50<4:31:49,  2.82s/it]  3%|â–Ž         | 208/6000 [09:52<4:27:35,  2.77s/it]                                                    {'loss': 2.7782, 'grad_norm': 3.500316858291626, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:52<4:27:35,  2.77s/it]  3%|â–Ž         | 209/6000 [09:55<4:25:48,  2.75s/it]                                                    {'loss': 2.7733, 'grad_norm': 2.285688638687134, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:55<4:25:48,  2.75s/it]  4%|â–Ž         | 210/6000 [09:58<4:37:46,  2.88s/it]                                                    {'loss': 2.7729, 'grad_norm': 1.8622863292694092, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:58<4:37:46,  2.88s/it]  4%|â–Ž         | 211/6000 [10:01<4:37:55,  2.88s/it]                                                    {'loss': 2.7907, 'grad_norm': 1.619878888130188, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:01<4:37:55,  2.88s/it]  4%|â–Ž         | 212/6000 [10:04<4:44:46,  2.95s/it]                                                    {'loss': 2.773, 'grad_norm': 1.2289693355560303, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:04<4:44:46,  2.95s/it]  4%|â–Ž         | 213/6000 [10:07<4:40:28,  2.91s/it]                                                    {'loss': 2.7784, 'grad_norm': 2.531451463699341, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:07<4:40:28,  2.91s/it]  4%|â–Ž         | 214/6000 [10:10<4:35:27,  2.86s/it]                                                    {'loss': 2.7783, 'grad_norm': 0.9697579741477966, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:10<4:35:27,  2.86s/it]  4%|â–Ž         | 215/6000 [10:13<4:33:11,  2.83s/it]                                                    {'loss': 2.8009, 'grad_norm': 2.9991214275360107, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:13<4:33:11,  2.83s/it]  4%|â–Ž         | 216/6000 [10:15<4:31:49,  2.82s/it]                                                    {'loss': 2.7724, 'grad_norm': 4.028842449188232, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:15<4:31:49,  2.82s/it]  4%|â–Ž         | 217/6000 [10:18<4:31:19,  2.82s/it]                                                    {'loss': 2.819, 'grad_norm': 2.3354544639587402, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:18<4:31:19,  2.82s/it]  4%|â–Ž         | 218/6000 [10:21<4:31:07,  2.81s/it]                                                    {'loss': 2.7994, 'grad_norm': 3.6491496562957764, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:21<4:31:07,  2.81s/it]  4%|â–Ž         | 219/6000 [10:24<4:28:31,  2.79s/it]                                                    {'loss': 2.8213, 'grad_norm': 9.227638244628906, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:24<4:28:31,  2.79s/it]  4%|â–Ž         | 220/6000 [10:27<4:26:37,  2.77s/it]                                                    {'loss': 2.7844, 'grad_norm': 10.23353385925293, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:27<4:26:37,  2.77s/it]  4%|â–Ž         | 221/6000 [10:29<4:26:29,  2.77s/it]                                                    {'loss': 2.8072, 'grad_norm': 11.569906234741211, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:29<4:26:29,  2.77s/it]  4%|â–Ž         | 222/6000 [10:32<4:25:48,  2.76s/it]                                                    {'loss': 2.7761, 'grad_norm': 2.460861921310425, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:32<4:25:48,  2.76s/it]  4%|â–Ž         | 223/6000 [10:35<4:24:38,  2.75s/it]                                                    {'loss': 2.7729, 'grad_norm': 2.4912471771240234, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:35<4:24:38,  2.75s/it]  4%|â–Ž         | 224/6000 [10:37<4:23:29,  2.74s/it]                                                    {'loss': 2.8281, 'grad_norm': 4.356234550476074, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:37<4:23:29,  2.74s/it]  4%|â–         | 225/6000 [10:40<4:25:33,  2.76s/it]                                                    {'loss': 2.7707, 'grad_norm': 4.995186805725098, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:40<4:25:33,  2.76s/it]  4%|â–         | 226/6000 [10:44<4:39:19,  2.90s/it]                                                    {'loss': 2.7835, 'grad_norm': 5.906493663787842, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:44<4:39:19,  2.90s/it]  4%|â–         | 227/6000 [10:46<4:33:39,  2.84s/it]                                                    {'loss': 2.8254, 'grad_norm': 8.956966400146484, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:46<4:33:39,  2.84s/it]  4%|â–         | 228/6000 [10:49<4:35:38,  2.87s/it]                                                    {'loss': 2.7742, 'grad_norm': 4.101074695587158, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:49<4:35:38,  2.87s/it]  4%|â–         | 229/6000 [10:52<4:29:44,  2.80s/it]                                                    {'loss': 2.7897, 'grad_norm': 5.78446102142334, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:52<4:29:44,  2.80s/it]  4%|â–         | 230/6000 [10:55<4:28:51,  2.80s/it]                                                    {'loss': 2.802, 'grad_norm': 3.575474500656128, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:55<4:28:51,  2.80s/it]  4%|â–         | 231/6000 [10:57<4:25:51,  2.77s/it]                                                    {'loss': 2.7796, 'grad_norm': 4.340979099273682, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:57<4:25:51,  2.77s/it]  4%|â–         | 232/6000 [11:00<4:25:20,  2.76s/it]                                                    {'loss': 2.7783, 'grad_norm': 2.4910032749176025, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [11:00<4:25:20,  2.76s/it]  4%|â–         | 233/6000 [11:03<4:34:59,  2.86s/it]                                                    {'loss': 2.7729, 'grad_norm': 3.7819416522979736, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:03<4:34:59,  2.86s/it]  4%|â–         | 234/6000 [11:06<4:39:42,  2.91s/it]                                                    {'loss': 2.7705, 'grad_norm': 16.989227294921875, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:06<4:39:42,  2.91s/it]  4%|â–         | 235/6000 [11:09<4:32:58,  2.84s/it]                                                    {'loss': 2.8423, 'grad_norm': 4.8822550773620605, 'learning_rate': 4.885593220338983e-05, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:09<4:32:58,  2.84s/it]  4%|â–         | 236/6000 [11:12<4:30:56,  2.82s/it]                                                    {'loss': 2.806, 'grad_norm': 9.368376731872559, 'learning_rate': 4.884745762711865e-05, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:12<4:30:56,  2.82s/it]  4%|â–         | 237/6000 [11:14<4:27:27,  2.78s/it]                                                    {'loss': 2.7736, 'grad_norm': 14.776261329650879, 'learning_rate': 4.883898305084746e-05, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:14<4:27:27,  2.78s/it]  4%|â–         | 238/6000 [11:17<4:39:04,  2.91s/it]                                                    {'loss': 2.8423, 'grad_norm': 5.072495937347412, 'learning_rate': 4.883050847457628e-05, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:17<4:39:04,  2.91s/it]  4%|â–         | 239/6000 [11:20<4:36:17,  2.88s/it]                                                    {'loss': 2.7753, 'grad_norm': 17.743165969848633, 'learning_rate': 4.882203389830508e-05, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:20<4:36:17,  2.88s/it]  4%|â–         | 240/6000 [11:23<4:32:28,  2.84s/it]                                                    {'loss': 2.7922, 'grad_norm': 10.832669258117676, 'learning_rate': 4.88135593220339e-05, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:23<4:32:28,  2.84s/it]  4%|â–         | 241/6000 [11:26<4:29:21,  2.81s/it]                                                    {'loss': 2.7754, 'grad_norm': 21.91278839111328, 'learning_rate': 4.880508474576271e-05, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:26<4:29:21,  2.81s/it]  4%|â–         | 242/6000 [11:28<4:25:34,  2.77s/it]                                                    {'loss': 2.7955, 'grad_norm': 8.948030471801758, 'learning_rate': 4.879661016949153e-05, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:28<4:25:34,  2.77s/it]  4%|â–         | 243/6000 [11:32<4:34:13,  2.86s/it]                                                    {'loss': 2.776, 'grad_norm': 11.97472095489502, 'learning_rate': 4.878813559322034e-05, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:32<4:34:13,  2.86s/it]  4%|â–         | 244/6000 [11:34<4:29:13,  2.81s/it]                                                    {'loss': 2.7704, 'grad_norm': 14.415307998657227, 'learning_rate': 4.877966101694916e-05, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:34<4:29:13,  2.81s/it]  4%|â–         | 245/6000 [11:37<4:29:00,  2.80s/it]                                                    {'loss': 2.7825, 'grad_norm': 5.199926376342773, 'learning_rate': 4.877118644067797e-05, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:37<4:29:00,  2.80s/it]  4%|â–         | 246/6000 [11:40<4:30:32,  2.82s/it]                                                    {'loss': 2.7961, 'grad_norm': 28.673513412475586, 'learning_rate': 4.876271186440678e-05, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:40<4:30:32,  2.82s/it]  4%|â–         | 247/6000 [11:43<4:25:39,  2.77s/it]                                                    {'loss': 2.7817, 'grad_norm': 17.845930099487305, 'learning_rate': 4.8754237288135593e-05, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:43<4:25:39,  2.77s/it]  4%|â–         | 248/6000 [11:46<4:32:01,  2.84s/it]                                                    {'loss': 2.7719, 'grad_norm': 5.698251247406006, 'learning_rate': 4.8745762711864405e-05, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:46<4:32:01,  2.84s/it]  4%|â–         | 249/6000 [11:48<4:28:12,  2.80s/it]                                                    {'loss': 2.7765, 'grad_norm': 6.506887912750244, 'learning_rate': 4.873728813559322e-05, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:48<4:28:12,  2.80s/it]  4%|â–         | 250/6000 [11:51<4:25:54,  2.77s/it]                                                    {'loss': 2.7974, 'grad_norm': 6.723686695098877, 'learning_rate': 4.8728813559322034e-05, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:51<4:25:54,  2.77s/it][2025-10-22 21:06:49,747] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
[2025-10-22 21:06:49,757] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:06:50,397] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [11:56<5:24:53,  3.39s/it]                                                    {'loss': 2.7858, 'grad_norm': 6.240324020385742, 'learning_rate': 4.872033898305085e-05, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:56<5:24:53,  3.39s/it]  4%|â–         | 252/6000 [11:59<5:11:39,  3.25s/it]                                                    {'loss': 2.8088, 'grad_norm': 4.248671054840088, 'learning_rate': 4.8711864406779663e-05, 'epoch': 0.04}
  4%|â–         | 252/6000 [11:59<5:11:39,  3.25s/it]  4%|â–         | 253/6000 [12:01<4:56:13,  3.09s/it]                                                    {'loss': 2.8072, 'grad_norm': 23.007240295410156, 'learning_rate': 4.8703389830508475e-05, 'epoch': 0.04}
  4%|â–         | 253/6000 [12:01<4:56:13,  3.09s/it]  4%|â–         | 254/6000 [12:04<4:47:58,  3.01s/it]                                                    {'loss': 2.7668, 'grad_norm': 6.708703994750977, 'learning_rate': 4.8694915254237286e-05, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:04<4:47:58,  3.01s/it]  4%|â–         | 255/6000 [12:07<4:37:09,  2.89s/it]                                                    {'loss': 2.7777, 'grad_norm': 4.560410976409912, 'learning_rate': 4.8686440677966104e-05, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:07<4:37:09,  2.89s/it]  4%|â–         | 256/6000 [12:10<4:45:10,  2.98s/it]                                                    {'loss': 2.7977, 'grad_norm': 8.228277206420898, 'learning_rate': 4.8677966101694915e-05, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:10<4:45:10,  2.98s/it]  4%|â–         | 257/6000 [12:13<4:36:29,  2.89s/it]                                                    {'loss': 2.7778, 'grad_norm': 5.160859107971191, 'learning_rate': 4.8669491525423733e-05, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:13<4:36:29,  2.89s/it]  4%|â–         | 258/6000 [12:15<4:30:46,  2.83s/it]                                                    {'loss': 2.7731, 'grad_norm': 6.6661577224731445, 'learning_rate': 4.8661016949152545e-05, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:15<4:30:46,  2.83s/it]  4%|â–         | 259/6000 [12:18<4:26:42,  2.79s/it]                                                    {'loss': 2.774, 'grad_norm': 3.41618013381958, 'learning_rate': 4.865254237288136e-05, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:18<4:26:42,  2.79s/it]  4%|â–         | 260/6000 [12:21<4:24:08,  2.76s/it]                                                    {'loss': 2.7932, 'grad_norm': 2.186298131942749, 'learning_rate': 4.8644067796610174e-05, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:21<4:24:08,  2.76s/it]  4%|â–         | 261/6000 [12:24<4:28:26,  2.81s/it]                                                    {'loss': 2.7826, 'grad_norm': 1.9173983335494995, 'learning_rate': 4.8635593220338985e-05, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:24<4:28:26,  2.81s/it]  4%|â–         | 262/6000 [12:26<4:26:34,  2.79s/it]                                                    {'loss': 2.7795, 'grad_norm': 6.376238822937012, 'learning_rate': 4.86271186440678e-05, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:26<4:26:34,  2.79s/it]  4%|â–         | 263/6000 [12:30<4:35:18,  2.88s/it]                                                    {'loss': 2.8484, 'grad_norm': 3.443526268005371, 'learning_rate': 4.8618644067796615e-05, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:30<4:35:18,  2.88s/it]  4%|â–         | 264/6000 [12:32<4:30:38,  2.83s/it]                                                    {'loss': 2.7822, 'grad_norm': 2.298802614212036, 'learning_rate': 4.8610169491525426e-05, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:32<4:30:38,  2.83s/it]  4%|â–         | 265/6000 [12:35<4:32:48,  2.85s/it]                                                    {'loss': 2.7778, 'grad_norm': 2.805837392807007, 'learning_rate': 4.8601694915254244e-05, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:35<4:32:48,  2.85s/it]  4%|â–         | 266/6000 [12:38<4:32:25,  2.85s/it]                                                    {'loss': 2.805, 'grad_norm': 1.508533239364624, 'learning_rate': 4.8593220338983055e-05, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:38<4:32:25,  2.85s/it]  4%|â–         | 267/6000 [12:41<4:27:24,  2.80s/it]                                                    {'loss': 2.7718, 'grad_norm': 1.875444769859314, 'learning_rate': 4.858474576271187e-05, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:41<4:27:24,  2.80s/it]  4%|â–         | 268/6000 [12:43<4:24:48,  2.77s/it]                                                    {'loss': 2.8425, 'grad_norm': 4.831098556518555, 'learning_rate': 4.857627118644068e-05, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:43<4:24:48,  2.77s/it]  4%|â–         | 269/6000 [12:46<4:27:48,  2.80s/it]                                                    {'loss': 2.7792, 'grad_norm': 2.246788501739502, 'learning_rate': 4.856779661016949e-05, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:46<4:27:48,  2.80s/it]  4%|â–         | 270/6000 [12:49<4:25:33,  2.78s/it]                                                    {'loss': 2.7779, 'grad_norm': 4.500433921813965, 'learning_rate': 4.855932203389831e-05, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:49<4:25:33,  2.78s/it]  5%|â–         | 271/6000 [12:52<4:22:14,  2.75s/it]                                                    {'loss': 2.7977, 'grad_norm': 2.58941912651062, 'learning_rate': 4.855084745762712e-05, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:52<4:22:14,  2.75s/it]  5%|â–         | 272/6000 [12:54<4:21:34,  2.74s/it]                                                    {'loss': 2.7811, 'grad_norm': 4.963543891906738, 'learning_rate': 4.8542372881355937e-05, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:54<4:21:34,  2.74s/it]  5%|â–         | 273/6000 [12:57<4:22:00,  2.75s/it]                                                    {'loss': 2.8017, 'grad_norm': 3.325831174850464, 'learning_rate': 4.853389830508475e-05, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:57<4:22:00,  2.75s/it]  5%|â–         | 274/6000 [13:00<4:32:43,  2.86s/it]                                                    {'loss': 2.7751, 'grad_norm': 1.5786733627319336, 'learning_rate': 4.8525423728813566e-05, 'epoch': 0.05}
  5%|â–         | 274/6000 [13:00<4:32:43,  2.86s/it]  5%|â–         | 275/6000 [13:03<4:29:37,  2.83s/it]                                                    {'loss': 2.794, 'grad_norm': 4.565934658050537, 'learning_rate': 4.851694915254237e-05, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:03<4:29:37,  2.83s/it]  5%|â–         | 276/6000 [13:06<4:28:14,  2.81s/it]                                                    {'loss': 2.848, 'grad_norm': 1.5774023532867432, 'learning_rate': 4.850847457627119e-05, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:06<4:28:14,  2.81s/it]  5%|â–         | 277/6000 [13:08<4:24:05,  2.77s/it]                                                    {'loss': 2.7738, 'grad_norm': 4.38441801071167, 'learning_rate': 4.85e-05, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:08<4:24:05,  2.77s/it]  5%|â–         | 278/6000 [13:11<4:21:26,  2.74s/it]                                                    {'loss': 2.7804, 'grad_norm': 10.107800483703613, 'learning_rate': 4.849152542372882e-05, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:11<4:21:26,  2.74s/it]  5%|â–         | 279/6000 [13:14<4:23:37,  2.76s/it]                                                    {'loss': 2.8035, 'grad_norm': 1.6506240367889404, 'learning_rate': 4.848305084745763e-05, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:14<4:23:37,  2.76s/it]  5%|â–         | 280/6000 [13:17<4:21:40,  2.74s/it]                                                    {'loss': 2.7849, 'grad_norm': 5.70479154586792, 'learning_rate': 4.847457627118645e-05, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:17<4:21:40,  2.74s/it]  5%|â–         | 281/6000 [13:19<4:21:51,  2.75s/it]                                                    {'loss': 2.7789, 'grad_norm': 1.5219488143920898, 'learning_rate': 4.846610169491526e-05, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:19<4:21:51,  2.75s/it]  5%|â–         | 282/6000 [13:23<4:33:23,  2.87s/it]                                                    {'loss': 2.7754, 'grad_norm': 2.5418622493743896, 'learning_rate': 4.845762711864407e-05, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:23<4:33:23,  2.87s/it]  5%|â–         | 283/6000 [13:26<4:39:32,  2.93s/it]                                                    {'loss': 2.7727, 'grad_norm': 5.616013050079346, 'learning_rate': 4.844915254237288e-05, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:26<4:39:32,  2.93s/it]  5%|â–         | 284/6000 [13:29<4:53:08,  3.08s/it]                                                    {'loss': 2.7776, 'grad_norm': 3.5094311237335205, 'learning_rate': 4.84406779661017e-05, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:29<4:53:08,  3.08s/it]  5%|â–         | 285/6000 [13:32<4:47:48,  3.02s/it]                                                    {'loss': 2.7776, 'grad_norm': 1.6192775964736938, 'learning_rate': 4.843220338983051e-05, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:32<4:47:48,  3.02s/it]  5%|â–         | 286/6000 [13:35<4:37:59,  2.92s/it]                                                    {'loss': 2.782, 'grad_norm': 7.128964424133301, 'learning_rate': 4.842372881355933e-05, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:35<4:37:59,  2.92s/it]  5%|â–         | 287/6000 [13:37<4:33:00,  2.87s/it]                                                    {'loss': 2.8076, 'grad_norm': 4.77205228805542, 'learning_rate': 4.841525423728814e-05, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:37<4:33:00,  2.87s/it]  5%|â–         | 288/6000 [13:40<4:29:04,  2.83s/it]                                                    {'loss': 2.7826, 'grad_norm': 1.18909752368927, 'learning_rate': 4.840677966101695e-05, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:40<4:29:04,  2.83s/it]  5%|â–         | 289/6000 [13:43<4:31:27,  2.85s/it]                                                    {'loss': 2.7691, 'grad_norm': 1.333228588104248, 'learning_rate': 4.839830508474576e-05, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:43<4:31:27,  2.85s/it]  5%|â–         | 290/6000 [13:46<4:28:31,  2.82s/it]                                                    {'loss': 2.7992, 'grad_norm': 2.1055335998535156, 'learning_rate': 4.8389830508474574e-05, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:46<4:28:31,  2.82s/it]  5%|â–         | 291/6000 [13:48<4:24:08,  2.78s/it]                                                    {'loss': 2.781, 'grad_norm': 8.08781909942627, 'learning_rate': 4.838135593220339e-05, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:48<4:24:08,  2.78s/it]  5%|â–         | 292/6000 [13:51<4:22:17,  2.76s/it]                                                    {'loss': 2.7717, 'grad_norm': 2.5533714294433594, 'learning_rate': 4.83728813559322e-05, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:51<4:22:17,  2.76s/it]  5%|â–         | 293/6000 [13:54<4:18:53,  2.72s/it]                                                    {'loss': 2.8452, 'grad_norm': 1.242727518081665, 'learning_rate': 4.836440677966102e-05, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:54<4:18:53,  2.72s/it]  5%|â–         | 294/6000 [13:57<4:18:50,  2.72s/it]                                                    {'loss': 2.7875, 'grad_norm': 2.7575619220733643, 'learning_rate': 4.835593220338983e-05, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:57<4:18:50,  2.72s/it]  5%|â–         | 295/6000 [13:59<4:17:38,  2.71s/it]                                                    {'loss': 2.7779, 'grad_norm': 7.616001129150391, 'learning_rate': 4.834745762711865e-05, 'epoch': 0.05}
  5%|â–         | 295/6000 [13:59<4:17:38,  2.71s/it]  5%|â–         | 296/6000 [14:02<4:21:45,  2.75s/it]                                                    {'loss': 2.7721, 'grad_norm': 4.187860488891602, 'learning_rate': 4.833898305084746e-05, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:02<4:21:45,  2.75s/it]  5%|â–         | 297/6000 [14:05<4:21:44,  2.75s/it]                                                    {'loss': 2.7895, 'grad_norm': 1.4138888120651245, 'learning_rate': 4.833050847457627e-05, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:05<4:21:44,  2.75s/it]  5%|â–         | 298/6000 [14:08<4:26:22,  2.80s/it]                                                    {'loss': 2.779, 'grad_norm': 1.2480313777923584, 'learning_rate': 4.8322033898305084e-05, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:08<4:26:22,  2.80s/it]  5%|â–         | 299/6000 [14:11<4:49:10,  3.04s/it]                                                    {'loss': 2.7888, 'grad_norm': 6.465165138244629, 'learning_rate': 4.83135593220339e-05, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:11<4:49:10,  3.04s/it]  5%|â–Œ         | 300/6000 [14:14<4:39:59,  2.95s/it]                                                    {'loss': 2.7726, 'grad_norm': 4.494490623474121, 'learning_rate': 4.8305084745762714e-05, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:14<4:39:59,  2.95s/it][2025-10-22 21:09:12,866] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
[2025-10-22 21:09:12,882] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:09:13,508] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [14:19<5:39:37,  3.58s/it]                                                    {'loss': 2.8009, 'grad_norm': 1.3212213516235352, 'learning_rate': 4.829661016949153e-05, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:19<5:39:37,  3.58s/it]  5%|â–Œ         | 302/6000 [14:22<5:15:15,  3.32s/it]                                                    {'loss': 2.78, 'grad_norm': 3.9774835109710693, 'learning_rate': 4.828813559322034e-05, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:22<5:15:15,  3.32s/it]  5%|â–Œ         | 303/6000 [14:25<5:00:00,  3.16s/it]                                                    {'loss': 2.7821, 'grad_norm': 7.371455669403076, 'learning_rate': 4.8279661016949154e-05, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:25<5:00:00,  3.16s/it]  5%|â–Œ         | 304/6000 [14:27<4:48:16,  3.04s/it]                                                    {'loss': 2.789, 'grad_norm': 5.070200443267822, 'learning_rate': 4.8271186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:27<4:48:16,  3.04s/it]  5%|â–Œ         | 305/6000 [14:30<4:42:45,  2.98s/it]                                                    {'loss': 2.7936, 'grad_norm': 3.7247092723846436, 'learning_rate': 4.8262711864406784e-05, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:30<4:42:45,  2.98s/it]  5%|â–Œ         | 306/6000 [14:33<4:34:53,  2.90s/it]                                                    {'loss': 2.7747, 'grad_norm': 2.795187473297119, 'learning_rate': 4.8254237288135595e-05, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:33<4:34:53,  2.90s/it]  5%|â–Œ         | 307/6000 [14:36<4:29:00,  2.84s/it]                                                    {'loss': 2.7842, 'grad_norm': 4.4154229164123535, 'learning_rate': 4.824576271186441e-05, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:36<4:29:00,  2.84s/it]  5%|â–Œ         | 308/6000 [14:38<4:25:52,  2.80s/it]                                                    {'loss': 2.7862, 'grad_norm': 6.503978252410889, 'learning_rate': 4.8237288135593224e-05, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:38<4:25:52,  2.80s/it]  5%|â–Œ         | 309/6000 [14:41<4:26:43,  2.81s/it]                                                    {'loss': 2.7696, 'grad_norm': 2.6626498699188232, 'learning_rate': 4.8228813559322036e-05, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:41<4:26:43,  2.81s/it]  5%|â–Œ         | 310/6000 [14:44<4:22:56,  2.77s/it]                                                    {'loss': 2.8163, 'grad_norm': 13.4700288772583, 'learning_rate': 4.822033898305085e-05, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:44<4:22:56,  2.77s/it]  5%|â–Œ         | 311/6000 [14:47<4:22:11,  2.77s/it]                                                    {'loss': 2.77, 'grad_norm': 2.9621920585632324, 'learning_rate': 4.821186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:47<4:22:11,  2.77s/it]  5%|â–Œ         | 312/6000 [14:49<4:20:02,  2.74s/it]                                                    {'loss': 2.7878, 'grad_norm': 8.24804973602295, 'learning_rate': 4.8203389830508476e-05, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:49<4:20:02,  2.74s/it]  5%|â–Œ         | 313/6000 [14:52<4:19:30,  2.74s/it]                                                    {'loss': 2.8482, 'grad_norm': 6.098508834838867, 'learning_rate': 4.819491525423729e-05, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:52<4:19:30,  2.74s/it]  5%|â–Œ         | 314/6000 [14:55<4:19:14,  2.74s/it]                                                    {'loss': 2.7923, 'grad_norm': 5.157170295715332, 'learning_rate': 4.8186440677966105e-05, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:55<4:19:14,  2.74s/it]  5%|â–Œ         | 315/6000 [14:57<4:17:04,  2.71s/it]                                                    {'loss': 2.7741, 'grad_norm': 5.630895137786865, 'learning_rate': 4.817796610169492e-05, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:57<4:17:04,  2.71s/it]  5%|â–Œ         | 316/6000 [15:00<4:16:19,  2.71s/it]                                                    {'loss': 2.7759, 'grad_norm': 2.475555419921875, 'learning_rate': 4.8169491525423735e-05, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [15:00<4:16:19,  2.71s/it]  5%|â–Œ         | 317/6000 [15:03<4:20:07,  2.75s/it]                                                    {'loss': 2.8446, 'grad_norm': 7.303768634796143, 'learning_rate': 4.8161016949152546e-05, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [15:03<4:20:07,  2.75s/it]  5%|â–Œ         | 318/6000 [15:06<4:28:53,  2.84s/it]                                                    {'loss': 2.7661, 'grad_norm': 8.705109596252441, 'learning_rate': 4.815254237288136e-05, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [15:06<4:28:53,  2.84s/it]  5%|â–Œ         | 319/6000 [15:09<4:25:22,  2.80s/it]                                                    {'loss': 2.7759, 'grad_norm': 2.49396014213562, 'learning_rate': 4.814406779661017e-05, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [15:09<4:25:22,  2.80s/it]  5%|â–Œ         | 320/6000 [15:12<4:26:01,  2.81s/it]                                                    {'loss': 2.7676, 'grad_norm': 11.493341445922852, 'learning_rate': 4.813559322033899e-05, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:12<4:26:01,  2.81s/it]  5%|â–Œ         | 321/6000 [15:15<4:35:54,  2.92s/it]                                                    {'loss': 2.7956, 'grad_norm': 8.973636627197266, 'learning_rate': 4.81271186440678e-05, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:15<4:35:54,  2.92s/it]  5%|â–Œ         | 322/6000 [15:17<4:30:31,  2.86s/it]                                                    {'loss': 2.7845, 'grad_norm': 7.810653209686279, 'learning_rate': 4.8118644067796616e-05, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:17<4:30:31,  2.86s/it]  5%|â–Œ         | 323/6000 [15:21<4:43:26,  3.00s/it]                                                    {'loss': 2.7786, 'grad_norm': 4.848935127258301, 'learning_rate': 4.811016949152543e-05, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:21<4:43:26,  3.00s/it]  5%|â–Œ         | 324/6000 [15:23<4:33:16,  2.89s/it]                                                    {'loss': 2.8, 'grad_norm': 2.3079354763031006, 'learning_rate': 4.810169491525424e-05, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:23<4:33:16,  2.89s/it]  5%|â–Œ         | 325/6000 [15:26<4:27:55,  2.83s/it]                                                    {'loss': 2.78, 'grad_norm': 6.182097911834717, 'learning_rate': 4.809322033898305e-05, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:26<4:27:55,  2.83s/it]  5%|â–Œ         | 326/6000 [15:29<4:24:15,  2.79s/it]                                                    {'loss': 2.7837, 'grad_norm': 3.672574281692505, 'learning_rate': 4.808474576271187e-05, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:29<4:24:15,  2.79s/it]  5%|â–Œ         | 327/6000 [15:31<4:20:54,  2.76s/it]                                                    {'loss': 2.7786, 'grad_norm': 6.337491035461426, 'learning_rate': 4.807627118644068e-05, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:31<4:20:54,  2.76s/it]  5%|â–Œ         | 328/6000 [15:34<4:19:59,  2.75s/it]                                                    {'loss': 2.7686, 'grad_norm': 3.0777761936187744, 'learning_rate': 4.80677966101695e-05, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:34<4:19:59,  2.75s/it]  5%|â–Œ         | 329/6000 [15:37<4:18:29,  2.73s/it]                                                    {'loss': 2.7892, 'grad_norm': 4.520074367523193, 'learning_rate': 4.805932203389831e-05, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:37<4:18:29,  2.73s/it]  6%|â–Œ         | 330/6000 [15:40<4:18:22,  2.73s/it]                                                    {'loss': 2.7936, 'grad_norm': 12.145711898803711, 'learning_rate': 4.805084745762712e-05, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:40<4:18:22,  2.73s/it]  6%|â–Œ         | 331/6000 [15:42<4:18:11,  2.73s/it]                                                    {'loss': 2.7847, 'grad_norm': 2.9931583404541016, 'learning_rate': 4.804237288135594e-05, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:42<4:18:11,  2.73s/it]  6%|â–Œ         | 332/6000 [15:45<4:15:29,  2.70s/it]                                                    {'loss': 2.7792, 'grad_norm': 3.071976900100708, 'learning_rate': 4.803389830508474e-05, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:45<4:15:29,  2.70s/it]  6%|â–Œ         | 333/6000 [15:48<4:15:00,  2.70s/it]                                                    {'loss': 2.7751, 'grad_norm': 11.460946083068848, 'learning_rate': 4.802542372881356e-05, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:48<4:15:00,  2.70s/it]  6%|â–Œ         | 334/6000 [15:51<4:19:46,  2.75s/it]                                                    {'loss': 2.7674, 'grad_norm': 5.555559158325195, 'learning_rate': 4.801694915254237e-05, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:51<4:19:46,  2.75s/it]  6%|â–Œ         | 335/6000 [15:53<4:20:30,  2.76s/it]                                                    {'loss': 2.7888, 'grad_norm': 4.483949184417725, 'learning_rate': 4.800847457627119e-05, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [15:53<4:20:30,  2.76s/it]  6%|â–Œ         | 336/6000 [15:56<4:17:55,  2.73s/it]                                                    {'loss': 2.7805, 'grad_norm': 9.969596862792969, 'learning_rate': 4.8e-05, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [15:56<4:17:55,  2.73s/it]  6%|â–Œ         | 337/6000 [15:59<4:19:37,  2.75s/it]                                                    {'loss': 2.801, 'grad_norm': 5.941389560699463, 'learning_rate': 4.799152542372882e-05, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [15:59<4:19:37,  2.75s/it]  6%|â–Œ         | 338/6000 [16:01<4:17:15,  2.73s/it]                                                    {'loss': 2.7888, 'grad_norm': 3.8293473720550537, 'learning_rate': 4.798305084745763e-05, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [16:01<4:17:15,  2.73s/it]  6%|â–Œ         | 339/6000 [16:04<4:17:38,  2.73s/it]                                                    {'loss': 2.7701, 'grad_norm': 3.729926586151123, 'learning_rate': 4.797457627118644e-05, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [16:04<4:17:38,  2.73s/it]  6%|â–Œ         | 340/6000 [16:07<4:24:20,  2.80s/it]                                                    {'loss': 2.7568, 'grad_norm': 2.7049553394317627, 'learning_rate': 4.796610169491525e-05, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [16:07<4:24:20,  2.80s/it]  6%|â–Œ         | 341/6000 [16:10<4:19:30,  2.75s/it]                                                    {'loss': 2.7694, 'grad_norm': 5.791712284088135, 'learning_rate': 4.795762711864407e-05, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [16:10<4:19:30,  2.75s/it]  6%|â–Œ         | 342/6000 [16:13<4:19:22,  2.75s/it]                                                    {'loss': 2.8026, 'grad_norm': 4.743657112121582, 'learning_rate': 4.794915254237288e-05, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [16:13<4:19:22,  2.75s/it]  6%|â–Œ         | 343/6000 [16:16<4:35:50,  2.93s/it]                                                    {'loss': 2.8283, 'grad_norm': 3.159383773803711, 'learning_rate': 4.79406779661017e-05, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [16:16<4:35:50,  2.93s/it]  6%|â–Œ         | 344/6000 [16:19<4:29:49,  2.86s/it]                                                    {'loss': 2.7838, 'grad_norm': 4.7080230712890625, 'learning_rate': 4.793220338983051e-05, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [16:19<4:29:49,  2.86s/it]  6%|â–Œ         | 345/6000 [16:21<4:24:26,  2.81s/it]                                                    {'loss': 2.8279, 'grad_norm': 8.512039184570312, 'learning_rate': 4.792372881355933e-05, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [16:21<4:24:26,  2.81s/it]  6%|â–Œ         | 346/6000 [16:24<4:21:16,  2.77s/it]                                                    {'loss': 2.8209, 'grad_norm': 9.646210670471191, 'learning_rate': 4.7915254237288134e-05, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:24<4:21:16,  2.77s/it]  6%|â–Œ         | 347/6000 [16:27<4:19:09,  2.75s/it]                                                    {'loss': 2.7873, 'grad_norm': 3.8064324855804443, 'learning_rate': 4.790677966101695e-05, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:27<4:19:09,  2.75s/it]  6%|â–Œ         | 348/6000 [16:29<4:20:14,  2.76s/it]                                                    {'loss': 2.7972, 'grad_norm': 5.053635597229004, 'learning_rate': 4.7898305084745764e-05, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:29<4:20:14,  2.76s/it]  6%|â–Œ         | 349/6000 [16:32<4:19:43,  2.76s/it]                                                    {'loss': 2.84, 'grad_norm': 21.88507843017578, 'learning_rate': 4.788983050847458e-05, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:32<4:19:43,  2.76s/it]  6%|â–Œ         | 350/6000 [16:35<4:18:36,  2.75s/it]                                                    {'loss': 2.7771, 'grad_norm': 3.309194564819336, 'learning_rate': 4.788135593220339e-05, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:35<4:18:36,  2.75s/it][2025-10-22 21:11:33,731] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
[2025-10-22 21:11:33,745] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:11:34,375] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [16:40<5:28:28,  3.49s/it]                                                    {'loss': 2.7779, 'grad_norm': 9.058079719543457, 'learning_rate': 4.7872881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:40<5:28:28,  3.49s/it]  6%|â–Œ         | 352/6000 [16:43<5:05:56,  3.25s/it]                                                    {'loss': 2.7576, 'grad_norm': 7.620345115661621, 'learning_rate': 4.786440677966102e-05, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:43<5:05:56,  3.25s/it]  6%|â–Œ         | 353/6000 [16:46<4:49:18,  3.07s/it]                                                    {'loss': 2.8159, 'grad_norm': 4.240007400512695, 'learning_rate': 4.7855932203389834e-05, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:46<4:49:18,  3.07s/it]  6%|â–Œ         | 354/6000 [16:48<4:38:54,  2.96s/it]                                                    {'loss': 2.7734, 'grad_norm': 3.284815549850464, 'learning_rate': 4.7847457627118645e-05, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:48<4:38:54,  2.96s/it]  6%|â–Œ         | 355/6000 [16:51<4:32:03,  2.89s/it]                                                    {'loss': 2.8178, 'grad_norm': 4.472558498382568, 'learning_rate': 4.7838983050847456e-05, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:51<4:32:03,  2.89s/it]  6%|â–Œ         | 356/6000 [16:54<4:32:54,  2.90s/it]                                                    {'loss': 2.7908, 'grad_norm': 4.264852046966553, 'learning_rate': 4.7830508474576274e-05, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [16:54<4:32:54,  2.90s/it]  6%|â–Œ         | 357/6000 [16:57<4:27:08,  2.84s/it]                                                    {'loss': 2.8234, 'grad_norm': 1.6022757291793823, 'learning_rate': 4.7822033898305086e-05, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [16:57<4:27:08,  2.84s/it]  6%|â–Œ         | 358/6000 [16:59<4:22:12,  2.79s/it]                                                    {'loss': 2.7846, 'grad_norm': 3.3040964603424072, 'learning_rate': 4.7813559322033904e-05, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [16:59<4:22:12,  2.79s/it]  6%|â–Œ         | 359/6000 [17:02<4:19:49,  2.76s/it]                                                    {'loss': 2.7757, 'grad_norm': 2.53460431098938, 'learning_rate': 4.7805084745762715e-05, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [17:02<4:19:49,  2.76s/it]  6%|â–Œ         | 360/6000 [17:05<4:17:40,  2.74s/it]                                                    {'loss': 2.7878, 'grad_norm': 1.9370299577713013, 'learning_rate': 4.7796610169491526e-05, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [17:05<4:17:40,  2.74s/it]  6%|â–Œ         | 361/6000 [17:07<4:15:27,  2.72s/it]                                                    {'loss': 2.7733, 'grad_norm': 1.1625950336456299, 'learning_rate': 4.778813559322034e-05, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [17:07<4:15:27,  2.72s/it]  6%|â–Œ         | 362/6000 [17:10<4:12:52,  2.69s/it]                                                    {'loss': 2.7743, 'grad_norm': 2.0595815181732178, 'learning_rate': 4.7779661016949156e-05, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [17:10<4:12:52,  2.69s/it]  6%|â–Œ         | 363/6000 [17:13<4:12:20,  2.69s/it]                                                    {'loss': 2.8079, 'grad_norm': 4.436467170715332, 'learning_rate': 4.777118644067797e-05, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [17:13<4:12:20,  2.69s/it]  6%|â–Œ         | 364/6000 [17:15<4:17:27,  2.74s/it]                                                    {'loss': 2.7705, 'grad_norm': 1.2170883417129517, 'learning_rate': 4.7762711864406785e-05, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [17:15<4:17:27,  2.74s/it]  6%|â–Œ         | 365/6000 [17:18<4:14:48,  2.71s/it]                                                    {'loss': 2.7826, 'grad_norm': 1.7518948316574097, 'learning_rate': 4.7754237288135596e-05, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [17:18<4:14:48,  2.71s/it]  6%|â–Œ         | 366/6000 [17:21<4:14:10,  2.71s/it]                                                    {'loss': 2.7777, 'grad_norm': 1.696432113647461, 'learning_rate': 4.7745762711864414e-05, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [17:21<4:14:10,  2.71s/it]  6%|â–Œ         | 367/6000 [17:24<4:14:51,  2.71s/it]                                                    {'loss': 2.7779, 'grad_norm': 1.327757716178894, 'learning_rate': 4.773728813559322e-05, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [17:24<4:14:51,  2.71s/it]  6%|â–Œ         | 368/6000 [17:27<4:29:03,  2.87s/it]                                                    {'loss': 2.7777, 'grad_norm': 2.841923475265503, 'learning_rate': 4.772881355932204e-05, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [17:27<4:29:03,  2.87s/it]  6%|â–Œ         | 369/6000 [17:29<4:24:15,  2.82s/it]                                                    {'loss': 2.7924, 'grad_norm': 1.072274923324585, 'learning_rate': 4.772033898305085e-05, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [17:29<4:24:15,  2.82s/it]  6%|â–Œ         | 370/6000 [17:32<4:21:17,  2.78s/it]                                                    {'loss': 2.7754, 'grad_norm': 0.9991334080696106, 'learning_rate': 4.7711864406779666e-05, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [17:32<4:21:17,  2.78s/it]  6%|â–Œ         | 371/6000 [17:35<4:19:04,  2.76s/it]                                                    {'loss': 2.7728, 'grad_norm': 3.125098705291748, 'learning_rate': 4.770338983050848e-05, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [17:35<4:19:04,  2.76s/it]  6%|â–Œ         | 372/6000 [17:38<4:17:03,  2.74s/it]                                                    {'loss': 2.7938, 'grad_norm': 5.622949123382568, 'learning_rate': 4.769491525423729e-05, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [17:38<4:17:03,  2.74s/it]  6%|â–Œ         | 373/6000 [17:40<4:14:31,  2.71s/it]                                                    {'loss': 2.9044, 'grad_norm': 1.3779869079589844, 'learning_rate': 4.768644067796611e-05, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [17:40<4:14:31,  2.71s/it]  6%|â–Œ         | 374/6000 [17:43<4:15:53,  2.73s/it]                                                    {'loss': 2.7816, 'grad_norm': 1.4104305505752563, 'learning_rate': 4.767796610169492e-05, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [17:43<4:15:53,  2.73s/it]  6%|â–‹         | 375/6000 [17:46<4:14:03,  2.71s/it]                                                    {'loss': 2.8044, 'grad_norm': 3.879417896270752, 'learning_rate': 4.766949152542373e-05, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [17:46<4:14:03,  2.71s/it]  6%|â–‹         | 376/6000 [17:48<4:12:19,  2.69s/it]                                                    {'loss': 2.7831, 'grad_norm': 2.2047958374023438, 'learning_rate': 4.766101694915254e-05, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [17:48<4:12:19,  2.69s/it]  6%|â–‹         | 377/6000 [17:51<4:12:11,  2.69s/it]                                                    {'loss': 2.8006, 'grad_norm': 2.410055160522461, 'learning_rate': 4.765254237288136e-05, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [17:51<4:12:11,  2.69s/it]  6%|â–‹         | 378/6000 [17:54<4:15:29,  2.73s/it]                                                    {'loss': 2.7808, 'grad_norm': 1.0550323724746704, 'learning_rate': 4.764406779661017e-05, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [17:54<4:15:29,  2.73s/it]  6%|â–‹         | 379/6000 [17:57<4:16:55,  2.74s/it]                                                    {'loss': 2.7823, 'grad_norm': 1.2910449504852295, 'learning_rate': 4.763559322033899e-05, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [17:57<4:16:55,  2.74s/it]  6%|â–‹         | 380/6000 [18:00<4:25:56,  2.84s/it]                                                    {'loss': 2.7789, 'grad_norm': 2.229741334915161, 'learning_rate': 4.76271186440678e-05, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [18:00<4:25:56,  2.84s/it]  6%|â–‹         | 381/6000 [18:02<4:21:34,  2.79s/it]                                                    {'loss': 2.7833, 'grad_norm': 1.8368051052093506, 'learning_rate': 4.761864406779661e-05, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [18:02<4:21:34,  2.79s/it]  6%|â–‹         | 382/6000 [18:05<4:19:05,  2.77s/it]                                                    {'loss': 2.7735, 'grad_norm': 1.3862577676773071, 'learning_rate': 4.761016949152542e-05, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [18:05<4:19:05,  2.77s/it]  6%|â–‹         | 383/6000 [18:08<4:17:42,  2.75s/it]                                                    {'loss': 2.8028, 'grad_norm': 2.3002891540527344, 'learning_rate': 4.760169491525424e-05, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [18:08<4:17:42,  2.75s/it]  6%|â–‹         | 384/6000 [18:11<4:21:53,  2.80s/it]                                                    {'loss': 2.776, 'grad_norm': 1.5887969732284546, 'learning_rate': 4.759322033898305e-05, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [18:11<4:21:53,  2.80s/it]  6%|â–‹         | 385/6000 [18:14<4:29:59,  2.89s/it]                                                    {'loss': 2.7788, 'grad_norm': 2.89860200881958, 'learning_rate': 4.758474576271187e-05, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [18:14<4:29:59,  2.89s/it]  6%|â–‹         | 386/6000 [18:16<4:26:14,  2.85s/it]                                                    {'loss': 2.7888, 'grad_norm': 3.9419972896575928, 'learning_rate': 4.757627118644068e-05, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [18:16<4:26:14,  2.85s/it]  6%|â–‹         | 387/6000 [18:20<4:34:13,  2.93s/it]                                                    {'loss': 2.7721, 'grad_norm': 5.0726494789123535, 'learning_rate': 4.75677966101695e-05, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [18:20<4:34:13,  2.93s/it]  6%|â–‹         | 388/6000 [18:22<4:32:38,  2.91s/it]                                                    {'loss': 2.8227, 'grad_norm': 5.678712368011475, 'learning_rate': 4.755932203389831e-05, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [18:23<4:32:38,  2.91s/it]  6%|â–‹         | 389/6000 [18:25<4:27:33,  2.86s/it]                                                    {'loss': 2.7777, 'grad_norm': 3.0326991081237793, 'learning_rate': 4.755084745762712e-05, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [18:25<4:27:33,  2.86s/it]  6%|â–‹         | 390/6000 [18:28<4:23:35,  2.82s/it]                                                    {'loss': 2.7743, 'grad_norm': 2.3178539276123047, 'learning_rate': 4.754237288135593e-05, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [18:28<4:23:35,  2.82s/it]  7%|â–‹         | 391/6000 [18:31<4:19:07,  2.77s/it]                                                    {'loss': 2.7764, 'grad_norm': 4.159079551696777, 'learning_rate': 4.7533898305084744e-05, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [18:31<4:19:07,  2.77s/it]  7%|â–‹         | 392/6000 [18:33<4:20:15,  2.78s/it]                                                    {'loss': 2.7741, 'grad_norm': 2.7604360580444336, 'learning_rate': 4.752542372881356e-05, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [18:33<4:20:15,  2.78s/it]  7%|â–‹         | 393/6000 [18:36<4:17:30,  2.76s/it]                                                    {'loss': 2.7831, 'grad_norm': 3.282407760620117, 'learning_rate': 4.751694915254237e-05, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [18:36<4:17:30,  2.76s/it]  7%|â–‹         | 394/6000 [18:39<4:18:24,  2.77s/it]                                                    {'loss': 2.7939, 'grad_norm': 2.7837603092193604, 'learning_rate': 4.750847457627119e-05, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [18:39<4:18:24,  2.77s/it]  7%|â–‹         | 395/6000 [18:42<4:18:24,  2.77s/it]                                                    {'loss': 2.7915, 'grad_norm': 2.9166126251220703, 'learning_rate': 4.75e-05, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [18:42<4:18:24,  2.77s/it]  7%|â–‹         | 396/6000 [18:44<4:16:13,  2.74s/it]                                                    {'loss': 2.7902, 'grad_norm': 1.9946346282958984, 'learning_rate': 4.7491525423728814e-05, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [18:44<4:16:13,  2.74s/it]  7%|â–‹         | 397/6000 [18:47<4:26:05,  2.85s/it]                                                    {'loss': 2.7805, 'grad_norm': 1.6208081245422363, 'learning_rate': 4.7483050847457625e-05, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [18:47<4:26:05,  2.85s/it]  7%|â–‹         | 398/6000 [18:50<4:21:21,  2.80s/it]                                                    {'loss': 2.7837, 'grad_norm': 2.386619806289673, 'learning_rate': 4.747457627118644e-05, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [18:50<4:21:21,  2.80s/it]  7%|â–‹         | 399/6000 [18:53<4:20:44,  2.79s/it]                                                    {'loss': 2.7899, 'grad_norm': 3.036025047302246, 'learning_rate': 4.7466101694915255e-05, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [18:53<4:20:44,  2.79s/it]  7%|â–‹         | 400/6000 [18:56<4:19:12,  2.78s/it]                                                    {'loss': 2.7779, 'grad_norm': 1.2317434549331665, 'learning_rate': 4.745762711864407e-05, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [18:56<4:19:12,  2.78s/it][2025-10-22 21:13:54,459] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
[2025-10-22 21:13:54,469] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-10-22 21:13:55,087] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [19:01<5:17:40,  3.40s/it]                                                    {'loss': 2.7722, 'grad_norm': 1.023919939994812, 'learning_rate': 4.7449152542372884e-05, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [19:01<5:17:40,  3.40s/it]  7%|â–‹         | 402/6000 [19:03<4:56:49,  3.18s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.395276427268982, 'learning_rate': 4.74406779661017e-05, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [19:03<4:56:49,  3.18s/it]  7%|â–‹         | 403/6000 [19:06<4:44:28,  3.05s/it]                                                    {'loss': 2.7741, 'grad_norm': 1.7427841424942017, 'learning_rate': 4.7432203389830506e-05, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [19:06<4:44:28,  3.05s/it]  7%|â–‹         | 404/6000 [19:09<4:38:55,  2.99s/it]                                                    {'loss': 2.7802, 'grad_norm': 6.046131134033203, 'learning_rate': 4.7423728813559325e-05, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [19:09<4:38:55,  2.99s/it]  7%|â–‹         | 405/6000 [19:12<4:31:24,  2.91s/it]                                                    {'loss': 2.7735, 'grad_norm': 1.7849775552749634, 'learning_rate': 4.7415254237288136e-05, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [19:12<4:31:24,  2.91s/it]  7%|â–‹         | 406/6000 [19:14<4:26:13,  2.86s/it]                                                    {'loss': 2.7778, 'grad_norm': 0.9590878486633301, 'learning_rate': 4.7406779661016954e-05, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [19:14<4:26:13,  2.86s/it]  7%|â–‹         | 407/6000 [19:17<4:25:56,  2.85s/it]                                                    {'loss': 2.7854, 'grad_norm': 2.639296293258667, 'learning_rate': 4.7398305084745765e-05, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [19:17<4:25:56,  2.85s/it]  7%|â–‹         | 408/6000 [19:21<4:44:05,  3.05s/it]                                                    {'loss': 2.8005, 'grad_norm': 1.3266656398773193, 'learning_rate': 4.738983050847458e-05, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [19:21<4:44:05,  3.05s/it]