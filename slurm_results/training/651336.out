==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token False --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 16:18:00.684000 131334531602240 torch/distributed/run.py:779] 
W1103 16:18:00.684000 131334531602240 torch/distributed/run.py:779] *****************************************
W1103 16:18:00.684000 131334531602240 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 16:18:00.684000 131334531602240 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 16:18:11,152] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.91it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.52it/s]
wandb: setting up run y2c44u6q
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_161811-y2c44u6q
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/y2c44u6q
[2025-11-03 16:18:12,863] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.25it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.16it/s]
[2025-11-03 16:18:13,488] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 16:18:22,456] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 16:18:23,645] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 16:18:23,646] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 16:18:28,424] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 16:18:28,424] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 16:18:29,279] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 16:18:29,280] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 16:18:29,280] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 16:18:29,281] INFO [src.utils:19] ==================================================
[2025-11-03 16:18:29,282] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 16:18:29,283] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 16:18:29,285] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 16:18:29,285] INFO [src.utils:19] ==================================================
[2025-11-03 16:18:31,091] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 16:18:31,091] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 16:18:31,091] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 16:18:31,091] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 16:18:31,091] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 16:18:31,092] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 16:18:31,092] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 16:18:31,092] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 16:18:31,092] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 16:18:31,092] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 16:18:31,093] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 16:18:31,093] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 16:18:31,093] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 16:18:31,094] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 16:18:31,100] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-11-03 16:18:31,102] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-11-03 16:18:31,106] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 16:18:31,110] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1103 16:18:34.388273782 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1103 16:18:34.404687481 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<7:54:04,  4.74s/it]                                                  {'loss': 19.1675, 'grad_norm': 998.566162109375, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<7:54:04,  4.74s/it]  0%|          | 2/6000 [00:08<6:28:16,  3.88s/it]                                                  {'loss': 20.0058, 'grad_norm': 1033.5416259765625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:08<6:28:16,  3.88s/it]  0%|          | 3/6000 [00:11<6:07:38,  3.68s/it]                                                  {'loss': 19.6774, 'grad_norm': 931.8052978515625, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:11<6:07:38,  3.68s/it]  0%|          | 4/6000 [00:14<5:59:47,  3.60s/it]                                                  {'loss': 19.2715, 'grad_norm': 894.2066040039062, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:14<5:59:47,  3.60s/it]  0%|          | 5/6000 [00:18<5:49:16,  3.50s/it]                                                  {'loss': 20.9813, 'grad_norm': 942.0269775390625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:18<5:49:16,  3.50s/it]  0%|          | 6/6000 [00:21<5:44:45,  3.45s/it]                                                  {'loss': 17.9731, 'grad_norm': 870.9005126953125, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:21<5:44:45,  3.45s/it]  0%|          | 7/6000 [00:24<5:41:30,  3.42s/it]                                                  {'loss': 19.2257, 'grad_norm': 944.9468383789062, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:24<5:41:30,  3.42s/it]  0%|          | 8/6000 [00:28<5:37:08,  3.38s/it]                                                  {'loss': 18.227, 'grad_norm': 902.8922729492188, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:28<5:37:08,  3.38s/it]  0%|          | 9/6000 [00:31<5:37:25,  3.38s/it]                                                  {'loss': 17.8863, 'grad_norm': 950.420166015625, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:31<5:37:25,  3.38s/it]  0%|          | 10/6000 [00:34<5:33:20,  3.34s/it]                                                   {'loss': 16.8571, 'grad_norm': 891.1262817382812, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:34<5:33:20,  3.34s/it]  0%|          | 11/6000 [00:38<5:44:52,  3.46s/it]                                                   {'loss': 20.0556, 'grad_norm': 1008.871337890625, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:38<5:44:52,  3.46s/it]  0%|          | 12/6000 [00:42<5:46:36,  3.47s/it]                                                   {'loss': 18.0703, 'grad_norm': 986.9176635742188, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:42<5:46:36,  3.47s/it]  0%|          | 13/6000 [00:45<5:42:10,  3.43s/it]                                                   {'loss': 18.1613, 'grad_norm': 870.794677734375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:45<5:42:10,  3.43s/it]  0%|          | 14/6000 [00:48<5:43:35,  3.44s/it]                                                   {'loss': 18.2921, 'grad_norm': 1090.495849609375, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:48<5:43:35,  3.44s/it]  0%|          | 15/6000 [00:52<5:39:54,  3.41s/it]                                                   {'loss': 17.3687, 'grad_norm': 837.676025390625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:52<5:39:54,  3.41s/it]  0%|          | 16/6000 [00:55<5:37:44,  3.39s/it]                                                   {'loss': 15.9479, 'grad_norm': 702.1694946289062, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:55<5:37:44,  3.39s/it]  0%|          | 17/6000 [00:58<5:36:11,  3.37s/it]                                                   {'loss': 16.0708, 'grad_norm': 662.61279296875, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:58<5:36:11,  3.37s/it]  0%|          | 18/6000 [01:02<5:36:22,  3.37s/it]                                                   {'loss': 15.0979, 'grad_norm': 677.7976684570312, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [01:02<5:36:22,  3.37s/it]  0%|          | 19/6000 [01:05<5:34:56,  3.36s/it]                                                   {'loss': 14.9156, 'grad_norm': 664.3360595703125, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [01:05<5:34:56,  3.36s/it]  0%|          | 20/6000 [01:09<5:35:58,  3.37s/it]                                                   {'loss': 14.8071, 'grad_norm': 659.3499145507812, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [01:09<5:35:58,  3.37s/it]  0%|          | 21/6000 [01:12<5:38:51,  3.40s/it]                                                   {'loss': 13.0195, 'grad_norm': 611.1314697265625, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:12<5:38:51,  3.40s/it]  0%|          | 22/6000 [01:15<5:39:08,  3.40s/it]                                                   {'loss': 13.4564, 'grad_norm': 504.4424743652344, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:15<5:39:08,  3.40s/it]  0%|          | 23/6000 [01:19<5:37:00,  3.38s/it]                                                   {'loss': 11.8825, 'grad_norm': 619.7420043945312, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:19<5:37:00,  3.38s/it]  0%|          | 24/6000 [01:22<5:39:25,  3.41s/it]                                                   {'loss': 12.9569, 'grad_norm': 593.7889404296875, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:22<5:39:25,  3.41s/it]  0%|          | 25/6000 [01:26<5:39:34,  3.41s/it]                                                   {'loss': 11.0751, 'grad_norm': 476.1031799316406, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:26<5:39:34,  3.41s/it]  0%|          | 26/6000 [01:29<5:39:58,  3.41s/it]                                                   {'loss': 11.1126, 'grad_norm': 512.3352661132812, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:29<5:39:58,  3.41s/it]  0%|          | 27/6000 [01:32<5:39:25,  3.41s/it]                                                   {'loss': 10.3564, 'grad_norm': 431.7817687988281, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:32<5:39:25,  3.41s/it]  0%|          | 28/6000 [01:37<6:05:10,  3.67s/it]                                                   {'loss': 10.6131, 'grad_norm': 434.4549255371094, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:37<6:05:10,  3.67s/it]  0%|          | 29/6000 [01:40<5:56:21,  3.58s/it]                                                   {'loss': 9.9558, 'grad_norm': 451.15753173828125, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:40<5:56:21,  3.58s/it]  0%|          | 30/6000 [01:43<5:49:10,  3.51s/it]                                                   {'loss': 9.8424, 'grad_norm': 412.97412109375, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:43<5:49:10,  3.51s/it]  1%|          | 31/6000 [01:47<5:43:33,  3.45s/it]                                                   {'loss': 8.3876, 'grad_norm': 467.6385803222656, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:47<5:43:33,  3.45s/it]  1%|          | 32/6000 [01:50<5:42:35,  3.44s/it]                                                   {'loss': 9.0608, 'grad_norm': 403.5184020996094, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:50<5:42:35,  3.44s/it]  1%|          | 33/6000 [01:54<5:40:22,  3.42s/it]                                                   {'loss': 9.2785, 'grad_norm': 361.0248718261719, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:54<5:40:22,  3.42s/it]  1%|          | 34/6000 [01:57<5:37:58,  3.40s/it]                                                   {'loss': 7.8063, 'grad_norm': 355.90972900390625, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:57<5:37:58,  3.40s/it]  1%|          | 35/6000 [02:00<5:37:06,  3.39s/it]                                                   {'loss': 8.6286, 'grad_norm': 413.27423095703125, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [02:00<5:37:06,  3.39s/it]  1%|          | 36/6000 [02:04<5:34:04,  3.36s/it]                                                   {'loss': 7.0371, 'grad_norm': 271.657958984375, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [02:04<5:34:04,  3.36s/it]  1%|          | 37/6000 [02:07<5:33:54,  3.36s/it]                                                   {'loss': 5.9865, 'grad_norm': 344.3564147949219, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [02:07<5:33:54,  3.36s/it]  1%|          | 38/6000 [02:10<5:31:51,  3.34s/it]                                                   {'loss': 6.583, 'grad_norm': 315.15972900390625, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [02:10<5:31:51,  3.34s/it]  1%|          | 39/6000 [02:14<5:30:45,  3.33s/it]                                                   {'loss': 6.0597, 'grad_norm': 335.9770812988281, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [02:14<5:30:45,  3.33s/it]  1%|          | 40/6000 [02:17<5:33:24,  3.36s/it]                                                   {'loss': 5.9022, 'grad_norm': 276.6205749511719, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [02:17<5:33:24,  3.36s/it]  1%|          | 41/6000 [02:20<5:32:31,  3.35s/it]                                                   {'loss': 6.9042, 'grad_norm': 308.9685363769531, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [02:20<5:32:31,  3.35s/it]  1%|          | 42/6000 [02:24<5:30:53,  3.33s/it]                                                   {'loss': 5.079, 'grad_norm': 276.0873718261719, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:24<5:30:53,  3.33s/it]  1%|          | 43/6000 [02:28<6:02:43,  3.65s/it]                                                   {'loss': 5.3037, 'grad_norm': 334.9602355957031, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:28<6:02:43,  3.65s/it]  1%|          | 44/6000 [02:32<6:08:52,  3.72s/it]                                                   {'loss': 6.2879, 'grad_norm': 316.3471984863281, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:32<6:08:52,  3.72s/it]  1%|          | 45/6000 [02:35<6:01:09,  3.64s/it]                                                   {'loss': 5.0364, 'grad_norm': 219.4678955078125, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:35<6:01:09,  3.64s/it]  1%|          | 46/6000 [02:39<5:55:08,  3.58s/it]                                                   {'loss': 4.4945, 'grad_norm': 251.1810760498047, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:39<5:55:08,  3.58s/it]  1%|          | 47/6000 [02:42<5:47:38,  3.50s/it]                                                   {'loss': 4.5412, 'grad_norm': 214.02806091308594, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:42<5:47:38,  3.50s/it]  1%|          | 48/6000 [02:45<5:43:48,  3.47s/it]                                                   {'loss': 5.0459, 'grad_norm': 263.47601318359375, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:45<5:43:48,  3.47s/it]  1%|          | 49/6000 [02:49<5:39:44,  3.43s/it]                                                   {'loss': 4.2773, 'grad_norm': 190.5050506591797, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:49<5:39:44,  3.43s/it]  1%|          | 50/6000 [02:52<5:42:30,  3.45s/it]                                                   {'loss': 3.8732, 'grad_norm': 215.8856964111328, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:52<5:42:30,  3.45s/it][2025-11-03 16:21:24,059] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:58<6:48:23,  4.12s/it]                                                   {'loss': 3.5548, 'grad_norm': 233.28758239746094, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:58<6:48:23,  4.12s/it]  1%|          | 52/6000 [03:01<6:24:49,  3.88s/it]                                                   {'loss': 3.9029, 'grad_norm': 209.0052490234375, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [03:01<6:24:49,  3.88s/it]  1%|          | 53/6000 [03:05<6:09:29,  3.73s/it]                                                   {'loss': 3.9372, 'grad_norm': 194.2515869140625, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [03:05<6:09:29,  3.73s/it]  1%|          | 54/6000 [03:08<5:56:31,  3.60s/it]                                                   {'loss': 3.6369, 'grad_norm': 151.79547119140625, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [03:08<5:56:31,  3.60s/it]  1%|          | 55/6000 [03:11<5:50:20,  3.54s/it]                                                   {'loss': 3.6449, 'grad_norm': 171.60231018066406, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [03:11<5:50:20,  3.54s/it]  1%|          | 56/6000 [03:15<5:44:06,  3.47s/it]                                                   {'loss': 3.4727, 'grad_norm': 187.4272003173828, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [03:15<5:44:06,  3.47s/it]  1%|          | 57/6000 [03:18<5:40:47,  3.44s/it]                                                   {'loss': 3.3684, 'grad_norm': 153.85858154296875, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [03:18<5:40:47,  3.44s/it]  1%|          | 58/6000 [03:21<5:39:50,  3.43s/it]                                                   {'loss': 2.8293, 'grad_norm': 137.93601989746094, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [03:21<5:39:50,  3.43s/it]  1%|          | 59/6000 [03:25<5:36:13,  3.40s/it]                                                   {'loss': 3.5222, 'grad_norm': 193.51104736328125, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:25<5:36:13,  3.40s/it]  1%|          | 60/6000 [03:28<5:36:47,  3.40s/it]                                                   {'loss': 3.0906, 'grad_norm': 130.50840759277344, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:28<5:36:47,  3.40s/it]  1%|          | 61/6000 [03:31<5:32:48,  3.36s/it]                                                   {'loss': 2.6837, 'grad_norm': 130.702880859375, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:31<5:32:48,  3.36s/it]  1%|          | 62/6000 [03:35<5:33:46,  3.37s/it]                                                   {'loss': 3.0406, 'grad_norm': 126.54613494873047, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:35<5:33:46,  3.37s/it]  1%|          | 63/6000 [03:38<5:33:38,  3.37s/it]                                                   {'loss': 2.7064, 'grad_norm': 207.22630310058594, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:38<5:33:38,  3.37s/it]  1%|          | 64/6000 [03:41<5:31:13,  3.35s/it]                                                   {'loss': 2.2761, 'grad_norm': 122.81224060058594, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:42<5:31:13,  3.35s/it]  1%|          | 65/6000 [03:45<5:30:48,  3.34s/it]                                                   {'loss': 2.3638, 'grad_norm': 125.95736694335938, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:45<5:30:48,  3.34s/it]  1%|          | 66/6000 [03:49<5:45:49,  3.50s/it]                                                   {'loss': 2.4071, 'grad_norm': 122.53291320800781, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:49<5:45:49,  3.50s/it]  1%|          | 67/6000 [03:52<5:41:54,  3.46s/it]                                                   {'loss': 2.4134, 'grad_norm': 154.54603576660156, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:52<5:41:54,  3.46s/it]  1%|          | 68/6000 [03:55<5:37:57,  3.42s/it]                                                   {'loss': 2.0925, 'grad_norm': 121.57135009765625, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:55<5:37:57,  3.42s/it]  1%|          | 69/6000 [03:59<5:36:13,  3.40s/it]                                                   {'loss': 2.4497, 'grad_norm': 105.09508514404297, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:59<5:36:13,  3.40s/it]  1%|          | 70/6000 [04:02<5:44:35,  3.49s/it]                                                   {'loss': 1.8447, 'grad_norm': 101.75428771972656, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [04:02<5:44:35,  3.49s/it]  1%|          | 71/6000 [04:06<5:38:58,  3.43s/it]                                                   {'loss': 2.1506, 'grad_norm': 100.41419219970703, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [04:06<5:38:58,  3.43s/it]  1%|          | 72/6000 [04:10<5:49:15,  3.53s/it]                                                   {'loss': 2.0061, 'grad_norm': 110.53437042236328, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [04:10<5:49:15,  3.53s/it]  1%|          | 73/6000 [04:13<5:45:34,  3.50s/it]                                                   {'loss': 1.9728, 'grad_norm': 124.69243621826172, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [04:13<5:45:34,  3.50s/it]  1%|          | 74/6000 [04:16<5:40:25,  3.45s/it]                                                   {'loss': 2.169, 'grad_norm': 103.08734130859375, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [04:16<5:40:25,  3.45s/it]  1%|â–         | 75/6000 [04:20<5:37:22,  3.42s/it]                                                   {'loss': 1.8877, 'grad_norm': 157.6186981201172, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [04:20<5:37:22,  3.42s/it]  1%|â–         | 76/6000 [04:23<5:38:19,  3.43s/it]                                                   {'loss': 2.4715, 'grad_norm': 121.55070495605469, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [04:23<5:38:19,  3.43s/it]  1%|â–         | 77/6000 [04:27<5:43:50,  3.48s/it]                                                   {'loss': 2.7744, 'grad_norm': 124.97144317626953, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [04:27<5:43:50,  3.48s/it]  1%|â–         | 78/6000 [04:30<5:52:44,  3.57s/it]                                                   {'loss': 1.7483, 'grad_norm': 105.56546783447266, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [04:30<5:52:44,  3.57s/it]  1%|â–         | 79/6000 [04:34<5:46:50,  3.51s/it]                                                   {'loss': 1.7585, 'grad_norm': 78.71699523925781, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [04:34<5:46:50,  3.51s/it]  1%|â–         | 80/6000 [04:37<5:41:28,  3.46s/it]                                                   {'loss': 1.7531, 'grad_norm': 92.09820556640625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:37<5:41:28,  3.46s/it]  1%|â–         | 81/6000 [04:40<5:37:39,  3.42s/it]                                                   {'loss': 2.2324, 'grad_norm': 166.98573303222656, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:40<5:37:39,  3.42s/it]  1%|â–         | 82/6000 [04:44<5:37:13,  3.42s/it]                                                   {'loss': 1.6453, 'grad_norm': 77.60248565673828, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:44<5:37:13,  3.42s/it]  1%|â–         | 83/6000 [04:47<5:37:52,  3.43s/it]                                                   {'loss': 2.0496, 'grad_norm': 108.126708984375, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:47<5:37:52,  3.43s/it]  1%|â–         | 84/6000 [04:51<5:38:21,  3.43s/it]                                                   {'loss': 1.4766, 'grad_norm': 88.96604919433594, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:51<5:38:21,  3.43s/it]  1%|â–         | 85/6000 [04:55<5:48:02,  3.53s/it]                                                   {'loss': 1.5236, 'grad_norm': 129.19033813476562, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:55<5:48:02,  3.53s/it]  1%|â–         | 86/6000 [04:58<5:44:12,  3.49s/it]                                                   {'loss': 1.8512, 'grad_norm': 136.7113037109375, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:58<5:44:12,  3.49s/it]  1%|â–         | 87/6000 [05:01<5:37:31,  3.42s/it]                                                   {'loss': 1.2773, 'grad_norm': 88.18226623535156, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [05:01<5:37:31,  3.42s/it]  1%|â–         | 88/6000 [05:05<5:33:41,  3.39s/it]                                                   {'loss': 1.381, 'grad_norm': 145.2624053955078, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [05:05<5:33:41,  3.39s/it]  1%|â–         | 89/6000 [05:08<5:32:15,  3.37s/it]                                                   {'loss': 1.1598, 'grad_norm': 92.16487884521484, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [05:08<5:32:15,  3.37s/it]  2%|â–         | 90/6000 [05:11<5:36:15,  3.41s/it]                                                   {'loss': 0.753, 'grad_norm': 64.97325134277344, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [05:11<5:36:15,  3.41s/it]  2%|â–         | 91/6000 [05:15<5:34:10,  3.39s/it]                                                   {'loss': 1.4242, 'grad_norm': 106.7462387084961, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [05:15<5:34:10,  3.39s/it]  2%|â–         | 92/6000 [05:18<5:45:02,  3.50s/it]                                                   {'loss': 1.2636, 'grad_norm': 122.44747924804688, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [05:18<5:45:02,  3.50s/it]  2%|â–         | 93/6000 [05:22<5:46:40,  3.52s/it]                                                   {'loss': 1.1049, 'grad_norm': 118.23700714111328, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [05:22<5:46:40,  3.52s/it]  2%|â–         | 94/6000 [05:25<5:40:49,  3.46s/it]                                                   {'loss': 1.2122, 'grad_norm': 129.3799285888672, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [05:25<5:40:49,  3.46s/it]  2%|â–         | 95/6000 [05:29<5:38:17,  3.44s/it]                                                   {'loss': 0.8519, 'grad_norm': 83.50314331054688, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [05:29<5:38:17,  3.44s/it]  2%|â–         | 96/6000 [05:32<5:35:48,  3.41s/it]                                                   {'loss': 0.825, 'grad_norm': 85.79582977294922, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [05:32<5:35:48,  3.41s/it]  2%|â–         | 97/6000 [05:35<5:33:26,  3.39s/it]                                                   {'loss': 1.5236, 'grad_norm': 205.09788513183594, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [05:35<5:33:26,  3.39s/it]  2%|â–         | 98/6000 [05:39<5:34:20,  3.40s/it]                                                   {'loss': 0.7419, 'grad_norm': 74.04022979736328, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [05:39<5:34:20,  3.40s/it]  2%|â–         | 99/6000 [05:42<5:31:40,  3.37s/it]                                                   {'loss': 0.7183, 'grad_norm': 70.98233032226562, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [05:42<5:31:40,  3.37s/it]  2%|â–         | 100/6000 [05:45<5:29:45,  3.35s/it]                                                    {'loss': 1.1124, 'grad_norm': 107.52603149414062, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [05:45<5:29:45,  3.35s/it][2025-11-03 16:24:17,248] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [05:52<7:04:53,  4.32s/it]                                                    {'loss': 0.7481, 'grad_norm': 65.0886459350586, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:52<7:04:53,  4.32s/it]  2%|â–         | 102/6000 [05:55<6:35:02,  4.02s/it]                                                    {'loss': 0.711, 'grad_norm': 76.41885375976562, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:55<6:35:02,  4.02s/it]  2%|â–         | 103/6000 [05:59<6:17:42,  3.84s/it]                                                    {'loss': 0.651, 'grad_norm': 65.29603576660156, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:59<6:17:42,  3.84s/it]  2%|â–         | 104/6000 [06:02<6:11:26,  3.78s/it]                                                    {'loss': 0.598, 'grad_norm': 59.930484771728516, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [06:02<6:11:26,  3.78s/it]  2%|â–         | 105/6000 [06:06<6:01:55,  3.68s/it]                                                    {'loss': 0.8436, 'grad_norm': 79.88368225097656, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [06:06<6:01:55,  3.68s/it]  2%|â–         | 106/6000 [06:09<5:57:30,  3.64s/it]                                                    {'loss': 0.4304, 'grad_norm': 45.84559631347656, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [06:09<5:57:30,  3.64s/it]  2%|â–         | 107/6000 [06:13<5:50:02,  3.56s/it]                                                    {'loss': 0.609, 'grad_norm': 57.37644958496094, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [06:13<5:50:02,  3.56s/it]  2%|â–         | 108/6000 [06:16<5:50:02,  3.56s/it]                                                    {'loss': 0.397, 'grad_norm': 41.59162521362305, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [06:16<5:50:02,  3.56s/it]  2%|â–         | 109/6000 [06:20<5:58:05,  3.65s/it]                                                    {'loss': 0.66, 'grad_norm': 71.7452163696289, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [06:20<5:58:05,  3.65s/it]  2%|â–         | 110/6000 [06:24<5:51:24,  3.58s/it]                                                    {'loss': 0.535, 'grad_norm': 45.11711120605469, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [06:24<5:51:24,  3.58s/it]  2%|â–         | 111/6000 [06:27<5:48:47,  3.55s/it]                                                    {'loss': 0.4318, 'grad_norm': 54.0627326965332, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [06:27<5:48:47,  3.55s/it]  2%|â–         | 112/6000 [06:31<6:01:27,  3.68s/it]                                                    {'loss': 0.6345, 'grad_norm': 61.9681282043457, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [06:31<6:01:27,  3.68s/it]  2%|â–         | 113/6000 [06:35<5:54:28,  3.61s/it]                                                    {'loss': 0.5199, 'grad_norm': 53.78449630737305, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [06:35<5:54:28,  3.61s/it]  2%|â–         | 114/6000 [06:38<5:48:19,  3.55s/it]                                                    {'loss': 0.3345, 'grad_norm': 42.67142105102539, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [06:38<5:48:19,  3.55s/it]  2%|â–         | 115/6000 [06:41<5:43:35,  3.50s/it]                                                    {'loss': 0.3602, 'grad_norm': 52.81337356567383, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [06:41<5:43:35,  3.50s/it]  2%|â–         | 116/6000 [06:45<5:40:25,  3.47s/it]                                                    {'loss': 0.5361, 'grad_norm': 74.42389678955078, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [06:45<5:40:25,  3.47s/it]  2%|â–         | 117/6000 [06:48<5:38:18,  3.45s/it]                                                    {'loss': 0.6186, 'grad_norm': 52.31220626831055, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [06:48<5:38:18,  3.45s/it]  2%|â–         | 118/6000 [06:52<5:54:33,  3.62s/it]                                                    {'loss': 0.41, 'grad_norm': 52.81177520751953, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:52<5:54:33,  3.62s/it]  2%|â–         | 119/6000 [06:55<5:45:01,  3.52s/it]                                                    {'loss': 0.2256, 'grad_norm': 29.180784225463867, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:55<5:45:01,  3.52s/it]  2%|â–         | 120/6000 [06:59<5:41:29,  3.48s/it]                                                    {'loss': 0.3429, 'grad_norm': 44.66416931152344, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:59<5:41:29,  3.48s/it]  2%|â–         | 121/6000 [07:02<5:38:41,  3.46s/it]                                                    {'loss': 0.4011, 'grad_norm': 73.17003631591797, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [07:02<5:38:41,  3.46s/it]  2%|â–         | 122/6000 [07:06<5:41:09,  3.48s/it]                                                    {'loss': 0.4141, 'grad_norm': 44.802284240722656, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [07:06<5:41:09,  3.48s/it]  2%|â–         | 123/6000 [07:09<5:35:23,  3.42s/it]                                                    {'loss': 0.3777, 'grad_norm': 37.141937255859375, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [07:09<5:35:23,  3.42s/it]  2%|â–         | 124/6000 [07:12<5:33:13,  3.40s/it]                                                    {'loss': 0.311, 'grad_norm': 34.769004821777344, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [07:12<5:33:13,  3.40s/it]  2%|â–         | 125/6000 [07:16<5:40:29,  3.48s/it]                                                    {'loss': 0.3464, 'grad_norm': 35.524044036865234, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [07:16<5:40:29,  3.48s/it]  2%|â–         | 126/6000 [07:20<5:39:03,  3.46s/it]                                                    {'loss': 0.672, 'grad_norm': 47.87737274169922, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [07:20<5:39:03,  3.46s/it]  2%|â–         | 127/6000 [07:23<5:42:56,  3.50s/it]                                                    {'loss': 0.3566, 'grad_norm': 37.36437225341797, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [07:23<5:42:56,  3.50s/it]  2%|â–         | 128/6000 [07:26<5:39:16,  3.47s/it]                                                    {'loss': 0.4855, 'grad_norm': 47.08952331542969, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [07:26<5:39:16,  3.47s/it]  2%|â–         | 129/6000 [07:30<5:37:56,  3.45s/it]                                                    {'loss': 0.7754, 'grad_norm': 74.30105590820312, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [07:30<5:37:56,  3.45s/it]  2%|â–         | 130/6000 [07:33<5:35:59,  3.43s/it]                                                    {'loss': 0.3155, 'grad_norm': 41.3006706237793, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [07:33<5:35:59,  3.43s/it]  2%|â–         | 131/6000 [07:37<5:34:46,  3.42s/it]                                                    {'loss': 0.4795, 'grad_norm': 44.57720184326172, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [07:37<5:34:46,  3.42s/it]  2%|â–         | 132/6000 [07:40<5:32:34,  3.40s/it]                                                    {'loss': 0.2213, 'grad_norm': 20.886402130126953, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [07:40<5:32:34,  3.40s/it]  2%|â–         | 133/6000 [07:43<5:32:01,  3.40s/it]                                                    {'loss': 0.4005, 'grad_norm': 58.63705825805664, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [07:43<5:32:01,  3.40s/it]  2%|â–         | 134/6000 [07:47<5:38:02,  3.46s/it]                                                    {'loss': 0.4674, 'grad_norm': 37.195560455322266, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [07:47<5:38:02,  3.46s/it]  2%|â–         | 135/6000 [07:50<5:35:47,  3.44s/it]                                                    {'loss': 0.5573, 'grad_norm': 51.03946304321289, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [07:50<5:35:47,  3.44s/it]  2%|â–         | 136/6000 [07:54<5:32:38,  3.40s/it]                                                    {'loss': 0.3711, 'grad_norm': 39.64960479736328, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [07:54<5:32:38,  3.40s/it]  2%|â–         | 137/6000 [07:58<5:46:36,  3.55s/it]                                                    {'loss': 0.2906, 'grad_norm': 31.942310333251953, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [07:58<5:46:36,  3.55s/it]  2%|â–         | 138/6000 [08:01<5:40:45,  3.49s/it]                                                    {'loss': 0.4165, 'grad_norm': 38.67998504638672, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [08:01<5:40:45,  3.49s/it]  2%|â–         | 139/6000 [08:05<5:42:04,  3.50s/it]                                                    {'loss': 0.3456, 'grad_norm': 29.47568130493164, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [08:05<5:42:04,  3.50s/it]  2%|â–         | 140/6000 [08:08<5:52:07,  3.61s/it]                                                    {'loss': 0.4772, 'grad_norm': 46.49858093261719, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [08:08<5:52:07,  3.61s/it]  2%|â–         | 141/6000 [08:12<5:45:18,  3.54s/it]                                                    {'loss': 0.5315, 'grad_norm': 34.56019592285156, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [08:12<5:45:18,  3.54s/it]  2%|â–         | 142/6000 [08:15<5:38:43,  3.47s/it]                                                    {'loss': 0.3167, 'grad_norm': 48.678585052490234, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [08:15<5:38:43,  3.47s/it]  2%|â–         | 143/6000 [08:18<5:35:30,  3.44s/it]                                                    {'loss': 0.397, 'grad_norm': 47.48749923706055, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [08:18<5:35:30,  3.44s/it]  2%|â–         | 144/6000 [08:22<5:36:01,  3.44s/it]                                                    {'loss': 0.2493, 'grad_norm': 28.819252014160156, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [08:22<5:36:01,  3.44s/it]  2%|â–         | 145/6000 [08:25<5:32:08,  3.40s/it]                                                    {'loss': 0.1651, 'grad_norm': 22.41165542602539, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [08:25<5:32:08,  3.40s/it]  2%|â–         | 146/6000 [08:29<5:35:43,  3.44s/it]                                                    {'loss': 0.3916, 'grad_norm': 49.31720733642578, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [08:29<5:35:43,  3.44s/it]  2%|â–         | 147/6000 [08:32<5:36:17,  3.45s/it]                                                    {'loss': 0.2327, 'grad_norm': 24.20166778564453, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [08:32<5:36:17,  3.45s/it]