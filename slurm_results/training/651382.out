==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 17:12:08.509000 134283673941824 torch/distributed/run.py:779] 
W1103 17:12:08.509000 134283673941824 torch/distributed/run.py:779] *****************************************
W1103 17:12:08.509000 134283673941824 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 17:12:08.509000 134283673941824 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 17:12:18,913] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.01it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.69it/s]
wandb: setting up run g3mhrerm
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_171219-g3mhrerm
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/g3mhrerm
[2025-11-03 17:12:20,718] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.05it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.77it/s]
[2025-11-03 17:12:21,369] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 17:12:30,103] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 17:12:31,618] INFO [src.utils:19] PeftModel(
  (base_model): LoraModel(
    (model): Qwen2VLForConditionalGeneration(
      (visual): Qwen2VisionTransformerPretrainedModel(
        (patch_embed): PatchEmbed(
          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
        )
        (rotary_pos_emb): VisionRotaryEmbedding()
        (blocks): ModuleList(
          (0-31): 32 x Qwen2VLVisionBlock(
            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (attn): VisionFlashAttention2(
              (qkv): Linear(in_features=1280, out_features=3840, bias=True)
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (mlp): VisionMlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): QuickGELUActivation()
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (merger): PatchMerger(
          (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (mlp): Sequential(
            (0): Linear(in_features=5120, out_features=5120, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=5120, out_features=1536, bias=True)
          )
        )
      )
      (model): Qwen2VLModel(
        (embed_tokens): Embedding(151936, 1536)
        (layers): ModuleList(
          (0-27): 28 x Qwen2VLDecoderLayer(
            (self_attn): Qwen2VLFlashAttention2(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (rotary_emb): Qwen2VLRotaryEmbedding()
            )
            (mlp): Qwen2MLP(
              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8960, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (act_fn): SiLU()
            )
            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          )
        )
        (norm): Qwen2RMSNorm((1536,), eps=1e-06)
        (rotary_emb): Qwen2VLRotaryEmbedding()
      )
      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
    )
  )
)
[2025-11-03 17:12:31,627] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 17:12:31,628] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 17:12:35,867] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 17:12:35,868] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 17:12:36,955] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 17:12:36,956] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 17:12:36,956] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 17:12:36,957] INFO [src.utils:19] ==================================================
[2025-11-03 17:12:36,958] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 17:12:36,959] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 17:12:36,961] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 17:12:36,961] INFO [src.utils:19] ==================================================
[2025-11-03 17:12:38,737] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 17:12:38,738] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 17:12:38,738] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 17:12:38,738] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 17:12:38,738] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 17:12:38,738] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 17:12:38,738] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 17:12:38,739] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 17:12:38,740] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 17:12:38,740] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 17:12:38,741] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 17:12:38,741] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 17:12:38,741] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 17:12:38,742] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 17:12:38,746] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 17:12:38,749] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 17:12:38,753] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 17:12:38,756] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:04<8:03:34,  4.84s/it]                                                  {'loss': 19.1675, 'grad_norm': 1009.9654541015625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<8:03:34,  4.84s/it]  0%|          | 2/6000 [00:08<6:34:04,  3.94s/it]                                                  {'loss': 20.0058, 'grad_norm': 1037.3819580078125, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:08<6:34:04,  3.94s/it]  0%|          | 3/6000 [00:11<6:10:19,  3.71s/it]                                                  {'loss': 19.7016, 'grad_norm': 927.710693359375, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:11<6:10:19,  3.71s/it]  0%|          | 4/6000 [00:15<6:01:43,  3.62s/it]                                                  {'loss': 19.2598, 'grad_norm': 901.7037963867188, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:15<6:01:43,  3.62s/it]  0%|          | 5/6000 [00:18<5:52:11,  3.52s/it]                                                  {'loss': 20.9033, 'grad_norm': 950.978515625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:18<5:52:11,  3.52s/it]  0%|          | 6/6000 [00:21<5:46:23,  3.47s/it]                                                  {'loss': 17.9376, 'grad_norm': 886.5466918945312, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:21<5:46:23,  3.47s/it]  0%|          | 7/6000 [00:25<5:42:19,  3.43s/it]                                                  {'loss': 19.2075, 'grad_norm': 938.25146484375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:25<5:42:19,  3.43s/it]  0%|          | 8/6000 [00:28<5:38:25,  3.39s/it]                                                  {'loss': 18.2949, 'grad_norm': 910.473388671875, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:28<5:38:25,  3.39s/it]  0%|          | 9/6000 [00:31<5:39:17,  3.40s/it]                                                  {'loss': 17.8898, 'grad_norm': 958.7743530273438, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:31<5:39:17,  3.40s/it]  0%|          | 10/6000 [00:35<5:34:11,  3.35s/it]                                                   {'loss': 16.9304, 'grad_norm': 920.7937622070312, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:35<5:34:11,  3.35s/it]  0%|          | 11/6000 [00:38<5:42:28,  3.43s/it]                                                   {'loss': 20.1026, 'grad_norm': 1071.7274169921875, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:38<5:42:28,  3.43s/it]  0%|          | 12/6000 [00:42<5:44:24,  3.45s/it]                                                   {'loss': 18.059, 'grad_norm': 986.8032836914062, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:42<5:44:24,  3.45s/it]  0%|          | 13/6000 [00:45<5:41:41,  3.42s/it]                                                   {'loss': 18.0417, 'grad_norm': 877.0560913085938, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:45<5:41:41,  3.42s/it]  0%|          | 14/6000 [00:49<5:42:37,  3.43s/it]                                                   {'loss': 18.4423, 'grad_norm': 926.041015625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:49<5:42:37,  3.43s/it]  0%|          | 15/6000 [00:52<5:41:01,  3.42s/it]                                                   {'loss': 17.3885, 'grad_norm': 828.7359619140625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:52<5:41:01,  3.42s/it]  0%|          | 16/6000 [00:55<5:38:38,  3.40s/it]                                                   {'loss': 15.9208, 'grad_norm': 716.417236328125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:55<5:38:38,  3.40s/it]  0%|          | 17/6000 [00:59<5:38:23,  3.39s/it]                                                   {'loss': 16.0801, 'grad_norm': 659.1031494140625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:59<5:38:23,  3.39s/it]  0%|          | 18/6000 [01:02<5:38:32,  3.40s/it]                                                   {'loss': 15.1741, 'grad_norm': 687.212890625, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [01:02<5:38:32,  3.40s/it]  0%|          | 19/6000 [01:05<5:35:43,  3.37s/it]                                                   {'loss': 14.8369, 'grad_norm': 672.4815673828125, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [01:05<5:35:43,  3.37s/it]  0%|          | 20/6000 [01:09<5:35:12,  3.36s/it]                                                   {'loss': 14.8098, 'grad_norm': 685.03662109375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [01:09<5:35:12,  3.36s/it]  0%|          | 21/6000 [01:12<5:38:03,  3.39s/it]                                                   {'loss': 13.0147, 'grad_norm': 602.8575439453125, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:12<5:38:03,  3.39s/it]  0%|          | 22/6000 [01:16<5:40:53,  3.42s/it]                                                   {'loss': 13.4379, 'grad_norm': 517.4732055664062, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:16<5:40:53,  3.42s/it]  0%|          | 23/6000 [01:19<5:37:54,  3.39s/it]                                                   {'loss': 11.8217, 'grad_norm': 590.0802612304688, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:19<5:37:54,  3.39s/it]  0%|          | 24/6000 [01:22<5:40:37,  3.42s/it]                                                   {'loss': 13.0719, 'grad_norm': 618.0725708007812, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:22<5:40:37,  3.42s/it]  0%|          | 25/6000 [01:26<5:40:18,  3.42s/it]                                                   {'loss': 11.0351, 'grad_norm': 496.4068298339844, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:26<5:40:18,  3.42s/it]  0%|          | 26/6000 [01:29<5:42:14,  3.44s/it]                                                   {'loss': 11.1279, 'grad_norm': 478.87408447265625, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:29<5:42:14,  3.44s/it]  0%|          | 27/6000 [01:33<5:41:12,  3.43s/it]                                                   {'loss': 10.2639, 'grad_norm': 460.2301025390625, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:33<5:41:12,  3.43s/it]  0%|          | 28/6000 [01:37<6:08:34,  3.70s/it]                                                   {'loss': 10.6295, 'grad_norm': 433.7909851074219, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:37<6:08:34,  3.70s/it]  0%|          | 29/6000 [01:40<5:58:03,  3.60s/it]                                                   {'loss': 9.9813, 'grad_norm': 456.16900634765625, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:40<5:58:03,  3.60s/it]  0%|          | 30/6000 [01:44<5:52:31,  3.54s/it]                                                   {'loss': 9.7957, 'grad_norm': 406.81622314453125, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:44<5:52:31,  3.54s/it]  1%|          | 31/6000 [01:47<5:46:26,  3.48s/it]                                                   {'loss': 8.4302, 'grad_norm': 475.1222229003906, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:47<5:46:26,  3.48s/it]  1%|          | 32/6000 [01:51<5:44:28,  3.46s/it]                                                   {'loss': 8.9763, 'grad_norm': 351.6454772949219, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:51<5:44:28,  3.46s/it]  1%|          | 33/6000 [01:54<5:42:19,  3.44s/it]                                                   {'loss': 9.197, 'grad_norm': 369.06298828125, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:54<5:42:19,  3.44s/it]  1%|          | 34/6000 [01:57<5:39:03,  3.41s/it]                                                   {'loss': 7.8112, 'grad_norm': 332.2106018066406, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:57<5:39:03,  3.41s/it]  1%|          | 35/6000 [02:01<5:38:52,  3.41s/it]                                                   {'loss': 8.5926, 'grad_norm': 407.4127502441406, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [02:01<5:38:52,  3.41s/it]  1%|          | 36/6000 [02:04<5:36:18,  3.38s/it]                                                   {'loss': 6.9985, 'grad_norm': 273.6069641113281, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [02:04<5:36:18,  3.38s/it]  1%|          | 37/6000 [02:07<5:35:26,  3.38s/it]                                                   {'loss': 5.9343, 'grad_norm': 332.5610046386719, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [02:07<5:35:26,  3.38s/it]  1%|          | 38/6000 [02:11<5:32:55,  3.35s/it]                                                   {'loss': 6.5653, 'grad_norm': 333.00909423828125, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [02:11<5:32:55,  3.35s/it]  1%|          | 39/6000 [02:14<5:31:39,  3.34s/it]                                                   {'loss': 6.0177, 'grad_norm': 329.24053955078125, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [02:14<5:31:39,  3.34s/it]  1%|          | 40/6000 [02:17<5:33:01,  3.35s/it]                                                   {'loss': 5.8957, 'grad_norm': 280.45654296875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [02:17<5:33:01,  3.35s/it]  1%|          | 41/6000 [02:21<5:31:17,  3.34s/it]                                                   {'loss': 6.8902, 'grad_norm': 305.9233703613281, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [02:21<5:31:17,  3.34s/it]  1%|          | 42/6000 [02:24<5:29:49,  3.32s/it]                                                   {'loss': 5.0931, 'grad_norm': 274.3407287597656, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:24<5:29:49,  3.32s/it]  1%|          | 43/6000 [02:28<6:03:04,  3.66s/it]                                                   {'loss': 5.174, 'grad_norm': 302.3445739746094, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:28<6:03:04,  3.66s/it]  1%|          | 44/6000 [02:32<6:09:33,  3.72s/it]                                                   {'loss': 6.1561, 'grad_norm': 312.2733154296875, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:32<6:09:33,  3.72s/it]  1%|          | 45/6000 [02:36<5:59:53,  3.63s/it]                                                   {'loss': 5.0216, 'grad_norm': 207.0375518798828, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:36<5:59:53,  3.63s/it]  1%|          | 46/6000 [02:39<5:54:49,  3.58s/it]                                                   {'loss': 4.4019, 'grad_norm': 250.01702880859375, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:39<5:54:49,  3.58s/it]  1%|          | 47/6000 [02:43<5:48:09,  3.51s/it]                                                   {'loss': 4.4347, 'grad_norm': 208.9263153076172, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:43<5:48:09,  3.51s/it]  1%|          | 48/6000 [02:46<5:46:26,  3.49s/it]                                                   {'loss': 5.0108, 'grad_norm': 266.848388671875, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:46<5:46:26,  3.49s/it]  1%|          | 49/6000 [02:49<5:41:35,  3.44s/it]                                                   {'loss': 4.2097, 'grad_norm': 188.4794464111328, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:49<5:41:35,  3.44s/it]  1%|          | 50/6000 [02:53<5:44:11,  3.47s/it]                                                   {'loss': 3.8092, 'grad_norm': 219.33815002441406, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:53<5:44:11,  3.47s/it]  1%|          | 51/6000 [02:56<5:39:30,  3.42s/it]                                                   {'loss': 3.6127, 'grad_norm': 237.27407836914062, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:56<5:39:30,  3.42s/it]  1%|          | 52/6000 [02:59<5:36:06,  3.39s/it]                                                   {'loss': 4.0022, 'grad_norm': 213.12388610839844, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:59<5:36:06,  3.39s/it]  1%|          | 53/6000 [03:03<5:37:45,  3.41s/it]                                                   {'loss': 3.9546, 'grad_norm': 186.24249267578125, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [03:03<5:37:45,  3.41s/it]  1%|          | 54/6000 [03:06<5:34:59,  3.38s/it]                                                   {'loss': 3.5963, 'grad_norm': 153.5782470703125, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [03:06<5:34:59,  3.38s/it]  1%|          | 55/6000 [03:10<5:33:28,  3.37s/it]                                                   {'loss': 3.6774, 'grad_norm': 183.31283569335938, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [03:10<5:33:28,  3.37s/it]  1%|          | 56/6000 [03:13<5:32:27,  3.36s/it]                                                   {'loss': 3.4697, 'grad_norm': 176.01641845703125, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [03:13<5:32:27,  3.36s/it]  1%|          | 57/6000 [03:16<5:32:24,  3.36s/it]                                                   {'loss': 3.4009, 'grad_norm': 153.6744384765625, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [03:16<5:32:24,  3.36s/it]  1%|          | 58/6000 [03:20<5:31:56,  3.35s/it]                                                   {'loss': 2.7727, 'grad_norm': 133.3096160888672, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [03:20<5:31:56,  3.35s/it]  1%|          | 59/6000 [03:23<5:28:56,  3.32s/it]                                                   {'loss': 3.4707, 'grad_norm': 197.1990966796875, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:23<5:28:56,  3.32s/it]  1%|          | 60/6000 [03:26<5:31:47,  3.35s/it]                                                   {'loss': 2.9746, 'grad_norm': 379.1207275390625, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:26<5:31:47,  3.35s/it]  1%|          | 61/6000 [03:30<5:30:41,  3.34s/it]                                                   {'loss': 2.7166, 'grad_norm': 129.975830078125, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:30<5:30:41,  3.34s/it]  1%|          | 62/6000 [03:33<5:32:22,  3.36s/it]                                                   {'loss': 3.1335, 'grad_norm': 125.10191345214844, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:33<5:32:22,  3.36s/it]  1%|          | 63/6000 [03:36<5:32:03,  3.36s/it]                                                   {'loss': 2.6528, 'grad_norm': 129.9091033935547, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:36<5:32:03,  3.36s/it]  1%|          | 64/6000 [03:40<5:29:43,  3.33s/it]                                                   {'loss': 2.318, 'grad_norm': 120.98270416259766, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:40<5:29:43,  3.33s/it]  1%|          | 65/6000 [03:43<5:28:27,  3.32s/it]                                                   {'loss': 2.3573, 'grad_norm': 132.01141357421875, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:43<5:28:27,  3.32s/it]  1%|          | 66/6000 [03:47<5:43:46,  3.48s/it]                                                   {'loss': 2.423, 'grad_norm': 129.67929077148438, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:47<5:43:46,  3.48s/it]  1%|          | 67/6000 [03:50<5:39:32,  3.43s/it]                                                   {'loss': 2.4377, 'grad_norm': 152.56822204589844, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:50<5:39:32,  3.43s/it]  1%|          | 68/6000 [03:53<5:34:01,  3.38s/it]                                                   {'loss': 2.0388, 'grad_norm': 139.82394409179688, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:53<5:34:01,  3.38s/it]  1%|          | 69/6000 [03:57<5:34:44,  3.39s/it]                                                   {'loss': 2.4515, 'grad_norm': 110.48854064941406, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:57<5:34:44,  3.39s/it]  1%|          | 70/6000 [04:00<5:36:53,  3.41s/it]                                                   {'loss': 1.7977, 'grad_norm': 94.34562683105469, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [04:00<5:36:53,  3.41s/it]  1%|          | 71/6000 [04:04<5:33:33,  3.38s/it]                                                   {'loss': 2.096, 'grad_norm': 104.95394897460938, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [04:04<5:33:33,  3.38s/it]  1%|          | 72/6000 [04:07<5:46:33,  3.51s/it]                                                   {'loss': 2.0031, 'grad_norm': 123.33870697021484, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [04:07<5:46:33,  3.51s/it]  1%|          | 73/6000 [04:11<5:43:37,  3.48s/it]                                                   {'loss': 2.0054, 'grad_norm': 167.07044982910156, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [04:11<5:43:37,  3.48s/it]  1%|          | 74/6000 [04:14<5:45:37,  3.50s/it]                                                   {'loss': 2.2659, 'grad_norm': 100.21560668945312, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [04:14<5:45:37,  3.50s/it]  1%|â–         | 75/6000 [04:18<5:39:10,  3.43s/it]                                                   {'loss': 2.0552, 'grad_norm': 215.73971557617188, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [04:18<5:39:10,  3.43s/it]  1%|â–         | 76/6000 [04:21<5:47:05,  3.52s/it]                                                   {'loss': 2.5902, 'grad_norm': 127.51815795898438, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [04:21<5:47:05,  3.52s/it]  1%|â–         | 77/6000 [04:25<5:43:24,  3.48s/it]                                                   {'loss': 2.7299, 'grad_norm': 136.86544799804688, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [04:25<5:43:24,  3.48s/it]  1%|â–         | 78/6000 [04:28<5:51:58,  3.57s/it]                                                   {'loss': 1.7282, 'grad_norm': 107.62805938720703, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [04:28<5:51:58,  3.57s/it]  1%|â–         | 79/6000 [04:32<5:45:06,  3.50s/it]                                                   {'loss': 1.7368, 'grad_norm': 97.95361328125, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [04:32<5:45:06,  3.50s/it]  1%|â–         | 80/6000 [04:35<5:40:06,  3.45s/it]                                                   {'loss': 1.6788, 'grad_norm': 102.7025146484375, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:35<5:40:06,  3.45s/it]  1%|â–         | 81/6000 [04:38<5:36:52,  3.41s/it]                                                   {'loss': 2.3008, 'grad_norm': 193.99241638183594, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:38<5:36:52,  3.41s/it]  1%|â–         | 82/6000 [04:42<5:37:19,  3.42s/it]                                                   {'loss': 1.6572, 'grad_norm': 103.63330078125, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:42<5:37:19,  3.42s/it]  1%|â–         | 83/6000 [04:45<5:37:04,  3.42s/it]                                                   {'loss': 2.1355, 'grad_norm': 124.17005157470703, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:45<5:37:04,  3.42s/it]  1%|â–         | 84/6000 [04:49<5:37:49,  3.43s/it]                                                   {'loss': 1.365, 'grad_norm': 90.99442291259766, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:49<5:37:49,  3.43s/it]  1%|â–         | 85/6000 [04:52<5:47:26,  3.52s/it]                                                   {'loss': 1.4849, 'grad_norm': 154.86392211914062, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:52<5:47:26,  3.52s/it]  1%|â–         | 86/6000 [04:56<5:43:27,  3.48s/it]                                                   {'loss': 1.648, 'grad_norm': 129.76536560058594, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:56<5:43:27,  3.48s/it]  1%|â–         | 87/6000 [04:59<5:36:19,  3.41s/it]                                                   {'loss': 1.1526, 'grad_norm': 100.0293197631836, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:59<5:36:19,  3.41s/it]  1%|â–         | 88/6000 [05:02<5:32:51,  3.38s/it]                                                   {'loss': 1.4736, 'grad_norm': 192.83306884765625, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [05:02<5:32:51,  3.38s/it]  1%|â–         | 89/6000 [05:06<5:30:37,  3.36s/it]                                                   {'loss': 1.2135, 'grad_norm': 106.63335418701172, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [05:06<5:30:37,  3.36s/it]  2%|â–         | 90/6000 [05:09<5:33:42,  3.39s/it]                                                   {'loss': 0.8375, 'grad_norm': 90.96680450439453, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [05:09<5:33:42,  3.39s/it]  2%|â–         | 91/6000 [05:13<5:32:43,  3.38s/it]                                                   {'loss': 1.3935, 'grad_norm': 119.63957214355469, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [05:13<5:32:43,  3.38s/it]  2%|â–         | 92/6000 [05:16<5:44:49,  3.50s/it]                                                   {'loss': 1.0829, 'grad_norm': 104.23125457763672, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [05:16<5:44:49,  3.50s/it]  2%|â–         | 93/6000 [05:20<5:47:25,  3.53s/it]                                                   {'loss': 1.186, 'grad_norm': 146.91029357910156, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [05:20<5:47:25,  3.53s/it]  2%|â–         | 94/6000 [05:23<5:40:49,  3.46s/it]                                                   {'loss': 1.2626, 'grad_norm': 141.12388610839844, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [05:23<5:40:49,  3.46s/it]  2%|â–         | 95/6000 [05:27<5:38:17,  3.44s/it]                                                   {'loss': 0.7597, 'grad_norm': 88.82855224609375, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [05:27<5:38:17,  3.44s/it]  2%|â–         | 96/6000 [05:30<5:34:59,  3.40s/it]                                                   {'loss': 0.8585, 'grad_norm': 90.24073791503906, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [05:30<5:34:59,  3.40s/it]  2%|â–         | 97/6000 [05:33<5:33:36,  3.39s/it]                                                   {'loss': 1.5552, 'grad_norm': 184.90234375, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [05:33<5:33:36,  3.39s/it]  2%|â–         | 98/6000 [05:37<5:32:45,  3.38s/it]                                                   {'loss': 0.6756, 'grad_norm': 75.51812744140625, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [05:37<5:32:45,  3.38s/it]  2%|â–         | 99/6000 [05:40<5:30:48,  3.36s/it]                                                   {'loss': 0.6869, 'grad_norm': 75.0660400390625, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [05:40<5:30:48,  3.36s/it]  2%|â–         | 100/6000 [05:43<5:28:03,  3.34s/it]                                                    {'loss': 1.1696, 'grad_norm': 123.74481201171875, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [05:43<5:28:03,  3.34s/it]  2%|â–         | 101/6000 [05:48<6:01:37,  3.68s/it]                                                    {'loss': 0.8401, 'grad_norm': 73.87579345703125, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:48<6:01:37,  3.68s/it]  2%|â–         | 102/6000 [05:51<5:51:19,  3.57s/it]                                                    {'loss': 0.7146, 'grad_norm': 79.67762756347656, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:51<5:51:19,  3.57s/it]  2%|â–         | 103/6000 [05:54<5:46:58,  3.53s/it]                                                    {'loss': 0.6205, 'grad_norm': 77.42737579345703, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:54<5:46:58,  3.53s/it]  2%|â–         | 104/6000 [05:58<5:49:56,  3.56s/it]                                                    {'loss': 0.5334, 'grad_norm': 57.00313949584961, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [05:58<5:49:56,  3.56s/it]  2%|â–         | 105/6000 [06:02<5:46:18,  3.52s/it]                                                    {'loss': 0.9544, 'grad_norm': 84.30747985839844, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [06:02<5:46:18,  3.52s/it]  2%|â–         | 106/6000 [06:05<5:46:31,  3.53s/it]                                                    {'loss': 0.435, 'grad_norm': 49.000125885009766, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [06:05<5:46:31,  3.53s/it]  2%|â–         | 107/6000 [06:08<5:42:19,  3.49s/it]                                                    {'loss': 0.6134, 'grad_norm': 59.507747650146484, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [06:08<5:42:19,  3.49s/it]  2%|â–         | 108/6000 [06:12<5:44:45,  3.51s/it]                                                    {'loss': 0.4411, 'grad_norm': 46.348785400390625, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [06:12<5:44:45,  3.51s/it]  2%|â–         | 109/6000 [06:16<5:53:33,  3.60s/it]                                                    {'loss': 0.6305, 'grad_norm': 75.61550903320312, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [06:16<5:53:33,  3.60s/it]  2%|â–         | 110/6000 [06:19<5:47:57,  3.54s/it]                                                    {'loss': 0.6579, 'grad_norm': 62.32790756225586, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [06:19<5:47:57,  3.54s/it]  2%|â–         | 111/6000 [06:23<5:46:37,  3.53s/it]                                                    {'loss': 0.4374, 'grad_norm': 60.328548431396484, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [06:23<5:46:37,  3.53s/it]  2%|â–         | 112/6000 [06:27<5:58:52,  3.66s/it]                                                    {'loss': 0.6381, 'grad_norm': 58.73530960083008, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [06:27<5:58:52,  3.66s/it]  2%|â–         | 113/6000 [06:30<5:53:15,  3.60s/it]                                                    {'loss': 0.4987, 'grad_norm': 54.47753143310547, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [06:30<5:53:15,  3.60s/it]  2%|â–         | 114/6000 [06:34<5:46:30,  3.53s/it]                                                    {'loss': 0.4885, 'grad_norm': 61.122432708740234, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [06:34<5:46:30,  3.53s/it]  2%|â–         | 115/6000 [06:37<5:42:30,  3.49s/it]                                                    {'loss': 0.417, 'grad_norm': 58.15972137451172, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [06:37<5:42:30,  3.49s/it]  2%|â–         | 116/6000 [06:40<5:38:20,  3.45s/it]                                                    {'loss': 0.6055, 'grad_norm': 79.66902923583984, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [06:40<5:38:20,  3.45s/it]  2%|â–         | 117/6000 [06:44<5:36:14,  3.43s/it]                                                    {'loss': 0.6631, 'grad_norm': 59.58155059814453, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [06:44<5:36:14,  3.43s/it]  2%|â–         | 118/6000 [06:48<5:53:00,  3.60s/it]                                                    {'loss': 0.4044, 'grad_norm': 32.20146179199219, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:48<5:53:00,  3.60s/it]  2%|â–         | 119/6000 [06:51<5:44:44,  3.52s/it]                                                    {'loss': 0.2368, 'grad_norm': 29.572439193725586, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:51<5:44:44,  3.52s/it]  2%|â–         | 120/6000 [06:54<5:41:25,  3.48s/it]                                                    {'loss': 0.4317, 'grad_norm': 50.59148406982422, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:54<5:41:25,  3.48s/it]  2%|â–         | 121/6000 [06:58<5:39:03,  3.46s/it]                                                    {'loss': 0.2689, 'grad_norm': 67.51133728027344, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [06:58<5:39:03,  3.46s/it]  2%|â–         | 122/6000 [07:01<5:41:08,  3.48s/it]                                                    {'loss': 0.4021, 'grad_norm': 37.6561164855957, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [07:01<5:41:08,  3.48s/it]  2%|â–         | 123/6000 [07:05<5:35:31,  3.43s/it]                                                    {'loss': 0.3896, 'grad_norm': 39.26847457885742, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [07:05<5:35:31,  3.43s/it]  2%|â–         | 124/6000 [07:08<5:32:42,  3.40s/it]                                                    {'loss': 0.2876, 'grad_norm': 28.550434112548828, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [07:08<5:32:42,  3.40s/it]  2%|â–         | 125/6000 [07:12<5:39:24,  3.47s/it]                                                    {'loss': 0.3626, 'grad_norm': 35.22950744628906, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [07:12<5:39:24,  3.47s/it]  2%|â–         | 126/6000 [07:15<5:38:55,  3.46s/it]                                                    {'loss': 0.5878, 'grad_norm': 46.844486236572266, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [07:15<5:38:55,  3.46s/it]  2%|â–         | 127/6000 [07:19<5:42:12,  3.50s/it]                                                    {'loss': 0.3261, 'grad_norm': 32.5357551574707, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [07:19<5:42:12,  3.50s/it]  2%|â–         | 128/6000 [07:22<5:36:33,  3.44s/it]                                                    {'loss': 0.4415, 'grad_norm': 41.88168716430664, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [07:22<5:36:33,  3.44s/it]  2%|â–         | 129/6000 [07:25<5:35:51,  3.43s/it]                                                    {'loss': 0.914, 'grad_norm': 69.88568115234375, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [07:25<5:35:51,  3.43s/it]