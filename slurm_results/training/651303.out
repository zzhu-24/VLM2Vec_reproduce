==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 15:49:57.172000 127109271701312 torch/distributed/run.py:779] 
W1103 15:49:57.172000 127109271701312 torch/distributed/run.py:779] *****************************************
W1103 15:49:57.172000 127109271701312 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 15:49:57.172000 127109271701312 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 15:50:06,794] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.82it/s]
wandb: setting up run loitp1fe
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_155007-loitp1fe
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/loitp1fe
[2025-11-03 15:50:08,370] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.18it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.02it/s]
[2025-11-03 15:50:08,993] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 15:50:17,918] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 15:50:19,040] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 15:50:19,041] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 15:50:23,523] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 15:50:23,524] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 15:50:24,367] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 15:50:24,368] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 15:50:24,368] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 15:50:24,370] INFO [src.utils:19] ==================================================
[2025-11-03 15:50:24,370] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 15:50:24,371] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:50:24,372] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:50:24,373] INFO [src.utils:19] ==================================================
[2025-11-03 15:50:26,127] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:50:26,127] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:50:26,127] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:50:26,127] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:50:26,127] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:50:26,127] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:50:26,127] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:50:26,127] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:50:26,128] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:50:26,128] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:50:26,129] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:50:26,129] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:50:26,129] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:50:26,129] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:50:26,135] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 15:50:26,137] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 15:50:26,142] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 15:50:26,144] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:04<6:47:42,  4.08s/it]                                                  {'loss': 21.6478, 'grad_norm': 613.95556640625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:47:42,  4.08s/it]  0%|          | 2/6000 [00:06<5:24:34,  3.25s/it]                                                  {'loss': 18.9604, 'grad_norm': 585.1402587890625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:24:34,  3.25s/it]  0%|          | 3/6000 [00:09<5:03:05,  3.03s/it]                                                  {'loss': 18.7079, 'grad_norm': 615.638916015625, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:03:05,  3.03s/it]  0%|          | 4/6000 [00:12<4:51:21,  2.92s/it]                                                  {'loss': 19.6626, 'grad_norm': 593.8137817382812, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:51:21,  2.92s/it]  0%|          | 5/6000 [00:14<4:41:47,  2.82s/it]                                                  {'loss': 20.628, 'grad_norm': 517.4874877929688, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:41:47,  2.82s/it]  0%|          | 6/6000 [00:17<4:39:24,  2.80s/it]                                                  {'loss': 20.3553, 'grad_norm': 553.1773681640625, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:39:24,  2.80s/it]  0%|          | 7/6000 [00:20<4:36:00,  2.76s/it]                                                  {'loss': 20.469, 'grad_norm': 587.16748046875, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:36:00,  2.76s/it]  0%|          | 8/6000 [00:22<4:31:35,  2.72s/it]                                                  {'loss': 19.6528, 'grad_norm': 550.2249145507812, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:31:35,  2.72s/it]  0%|          | 9/6000 [00:25<4:33:34,  2.74s/it]                                                  {'loss': 14.5111, 'grad_norm': 507.27960205078125, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:33:34,  2.74s/it]  0%|          | 10/6000 [00:28<4:31:08,  2.72s/it]                                                   {'loss': 18.8414, 'grad_norm': 521.7781372070312, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:31:08,  2.72s/it]  0%|          | 11/6000 [00:31<4:41:21,  2.82s/it]                                                   {'loss': 23.077, 'grad_norm': 550.3972778320312, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:41:21,  2.82s/it]  0%|          | 12/6000 [00:34<4:43:48,  2.84s/it]                                                   {'loss': 19.5204, 'grad_norm': 488.4038391113281, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:43:48,  2.84s/it]  0%|          | 13/6000 [00:37<4:42:58,  2.84s/it]                                                   {'loss': 19.2553, 'grad_norm': 482.2967529296875, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:42:58,  2.84s/it]  0%|          | 14/6000 [00:40<4:43:37,  2.84s/it]                                                   {'loss': 21.7685, 'grad_norm': 498.8329162597656, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:40<4:43:37,  2.84s/it]  0%|          | 15/6000 [00:42<4:38:39,  2.79s/it]                                                   {'loss': 17.6452, 'grad_norm': 454.6690368652344, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:38:39,  2.79s/it]  0%|          | 16/6000 [00:45<4:34:31,  2.75s/it]                                                   {'loss': 18.1624, 'grad_norm': 461.73138427734375, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:34:31,  2.75s/it]  0%|          | 17/6000 [00:48<4:33:34,  2.74s/it]                                                   {'loss': 18.1484, 'grad_norm': 439.1653747558594, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:48<4:33:34,  2.74s/it]  0%|          | 18/6000 [00:50<4:34:33,  2.75s/it]                                                   {'loss': 15.6721, 'grad_norm': 358.4112548828125, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:34:33,  2.75s/it]  0%|          | 19/6000 [00:53<4:32:34,  2.73s/it]                                                   {'loss': 17.9799, 'grad_norm': 414.6470947265625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:32:34,  2.73s/it]  0%|          | 20/6000 [00:56<4:31:55,  2.73s/it]                                                   {'loss': 18.8793, 'grad_norm': 458.55938720703125, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:56<4:31:55,  2.73s/it]  0%|          | 21/6000 [00:59<4:36:16,  2.77s/it]                                                   {'loss': 15.6829, 'grad_norm': 395.33197021484375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:59<4:36:16,  2.77s/it]  0%|          | 22/6000 [01:01<4:37:00,  2.78s/it]                                                   {'loss': 15.7332, 'grad_norm': 353.8860168457031, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:37:00,  2.78s/it]  0%|          | 23/6000 [01:04<4:35:00,  2.76s/it]                                                   {'loss': 14.4147, 'grad_norm': 365.77154541015625, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:35:00,  2.76s/it]  0%|          | 24/6000 [01:07<4:37:05,  2.78s/it]                                                   {'loss': 11.6938, 'grad_norm': 350.73309326171875, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:37:05,  2.78s/it]  0%|          | 25/6000 [01:10<4:35:34,  2.77s/it]                                                   {'loss': 14.1325, 'grad_norm': 425.4868469238281, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:10<4:35:34,  2.77s/it]  0%|          | 26/6000 [01:13<4:36:42,  2.78s/it]                                                   {'loss': 15.2704, 'grad_norm': 527.10205078125, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:13<4:36:42,  2.78s/it]  0%|          | 27/6000 [01:15<4:36:12,  2.77s/it]                                                   {'loss': 13.7894, 'grad_norm': 598.6448364257812, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:36:12,  2.77s/it]  0%|          | 28/6000 [01:19<5:01:28,  3.03s/it]                                                   {'loss': 12.7705, 'grad_norm': 425.42169189453125, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:19<5:01:28,  3.03s/it]  0%|          | 29/6000 [01:22<4:50:46,  2.92s/it]                                                   {'loss': 12.1114, 'grad_norm': 438.9566955566406, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:22<4:50:46,  2.92s/it]  0%|          | 30/6000 [01:24<4:46:08,  2.88s/it]                                                   {'loss': 11.9297, 'grad_norm': 512.8031616210938, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:46:08,  2.88s/it]  1%|          | 31/6000 [01:27<4:41:25,  2.83s/it]                                                   {'loss': 10.7239, 'grad_norm': 327.97906494140625, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:27<4:41:25,  2.83s/it]  1%|          | 32/6000 [01:30<4:37:51,  2.79s/it]                                                   {'loss': 10.4733, 'grad_norm': 439.39251708984375, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:30<4:37:51,  2.79s/it]  1%|          | 33/6000 [01:33<4:37:24,  2.79s/it]                                                   {'loss': 10.9208, 'grad_norm': 392.0653076171875, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:33<4:37:24,  2.79s/it]  1%|          | 34/6000 [01:35<4:35:44,  2.77s/it]                                                   {'loss': 9.8496, 'grad_norm': 275.48895263671875, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:35:44,  2.77s/it]  1%|          | 35/6000 [01:38<4:36:05,  2.78s/it]                                                   {'loss': 8.7471, 'grad_norm': 293.2976989746094, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:38<4:36:05,  2.78s/it]  1%|          | 36/6000 [01:41<4:32:59,  2.75s/it]                                                   {'loss': 8.4474, 'grad_norm': 422.3579406738281, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:41<4:32:59,  2.75s/it]  1%|          | 37/6000 [01:44<4:32:33,  2.74s/it]                                                   {'loss': 7.8736, 'grad_norm': 443.1771545410156, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:44<4:32:33,  2.74s/it]  1%|          | 38/6000 [01:46<4:30:10,  2.72s/it]                                                   {'loss': 7.4847, 'grad_norm': 317.8481140136719, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:46<4:30:10,  2.72s/it]  1%|          | 39/6000 [01:49<4:29:55,  2.72s/it]                                                   {'loss': 7.1411, 'grad_norm': 282.6275634765625, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:49<4:29:55,  2.72s/it]  1%|          | 40/6000 [01:52<4:30:24,  2.72s/it]                                                   {'loss': 7.0584, 'grad_norm': 295.7922058105469, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:52<4:30:24,  2.72s/it]  1%|          | 41/6000 [01:54<4:30:40,  2.73s/it]                                                   {'loss': 8.088, 'grad_norm': 393.04351806640625, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:54<4:30:40,  2.73s/it]  1%|          | 42/6000 [01:57<4:31:49,  2.74s/it]                                                   {'loss': 7.465, 'grad_norm': 249.83004760742188, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:57<4:31:49,  2.74s/it]  1%|          | 43/6000 [02:01<5:03:07,  3.05s/it]                                                   {'loss': 6.3142, 'grad_norm': 207.72215270996094, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:01<5:03:07,  3.05s/it]  1%|          | 44/6000 [02:04<5:08:16,  3.11s/it]                                                   {'loss': 6.1057, 'grad_norm': 221.65045166015625, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:04<5:08:16,  3.11s/it]  1%|          | 45/6000 [02:07<4:58:01,  3.00s/it]                                                   {'loss': 6.9282, 'grad_norm': 292.0779724121094, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:07<4:58:01,  3.00s/it]  1%|          | 46/6000 [02:10<4:53:19,  2.96s/it]                                                   {'loss': 5.9157, 'grad_norm': 247.25990295410156, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:10<4:53:19,  2.96s/it]  1%|          | 47/6000 [02:12<4:45:57,  2.88s/it]                                                   {'loss': 5.7465, 'grad_norm': 327.3296203613281, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:12<4:45:57,  2.88s/it]  1%|          | 48/6000 [02:15<4:43:51,  2.86s/it]                                                   {'loss': 5.6335, 'grad_norm': 150.97674560546875, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:15<4:43:51,  2.86s/it]  1%|          | 49/6000 [02:18<4:37:43,  2.80s/it]                                                   {'loss': 5.6388, 'grad_norm': 451.6947937011719, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:18<4:37:43,  2.80s/it]  1%|          | 50/6000 [02:21<4:40:39,  2.83s/it]                                                   {'loss': 4.4599, 'grad_norm': 188.1403045654297, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:21<4:40:39,  2.83s/it][2025-11-03 15:52:47,686] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-03 15:52:47,700] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:52:48,376] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:26<5:41:58,  3.45s/it]                                                   {'loss': 5.2826, 'grad_norm': 163.04367065429688, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:26<5:41:58,  3.45s/it]  1%|          | 52/6000 [02:28<5:19:18,  3.22s/it]                                                   {'loss': 4.6056, 'grad_norm': 328.1529846191406, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:28<5:19:18,  3.22s/it]  1%|          | 53/6000 [02:31<5:06:04,  3.09s/it]                                                   {'loss': 5.9983, 'grad_norm': 218.89215087890625, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:31<5:06:04,  3.09s/it]  1%|          | 54/6000 [02:34<4:53:02,  2.96s/it]                                                   {'loss': 4.5754, 'grad_norm': 133.27833557128906, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:34<4:53:02,  2.96s/it]  1%|          | 55/6000 [02:37<4:45:23,  2.88s/it]                                                   {'loss': 4.7293, 'grad_norm': 137.34564208984375, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:37<4:45:23,  2.88s/it]  1%|          | 56/6000 [02:39<4:41:53,  2.85s/it]                                                   {'loss': 3.7119, 'grad_norm': 86.82305908203125, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:39<4:41:53,  2.85s/it]  1%|          | 57/6000 [02:42<4:38:54,  2.82s/it]                                                   {'loss': 4.0627, 'grad_norm': 116.65727996826172, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:42<4:38:54,  2.82s/it]  1%|          | 58/6000 [02:45<4:35:58,  2.79s/it]                                                   {'loss': 3.4615, 'grad_norm': 77.78328704833984, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:45<4:35:58,  2.79s/it]  1%|          | 59/6000 [02:47<4:31:48,  2.75s/it]                                                   {'loss': 4.6209, 'grad_norm': 146.06591796875, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [02:47<4:31:48,  2.75s/it]  1%|          | 60/6000 [02:50<4:30:17,  2.73s/it]                                                   {'loss': 4.0312, 'grad_norm': 92.2497787475586, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:50<4:30:17,  2.73s/it]  1%|          | 61/6000 [02:53<4:28:33,  2.71s/it]                                                   {'loss': 3.1965, 'grad_norm': 60.46543502807617, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:53<4:28:33,  2.71s/it]  1%|          | 62/6000 [02:56<4:28:13,  2.71s/it]                                                   {'loss': 3.7965, 'grad_norm': 106.48786926269531, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:56<4:28:13,  2.71s/it]  1%|          | 63/6000 [02:58<4:27:30,  2.70s/it]                                                   {'loss': 3.4652, 'grad_norm': 100.66732788085938, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:58<4:27:30,  2.70s/it]  1%|          | 64/6000 [03:01<4:25:47,  2.69s/it]                                                   {'loss': 3.1926, 'grad_norm': 68.51116943359375, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:01<4:25:47,  2.69s/it]  1%|          | 65/6000 [03:04<4:25:06,  2.68s/it]                                                   {'loss': 3.8265, 'grad_norm': 125.62710571289062, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:04<4:25:06,  2.68s/it]  1%|          | 66/6000 [03:07<4:39:02,  2.82s/it]                                                   {'loss': 3.4174, 'grad_norm': 89.40995025634766, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:07<4:39:02,  2.82s/it]  1%|          | 67/6000 [03:09<4:35:16,  2.78s/it]                                                   {'loss': 3.5351, 'grad_norm': 210.40304565429688, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:09<4:35:16,  2.78s/it]  1%|          | 68/6000 [03:12<4:31:39,  2.75s/it]                                                   {'loss': 3.1277, 'grad_norm': 76.05402374267578, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:12<4:31:39,  2.75s/it]  1%|          | 69/6000 [03:15<4:30:36,  2.74s/it]                                                   {'loss': 3.2955, 'grad_norm': 183.8122100830078, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:15<4:30:36,  2.74s/it]  1%|          | 70/6000 [03:18<4:32:50,  2.76s/it]                                                   {'loss': 3.0281, 'grad_norm': 71.43955993652344, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:18<4:32:50,  2.76s/it]  1%|          | 71/6000 [03:20<4:31:13,  2.74s/it]                                                   {'loss': 3.1059, 'grad_norm': 58.81960678100586, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:20<4:31:13,  2.74s/it]  1%|          | 72/6000 [03:23<4:42:25,  2.86s/it]                                                   {'loss': 3.1595, 'grad_norm': 85.01287841796875, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:23<4:42:25,  2.86s/it]  1%|          | 73/6000 [03:26<4:47:06,  2.91s/it]                                                   {'loss': 2.9197, 'grad_norm': 30.773452758789062, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:26<4:47:06,  2.91s/it]  1%|          | 74/6000 [03:29<4:39:05,  2.83s/it]                                                   {'loss': 3.1002, 'grad_norm': 49.9642448425293, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:29<4:39:05,  2.83s/it]  1%|â–         | 75/6000 [03:32<4:34:28,  2.78s/it]                                                   {'loss': 2.9197, 'grad_norm': 47.01898956298828, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:32<4:34:28,  2.78s/it]  1%|â–         | 76/6000 [03:35<4:38:19,  2.82s/it]                                                   {'loss': 3.1555, 'grad_norm': 54.723480224609375, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:35<4:38:19,  2.82s/it]  1%|â–         | 77/6000 [03:37<4:38:18,  2.82s/it]                                                   {'loss': 3.3954, 'grad_norm': 70.11213684082031, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:37<4:38:18,  2.82s/it]  1%|â–         | 78/6000 [03:41<4:47:03,  2.91s/it]                                                   {'loss': 3.3665, 'grad_norm': 99.00674438476562, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:41<4:47:03,  2.91s/it]  1%|â–         | 79/6000 [03:43<4:41:12,  2.85s/it]                                                   {'loss': 2.9789, 'grad_norm': 49.839324951171875, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:43<4:41:12,  2.85s/it]  1%|â–         | 80/6000 [03:46<4:45:49,  2.90s/it]                                                   {'loss': 3.0798, 'grad_norm': 64.32300567626953, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:46<4:45:49,  2.90s/it]  1%|â–         | 81/6000 [03:49<4:39:28,  2.83s/it]                                                   {'loss': 2.9958, 'grad_norm': 70.61902618408203, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:49<4:39:28,  2.83s/it]  1%|â–         | 82/6000 [03:52<4:36:42,  2.81s/it]                                                   {'loss': 2.9316, 'grad_norm': 36.61904525756836, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:52<4:36:42,  2.81s/it]  1%|â–         | 83/6000 [03:54<4:34:05,  2.78s/it]                                                   {'loss': 3.2743, 'grad_norm': 101.22526550292969, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:54<4:34:05,  2.78s/it]  1%|â–         | 84/6000 [03:57<4:35:07,  2.79s/it]                                                   {'loss': 3.1355, 'grad_norm': 141.07565307617188, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:57<4:35:07,  2.79s/it]  1%|â–         | 85/6000 [04:00<4:43:40,  2.88s/it]                                                   {'loss': 3.0331, 'grad_norm': 40.91560363769531, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:00<4:43:40,  2.88s/it]  1%|â–         | 86/6000 [04:03<4:39:30,  2.84s/it]                                                   {'loss': 3.1723, 'grad_norm': 61.1306037902832, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:03<4:39:30,  2.84s/it]  1%|â–         | 87/6000 [04:06<4:35:39,  2.80s/it]                                                   {'loss': 3.1165, 'grad_norm': 82.92560577392578, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:06<4:35:39,  2.80s/it]  1%|â–         | 88/6000 [04:08<4:32:09,  2.76s/it]                                                   {'loss': 3.3191, 'grad_norm': 68.85009765625, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:08<4:32:09,  2.76s/it]  1%|â–         | 89/6000 [04:11<4:29:13,  2.73s/it]                                                   {'loss': 2.9867, 'grad_norm': 45.00218200683594, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:11<4:29:13,  2.73s/it]  2%|â–         | 90/6000 [04:14<4:32:02,  2.76s/it]                                                   {'loss': 2.9116, 'grad_norm': 33.701820373535156, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:14<4:32:02,  2.76s/it]  2%|â–         | 91/6000 [04:17<4:32:33,  2.77s/it]                                                   {'loss': 2.8672, 'grad_norm': 30.365724563598633, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:17<4:32:33,  2.77s/it]  2%|â–         | 92/6000 [04:20<4:42:43,  2.87s/it]                                                   {'loss': 3.0774, 'grad_norm': 122.29731750488281, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:20<4:42:43,  2.87s/it]  2%|â–         | 93/6000 [04:23<4:42:42,  2.87s/it]                                                   {'loss': 2.8862, 'grad_norm': 41.398712158203125, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:23<4:42:42,  2.87s/it]  2%|â–         | 94/6000 [04:25<4:37:50,  2.82s/it]                                                   {'loss': 2.9338, 'grad_norm': 32.72338104248047, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:25<4:37:50,  2.82s/it]  2%|â–         | 95/6000 [04:28<4:35:45,  2.80s/it]                                                   {'loss': 2.9205, 'grad_norm': 29.06212043762207, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:28<4:35:45,  2.80s/it]  2%|â–         | 96/6000 [04:31<4:32:28,  2.77s/it]                                                   {'loss': 2.9189, 'grad_norm': 23.629695892333984, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:31<4:32:28,  2.77s/it]  2%|â–         | 97/6000 [04:34<4:29:47,  2.74s/it]                                                   {'loss': 2.9557, 'grad_norm': 38.39376449584961, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:34<4:29:47,  2.74s/it]  2%|â–         | 98/6000 [04:36<4:29:00,  2.73s/it]                                                   {'loss': 2.8559, 'grad_norm': 29.370925903320312, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:36<4:29:00,  2.73s/it]  2%|â–         | 99/6000 [04:39<4:28:55,  2.73s/it]                                                   {'loss': 2.8197, 'grad_norm': 22.763813018798828, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:39<4:28:55,  2.73s/it]  2%|â–         | 100/6000 [04:42<4:26:30,  2.71s/it]                                                    {'loss': 2.8942, 'grad_norm': 24.183441162109375, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:42<4:26:30,  2.71s/it][2025-11-03 15:55:08,478] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-03 15:55:08,489] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:55:09,102] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:47<5:58:13,  3.64s/it]                                                    {'loss': 3.1263, 'grad_norm': 181.69891357421875, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:47<5:58:13,  3.64s/it]  2%|â–         | 102/6000 [04:50<5:30:53,  3.37s/it]                                                    {'loss': 2.8994, 'grad_norm': 34.09295654296875, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:50<5:30:53,  3.37s/it]  2%|â–         | 103/6000 [04:53<5:13:33,  3.19s/it]                                                    {'loss': 2.8538, 'grad_norm': 27.109922409057617, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:53<5:13:33,  3.19s/it]  2%|â–         | 104/6000 [04:56<5:07:49,  3.13s/it]                                                    {'loss': 2.8506, 'grad_norm': 19.649805068969727, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:56<5:07:49,  3.13s/it]  2%|â–         | 105/6000 [04:59<4:58:12,  3.04s/it]                                                    {'loss': 2.9318, 'grad_norm': 75.60235595703125, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:59<4:58:12,  3.04s/it]  2%|â–         | 106/6000 [05:02<4:54:29,  3.00s/it]                                                    {'loss': 2.8338, 'grad_norm': 18.80006217956543, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:02<4:54:29,  3.00s/it]  2%|â–         | 107/6000 [05:04<4:45:08,  2.90s/it]                                                    {'loss': 2.8851, 'grad_norm': 41.367340087890625, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:04<4:45:08,  2.90s/it]  2%|â–         | 108/6000 [05:07<4:43:28,  2.89s/it]                                                    {'loss': 2.8313, 'grad_norm': 34.2530517578125, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:07<4:43:28,  2.89s/it]  2%|â–         | 109/6000 [05:10<4:52:17,  2.98s/it]                                                    {'loss': 2.8775, 'grad_norm': 25.32151985168457, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:10<4:52:17,  2.98s/it]  2%|â–         | 110/6000 [05:13<4:45:23,  2.91s/it]                                                    {'loss': 2.9185, 'grad_norm': 21.72759246826172, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:13<4:45:23,  2.91s/it]  2%|â–         | 111/6000 [05:16<4:44:26,  2.90s/it]                                                    {'loss': 2.8233, 'grad_norm': 20.619741439819336, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:16<4:44:26,  2.90s/it]  2%|â–         | 112/6000 [05:19<4:56:28,  3.02s/it]                                                    {'loss': 3.0908, 'grad_norm': 55.00181579589844, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:19<4:56:28,  3.02s/it]  2%|â–         | 113/6000 [05:22<4:47:23,  2.93s/it]                                                    {'loss': 2.8524, 'grad_norm': 29.819242477416992, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:22<4:47:23,  2.93s/it]  2%|â–         | 114/6000 [05:25<4:43:00,  2.88s/it]                                                    {'loss': 2.894, 'grad_norm': 34.345367431640625, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:25<4:43:00,  2.88s/it]  2%|â–         | 115/6000 [05:28<4:39:46,  2.85s/it]                                                    {'loss': 2.8911, 'grad_norm': 22.758634567260742, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:28<4:39:46,  2.85s/it]  2%|â–         | 116/6000 [05:30<4:37:06,  2.83s/it]                                                    {'loss': 2.9069, 'grad_norm': 35.08347702026367, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:30<4:37:06,  2.83s/it]  2%|â–         | 117/6000 [05:33<4:35:30,  2.81s/it]                                                    {'loss': 3.2207, 'grad_norm': 61.87447738647461, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:33<4:35:30,  2.81s/it]  2%|â–         | 118/6000 [05:36<4:50:14,  2.96s/it]                                                    {'loss': 2.8442, 'grad_norm': 22.240333557128906, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:36<4:50:14,  2.96s/it]  2%|â–         | 119/6000 [05:39<4:43:58,  2.90s/it]                                                    {'loss': 2.8152, 'grad_norm': 13.293835639953613, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:39<4:43:58,  2.90s/it]  2%|â–         | 120/6000 [05:42<4:40:15,  2.86s/it]                                                    {'loss': 2.8841, 'grad_norm': 44.10771560668945, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:42<4:40:15,  2.86s/it]  2%|â–         | 121/6000 [05:45<4:38:42,  2.84s/it]                                                    {'loss': 2.8412, 'grad_norm': 24.01048469543457, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:45<4:38:42,  2.84s/it]  2%|â–         | 122/6000 [05:48<4:39:01,  2.85s/it]                                                    {'loss': 2.8338, 'grad_norm': 22.17292022705078, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:48<4:39:01,  2.85s/it]  2%|â–         | 123/6000 [05:50<4:35:23,  2.81s/it]                                                    {'loss': 3.0015, 'grad_norm': 35.46199417114258, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:50<4:35:23,  2.81s/it]  2%|â–         | 124/6000 [05:53<4:30:40,  2.76s/it]                                                    {'loss': 2.9045, 'grad_norm': 23.755268096923828, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:53<4:30:40,  2.76s/it]  2%|â–         | 125/6000 [05:56<4:38:12,  2.84s/it]                                                    {'loss': 2.9761, 'grad_norm': 15.051709175109863, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:56<4:38:12,  2.84s/it]  2%|â–         | 126/6000 [05:59<4:37:05,  2.83s/it]                                                    {'loss': 2.7926, 'grad_norm': 15.24123477935791, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:59<4:37:05,  2.83s/it]  2%|â–         | 127/6000 [06:02<4:39:23,  2.85s/it]                                                    {'loss': 2.9163, 'grad_norm': 38.74528884887695, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:02<4:39:23,  2.85s/it]  2%|â–         | 128/6000 [06:04<4:35:19,  2.81s/it]                                                    {'loss': 2.8396, 'grad_norm': 21.022296905517578, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:04<4:35:19,  2.81s/it]  2%|â–         | 129/6000 [06:07<4:33:33,  2.80s/it]                                                    {'loss': 3.0403, 'grad_norm': 28.017005920410156, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:07<4:33:33,  2.80s/it]  2%|â–         | 130/6000 [06:10<4:31:50,  2.78s/it]                                                    {'loss': 2.8913, 'grad_norm': 29.374433517456055, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:10<4:31:50,  2.78s/it]  2%|â–         | 131/6000 [06:13<4:31:30,  2.78s/it]                                                    {'loss': 2.8115, 'grad_norm': 24.13486099243164, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:13<4:31:30,  2.78s/it]  2%|â–         | 132/6000 [06:15<4:28:45,  2.75s/it]                                                    {'loss': 2.841, 'grad_norm': 34.892642974853516, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:15<4:28:45,  2.75s/it]  2%|â–         | 133/6000 [06:18<4:29:02,  2.75s/it]                                                    {'loss': 2.8505, 'grad_norm': 20.30352210998535, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:18<4:29:02,  2.75s/it]  2%|â–         | 134/6000 [06:21<4:34:03,  2.80s/it]                                                    {'loss': 2.861, 'grad_norm': 27.772079467773438, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:21<4:34:03,  2.80s/it]  2%|â–         | 135/6000 [06:24<4:31:12,  2.77s/it]                                                    {'loss': 2.8485, 'grad_norm': 16.856287002563477, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:24<4:31:12,  2.77s/it]  2%|â–         | 136/6000 [06:27<4:29:55,  2.76s/it]                                                    {'loss': 2.9002, 'grad_norm': 24.476539611816406, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:27<4:29:55,  2.76s/it]  2%|â–         | 137/6000 [06:30<4:44:15,  2.91s/it]                                                    {'loss': 2.8532, 'grad_norm': 24.40777015686035, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:30<4:44:15,  2.91s/it]  2%|â–         | 138/6000 [06:32<4:37:52,  2.84s/it]                                                    {'loss': 2.8014, 'grad_norm': 12.675830841064453, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:32<4:37:52,  2.84s/it]  2%|â–         | 139/6000 [06:35<4:39:15,  2.86s/it]                                                    {'loss': 2.8678, 'grad_norm': 12.696648597717285, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:35<4:39:15,  2.86s/it]  2%|â–         | 140/6000 [06:39<4:49:26,  2.96s/it]                                                    {'loss': 2.8154, 'grad_norm': 16.86056900024414, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:39<4:49:26,  2.96s/it]  2%|â–         | 141/6000 [06:41<4:44:37,  2.91s/it]                                                    {'loss': 2.8407, 'grad_norm': 23.422103881835938, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:41<4:44:37,  2.91s/it]  2%|â–         | 142/6000 [06:44<4:37:38,  2.84s/it]                                                    {'loss': 2.8108, 'grad_norm': 17.090757369995117, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:44<4:37:38,  2.84s/it]  2%|â–         | 143/6000 [06:47<4:33:22,  2.80s/it]                                                    {'loss': 2.8798, 'grad_norm': 27.787593841552734, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:47<4:33:22,  2.80s/it]  2%|â–         | 144/6000 [06:50<4:32:50,  2.80s/it]                                                    {'loss': 2.903, 'grad_norm': 32.30344009399414, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:50<4:32:50,  2.80s/it]  2%|â–         | 145/6000 [06:52<4:29:46,  2.76s/it]                                                    {'loss': 2.8728, 'grad_norm': 20.490909576416016, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:52<4:29:46,  2.76s/it]  2%|â–         | 146/6000 [06:55<4:31:06,  2.78s/it]                                                    {'loss': 2.8762, 'grad_norm': 20.503427505493164, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:55<4:31:06,  2.78s/it]  2%|â–         | 147/6000 [06:58<4:29:18,  2.76s/it]                                                    {'loss': 2.8196, 'grad_norm': 14.281453132629395, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:58<4:29:18,  2.76s/it]  2%|â–         | 148/6000 [07:01<4:29:01,  2.76s/it]                                                    {'loss': 2.8312, 'grad_norm': 15.040863990783691, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [07:01<4:29:01,  2.76s/it]  2%|â–         | 149/6000 [07:03<4:28:13,  2.75s/it]                                                    {'loss': 2.8386, 'grad_norm': 20.147071838378906, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:03<4:28:13,  2.75s/it]  2%|â–Ž         | 150/6000 [07:06<4:27:03,  2.74s/it]                                                    {'loss': 2.8519, 'grad_norm': 14.155000686645508, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:06<4:27:03,  2.74s/it][2025-11-03 15:57:32,802] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-03 15:57:32,818] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:57:33,461] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:11<5:34:52,  3.44s/it]                                                    {'loss': 2.8097, 'grad_norm': 12.571093559265137, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:11<5:34:52,  3.44s/it]  3%|â–Ž         | 152/6000 [07:14<5:14:00,  3.22s/it]                                                    {'loss': 2.8861, 'grad_norm': 35.57714080810547, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:14<5:14:00,  3.22s/it]  3%|â–Ž         | 153/6000 [07:16<4:58:30,  3.06s/it]                                                    {'loss': 2.818, 'grad_norm': 16.86784553527832, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:16<4:58:30,  3.06s/it]  3%|â–Ž         | 154/6000 [07:19<4:52:41,  3.00s/it]                                                    {'loss': 2.8413, 'grad_norm': 21.12421989440918, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:19<4:52:41,  3.00s/it]  3%|â–Ž         | 155/6000 [07:22<4:47:41,  2.95s/it]                                                    {'loss': 2.8366, 'grad_norm': 14.035982131958008, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:22<4:47:41,  2.95s/it]  3%|â–Ž         | 156/6000 [07:25<4:43:32,  2.91s/it]                                                    {'loss': 2.8808, 'grad_norm': 13.60049819946289, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:25<4:43:32,  2.91s/it]  3%|â–Ž         | 157/6000 [07:28<4:52:29,  3.00s/it]                                                    {'loss': 2.8064, 'grad_norm': 13.471053123474121, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:28<4:52:29,  3.00s/it]  3%|â–Ž         | 158/6000 [07:31<4:44:13,  2.92s/it]                                                    {'loss': 2.819, 'grad_norm': 12.33068561553955, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:31<4:44:13,  2.92s/it]  3%|â–Ž         | 159/6000 [07:34<4:38:47,  2.86s/it]                                                    {'loss': 2.7986, 'grad_norm': 9.813230514526367, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:34<4:38:47,  2.86s/it]  3%|â–Ž         | 160/6000 [07:37<4:52:28,  3.00s/it]                                                    {'loss': 2.8334, 'grad_norm': 18.598011016845703, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:37<4:52:28,  3.00s/it]  3%|â–Ž         | 161/6000 [07:40<4:49:09,  2.97s/it]                                                    {'loss': 2.7797, 'grad_norm': 12.081500053405762, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:40<4:49:09,  2.97s/it]  3%|â–Ž         | 162/6000 [07:43<4:41:58,  2.90s/it]                                                    {'loss': 2.7947, 'grad_norm': 12.411118507385254, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:43<4:41:58,  2.90s/it]  3%|â–Ž         | 163/6000 [07:46<4:49:15,  2.97s/it]                                                    {'loss': 2.7896, 'grad_norm': 13.675361633300781, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:46<4:49:15,  2.97s/it]  3%|â–Ž         | 164/6000 [07:48<4:41:56,  2.90s/it]                                                    {'loss': 2.9349, 'grad_norm': 11.952657699584961, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:48<4:41:56,  2.90s/it]  3%|â–Ž         | 165/6000 [07:51<4:43:01,  2.91s/it]                                                    {'loss': 2.8138, 'grad_norm': 15.15072250366211, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:51<4:43:01,  2.91s/it]  3%|â–Ž         | 166/6000 [07:54<4:36:35,  2.84s/it]                                                    {'loss': 2.8431, 'grad_norm': 20.0369930267334, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:54<4:36:35,  2.84s/it]  3%|â–Ž         | 167/6000 [07:57<4:33:59,  2.82s/it]                                                    {'loss': 2.8389, 'grad_norm': 14.532055854797363, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:57<4:33:59,  2.82s/it]  3%|â–Ž         | 168/6000 [08:00<4:33:13,  2.81s/it]                                                    {'loss': 2.9082, 'grad_norm': 20.94728660583496, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [08:00<4:33:13,  2.81s/it]  3%|â–Ž         | 169/6000 [08:03<4:43:50,  2.92s/it]                                                    {'loss': 2.846, 'grad_norm': 11.7189359664917, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:03<4:43:50,  2.92s/it]  3%|â–Ž         | 170/6000 [08:06<4:37:42,  2.86s/it]                                                    {'loss': 2.8064, 'grad_norm': 12.026209831237793, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:06<4:37:42,  2.86s/it]  3%|â–Ž         | 171/6000 [08:08<4:32:52,  2.81s/it]                                                    {'loss': 2.7984, 'grad_norm': 11.788580894470215, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:08<4:32:52,  2.81s/it]  3%|â–Ž         | 172/6000 [08:11<4:33:49,  2.82s/it]                                                    {'loss': 2.8181, 'grad_norm': 18.91037368774414, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:11<4:33:49,  2.82s/it]  3%|â–Ž         | 173/6000 [08:14<4:32:35,  2.81s/it]                                                    {'loss': 2.8164, 'grad_norm': 16.20733070373535, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:14<4:32:35,  2.81s/it]  3%|â–Ž         | 174/6000 [08:17<4:52:16,  3.01s/it]                                                    {'loss': 2.8326, 'grad_norm': 15.736054420471191, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:17<4:52:16,  3.01s/it]  3%|â–Ž         | 175/6000 [08:20<4:48:23,  2.97s/it]                                                    {'loss': 2.8235, 'grad_norm': 13.871894836425781, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:20<4:48:23,  2.97s/it]  3%|â–Ž         | 176/6000 [08:23<4:51:00,  3.00s/it]                                                    {'loss': 2.8335, 'grad_norm': 20.530851364135742, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:23<4:51:00,  3.00s/it]  3%|â–Ž         | 177/6000 [08:26<4:43:17,  2.92s/it]                                                    {'loss': 2.8327, 'grad_norm': 15.218785285949707, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:26<4:43:17,  2.92s/it]  3%|â–Ž         | 178/6000 [08:29<4:39:07,  2.88s/it]                                                    {'loss': 2.8146, 'grad_norm': 15.00706958770752, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:29<4:39:07,  2.88s/it]  3%|â–Ž         | 179/6000 [08:31<4:34:30,  2.83s/it]                                                    {'loss': 2.8568, 'grad_norm': 19.598482131958008, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:31<4:34:30,  2.83s/it]  3%|â–Ž         | 180/6000 [08:34<4:36:02,  2.85s/it]                                                    {'loss': 2.861, 'grad_norm': 22.269201278686523, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:34<4:36:02,  2.85s/it]  3%|â–Ž         | 181/6000 [08:37<4:32:55,  2.81s/it]                                                    {'loss': 2.8657, 'grad_norm': 17.77730369567871, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:37<4:32:55,  2.81s/it]  3%|â–Ž         | 182/6000 [08:40<4:31:04,  2.80s/it]                                                    {'loss': 2.8719, 'grad_norm': 20.56247329711914, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:40<4:31:04,  2.80s/it]  3%|â–Ž         | 183/6000 [08:43<4:29:56,  2.78s/it]                                                    {'loss': 2.7885, 'grad_norm': 9.667661666870117, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:43<4:29:56,  2.78s/it]  3%|â–Ž         | 184/6000 [08:45<4:28:57,  2.77s/it]                                                    {'loss': 2.8174, 'grad_norm': 24.024850845336914, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:45<4:28:57,  2.77s/it]  3%|â–Ž         | 185/6000 [08:48<4:28:40,  2.77s/it]                                                    {'loss': 2.8485, 'grad_norm': 20.376741409301758, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:48<4:28:40,  2.77s/it]  3%|â–Ž         | 186/6000 [08:51<4:28:38,  2.77s/it]                                                    {'loss': 2.8371, 'grad_norm': 15.500617027282715, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:51<4:28:38,  2.77s/it]  3%|â–Ž         | 187/6000 [08:54<4:26:34,  2.75s/it]                                                    {'loss': 2.8001, 'grad_norm': 12.734292030334473, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:54<4:26:34,  2.75s/it]  3%|â–Ž         | 188/6000 [08:56<4:26:03,  2.75s/it]                                                    {'loss': 2.8305, 'grad_norm': 17.667871475219727, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:56<4:26:03,  2.75s/it]  3%|â–Ž         | 189/6000 [08:59<4:27:30,  2.76s/it]                                                    {'loss': 2.8147, 'grad_norm': 13.218594551086426, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:59<4:27:30,  2.76s/it]  3%|â–Ž         | 190/6000 [09:02<4:25:13,  2.74s/it]                                                    {'loss': 2.8175, 'grad_norm': 15.404556274414062, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [09:02<4:25:13,  2.74s/it]  3%|â–Ž         | 191/6000 [09:05<4:26:40,  2.75s/it]                                                    {'loss': 2.8425, 'grad_norm': 18.293590545654297, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:05<4:26:40,  2.75s/it]  3%|â–Ž         | 192/6000 [09:08<4:45:25,  2.95s/it]                                                    {'loss': 2.9198, 'grad_norm': 13.2605562210083, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:08<4:45:25,  2.95s/it]  3%|â–Ž         | 193/6000 [09:11<4:50:58,  3.01s/it]                                                    {'loss': 2.7588, 'grad_norm': 9.666754722595215, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:11<4:50:58,  3.01s/it]  3%|â–Ž         | 194/6000 [09:14<4:40:58,  2.90s/it]                                                    {'loss': 2.8231, 'grad_norm': 12.739188194274902, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:14<4:40:58,  2.90s/it]  3%|â–Ž         | 195/6000 [09:17<4:36:59,  2.86s/it]                                                    {'loss': 2.8416, 'grad_norm': 11.523448944091797, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:17<4:36:59,  2.86s/it]  3%|â–Ž         | 196/6000 [09:19<4:33:24,  2.83s/it]                                                    {'loss': 2.8116, 'grad_norm': 11.563443183898926, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:19<4:33:24,  2.83s/it]  3%|â–Ž         | 197/6000 [09:22<4:36:47,  2.86s/it]                                                    {'loss': 2.8446, 'grad_norm': 13.157477378845215, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:22<4:36:47,  2.86s/it]  3%|â–Ž         | 198/6000 [09:25<4:37:49,  2.87s/it]                                                    {'loss': 2.7834, 'grad_norm': 9.907590866088867, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:25<4:37:49,  2.87s/it]  3%|â–Ž         | 199/6000 [09:28<4:33:27,  2.83s/it]                                                    {'loss': 2.8232, 'grad_norm': 14.300616264343262, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:28<4:33:27,  2.83s/it]  3%|â–Ž         | 200/6000 [09:31<4:43:28,  2.93s/it]                                                    {'loss': 2.8129, 'grad_norm': 13.661449432373047, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:31<4:43:28,  2.93s/it][2025-11-03 15:59:57,935] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-11-03 15:59:57,951] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:59:58,587] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:36<5:42:20,  3.54s/it]                                                    {'loss': 2.8134, 'grad_norm': 10.619412422180176, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:36<5:42:20,  3.54s/it]  3%|â–Ž         | 202/6000 [09:39<5:17:42,  3.29s/it]                                                    {'loss': 2.7988, 'grad_norm': 9.244013786315918, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:39<5:17:42,  3.29s/it]  3%|â–Ž         | 203/6000 [09:41<5:02:03,  3.13s/it]                                                    {'loss': 2.825, 'grad_norm': 12.691615104675293, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:41<5:02:03,  3.13s/it]  3%|â–Ž         | 204/6000 [09:44<4:53:39,  3.04s/it]                                                    {'loss': 2.8071, 'grad_norm': 8.74972915649414, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:44<4:53:39,  3.04s/it]  3%|â–Ž         | 205/6000 [09:47<4:42:57,  2.93s/it]                                                    {'loss': 2.7804, 'grad_norm': 9.68152141571045, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:47<4:42:57,  2.93s/it]  3%|â–Ž         | 206/6000 [09:50<4:34:26,  2.84s/it]                                                    {'loss': 2.8604, 'grad_norm': 12.557233810424805, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:50<4:34:26,  2.84s/it]  3%|â–Ž         | 207/6000 [09:52<4:29:51,  2.79s/it]                                                    {'loss': 2.891, 'grad_norm': 8.883365631103516, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:52<4:29:51,  2.79s/it]  3%|â–Ž         | 208/6000 [09:55<4:25:06,  2.75s/it]                                                    {'loss': 2.8084, 'grad_norm': 10.841483116149902, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:55<4:25:06,  2.75s/it]  3%|â–Ž         | 209/6000 [09:58<4:25:50,  2.75s/it]                                                    {'loss': 2.7945, 'grad_norm': 9.923453330993652, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:58<4:25:50,  2.75s/it]  4%|â–Ž         | 210/6000 [10:01<4:38:18,  2.88s/it]                                                    {'loss': 2.7848, 'grad_norm': 12.89796257019043, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [10:01<4:38:18,  2.88s/it]  4%|â–Ž         | 211/6000 [10:04<4:39:19,  2.89s/it]                                                    {'loss': 2.8039, 'grad_norm': 10.563459396362305, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:04<4:39:19,  2.89s/it]  4%|â–Ž         | 212/6000 [10:07<4:45:16,  2.96s/it]                                                    {'loss': 2.817, 'grad_norm': 13.553089141845703, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:07<4:45:16,  2.96s/it]  4%|â–Ž         | 213/6000 [10:10<4:40:52,  2.91s/it]                                                    {'loss': 2.8386, 'grad_norm': 20.3945255279541, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:10<4:40:52,  2.91s/it]  4%|â–Ž         | 214/6000 [10:12<4:34:34,  2.85s/it]                                                    {'loss': 2.7907, 'grad_norm': 10.312806129455566, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:12<4:34:34,  2.85s/it]  4%|â–Ž         | 215/6000 [10:15<4:31:12,  2.81s/it]                                                    {'loss': 2.8078, 'grad_norm': 9.16081428527832, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:15<4:31:12,  2.81s/it]  4%|â–Ž         | 216/6000 [10:18<4:31:19,  2.81s/it]                                                    {'loss': 2.7754, 'grad_norm': 9.452842712402344, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:18<4:31:19,  2.81s/it]  4%|â–Ž         | 217/6000 [10:21<4:33:00,  2.83s/it]                                                    {'loss': 2.8444, 'grad_norm': 12.374853134155273, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:21<4:33:00,  2.83s/it]  4%|â–Ž         | 218/6000 [10:24<4:33:23,  2.84s/it]                                                    {'loss': 2.8274, 'grad_norm': 13.38427448272705, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:24<4:33:23,  2.84s/it]  4%|â–Ž         | 219/6000 [10:26<4:29:40,  2.80s/it]                                                    {'loss': 2.8484, 'grad_norm': 11.303293228149414, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:26<4:29:40,  2.80s/it]  4%|â–Ž         | 220/6000 [10:29<4:27:13,  2.77s/it]                                                    {'loss': 2.7875, 'grad_norm': 9.42563247680664, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:29<4:27:13,  2.77s/it]  4%|â–Ž         | 221/6000 [10:32<4:25:35,  2.76s/it]                                                    {'loss': 2.8178, 'grad_norm': 9.139187812805176, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:32<4:25:35,  2.76s/it]  4%|â–Ž         | 222/6000 [10:35<4:24:53,  2.75s/it]                                                    {'loss': 2.8068, 'grad_norm': 11.60484790802002, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:35<4:24:53,  2.75s/it]  4%|â–Ž         | 223/6000 [10:37<4:25:34,  2.76s/it]                                                    {'loss': 2.7847, 'grad_norm': 9.015480041503906, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:37<4:25:34,  2.76s/it]  4%|â–Ž         | 224/6000 [10:40<4:23:38,  2.74s/it]                                                    {'loss': 2.8653, 'grad_norm': 10.435078620910645, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:40<4:23:38,  2.74s/it]  4%|â–         | 225/6000 [10:43<4:25:56,  2.76s/it]                                                    {'loss': 2.8046, 'grad_norm': 14.149426460266113, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:43<4:25:56,  2.76s/it]  4%|â–         | 226/6000 [10:46<4:40:01,  2.91s/it]                                                    {'loss': 2.8225, 'grad_norm': 18.326589584350586, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:46<4:40:01,  2.91s/it]  4%|â–         | 227/6000 [10:49<4:34:03,  2.85s/it]                                                    {'loss': 2.8586, 'grad_norm': 12.425045013427734, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:49<4:34:03,  2.85s/it]  4%|â–         | 228/6000 [10:52<4:32:42,  2.83s/it]                                                    {'loss': 2.8093, 'grad_norm': 15.810729026794434, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:52<4:32:42,  2.83s/it]  4%|â–         | 229/6000 [10:54<4:27:58,  2.79s/it]                                                    {'loss': 2.8145, 'grad_norm': 9.074841499328613, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:54<4:27:58,  2.79s/it]  4%|â–         | 230/6000 [10:57<4:29:20,  2.80s/it]                                                    {'loss': 2.8207, 'grad_norm': 8.15961742401123, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:57<4:29:20,  2.80s/it]  4%|â–         | 231/6000 [11:00<4:26:25,  2.77s/it]                                                    {'loss': 2.8149, 'grad_norm': 10.824201583862305, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [11:00<4:26:25,  2.77s/it]  4%|â–         | 232/6000 [11:03<4:30:27,  2.81s/it]                                                    {'loss': 2.8083, 'grad_norm': 17.23159408569336, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [11:03<4:30:27,  2.81s/it]  4%|â–         | 233/6000 [11:06<4:39:26,  2.91s/it]                                                    {'loss': 2.8187, 'grad_norm': 12.368969917297363, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:06<4:39:26,  2.91s/it]  4%|â–         | 234/6000 [11:09<4:43:10,  2.95s/it]                                                    {'loss': 2.7902, 'grad_norm': 10.656353950500488, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:09<4:43:10,  2.95s/it]  4%|â–         | 235/6000 [11:12<4:36:19,  2.88s/it]                                                    {'loss': 2.8821, 'grad_norm': 12.045662879943848, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:12<4:36:19,  2.88s/it]  4%|â–         | 236/6000 [11:14<4:34:07,  2.85s/it]                                                    {'loss': 2.8182, 'grad_norm': 8.615118980407715, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:14<4:34:07,  2.85s/it]  4%|â–         | 237/6000 [11:17<4:31:11,  2.82s/it]                                                    {'loss': 2.8142, 'grad_norm': 9.053400039672852, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:17<4:31:11,  2.82s/it]  4%|â–         | 238/6000 [11:20<4:42:54,  2.95s/it]                                                    {'loss': 2.8664, 'grad_norm': 12.422502517700195, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:20<4:42:54,  2.95s/it]  4%|â–         | 239/6000 [11:23<4:40:12,  2.92s/it]                                                    {'loss': 2.7795, 'grad_norm': 8.725557327270508, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:23<4:40:12,  2.92s/it]  4%|â–         | 240/6000 [11:26<4:36:39,  2.88s/it]                                                    {'loss': 2.8293, 'grad_norm': 15.435830116271973, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:26<4:36:39,  2.88s/it]  4%|â–         | 241/6000 [11:29<4:34:11,  2.86s/it]                                                    {'loss': 2.8162, 'grad_norm': 15.207436561584473, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:29<4:34:11,  2.86s/it]  4%|â–         | 242/6000 [11:32<4:29:42,  2.81s/it]                                                    {'loss': 2.8335, 'grad_norm': 10.697815895080566, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:32<4:29:42,  2.81s/it]  4%|â–         | 243/6000 [11:35<4:37:44,  2.89s/it]                                                    {'loss': 2.787, 'grad_norm': 7.890920162200928, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:35<4:37:44,  2.89s/it]  4%|â–         | 244/6000 [11:37<4:32:54,  2.84s/it]                                                    {'loss': 2.7899, 'grad_norm': 10.512275695800781, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:37<4:32:54,  2.84s/it]  4%|â–         | 245/6000 [11:40<4:31:32,  2.83s/it]                                                    {'loss': 2.8068, 'grad_norm': 10.727083206176758, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:40<4:31:32,  2.83s/it]  4%|â–         | 246/6000 [11:43<4:31:49,  2.83s/it]                                                    {'loss': 2.7974, 'grad_norm': 8.736770629882812, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:43<4:31:49,  2.83s/it]  4%|â–         | 247/6000 [11:46<4:27:50,  2.79s/it]                                                    {'loss': 2.7898, 'grad_norm': 8.474230766296387, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:46<4:27:50,  2.79s/it]  4%|â–         | 248/6000 [11:49<4:29:44,  2.81s/it]                                                    {'loss': 2.7961, 'grad_norm': 11.270807266235352, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:49<4:29:44,  2.81s/it]  4%|â–         | 249/6000 [11:51<4:25:44,  2.77s/it]                                                    {'loss': 2.8162, 'grad_norm': 10.357684135437012, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:51<4:25:44,  2.77s/it]  4%|â–         | 250/6000 [11:54<4:29:04,  2.81s/it]                                                    {'loss': 2.8039, 'grad_norm': 7.7134246826171875, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:54<4:29:04,  2.81s/it][2025-11-03 16:02:21,015] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
[2025-11-03 16:02:21,031] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 16:02:21,646] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [11:59<5:25:40,  3.40s/it]                                                    {'loss': 2.8339, 'grad_norm': 13.400704383850098, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:59<5:25:40,  3.40s/it]  4%|â–         | 252/6000 [12:02<5:11:40,  3.25s/it]                                                    {'loss': 2.8077, 'grad_norm': 8.967524528503418, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [12:02<5:11:40,  3.25s/it]  4%|â–         | 253/6000 [12:05<4:57:19,  3.10s/it]                                                    {'loss': 2.8245, 'grad_norm': 6.441555023193359, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [12:05<4:57:19,  3.10s/it]  4%|â–         | 254/6000 [12:07<4:48:21,  3.01s/it]                                                    {'loss': 2.7798, 'grad_norm': 9.762452125549316, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:07<4:48:21,  3.01s/it]  4%|â–         | 255/6000 [12:10<4:38:24,  2.91s/it]                                                    {'loss': 2.8199, 'grad_norm': 13.160913467407227, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:10<4:38:24,  2.91s/it]  4%|â–         | 256/6000 [12:13<4:46:28,  2.99s/it]                                                    {'loss': 2.8058, 'grad_norm': 10.679241180419922, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:13<4:46:28,  2.99s/it]  4%|â–         | 257/6000 [12:16<4:38:16,  2.91s/it]                                                    {'loss': 2.7899, 'grad_norm': 8.466862678527832, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:16<4:38:16,  2.91s/it]  4%|â–         | 258/6000 [12:19<4:33:19,  2.86s/it]                                                    {'loss': 2.7929, 'grad_norm': 12.555041313171387, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:19<4:33:19,  2.86s/it]  4%|â–         | 259/6000 [12:21<4:27:57,  2.80s/it]                                                    {'loss': 2.788, 'grad_norm': 9.099239349365234, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:21<4:27:57,  2.80s/it]  4%|â–         | 260/6000 [12:24<4:25:34,  2.78s/it]                                                    {'loss': 2.8028, 'grad_norm': 7.798229694366455, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:24<4:25:34,  2.78s/it]  4%|â–         | 261/6000 [12:27<4:28:47,  2.81s/it]                                                    {'loss': 2.7935, 'grad_norm': 8.094072341918945, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:27<4:28:47,  2.81s/it]  4%|â–         | 262/6000 [12:30<4:27:46,  2.80s/it]                                                    {'loss': 2.8103, 'grad_norm': 10.130617141723633, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:30<4:27:46,  2.80s/it]  4%|â–         | 263/6000 [12:33<4:37:44,  2.90s/it]                                                    {'loss': 2.8602, 'grad_norm': 9.802541732788086, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:33<4:37:44,  2.90s/it]  4%|â–         | 264/6000 [12:36<4:30:34,  2.83s/it]                                                    {'loss': 2.7949, 'grad_norm': 9.833252906799316, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:36<4:30:34,  2.83s/it]  4%|â–         | 265/6000 [12:38<4:32:35,  2.85s/it]                                                    {'loss': 2.7842, 'grad_norm': 7.708834648132324, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:38<4:32:35,  2.85s/it]  4%|â–         | 266/6000 [12:41<4:33:48,  2.87s/it]                                                    {'loss': 2.8147, 'grad_norm': 7.886467456817627, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:41<4:33:48,  2.87s/it]  4%|â–         | 267/6000 [12:44<4:29:45,  2.82s/it]                                                    {'loss': 2.7816, 'grad_norm': 7.387453556060791, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:44<4:29:45,  2.82s/it]  4%|â–         | 268/6000 [12:47<4:27:15,  2.80s/it]                                                    {'loss': 2.8799, 'grad_norm': 10.934897422790527, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:47<4:27:15,  2.80s/it]  4%|â–         | 269/6000 [12:50<4:31:05,  2.84s/it]                                                    {'loss': 2.7823, 'grad_norm': 6.683724403381348, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:50<4:31:05,  2.84s/it]  4%|â–         | 270/6000 [12:52<4:27:55,  2.81s/it]                                                    {'loss': 2.7938, 'grad_norm': 8.588798522949219, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:53<4:27:55,  2.81s/it]  5%|â–         | 271/6000 [12:55<4:24:20,  2.77s/it]                                                    {'loss': 2.8115, 'grad_norm': 7.784670352935791, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:55<4:24:20,  2.77s/it]  5%|â–         | 272/6000 [12:58<4:23:44,  2.76s/it]                                                    {'loss': 2.7911, 'grad_norm': 11.844965934753418, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:58<4:23:44,  2.76s/it]  5%|â–         | 273/6000 [13:01<4:21:33,  2.74s/it]                                                    {'loss': 2.8353, 'grad_norm': 7.1898932456970215, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [13:01<4:21:33,  2.74s/it]  5%|â–         | 274/6000 [13:04<4:32:38,  2.86s/it]                                                    {'loss': 2.7999, 'grad_norm': 16.68136978149414, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [13:04<4:32:38,  2.86s/it]  5%|â–         | 275/6000 [13:07<4:30:21,  2.83s/it]                                                    {'loss': 2.8111, 'grad_norm': 9.334090232849121, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:07<4:30:21,  2.83s/it]  5%|â–         | 276/6000 [13:09<4:28:28,  2.81s/it]                                                    {'loss': 2.8851, 'grad_norm': 10.931802749633789, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:09<4:28:28,  2.81s/it]  5%|â–         | 277/6000 [13:12<4:24:48,  2.78s/it]                                                    {'loss': 2.7819, 'grad_norm': 9.541181564331055, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:12<4:24:48,  2.78s/it]  5%|â–         | 278/6000 [13:15<4:22:30,  2.75s/it]                                                    {'loss': 2.8039, 'grad_norm': 14.767139434814453, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:15<4:22:30,  2.75s/it]  5%|â–         | 279/6000 [13:18<4:24:37,  2.78s/it]                                                    {'loss': 2.8114, 'grad_norm': 7.895560264587402, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:18<4:24:37,  2.78s/it]  5%|â–         | 280/6000 [13:20<4:22:56,  2.76s/it]                                                    {'loss': 2.7942, 'grad_norm': 8.563264846801758, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:20<4:22:56,  2.76s/it]  5%|â–         | 281/6000 [13:23<4:23:33,  2.77s/it]                                                    {'loss': 2.7924, 'grad_norm': 8.987013816833496, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:23<4:23:33,  2.77s/it]  5%|â–         | 282/6000 [13:26<4:36:04,  2.90s/it]                                                    {'loss': 2.8109, 'grad_norm': 15.719325065612793, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:26<4:36:04,  2.90s/it]  5%|â–         | 283/6000 [13:29<4:42:04,  2.96s/it]                                                    {'loss': 2.8084, 'grad_norm': 8.411255836486816, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:29<4:42:04,  2.96s/it]  5%|â–         | 284/6000 [13:33<4:54:56,  3.10s/it]                                                    {'loss': 2.7909, 'grad_norm': 11.553914070129395, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:33<4:54:56,  3.10s/it]  5%|â–         | 285/6000 [13:36<4:49:51,  3.04s/it]                                                    {'loss': 2.8034, 'grad_norm': 11.840627670288086, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:36<4:49:51,  3.04s/it]  5%|â–         | 286/6000 [13:38<4:40:13,  2.94s/it]                                                    {'loss': 2.8516, 'grad_norm': 22.924545288085938, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:38<4:40:13,  2.94s/it]  5%|â–         | 287/6000 [13:41<4:35:38,  2.89s/it]                                                    {'loss': 2.8161, 'grad_norm': 6.615236282348633, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:41<4:35:38,  2.89s/it]  5%|â–         | 288/6000 [13:44<4:30:28,  2.84s/it]                                                    {'loss': 2.8008, 'grad_norm': 8.693466186523438, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:44<4:30:28,  2.84s/it]  5%|â–         | 289/6000 [13:47<4:30:55,  2.85s/it]                                                    {'loss': 2.7964, 'grad_norm': 11.79524040222168, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:47<4:30:55,  2.85s/it]  5%|â–         | 290/6000 [13:50<4:29:10,  2.83s/it]                                                    {'loss': 2.8308, 'grad_norm': 13.70689868927002, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:50<4:29:10,  2.83s/it]  5%|â–         | 291/6000 [13:52<4:25:17,  2.79s/it]                                                    {'loss': 2.7981, 'grad_norm': 8.976876258850098, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:52<4:25:17,  2.79s/it]  5%|â–         | 292/6000 [13:55<4:23:27,  2.77s/it]                                                    {'loss': 2.7909, 'grad_norm': 7.043808460235596, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:55<4:23:27,  2.77s/it]  5%|â–         | 293/6000 [13:58<4:20:09,  2.74s/it]                                                    {'loss': 2.8542, 'grad_norm': 8.325329780578613, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:58<4:20:09,  2.74s/it]  5%|â–         | 294/6000 [14:00<4:20:03,  2.73s/it]                                                    {'loss': 2.7951, 'grad_norm': 7.37506628036499, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [14:00<4:20:03,  2.73s/it]  5%|â–         | 295/6000 [14:03<4:19:30,  2.73s/it]                                                    {'loss': 2.7991, 'grad_norm': 8.404562950134277, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [14:03<4:19:30,  2.73s/it]  5%|â–         | 296/6000 [14:06<4:23:41,  2.77s/it]                                                    {'loss': 2.7794, 'grad_norm': 9.443883895874023, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:06<4:23:41,  2.77s/it]  5%|â–         | 297/6000 [14:09<4:22:48,  2.76s/it]                                                    {'loss': 2.7981, 'grad_norm': 7.730186939239502, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:09<4:22:48,  2.76s/it]  5%|â–         | 298/6000 [14:12<4:28:34,  2.83s/it]                                                    {'loss': 2.8047, 'grad_norm': 8.88751220703125, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:12<4:28:34,  2.83s/it]  5%|â–         | 299/6000 [14:15<4:50:43,  3.06s/it]                                                    {'loss': 2.8327, 'grad_norm': 24.421024322509766, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:15<4:50:43,  3.06s/it]  5%|â–Œ         | 300/6000 [14:18<4:42:24,  2.97s/it]                                                    {'loss': 2.7812, 'grad_norm': 7.138636112213135, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:18<4:42:24,  2.97s/it][2025-11-03 16:04:44,841] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
[2025-11-03 16:04:44,853] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 16:04:45,480] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [14:23<5:41:16,  3.59s/it]                                                    {'loss': 2.8019, 'grad_norm': 8.402046203613281, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:23<5:41:16,  3.59s/it]  5%|â–Œ         | 302/6000 [14:26<5:16:23,  3.33s/it]                                                    {'loss': 2.7979, 'grad_norm': 7.858874797821045, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:26<5:16:23,  3.33s/it]  5%|â–Œ         | 303/6000 [14:29<5:00:47,  3.17s/it]                                                    {'loss': 2.7979, 'grad_norm': 7.386464595794678, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:29<5:00:47,  3.17s/it]  5%|â–Œ         | 304/6000 [14:31<4:49:46,  3.05s/it]                                                    {'loss': 2.7938, 'grad_norm': 8.318089485168457, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:31<4:49:46,  3.05s/it]  5%|â–Œ         | 305/6000 [14:34<4:38:27,  2.93s/it]                                                    {'loss': 2.7966, 'grad_norm': 6.848967552185059, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:34<4:38:27,  2.93s/it]  5%|â–Œ         | 306/6000 [14:37<4:31:25,  2.86s/it]                                                    {'loss': 2.8029, 'grad_norm': 7.329380512237549, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:37<4:31:25,  2.86s/it]  5%|â–Œ         | 307/6000 [14:39<4:27:56,  2.82s/it]                                                    {'loss': 2.8046, 'grad_norm': 10.503196716308594, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:39<4:27:56,  2.82s/it]  5%|â–Œ         | 308/6000 [14:42<4:24:25,  2.79s/it]                                                    {'loss': 2.7983, 'grad_norm': 8.889725685119629, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:42<4:24:25,  2.79s/it]  5%|â–Œ         | 309/6000 [14:45<4:30:06,  2.85s/it]                                                    {'loss': 2.7674, 'grad_norm': 6.041208744049072, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:45<4:30:06,  2.85s/it]  5%|â–Œ         | 310/6000 [14:48<4:26:41,  2.81s/it]                                                    {'loss': 2.8275, 'grad_norm': 9.671391487121582, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:48<4:26:41,  2.81s/it]