==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name test3-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/train.log
W1022 20:14:34.483000 129864347084608 torch/distributed/run.py:779] 
W1022 20:14:34.483000 129864347084608 torch/distributed/run.py:779] *****************************************
W1022 20:14:34.483000 129864347084608 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1022 20:14:34.483000 129864347084608 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-22 20:14:44,969] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.02it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.75it/s]
wandb: setting up run 6cbv9qma
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251022_201445-6cbv9qma
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run test3-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/6cbv9qma
[2025-10-22 20:14:46,529] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.29it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.13it/s]
[2025-10-22 20:14:47,157] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-10-22 20:14:56,108] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-22 20:14:57,268] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-22 20:14:57,269] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-22 20:15:01,501] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-22 20:15:01,502] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-22 20:15:02,490] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-22 20:15:02,491] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-22 20:15:02,491] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-22 20:15:02,492] INFO [src.utils:19] ==================================================
[2025-10-22 20:15:02,492] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-22 20:15:02,493] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 20:15:02,494] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-22 20:15:02,494] INFO [src.utils:19] ==================================================
[2025-10-22 20:15:04,323] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 20:15:04,323] INFO [src.trainer:342] ***** Running training *****
[2025-10-22 20:15:04,323] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 20:15:04,323] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 20:15:04,323] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 20:15:04,323] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 20:15:04,323] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 20:15:04,323] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 20:15:04,324] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-22 20:15:04,324] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-22 20:15:04,325] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-22 20:15:04,325] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-22 20:15:04,325] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-22 20:15:04,325] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-22 20:15:04,333] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 20:15:04,335] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-10-22 20:15:04,341] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-10-22 20:15:04,343] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1022 20:15:07.077430138 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1022 20:15:07.116366194 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:48:35,  4.09s/it]                                                  {'loss': 10.7162, 'grad_norm': 2486.785888671875, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:48:35,  4.09s/it]  0%|          | 2/6000 [00:06<5:25:50,  3.26s/it]                                                  {'loss': 8.3867, 'grad_norm': 1878.374755859375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:25:50,  3.26s/it]  0%|          | 3/6000 [00:09<5:01:46,  3.02s/it]                                                  {'loss': 8.4721, 'grad_norm': 2263.275634765625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:01:46,  3.02s/it]  0%|          | 4/6000 [00:12<4:49:31,  2.90s/it]                                                  {'loss': 7.7289, 'grad_norm': 1357.7490234375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:49:31,  2.90s/it]  0%|          | 5/6000 [00:14<4:42:59,  2.83s/it]                                                  {'loss': 9.1545, 'grad_norm': 1808.7178955078125, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:42:59,  2.83s/it]  0%|          | 6/6000 [00:17<4:37:43,  2.78s/it]                                                  {'loss': 8.8539, 'grad_norm': 1562.546142578125, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:37:43,  2.78s/it]  0%|          | 7/6000 [00:20<4:34:51,  2.75s/it]                                                  {'loss': 8.6696, 'grad_norm': 1201.3997802734375, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:34:51,  2.75s/it]  0%|          | 8/6000 [00:22<4:30:49,  2.71s/it]                                                  {'loss': 7.9969, 'grad_norm': 1112.8858642578125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:30:49,  2.71s/it]  0%|          | 9/6000 [00:25<4:31:10,  2.72s/it]                                                  {'loss': 5.8219, 'grad_norm': 672.8592529296875, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:31:10,  2.72s/it]  0%|          | 10/6000 [00:28<4:28:48,  2.69s/it]                                                   {'loss': 7.2417, 'grad_norm': 916.1229248046875, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:28:48,  2.69s/it]  0%|          | 11/6000 [00:31<4:39:34,  2.80s/it]                                                   {'loss': 7.8021, 'grad_norm': 1352.5997314453125, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:39:34,  2.80s/it]  0%|          | 12/6000 [00:34<4:42:21,  2.83s/it]                                                   {'loss': 6.17, 'grad_norm': 1030.1949462890625, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:42:21,  2.83s/it]  0%|          | 13/6000 [00:36<4:39:28,  2.80s/it]                                                   {'loss': 5.97, 'grad_norm': 1040.78662109375, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:39:28,  2.80s/it]  0%|          | 14/6000 [00:39<4:41:23,  2.82s/it]                                                   {'loss': 5.1566, 'grad_norm': 924.1384887695312, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:39<4:41:23,  2.82s/it]  0%|          | 15/6000 [00:42<4:38:04,  2.79s/it]                                                   {'loss': 4.1172, 'grad_norm': 1041.832275390625, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:38:04,  2.79s/it]  0%|          | 16/6000 [00:45<4:33:51,  2.75s/it]                                                   {'loss': 4.1241, 'grad_norm': 983.0614624023438, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:33:51,  2.75s/it]  0%|          | 17/6000 [00:47<4:32:56,  2.74s/it]                                                   {'loss': 3.3887, 'grad_norm': 341.0491027832031, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:47<4:32:56,  2.74s/it]  0%|          | 18/6000 [00:50<4:32:07,  2.73s/it]                                                   {'loss': 3.2594, 'grad_norm': 554.7603759765625, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:32:07,  2.73s/it]  0%|          | 19/6000 [00:53<4:29:38,  2.70s/it]                                                   {'loss': 3.4235, 'grad_norm': 340.21905517578125, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:29:38,  2.70s/it]  0%|          | 20/6000 [00:55<4:30:02,  2.71s/it]                                                   {'loss': 3.6236, 'grad_norm': 892.6478271484375, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:56<4:30:02,  2.71s/it]  0%|          | 21/6000 [00:58<4:34:58,  2.76s/it]                                                   {'loss': 3.0672, 'grad_norm': 211.194091796875, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:58<4:34:58,  2.76s/it]  0%|          | 22/6000 [01:01<4:35:10,  2.76s/it]                                                   {'loss': 3.1554, 'grad_norm': 319.0102233886719, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:35:10,  2.76s/it]  0%|          | 23/6000 [01:04<4:32:01,  2.73s/it]                                                   {'loss': 3.0834, 'grad_norm': 282.16827392578125, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:32:01,  2.73s/it]  0%|          | 24/6000 [01:07<4:32:51,  2.74s/it]                                                   {'loss': 3.0218, 'grad_norm': 101.1982192993164, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:32:51,  2.74s/it]  0%|          | 25/6000 [01:09<4:31:44,  2.73s/it]                                                   {'loss': 2.9416, 'grad_norm': 143.70005798339844, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:09<4:31:44,  2.73s/it]  0%|          | 26/6000 [01:12<4:31:55,  2.73s/it]                                                   {'loss': 2.8703, 'grad_norm': 97.53207397460938, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:12<4:31:55,  2.73s/it]  0%|          | 27/6000 [01:15<4:31:20,  2.73s/it]                                                   {'loss': 2.9721, 'grad_norm': 171.07589721679688, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:31:20,  2.73s/it]  0%|          | 28/6000 [01:18<4:57:49,  2.99s/it]                                                   {'loss': 2.8282, 'grad_norm': 136.2085723876953, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:18<4:57:49,  2.99s/it]  0%|          | 29/6000 [01:21<4:46:57,  2.88s/it]                                                   {'loss': 2.8412, 'grad_norm': 83.0682601928711, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:21<4:46:57,  2.88s/it]  0%|          | 30/6000 [01:24<4:41:00,  2.82s/it]                                                   {'loss': 2.8456, 'grad_norm': 177.36936950683594, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:41:00,  2.82s/it]  1%|          | 31/6000 [01:26<4:38:31,  2.80s/it]                                                   {'loss': 2.8345, 'grad_norm': 60.41736602783203, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:26<4:38:31,  2.80s/it]  1%|          | 32/6000 [01:29<4:35:15,  2.77s/it]                                                   {'loss': 2.8204, 'grad_norm': 64.08702850341797, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:29<4:35:15,  2.77s/it]  1%|          | 33/6000 [01:32<4:33:20,  2.75s/it]                                                   {'loss': 2.799, 'grad_norm': 64.72714233398438, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:32<4:33:20,  2.75s/it]  1%|          | 34/6000 [01:34<4:31:21,  2.73s/it]                                                   {'loss': 2.852, 'grad_norm': 39.379173278808594, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:34<4:31:21,  2.73s/it]  1%|          | 35/6000 [01:37<4:32:47,  2.74s/it]                                                   {'loss': 2.8319, 'grad_norm': 92.43926239013672, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:37<4:32:47,  2.74s/it]  1%|          | 36/6000 [01:40<4:30:14,  2.72s/it]                                                   {'loss': 2.817, 'grad_norm': 47.91808319091797, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:40<4:30:14,  2.72s/it]  1%|          | 37/6000 [01:43<4:28:58,  2.71s/it]                                                   {'loss': 2.8142, 'grad_norm': 39.51469421386719, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:28:58,  2.71s/it]  1%|          | 38/6000 [01:45<4:26:14,  2.68s/it]                                                   {'loss': 2.8011, 'grad_norm': 27.79290008544922, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:45<4:26:14,  2.68s/it]  1%|          | 39/6000 [01:48<4:25:59,  2.68s/it]                                                   {'loss': 2.8525, 'grad_norm': 19.825828552246094, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:48<4:25:59,  2.68s/it]  1%|          | 40/6000 [01:51<4:26:13,  2.68s/it]                                                   {'loss': 2.8702, 'grad_norm': 73.80433654785156, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:51<4:26:13,  2.68s/it]  1%|          | 41/6000 [01:53<4:25:41,  2.68s/it]                                                   {'loss': 2.7861, 'grad_norm': 54.2921028137207, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:53<4:25:41,  2.68s/it]  1%|          | 42/6000 [01:56<4:25:29,  2.67s/it]                                                   {'loss': 2.797, 'grad_norm': 31.951976776123047, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:56<4:25:29,  2.67s/it]  1%|          | 43/6000 [02:00<4:57:00,  2.99s/it]                                                   {'loss': 2.7996, 'grad_norm': 52.02847671508789, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [02:00<4:57:00,  2.99s/it]  1%|          | 44/6000 [02:03<5:02:07,  3.04s/it]                                                   {'loss': 2.7756, 'grad_norm': 19.04706573486328, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:03<5:02:07,  3.04s/it]  1%|          | 45/6000 [02:06<4:54:07,  2.96s/it]                                                   {'loss': 2.7904, 'grad_norm': 15.444074630737305, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:06<4:54:07,  2.96s/it]  1%|          | 46/6000 [02:08<4:49:48,  2.92s/it]                                                   {'loss': 2.7731, 'grad_norm': 28.77065658569336, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:08<4:49:48,  2.92s/it]  1%|          | 47/6000 [02:11<4:45:04,  2.87s/it]                                                   {'loss': 2.7748, 'grad_norm': 37.118412017822266, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:11<4:45:04,  2.87s/it]  1%|          | 48/6000 [02:14<4:42:12,  2.84s/it]                                                   {'loss': 2.7983, 'grad_norm': 41.62014389038086, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:14<4:42:12,  2.84s/it]  1%|          | 49/6000 [02:17<4:36:26,  2.79s/it]                                                   {'loss': 2.7876, 'grad_norm': 50.52492141723633, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:17<4:36:26,  2.79s/it]  1%|          | 50/6000 [02:19<4:39:18,  2.82s/it]                                                   {'loss': 2.7744, 'grad_norm': 30.302839279174805, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:19<4:39:18,  2.82s/it][2025-10-22 20:17:24,493] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-10-22 20:17:24,496] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:24<5:39:24,  3.42s/it]                                                   {'loss': 2.7888, 'grad_norm': 19.9791202545166, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:24<5:39:24,  3.42s/it]  1%|          | 52/6000 [02:27<5:16:56,  3.20s/it]                                                   {'loss': 2.7841, 'grad_norm': 23.741018295288086, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:27<5:16:56,  3.20s/it]  1%|          | 53/6000 [02:30<5:04:09,  3.07s/it]                                                   {'loss': 2.8603, 'grad_norm': 64.46707153320312, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:30<5:04:09,  3.07s/it]  1%|          | 54/6000 [02:32<4:52:31,  2.95s/it]                                                   {'loss': 2.7884, 'grad_norm': 26.63908576965332, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:32<4:52:31,  2.95s/it]  1%|          | 55/6000 [02:35<4:45:50,  2.88s/it]                                                   {'loss': 2.7979, 'grad_norm': 19.70147705078125, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:35<4:45:50,  2.88s/it]  1%|          | 56/6000 [02:38<4:41:47,  2.84s/it]                                                   {'loss': 2.7738, 'grad_norm': 24.478574752807617, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:38<4:41:47,  2.84s/it]  1%|          | 57/6000 [02:41<4:38:31,  2.81s/it]                                                   {'loss': 2.7841, 'grad_norm': 11.218323707580566, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:41<4:38:31,  2.81s/it]  1%|          | 58/6000 [02:43<4:35:01,  2.78s/it]                                                   {'loss': 2.776, 'grad_norm': 37.478843688964844, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:43<4:35:01,  2.78s/it]  1%|          | 59/6000 [02:46<4:31:51,  2.75s/it]                                                   {'loss': 2.7805, 'grad_norm': 31.31574058532715, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:46<4:31:51,  2.75s/it]  1%|          | 60/6000 [02:49<4:29:10,  2.72s/it]                                                   {'loss': 2.7821, 'grad_norm': 17.243864059448242, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:49<4:29:10,  2.72s/it]  1%|          | 61/6000 [02:51<4:26:53,  2.70s/it]                                                   {'loss': 2.8034, 'grad_norm': 18.610172271728516, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:51<4:26:53,  2.70s/it]  1%|          | 62/6000 [02:54<4:28:01,  2.71s/it]                                                   {'loss': 2.7993, 'grad_norm': 28.642127990722656, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:54<4:28:01,  2.71s/it]  1%|          | 63/6000 [02:57<4:26:41,  2.70s/it]                                                   {'loss': 2.7809, 'grad_norm': 11.503273010253906, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:57<4:26:41,  2.70s/it]  1%|          | 64/6000 [02:59<4:26:24,  2.69s/it]                                                   {'loss': 2.7777, 'grad_norm': 10.781051635742188, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [02:59<4:26:24,  2.69s/it]  1%|          | 65/6000 [03:02<4:24:37,  2.68s/it]                                                   {'loss': 2.7872, 'grad_norm': 13.744529724121094, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [03:02<4:24:37,  2.68s/it]  1%|          | 66/6000 [03:05<4:37:48,  2.81s/it]                                                   {'loss': 2.7792, 'grad_norm': 16.394948959350586, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:05<4:37:48,  2.81s/it]  1%|          | 67/6000 [03:08<4:34:43,  2.78s/it]                                                   {'loss': 2.7893, 'grad_norm': 10.751081466674805, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:08<4:34:43,  2.78s/it]  1%|          | 68/6000 [03:11<4:31:11,  2.74s/it]                                                   {'loss': 2.7896, 'grad_norm': 36.0557746887207, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:11<4:31:11,  2.74s/it]  1%|          | 69/6000 [03:13<4:29:52,  2.73s/it]                                                   {'loss': 2.7885, 'grad_norm': 44.92995834350586, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:13<4:29:52,  2.73s/it]  1%|          | 70/6000 [03:16<4:31:41,  2.75s/it]                                                   {'loss': 2.7935, 'grad_norm': 36.94851303100586, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:16<4:31:41,  2.75s/it]  1%|          | 71/6000 [03:19<4:29:04,  2.72s/it]                                                   {'loss': 2.7812, 'grad_norm': 46.238319396972656, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:19<4:29:04,  2.72s/it]  1%|          | 72/6000 [03:22<4:47:56,  2.91s/it]                                                   {'loss': 2.7715, 'grad_norm': 10.490836143493652, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:22<4:47:56,  2.91s/it]  1%|          | 73/6000 [03:25<4:43:14,  2.87s/it]                                                   {'loss': 2.8297, 'grad_norm': 14.062565803527832, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:25<4:43:14,  2.87s/it]  1%|          | 74/6000 [03:27<4:36:28,  2.80s/it]                                                   {'loss': 2.7708, 'grad_norm': 18.090967178344727, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:27<4:36:28,  2.80s/it]  1%|â–         | 75/6000 [03:30<4:31:37,  2.75s/it]                                                   {'loss': 2.8048, 'grad_norm': 74.86763763427734, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:30<4:31:37,  2.75s/it]  1%|â–         | 76/6000 [03:33<4:34:06,  2.78s/it]                                                   {'loss': 2.8114, 'grad_norm': 20.57781219482422, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:33<4:34:06,  2.78s/it]  1%|â–         | 77/6000 [03:36<4:32:57,  2.76s/it]                                                   {'loss': 2.8494, 'grad_norm': 7.805633544921875, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:36<4:32:57,  2.76s/it]  1%|â–         | 78/6000 [03:39<4:43:34,  2.87s/it]                                                   {'loss': 2.7722, 'grad_norm': 20.59967803955078, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:39<4:43:34,  2.87s/it]  1%|â–         | 79/6000 [03:42<4:40:24,  2.84s/it]                                                   {'loss': 2.7652, 'grad_norm': 17.10293197631836, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:42<4:40:24,  2.84s/it]  1%|â–         | 80/6000 [03:45<4:44:45,  2.89s/it]                                                   {'loss': 2.7735, 'grad_norm': 18.56514549255371, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:45<4:44:45,  2.89s/it]  1%|â–         | 81/6000 [03:47<4:38:36,  2.82s/it]                                                   {'loss': 2.7909, 'grad_norm': 32.8306884765625, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:47<4:38:36,  2.82s/it]  1%|â–         | 82/6000 [03:50<4:36:16,  2.80s/it]                                                   {'loss': 2.7645, 'grad_norm': 17.828336715698242, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:50<4:36:16,  2.80s/it]  1%|â–         | 83/6000 [03:53<4:33:08,  2.77s/it]                                                   {'loss': 2.8068, 'grad_norm': 31.755008697509766, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:53<4:33:08,  2.77s/it]  1%|â–         | 84/6000 [03:55<4:34:39,  2.79s/it]                                                   {'loss': 2.7834, 'grad_norm': 22.35011100769043, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:55<4:34:39,  2.79s/it]  1%|â–         | 85/6000 [03:59<4:43:51,  2.88s/it]                                                   {'loss': 2.7825, 'grad_norm': 116.08304595947266, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:59<4:43:51,  2.88s/it]  1%|â–         | 86/6000 [04:01<4:38:44,  2.83s/it]                                                   {'loss': 2.793, 'grad_norm': 24.6314640045166, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:01<4:38:44,  2.83s/it]  1%|â–         | 87/6000 [04:04<4:33:50,  2.78s/it]                                                   {'loss': 2.7905, 'grad_norm': 18.767065048217773, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:04<4:33:50,  2.78s/it]  1%|â–         | 88/6000 [04:07<4:30:32,  2.75s/it]                                                   {'loss': 2.7883, 'grad_norm': 11.96972370147705, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:07<4:30:32,  2.75s/it]  1%|â–         | 89/6000 [04:09<4:28:11,  2.72s/it]                                                   {'loss': 2.7746, 'grad_norm': 13.974361419677734, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:09<4:28:11,  2.72s/it]  2%|â–         | 90/6000 [04:12<4:29:34,  2.74s/it]                                                   {'loss': 2.8317, 'grad_norm': 17.852197647094727, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:12<4:29:34,  2.74s/it]  2%|â–         | 91/6000 [04:15<4:29:39,  2.74s/it]                                                   {'loss': 2.7745, 'grad_norm': 16.27545928955078, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:15<4:29:39,  2.74s/it]  2%|â–         | 92/6000 [04:18<4:40:34,  2.85s/it]                                                   {'loss': 2.7656, 'grad_norm': 46.270179748535156, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:18<4:40:34,  2.85s/it]  2%|â–         | 93/6000 [04:21<4:39:59,  2.84s/it]                                                   {'loss': 2.7683, 'grad_norm': 42.089656829833984, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:21<4:39:59,  2.84s/it]  2%|â–         | 94/6000 [04:23<4:36:07,  2.81s/it]                                                   {'loss': 2.791, 'grad_norm': 8.85075855255127, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:23<4:36:07,  2.81s/it]  2%|â–         | 95/6000 [04:26<4:33:48,  2.78s/it]                                                   {'loss': 2.7779, 'grad_norm': 18.891620635986328, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:26<4:33:48,  2.78s/it]  2%|â–         | 96/6000 [04:29<4:31:54,  2.76s/it]                                                   {'loss': 2.7739, 'grad_norm': 13.239470481872559, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:29<4:31:54,  2.76s/it]  2%|â–         | 97/6000 [04:32<4:29:31,  2.74s/it]                                                   {'loss': 2.7841, 'grad_norm': 21.07511329650879, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:32<4:29:31,  2.74s/it]  2%|â–         | 98/6000 [04:34<4:26:57,  2.71s/it]                                                   {'loss': 2.7712, 'grad_norm': 34.42325210571289, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:34<4:26:57,  2.71s/it]  2%|â–         | 99/6000 [04:37<4:24:48,  2.69s/it]                                                   {'loss': 2.769, 'grad_norm': 38.22214889526367, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:37<4:24:48,  2.69s/it]  2%|â–         | 100/6000 [04:40<4:24:41,  2.69s/it]                                                    {'loss': 2.7648, 'grad_norm': 36.27690124511719, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:40<4:24:41,  2.69s/it][2025-10-22 20:19:44,591] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-10-22 20:19:44,593] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:45<5:56:26,  3.63s/it]                                                    {'loss': 2.8258, 'grad_norm': 32.25993728637695, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:45<5:56:26,  3.63s/it]  2%|â–         | 102/6000 [04:48<5:29:16,  3.35s/it]                                                    {'loss': 2.7811, 'grad_norm': 26.548799514770508, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:48<5:29:16,  3.35s/it]  2%|â–         | 103/6000 [04:51<5:12:53,  3.18s/it]                                                    {'loss': 2.7907, 'grad_norm': 25.94145965576172, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:51<5:12:53,  3.18s/it]  2%|â–         | 104/6000 [04:54<5:05:27,  3.11s/it]                                                    {'loss': 2.7705, 'grad_norm': 34.249263763427734, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:54<5:05:27,  3.11s/it]  2%|â–         | 105/6000 [04:57<4:55:55,  3.01s/it]                                                    {'loss': 2.7754, 'grad_norm': 36.211280822753906, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:57<4:55:55,  3.01s/it]  2%|â–         | 106/6000 [04:59<4:50:42,  2.96s/it]                                                    {'loss': 2.775, 'grad_norm': 6.830355644226074, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:59<4:50:42,  2.96s/it]  2%|â–         | 107/6000 [05:02<4:41:20,  2.86s/it]                                                    {'loss': 2.7968, 'grad_norm': 18.96885108947754, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:02<4:41:20,  2.86s/it]  2%|â–         | 108/6000 [05:05<4:40:09,  2.85s/it]                                                    {'loss': 2.7794, 'grad_norm': 13.4158353805542, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:05<4:40:09,  2.85s/it]  2%|â–         | 109/6000 [05:08<4:49:17,  2.95s/it]                                                    {'loss': 2.8137, 'grad_norm': 10.604024887084961, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:08<4:49:17,  2.95s/it]  2%|â–         | 110/6000 [05:11<4:42:45,  2.88s/it]                                                    {'loss': 2.851, 'grad_norm': 17.853605270385742, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:11<4:42:45,  2.88s/it]  2%|â–         | 111/6000 [05:14<4:41:31,  2.87s/it]                                                    {'loss': 2.7481, 'grad_norm': 17.66542625427246, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:14<4:41:31,  2.87s/it]  2%|â–         | 112/6000 [05:17<4:52:52,  2.98s/it]                                                    {'loss': 2.7816, 'grad_norm': 33.97306442260742, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:17<4:52:52,  2.98s/it]  2%|â–         | 113/6000 [05:20<4:45:02,  2.91s/it]                                                    {'loss': 2.8374, 'grad_norm': 49.224361419677734, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:20<4:45:02,  2.91s/it]  2%|â–         | 114/6000 [05:22<4:40:29,  2.86s/it]                                                    {'loss': 2.7715, 'grad_norm': 20.039567947387695, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:22<4:40:29,  2.86s/it]  2%|â–         | 115/6000 [05:25<4:36:32,  2.82s/it]                                                    {'loss': 2.8663, 'grad_norm': 35.65634536743164, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:25<4:36:32,  2.82s/it]  2%|â–         | 116/6000 [05:28<4:33:24,  2.79s/it]                                                    {'loss': 2.8024, 'grad_norm': 19.156986236572266, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:28<4:33:24,  2.79s/it]  2%|â–         | 117/6000 [05:31<4:32:35,  2.78s/it]                                                    {'loss': 2.9107, 'grad_norm': 12.918953895568848, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:31<4:32:35,  2.78s/it]  2%|â–         | 118/6000 [05:34<4:47:47,  2.94s/it]                                                    {'loss': 2.7797, 'grad_norm': 16.981090545654297, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:34<4:47:47,  2.94s/it]  2%|â–         | 119/6000 [05:37<4:41:28,  2.87s/it]                                                    {'loss': 2.7845, 'grad_norm': 76.49614715576172, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:37<4:41:28,  2.87s/it]  2%|â–         | 120/6000 [05:39<4:38:17,  2.84s/it]                                                    {'loss': 2.7977, 'grad_norm': 12.453701972961426, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:39<4:38:17,  2.84s/it]  2%|â–         | 121/6000 [05:42<4:36:47,  2.82s/it]                                                    {'loss': 2.7779, 'grad_norm': 8.588292121887207, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:42<4:36:47,  2.82s/it]  2%|â–         | 122/6000 [05:45<4:37:35,  2.83s/it]                                                    {'loss': 2.7787, 'grad_norm': 8.492985725402832, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:45<4:37:35,  2.83s/it]  2%|â–         | 123/6000 [05:48<4:33:01,  2.79s/it]                                                    {'loss': 2.7937, 'grad_norm': 9.228568077087402, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:48<4:33:01,  2.79s/it]  2%|â–         | 124/6000 [05:50<4:28:05,  2.74s/it]                                                    {'loss': 2.827, 'grad_norm': 4.485071182250977, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:50<4:28:05,  2.74s/it]  2%|â–         | 125/6000 [05:53<4:34:33,  2.80s/it]                                                    {'loss': 2.9393, 'grad_norm': 4.297375202178955, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:53<4:34:33,  2.80s/it]  2%|â–         | 126/6000 [05:56<4:34:33,  2.80s/it]                                                    {'loss': 2.769, 'grad_norm': 9.553047180175781, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:56<4:34:33,  2.80s/it]  2%|â–         | 127/6000 [05:59<4:36:23,  2.82s/it]                                                    {'loss': 2.7825, 'grad_norm': 8.594812393188477, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:59<4:36:23,  2.82s/it]  2%|â–         | 128/6000 [06:02<4:32:31,  2.78s/it]                                                    {'loss': 2.7918, 'grad_norm': 5.658100128173828, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:02<4:32:31,  2.78s/it]  2%|â–         | 129/6000 [06:04<4:29:34,  2.76s/it]                                                    {'loss': 2.8489, 'grad_norm': 7.227632522583008, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:04<4:29:34,  2.76s/it]  2%|â–         | 130/6000 [06:07<4:29:25,  2.75s/it]                                                    {'loss': 2.7693, 'grad_norm': 9.781044960021973, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:07<4:29:25,  2.75s/it]  2%|â–         | 131/6000 [06:10<4:29:25,  2.75s/it]                                                    {'loss': 2.7841, 'grad_norm': 12.837019920349121, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:10<4:29:25,  2.75s/it]  2%|â–         | 132/6000 [06:12<4:27:06,  2.73s/it]                                                    {'loss': 2.8068, 'grad_norm': 9.652052879333496, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:12<4:27:06,  2.73s/it]  2%|â–         | 133/6000 [06:15<4:26:50,  2.73s/it]                                                    {'loss': 2.7767, 'grad_norm': 7.556302070617676, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:15<4:26:50,  2.73s/it]  2%|â–         | 134/6000 [06:18<4:32:08,  2.78s/it]                                                    {'loss': 2.7763, 'grad_norm': 5.769701957702637, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:18<4:32:08,  2.78s/it]  2%|â–         | 135/6000 [06:21<4:28:48,  2.75s/it]                                                    {'loss': 2.7897, 'grad_norm': 4.416922092437744, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:21<4:28:48,  2.75s/it]  2%|â–         | 136/6000 [06:23<4:26:37,  2.73s/it]                                                    {'loss': 2.7799, 'grad_norm': 3.643690586090088, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:23<4:26:37,  2.73s/it]  2%|â–         | 137/6000 [06:27<4:44:22,  2.91s/it]                                                    {'loss': 2.7771, 'grad_norm': 3.3852450847625732, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:27<4:44:22,  2.91s/it]  2%|â–         | 138/6000 [06:29<4:37:03,  2.84s/it]                                                    {'loss': 2.7751, 'grad_norm': 2.667890787124634, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:29<4:37:03,  2.84s/it]  2%|â–         | 139/6000 [06:32<4:37:33,  2.84s/it]                                                    {'loss': 2.8244, 'grad_norm': 3.8523666858673096, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:32<4:37:33,  2.84s/it]  2%|â–         | 140/6000 [06:35<4:45:05,  2.92s/it]                                                    {'loss': 2.7797, 'grad_norm': 2.5958659648895264, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:35<4:45:05,  2.92s/it]  2%|â–         | 141/6000 [06:38<4:40:30,  2.87s/it]                                                    {'loss': 2.7834, 'grad_norm': 1.8199514150619507, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:38<4:40:30,  2.87s/it]  2%|â–         | 142/6000 [06:41<4:34:40,  2.81s/it]                                                    {'loss': 2.7753, 'grad_norm': 1.9367121458053589, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:41<4:34:40,  2.81s/it]  2%|â–         | 143/6000 [06:44<4:31:46,  2.78s/it]                                                    {'loss': 2.7739, 'grad_norm': 3.2047455310821533, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:44<4:31:46,  2.78s/it]  2%|â–         | 144/6000 [06:46<4:30:13,  2.77s/it]                                                    {'loss': 2.7913, 'grad_norm': 3.1859753131866455, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:46<4:30:13,  2.77s/it]  2%|â–         | 145/6000 [06:49<4:27:10,  2.74s/it]                                                    {'loss': 2.8048, 'grad_norm': 3.1100244522094727, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:49<4:27:10,  2.74s/it]  2%|â–         | 146/6000 [06:52<4:26:59,  2.74s/it]                                                    {'loss': 2.8063, 'grad_norm': 2.011033296585083, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:52<4:26:59,  2.74s/it]  2%|â–         | 147/6000 [06:54<4:24:37,  2.71s/it]                                                    {'loss': 2.7821, 'grad_norm': 1.6356762647628784, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:54<4:24:37,  2.71s/it]  2%|â–         | 148/6000 [06:57<4:25:24,  2.72s/it]                                                    {'loss': 2.7917, 'grad_norm': 2.1346092224121094, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:57<4:25:24,  2.72s/it]  2%|â–         | 149/6000 [07:00<4:23:44,  2.70s/it]                                                    {'loss': 2.7679, 'grad_norm': 1.9020308256149292, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:00<4:23:44,  2.70s/it]  2%|â–Ž         | 150/6000 [07:02<4:23:08,  2.70s/it]                                                    {'loss': 2.7914, 'grad_norm': 1.8819737434387207, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:02<4:23:08,  2.70s/it][2025-10-22 20:22:07,484] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-10-22 20:22:07,486] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:07<5:30:47,  3.39s/it]                                                    {'loss': 2.7759, 'grad_norm': 2.7499170303344727, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:07<5:30:47,  3.39s/it]  3%|â–Ž         | 152/6000 [07:10<5:10:02,  3.18s/it]                                                    {'loss': 2.7941, 'grad_norm': 6.2914886474609375, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:10<5:10:02,  3.18s/it]  3%|â–Ž         | 153/6000 [07:13<4:54:43,  3.02s/it]                                                    {'loss': 2.7854, 'grad_norm': 3.0186338424682617, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:13<4:54:43,  3.02s/it]  3%|â–Ž         | 154/6000 [07:16<4:48:26,  2.96s/it]                                                    {'loss': 2.7779, 'grad_norm': 2.5776658058166504, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:16<4:48:26,  2.96s/it]  3%|â–Ž         | 155/6000 [07:18<4:42:07,  2.90s/it]                                                    {'loss': 2.802, 'grad_norm': 1.9590880870819092, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:18<4:42:07,  2.90s/it]  3%|â–Ž         | 156/6000 [07:21<4:38:38,  2.86s/it]                                                    {'loss': 2.8452, 'grad_norm': 4.63101863861084, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:21<4:38:38,  2.86s/it]  3%|â–Ž         | 157/6000 [07:24<4:49:13,  2.97s/it]                                                    {'loss': 2.7666, 'grad_norm': 6.072098731994629, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:24<4:49:13,  2.97s/it]  3%|â–Ž         | 158/6000 [07:27<4:40:31,  2.88s/it]                                                    {'loss': 2.7808, 'grad_norm': 2.5820610523223877, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:27<4:40:31,  2.88s/it]  3%|â–Ž         | 159/6000 [07:30<4:36:07,  2.84s/it]                                                    {'loss': 2.7875, 'grad_norm': 3.473503828048706, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:30<4:36:07,  2.84s/it]  3%|â–Ž         | 160/6000 [07:33<4:49:58,  2.98s/it]                                                    {'loss': 2.8067, 'grad_norm': 3.453101873397827, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:33<4:49:58,  2.98s/it]  3%|â–Ž         | 161/6000 [07:36<4:46:01,  2.94s/it]                                                    {'loss': 2.7704, 'grad_norm': 3.8911914825439453, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:36<4:46:01,  2.94s/it]  3%|â–Ž         | 162/6000 [07:39<4:40:18,  2.88s/it]                                                    {'loss': 2.7772, 'grad_norm': 6.220988750457764, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:39<4:40:18,  2.88s/it]  3%|â–Ž         | 163/6000 [07:42<4:53:31,  3.02s/it]                                                    {'loss': 2.7927, 'grad_norm': 7.536847114562988, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:42<4:53:31,  3.02s/it]  3%|â–Ž         | 164/6000 [07:45<4:43:35,  2.92s/it]                                                    {'loss': 2.8989, 'grad_norm': 6.5768961906433105, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:45<4:43:35,  2.92s/it]  3%|â–Ž         | 165/6000 [07:48<4:41:27,  2.89s/it]                                                    {'loss': 2.7675, 'grad_norm': 5.020320415496826, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:48<4:41:27,  2.89s/it]  3%|â–Ž         | 166/6000 [07:50<4:36:21,  2.84s/it]                                                    {'loss': 2.7822, 'grad_norm': 7.1814727783203125, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:50<4:36:21,  2.84s/it]  3%|â–Ž         | 167/6000 [07:53<4:33:59,  2.82s/it]                                                    {'loss': 2.8164, 'grad_norm': 15.64402961730957, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:53<4:33:59,  2.82s/it]  3%|â–Ž         | 168/6000 [07:56<4:32:49,  2.81s/it]                                                    {'loss': 2.8206, 'grad_norm': 6.06265926361084, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:56<4:32:49,  2.81s/it]  3%|â–Ž         | 169/6000 [07:59<4:43:32,  2.92s/it]                                                    {'loss': 2.8066, 'grad_norm': 4.122326850891113, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:59<4:43:32,  2.92s/it]  3%|â–Ž         | 170/6000 [08:02<4:36:10,  2.84s/it]                                                    {'loss': 2.7812, 'grad_norm': 5.9602532386779785, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:02<4:36:10,  2.84s/it]  3%|â–Ž         | 171/6000 [08:04<4:30:30,  2.78s/it]                                                    {'loss': 2.7703, 'grad_norm': 6.282868385314941, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:04<4:30:30,  2.78s/it]  3%|â–Ž         | 172/6000 [08:07<4:32:39,  2.81s/it]                                                    {'loss': 2.79, 'grad_norm': 3.412135601043701, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:07<4:32:39,  2.81s/it]  3%|â–Ž         | 173/6000 [08:10<4:31:06,  2.79s/it]                                                    {'loss': 2.7727, 'grad_norm': 6.529079914093018, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:10<4:31:06,  2.79s/it]  3%|â–Ž         | 174/6000 [08:13<4:52:15,  3.01s/it]                                                    {'loss': 2.7734, 'grad_norm': 6.892768859863281, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:13<4:52:15,  3.01s/it]  3%|â–Ž         | 175/6000 [08:16<4:45:45,  2.94s/it]                                                    {'loss': 2.797, 'grad_norm': 9.814764976501465, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:16<4:45:45,  2.94s/it]  3%|â–Ž         | 176/6000 [08:19<4:49:55,  2.99s/it]                                                    {'loss': 2.7777, 'grad_norm': 7.097207546234131, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:19<4:49:55,  2.99s/it]  3%|â–Ž         | 177/6000 [08:22<4:41:45,  2.90s/it]                                                    {'loss': 2.7853, 'grad_norm': 5.6781325340271, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:22<4:41:45,  2.90s/it]  3%|â–Ž         | 178/6000 [08:25<4:37:21,  2.86s/it]                                                    {'loss': 2.7878, 'grad_norm': 6.195909023284912, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:25<4:37:21,  2.86s/it]  3%|â–Ž         | 179/6000 [08:27<4:32:58,  2.81s/it]                                                    {'loss': 2.7978, 'grad_norm': 9.986573219299316, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:27<4:32:58,  2.81s/it]  3%|â–Ž         | 180/6000 [08:30<4:35:05,  2.84s/it]                                                    {'loss': 2.7882, 'grad_norm': 3.6569132804870605, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:30<4:35:05,  2.84s/it]  3%|â–Ž         | 181/6000 [08:33<4:33:01,  2.82s/it]                                                    {'loss': 2.8022, 'grad_norm': 4.286755084991455, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:33<4:33:01,  2.82s/it]  3%|â–Ž         | 182/6000 [08:36<4:31:37,  2.80s/it]                                                    {'loss': 2.8233, 'grad_norm': 4.384373664855957, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:36<4:31:37,  2.80s/it]  3%|â–Ž         | 183/6000 [08:39<4:33:08,  2.82s/it]                                                    {'loss': 2.7735, 'grad_norm': 4.613771438598633, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:39<4:33:08,  2.82s/it]  3%|â–Ž         | 184/6000 [08:41<4:30:04,  2.79s/it]                                                    {'loss': 2.7923, 'grad_norm': 5.563693523406982, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:41<4:30:04,  2.79s/it]  3%|â–Ž         | 185/6000 [08:44<4:30:42,  2.79s/it]                                                    {'loss': 2.7964, 'grad_norm': 4.878602504730225, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:44<4:30:42,  2.79s/it]  3%|â–Ž         | 186/6000 [08:47<4:30:04,  2.79s/it]                                                    {'loss': 2.779, 'grad_norm': 4.440022945404053, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:47<4:30:04,  2.79s/it]  3%|â–Ž         | 187/6000 [08:50<4:29:02,  2.78s/it]                                                    {'loss': 2.7876, 'grad_norm': 4.891303062438965, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:50<4:29:02,  2.78s/it]  3%|â–Ž         | 188/6000 [08:53<4:27:50,  2.77s/it]                                                    {'loss': 2.7851, 'grad_norm': 4.6993088722229, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:53<4:27:50,  2.77s/it]  3%|â–Ž         | 189/6000 [08:55<4:27:40,  2.76s/it]                                                    {'loss': 2.7841, 'grad_norm': 7.015982151031494, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:55<4:27:40,  2.76s/it]  3%|â–Ž         | 190/6000 [08:58<4:24:59,  2.74s/it]                                                    {'loss': 2.7823, 'grad_norm': 3.8372910022735596, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:58<4:24:59,  2.74s/it]  3%|â–Ž         | 191/6000 [09:01<4:27:39,  2.76s/it]                                                    {'loss': 2.7789, 'grad_norm': 8.64190673828125, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:01<4:27:39,  2.76s/it]  3%|â–Ž         | 192/6000 [09:04<4:44:47,  2.94s/it]                                                    {'loss': 2.8884, 'grad_norm': 12.7933931350708, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:04<4:44:47,  2.94s/it]  3%|â–Ž         | 193/6000 [09:07<4:48:26,  2.98s/it]                                                    {'loss': 2.7807, 'grad_norm': 7.948419094085693, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:07<4:48:26,  2.98s/it]  3%|â–Ž         | 194/6000 [09:10<4:40:07,  2.89s/it]                                                    {'loss': 2.7937, 'grad_norm': 7.54423713684082, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:10<4:40:07,  2.89s/it]  3%|â–Ž         | 195/6000 [09:13<4:37:32,  2.87s/it]                                                    {'loss': 2.8131, 'grad_norm': 3.0123069286346436, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:13<4:37:32,  2.87s/it]  3%|â–Ž         | 196/6000 [09:16<4:38:18,  2.88s/it]                                                    {'loss': 2.7904, 'grad_norm': 1.7112513780593872, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:16<4:38:18,  2.88s/it]  3%|â–Ž         | 197/6000 [09:19<4:38:51,  2.88s/it]                                                    {'loss': 2.7926, 'grad_norm': 1.0758676528930664, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:19<4:38:51,  2.88s/it]  3%|â–Ž         | 198/6000 [09:21<4:38:17,  2.88s/it]                                                    {'loss': 2.7835, 'grad_norm': 1.2295756340026855, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:21<4:38:17,  2.88s/it]  3%|â–Ž         | 199/6000 [09:24<4:32:59,  2.82s/it]                                                    {'loss': 2.7747, 'grad_norm': 1.0867819786071777, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:24<4:32:59,  2.82s/it]  3%|â–Ž         | 200/6000 [09:27<4:45:20,  2.95s/it]                                                    {'loss': 2.7829, 'grad_norm': 1.3765052556991577, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:27<4:45:20,  2.95s/it][2025-10-22 20:24:32,379] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-10-22 20:24:32,533] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:34<6:40:49,  4.15s/it]                                                    {'loss': 2.8031, 'grad_norm': 0.9796223640441895, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:34<6:40:49,  4.15s/it]  3%|â–Ž         | 202/6000 [09:37<6:00:40,  3.73s/it]                                                    {'loss': 2.7789, 'grad_norm': 1.2431185245513916, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:37<6:00:40,  3.73s/it]  3%|â–Ž         | 203/6000 [09:40<5:36:11,  3.48s/it]                                                    {'loss': 2.7894, 'grad_norm': 1.4783697128295898, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:40<5:36:11,  3.48s/it]  3%|â–Ž         | 204/6000 [09:43<5:17:40,  3.29s/it]                                                    {'loss': 2.7785, 'grad_norm': 0.8306596279144287, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:43<5:17:40,  3.29s/it]  3%|â–Ž         | 205/6000 [09:45<4:59:38,  3.10s/it]                                                    {'loss': 2.7723, 'grad_norm': 0.9388056993484497, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:45<4:59:38,  3.10s/it]  3%|â–Ž         | 206/6000 [09:48<4:48:30,  2.99s/it]                                                    {'loss': 2.8238, 'grad_norm': 0.8198786973953247, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:48<4:48:30,  2.99s/it]  3%|â–Ž         | 207/6000 [09:51<4:39:42,  2.90s/it]                                                    {'loss': 2.8688, 'grad_norm': 1.4244425296783447, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:51<4:39:42,  2.90s/it]  3%|â–Ž         | 208/6000 [09:54<4:33:58,  2.84s/it]                                                    {'loss': 2.7761, 'grad_norm': 1.4311851263046265, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:54<4:33:58,  2.84s/it]  3%|â–Ž         | 209/6000 [09:56<4:30:55,  2.81s/it]                                                    {'loss': 2.7833, 'grad_norm': 6.063994407653809, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:56<4:30:55,  2.81s/it]  4%|â–Ž         | 210/6000 [09:59<4:42:00,  2.92s/it]                                                    {'loss': 2.7783, 'grad_norm': 2.132195234298706, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:59<4:42:00,  2.92s/it]  4%|â–Ž         | 211/6000 [10:02<4:41:08,  2.91s/it]                                                    {'loss': 2.7904, 'grad_norm': 1.9542733430862427, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [10:02<4:41:08,  2.91s/it]  4%|â–Ž         | 212/6000 [10:06<4:48:49,  2.99s/it]                                                    {'loss': 2.7691, 'grad_norm': 2.1588103771209717, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:06<4:48:49,  2.99s/it]  4%|â–Ž         | 213/6000 [10:08<4:44:26,  2.95s/it]                                                    {'loss': 2.7836, 'grad_norm': 1.7835458517074585, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:08<4:44:26,  2.95s/it]  4%|â–Ž         | 214/6000 [10:11<4:36:54,  2.87s/it]                                                    {'loss': 2.773, 'grad_norm': 2.92547345161438, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:11<4:36:54,  2.87s/it]  4%|â–Ž         | 215/6000 [10:14<4:33:02,  2.83s/it]                                                    {'loss': 2.8289, 'grad_norm': 25.642658233642578, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:14<4:33:02,  2.83s/it]  4%|â–Ž         | 216/6000 [10:17<4:32:15,  2.82s/it]                                                    {'loss': 2.8022, 'grad_norm': 24.389310836791992, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:17<4:32:15,  2.82s/it]  4%|â–Ž         | 217/6000 [10:19<4:32:29,  2.83s/it]                                                    {'loss': 2.8245, 'grad_norm': 3.6533756256103516, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:19<4:32:29,  2.83s/it]  4%|â–Ž         | 218/6000 [10:22<4:31:27,  2.82s/it]                                                    {'loss': 2.7903, 'grad_norm': 4.641078472137451, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:22<4:31:27,  2.82s/it]  4%|â–Ž         | 219/6000 [10:25<4:26:57,  2.77s/it]                                                    {'loss': 2.8242, 'grad_norm': 2.1152596473693848, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:25<4:26:57,  2.77s/it]  4%|â–Ž         | 220/6000 [10:28<4:25:50,  2.76s/it]                                                    {'loss': 2.7727, 'grad_norm': 1.7154436111450195, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:28<4:25:50,  2.76s/it]  4%|â–Ž         | 221/6000 [10:30<4:25:36,  2.76s/it]                                                    {'loss': 2.804, 'grad_norm': 1.6259206533432007, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:30<4:25:36,  2.76s/it]  4%|â–Ž         | 222/6000 [10:33<4:24:06,  2.74s/it]                                                    {'loss': 2.7716, 'grad_norm': 1.3355408906936646, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:33<4:24:06,  2.74s/it]  4%|â–Ž         | 223/6000 [10:36<4:22:32,  2.73s/it]                                                    {'loss': 2.77, 'grad_norm': 1.2975107431411743, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:36<4:22:32,  2.73s/it]  4%|â–Ž         | 224/6000 [10:38<4:20:03,  2.70s/it]                                                    {'loss': 2.8234, 'grad_norm': 2.135507822036743, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:38<4:20:03,  2.70s/it]  4%|â–         | 225/6000 [10:41<4:21:28,  2.72s/it]                                                    {'loss': 2.7668, 'grad_norm': 2.303654670715332, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:41<4:21:28,  2.72s/it]  4%|â–         | 226/6000 [10:44<4:36:07,  2.87s/it]                                                    {'loss': 2.7755, 'grad_norm': 2.449899196624756, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:44<4:36:07,  2.87s/it]  4%|â–         | 227/6000 [10:47<4:29:59,  2.81s/it]                                                    {'loss': 2.8268, 'grad_norm': 2.2655506134033203, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:47<4:29:59,  2.81s/it]  4%|â–         | 228/6000 [10:50<4:29:09,  2.80s/it]                                                    {'loss': 2.7751, 'grad_norm': 2.92769455909729, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:50<4:29:09,  2.80s/it]  4%|â–         | 229/6000 [10:53<4:29:32,  2.80s/it]                                                    {'loss': 2.7784, 'grad_norm': 1.847387433052063, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:53<4:29:32,  2.80s/it]  4%|â–         | 230/6000 [10:55<4:28:56,  2.80s/it]                                                    {'loss': 2.8062, 'grad_norm': 3.906981945037842, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:55<4:28:56,  2.80s/it]  4%|â–         | 231/6000 [10:58<4:26:26,  2.77s/it]                                                    {'loss': 2.7754, 'grad_norm': 3.8053910732269287, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:58<4:26:26,  2.77s/it]  4%|â–         | 232/6000 [11:01<4:25:57,  2.77s/it]                                                    {'loss': 2.7708, 'grad_norm': 3.856199264526367, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [11:01<4:25:57,  2.77s/it]  4%|â–         | 233/6000 [11:04<4:34:56,  2.86s/it]                                                    {'loss': 2.7798, 'grad_norm': 2.7266733646392822, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:04<4:34:56,  2.86s/it]  4%|â–         | 234/6000 [11:07<4:39:48,  2.91s/it]                                                    {'loss': 2.776, 'grad_norm': 3.906832695007324, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:07<4:39:48,  2.91s/it]  4%|â–         | 235/6000 [11:10<4:31:40,  2.83s/it]                                                    {'loss': 2.85, 'grad_norm': 3.2844326496124268, 'learning_rate': 4.885593220338983e-05, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:10<4:31:40,  2.83s/it]  4%|â–         | 236/6000 [11:12<4:30:05,  2.81s/it]                                                    {'loss': 2.804, 'grad_norm': 5.263612270355225, 'learning_rate': 4.884745762711865e-05, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:12<4:30:05,  2.81s/it]  4%|â–         | 237/6000 [11:15<4:27:19,  2.78s/it]                                                    {'loss': 2.7975, 'grad_norm': 4.756795883178711, 'learning_rate': 4.883898305084746e-05, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:15<4:27:19,  2.78s/it]  4%|â–         | 238/6000 [11:18<4:40:22,  2.92s/it]                                                    {'loss': 2.8463, 'grad_norm': 1.6566975116729736, 'learning_rate': 4.883050847457628e-05, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:18<4:40:22,  2.92s/it]  4%|â–         | 239/6000 [11:21<4:36:38,  2.88s/it]                                                    {'loss': 2.7828, 'grad_norm': 3.039581060409546, 'learning_rate': 4.882203389830508e-05, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:21<4:36:38,  2.88s/it]  4%|â–         | 240/6000 [11:24<4:32:54,  2.84s/it]                                                    {'loss': 2.794, 'grad_norm': 1.5417120456695557, 'learning_rate': 4.88135593220339e-05, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:24<4:32:54,  2.84s/it]  4%|â–         | 241/6000 [11:27<4:29:35,  2.81s/it]                                                    {'loss': 2.7756, 'grad_norm': 2.046166181564331, 'learning_rate': 4.880508474576271e-05, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:27<4:29:35,  2.81s/it]  4%|â–         | 242/6000 [11:29<4:25:58,  2.77s/it]                                                    {'loss': 2.7987, 'grad_norm': 1.5502080917358398, 'learning_rate': 4.879661016949153e-05, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:29<4:25:58,  2.77s/it]  4%|â–         | 243/6000 [11:32<4:33:54,  2.85s/it]                                                    {'loss': 2.7721, 'grad_norm': 1.8571887016296387, 'learning_rate': 4.878813559322034e-05, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:32<4:33:54,  2.85s/it]  4%|â–         | 244/6000 [11:35<4:29:24,  2.81s/it]                                                    {'loss': 2.764, 'grad_norm': 2.625030517578125, 'learning_rate': 4.877966101694916e-05, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:35<4:29:24,  2.81s/it]  4%|â–         | 245/6000 [11:38<4:29:46,  2.81s/it]                                                    {'loss': 2.7823, 'grad_norm': 1.1319575309753418, 'learning_rate': 4.877118644067797e-05, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:38<4:29:46,  2.81s/it]  4%|â–         | 246/6000 [11:41<4:31:58,  2.84s/it]                                                    {'loss': 2.7736, 'grad_norm': 2.2907156944274902, 'learning_rate': 4.876271186440678e-05, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:41<4:31:58,  2.84s/it]  4%|â–         | 247/6000 [11:43<4:27:15,  2.79s/it]                                                    {'loss': 2.7791, 'grad_norm': 2.005984306335449, 'learning_rate': 4.8754237288135593e-05, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:43<4:27:15,  2.79s/it]  4%|â–         | 248/6000 [11:47<4:33:51,  2.86s/it]                                                    {'loss': 2.7753, 'grad_norm': 1.49876070022583, 'learning_rate': 4.8745762711864405e-05, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:47<4:33:51,  2.86s/it]  4%|â–         | 249/6000 [11:49<4:28:23,  2.80s/it]                                                    {'loss': 2.7725, 'grad_norm': 2.0019116401672363, 'learning_rate': 4.873728813559322e-05, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:49<4:28:23,  2.80s/it]  4%|â–         | 250/6000 [11:52<4:26:23,  2.78s/it]                                                    {'loss': 2.8022, 'grad_norm': 2.908724546432495, 'learning_rate': 4.8728813559322034e-05, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:52<4:26:23,  2.78s/it][2025-10-22 20:26:56,941] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
[2025-10-22 20:26:56,944] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [11:57<5:24:06,  3.38s/it]                                                    {'loss': 2.7891, 'grad_norm': 2.2681353092193604, 'learning_rate': 4.872033898305085e-05, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:57<5:24:06,  3.38s/it]  4%|â–         | 252/6000 [12:00<5:10:34,  3.24s/it]                                                    {'loss': 2.7977, 'grad_norm': 2.4777960777282715, 'learning_rate': 4.8711864406779663e-05, 'epoch': 0.04}
  4%|â–         | 252/6000 [12:00<5:10:34,  3.24s/it]  4%|â–         | 253/6000 [12:02<4:56:32,  3.10s/it]                                                    {'loss': 2.8042, 'grad_norm': 4.854219913482666, 'learning_rate': 4.8703389830508475e-05, 'epoch': 0.04}
  4%|â–         | 253/6000 [12:02<4:56:32,  3.10s/it]  4%|â–         | 254/6000 [12:05<4:47:46,  3.01s/it]                                                    {'loss': 2.7682, 'grad_norm': 3.264678716659546, 'learning_rate': 4.8694915254237286e-05, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:05<4:47:46,  3.01s/it]  4%|â–         | 255/6000 [12:08<4:37:48,  2.90s/it]                                                    {'loss': 2.7836, 'grad_norm': 3.631281614303589, 'learning_rate': 4.8686440677966104e-05, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:08<4:37:48,  2.90s/it]  4%|â–         | 256/6000 [12:11<4:45:35,  2.98s/it]                                                    {'loss': 2.7899, 'grad_norm': 2.9206204414367676, 'learning_rate': 4.8677966101694915e-05, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:11<4:45:35,  2.98s/it]  4%|â–         | 257/6000 [12:14<4:38:13,  2.91s/it]                                                    {'loss': 2.7709, 'grad_norm': 1.6554033756256104, 'learning_rate': 4.8669491525423733e-05, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:14<4:38:13,  2.91s/it]  4%|â–         | 258/6000 [12:16<4:33:07,  2.85s/it]                                                    {'loss': 2.8018, 'grad_norm': 7.162033557891846, 'learning_rate': 4.8661016949152545e-05, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:16<4:33:07,  2.85s/it]  4%|â–         | 259/6000 [12:19<4:28:54,  2.81s/it]                                                    {'loss': 2.7783, 'grad_norm': 4.492692947387695, 'learning_rate': 4.865254237288136e-05, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:19<4:28:54,  2.81s/it]  4%|â–         | 260/6000 [12:22<4:25:50,  2.78s/it]                                                    {'loss': 2.8054, 'grad_norm': 5.467646598815918, 'learning_rate': 4.8644067796610174e-05, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:22<4:25:50,  2.78s/it]  4%|â–         | 261/6000 [12:25<4:29:20,  2.82s/it]                                                    {'loss': 2.7873, 'grad_norm': 1.7064380645751953, 'learning_rate': 4.8635593220338985e-05, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:25<4:29:20,  2.82s/it]  4%|â–         | 262/6000 [12:28<4:28:57,  2.81s/it]                                                    {'loss': 2.7796, 'grad_norm': 2.521291732788086, 'learning_rate': 4.86271186440678e-05, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:28<4:28:57,  2.81s/it]  4%|â–         | 263/6000 [12:31<4:37:23,  2.90s/it]                                                    {'loss': 2.8429, 'grad_norm': 1.6857643127441406, 'learning_rate': 4.8618644067796615e-05, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:31<4:37:23,  2.90s/it]  4%|â–         | 264/6000 [12:33<4:30:10,  2.83s/it]                                                    {'loss': 2.7812, 'grad_norm': 2.1153855323791504, 'learning_rate': 4.8610169491525426e-05, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:33<4:30:10,  2.83s/it]  4%|â–         | 265/6000 [12:36<4:32:13,  2.85s/it]                                                    {'loss': 2.7807, 'grad_norm': 1.2731637954711914, 'learning_rate': 4.8601694915254244e-05, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:36<4:32:13,  2.85s/it]  4%|â–         | 266/6000 [12:39<4:31:53,  2.85s/it]                                                    {'loss': 2.8033, 'grad_norm': 1.4182486534118652, 'learning_rate': 4.8593220338983055e-05, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:39<4:31:53,  2.85s/it]  4%|â–         | 267/6000 [12:42<4:26:43,  2.79s/it]                                                    {'loss': 2.7719, 'grad_norm': 1.408191442489624, 'learning_rate': 4.858474576271187e-05, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:42<4:26:43,  2.79s/it]  4%|â–         | 268/6000 [12:44<4:24:36,  2.77s/it]                                                    {'loss': 2.8403, 'grad_norm': 1.4944802522659302, 'learning_rate': 4.857627118644068e-05, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:44<4:24:36,  2.77s/it]  4%|â–         | 269/6000 [12:47<4:27:27,  2.80s/it]                                                    {'loss': 2.7614, 'grad_norm': 2.8739161491394043, 'learning_rate': 4.856779661016949e-05, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:47<4:27:27,  2.80s/it]  4%|â–         | 270/6000 [12:50<4:24:46,  2.77s/it]                                                    {'loss': 2.7851, 'grad_norm': 2.541168689727783, 'learning_rate': 4.855932203389831e-05, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:50<4:24:46,  2.77s/it]  5%|â–         | 271/6000 [12:53<4:21:39,  2.74s/it]                                                    {'loss': 2.7943, 'grad_norm': 2.0608861446380615, 'learning_rate': 4.855084745762712e-05, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:53<4:21:39,  2.74s/it]  5%|â–         | 272/6000 [12:55<4:19:21,  2.72s/it]                                                    {'loss': 2.7879, 'grad_norm': 3.1836471557617188, 'learning_rate': 4.8542372881355937e-05, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:55<4:19:21,  2.72s/it]  5%|â–         | 273/6000 [12:58<4:18:46,  2.71s/it]                                                    {'loss': 2.8049, 'grad_norm': 1.7192577123641968, 'learning_rate': 4.853389830508475e-05, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:58<4:18:46,  2.71s/it]  5%|â–         | 274/6000 [13:01<4:29:12,  2.82s/it]                                                    {'loss': 2.7763, 'grad_norm': 2.340169906616211, 'learning_rate': 4.8525423728813566e-05, 'epoch': 0.05}
  5%|â–         | 274/6000 [13:01<4:29:12,  2.82s/it]  5%|â–         | 275/6000 [13:04<4:26:18,  2.79s/it]                                                    {'loss': 2.7925, 'grad_norm': 1.623023271560669, 'learning_rate': 4.851694915254237e-05, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:04<4:26:18,  2.79s/it]  5%|â–         | 276/6000 [13:07<4:24:55,  2.78s/it]                                                    {'loss': 2.8415, 'grad_norm': 2.341083526611328, 'learning_rate': 4.850847457627119e-05, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:07<4:24:55,  2.78s/it]  5%|â–         | 277/6000 [13:09<4:20:30,  2.73s/it]                                                    {'loss': 2.778, 'grad_norm': 1.5940214395523071, 'learning_rate': 4.85e-05, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:09<4:20:30,  2.73s/it]  5%|â–         | 278/6000 [13:12<4:18:43,  2.71s/it]                                                    {'loss': 2.7779, 'grad_norm': 1.7187060117721558, 'learning_rate': 4.849152542372882e-05, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:12<4:18:43,  2.71s/it]  5%|â–         | 279/6000 [13:15<4:20:13,  2.73s/it]                                                    {'loss': 2.8018, 'grad_norm': 2.3276655673980713, 'learning_rate': 4.848305084745763e-05, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:15<4:20:13,  2.73s/it]  5%|â–         | 280/6000 [13:17<4:18:44,  2.71s/it]                                                    {'loss': 2.7867, 'grad_norm': 2.289210081100464, 'learning_rate': 4.847457627118645e-05, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:17<4:18:44,  2.71s/it]  5%|â–         | 281/6000 [13:20<4:20:26,  2.73s/it]                                                    {'loss': 2.778, 'grad_norm': 1.446225643157959, 'learning_rate': 4.846610169491526e-05, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:20<4:20:26,  2.73s/it]  5%|â–         | 282/6000 [13:23<4:31:33,  2.85s/it]                                                    {'loss': 2.7808, 'grad_norm': 2.0188753604888916, 'learning_rate': 4.845762711864407e-05, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:23<4:31:33,  2.85s/it]  5%|â–         | 283/6000 [13:26<4:38:02,  2.92s/it]                                                    {'loss': 2.7702, 'grad_norm': 2.2887210845947266, 'learning_rate': 4.844915254237288e-05, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:26<4:38:02,  2.92s/it]  5%|â–         | 284/6000 [13:30<4:52:11,  3.07s/it]                                                    {'loss': 2.7813, 'grad_norm': 3.0732176303863525, 'learning_rate': 4.84406779661017e-05, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:30<4:52:11,  3.07s/it]  5%|â–         | 285/6000 [13:33<4:47:43,  3.02s/it]                                                    {'loss': 2.7694, 'grad_norm': 1.7994308471679688, 'learning_rate': 4.843220338983051e-05, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:33<4:47:43,  3.02s/it]  5%|â–         | 286/6000 [13:35<4:36:56,  2.91s/it]                                                    {'loss': 2.7812, 'grad_norm': 1.7796021699905396, 'learning_rate': 4.842372881355933e-05, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:35<4:36:56,  2.91s/it]  5%|â–         | 287/6000 [13:38<4:32:18,  2.86s/it]                                                    {'loss': 2.7953, 'grad_norm': 2.064307451248169, 'learning_rate': 4.841525423728814e-05, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:38<4:32:18,  2.86s/it]  5%|â–         | 288/6000 [13:41<4:28:39,  2.82s/it]                                                    {'loss': 2.7668, 'grad_norm': 1.8449052572250366, 'learning_rate': 4.840677966101695e-05, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:41<4:28:39,  2.82s/it]  5%|â–         | 289/6000 [13:44<4:29:35,  2.83s/it]                                                    {'loss': 2.764, 'grad_norm': 1.7810351848602295, 'learning_rate': 4.839830508474576e-05, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:44<4:29:35,  2.83s/it]  5%|â–         | 290/6000 [13:46<4:26:44,  2.80s/it]                                                    {'loss': 2.7971, 'grad_norm': 1.796352744102478, 'learning_rate': 4.8389830508474574e-05, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:46<4:26:44,  2.80s/it]  5%|â–         | 291/6000 [13:49<4:22:32,  2.76s/it]                                                    {'loss': 2.7874, 'grad_norm': 8.524092674255371, 'learning_rate': 4.838135593220339e-05, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:49<4:22:32,  2.76s/it]  5%|â–         | 292/6000 [13:52<4:21:07,  2.74s/it]                                                    {'loss': 2.7804, 'grad_norm': 3.0204050540924072, 'learning_rate': 4.83728813559322e-05, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:52<4:21:07,  2.74s/it]  5%|â–         | 293/6000 [13:54<4:18:57,  2.72s/it]                                                    {'loss': 2.8418, 'grad_norm': 1.4787309169769287, 'learning_rate': 4.836440677966102e-05, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:54<4:18:57,  2.72s/it]  5%|â–         | 294/6000 [13:57<4:18:20,  2.72s/it]                                                    {'loss': 2.7847, 'grad_norm': 1.9239963293075562, 'learning_rate': 4.835593220338983e-05, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:57<4:18:20,  2.72s/it]  5%|â–         | 295/6000 [14:00<4:18:57,  2.72s/it]                                                    {'loss': 2.7782, 'grad_norm': 3.1002140045166016, 'learning_rate': 4.834745762711865e-05, 'epoch': 0.05}
  5%|â–         | 295/6000 [14:00<4:18:57,  2.72s/it]  5%|â–         | 296/6000 [14:03<4:22:24,  2.76s/it]                                                    {'loss': 2.7929, 'grad_norm': 7.939688682556152, 'learning_rate': 4.833898305084746e-05, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:03<4:22:24,  2.76s/it]  5%|â–         | 297/6000 [14:05<4:20:50,  2.74s/it]                                                    {'loss': 2.7966, 'grad_norm': 5.7377824783325195, 'learning_rate': 4.833050847457627e-05, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:05<4:20:50,  2.74s/it]  5%|â–         | 298/6000 [14:08<4:25:25,  2.79s/it]                                                    {'loss': 2.7838, 'grad_norm': 1.4600610733032227, 'learning_rate': 4.8322033898305084e-05, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:08<4:25:25,  2.79s/it]  5%|â–         | 299/6000 [14:12<4:48:19,  3.03s/it]                                                    {'loss': 2.7947, 'grad_norm': 4.188002586364746, 'learning_rate': 4.83135593220339e-05, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:12<4:48:19,  3.03s/it]  5%|â–Œ         | 300/6000 [14:15<4:38:48,  2.93s/it]                                                    {'loss': 2.7744, 'grad_norm': 2.1520819664001465, 'learning_rate': 4.8305084745762714e-05, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:15<4:38:48,  2.93s/it][2025-10-22 20:29:19,637] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
[2025-10-22 20:29:19,639] INFO [src.utils:19]   Saved tail_token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/test3-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [14:20<5:37:43,  3.56s/it]                                                    {'loss': 2.8106, 'grad_norm': 4.884154796600342, 'learning_rate': 4.829661016949153e-05, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:20<5:37:43,  3.56s/it]  5%|â–Œ         | 302/6000 [14:22<5:14:57,  3.32s/it]                                                    {'loss': 2.7746, 'grad_norm': 1.965882420539856, 'learning_rate': 4.828813559322034e-05, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:22<5:14:57,  3.32s/it]  5%|â–Œ         | 303/6000 [14:25<4:58:49,  3.15s/it]                                                    {'loss': 2.7957, 'grad_norm': 5.756857872009277, 'learning_rate': 4.8279661016949154e-05, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:25<4:58:49,  3.15s/it]  5%|â–Œ         | 304/6000 [14:28<4:47:11,  3.03s/it]                                                    {'loss': 2.8581, 'grad_norm': 8.956212997436523, 'learning_rate': 4.8271186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:28<4:47:11,  3.03s/it]  5%|â–Œ         | 305/6000 [14:31<4:42:56,  2.98s/it]                                                    {'loss': 2.7704, 'grad_norm': 3.8761110305786133, 'learning_rate': 4.8262711864406784e-05, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:31<4:42:56,  2.98s/it]  5%|â–Œ         | 306/6000 [14:33<4:34:46,  2.90s/it]                                                    {'loss': 2.7715, 'grad_norm': 3.919635057449341, 'learning_rate': 4.8254237288135595e-05, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:33<4:34:46,  2.90s/it]  5%|â–Œ         | 307/6000 [14:36<4:28:28,  2.83s/it]                                                    {'loss': 2.7837, 'grad_norm': 2.5913259983062744, 'learning_rate': 4.824576271186441e-05, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:36<4:28:28,  2.83s/it]  5%|â–Œ         | 308/6000 [14:39<4:23:34,  2.78s/it]                                                    {'loss': 2.7831, 'grad_norm': 1.1913292407989502, 'learning_rate': 4.8237288135593224e-05, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:39<4:23:34,  2.78s/it]  5%|â–Œ         | 309/6000 [14:42<4:25:59,  2.80s/it]                                                    {'loss': 2.778, 'grad_norm': 1.36054527759552, 'learning_rate': 4.8228813559322036e-05, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:42<4:25:59,  2.80s/it]  5%|â–Œ         | 310/6000 [14:44<4:22:38,  2.77s/it]                                                    {'loss': 2.8133, 'grad_norm': 2.478688955307007, 'learning_rate': 4.822033898305085e-05, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:44<4:22:38,  2.77s/it]  5%|â–Œ         | 311/6000 [14:47<4:20:36,  2.75s/it]                                                    {'loss': 2.7743, 'grad_norm': 3.4322705268859863, 'learning_rate': 4.821186440677966e-05, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:47<4:20:36,  2.75s/it]  5%|â–Œ         | 312/6000 [14:50<4:19:20,  2.74s/it]                                                    {'loss': 2.7918, 'grad_norm': 3.4862782955169678, 'learning_rate': 4.8203389830508476e-05, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:50<4:19:20,  2.74s/it]  5%|â–Œ         | 313/6000 [14:52<4:18:53,  2.73s/it]                                                    {'loss': 2.8507, 'grad_norm': 2.8354241847991943, 'learning_rate': 4.819491525423729e-05, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:52<4:18:53,  2.73s/it]  5%|â–Œ         | 314/6000 [14:55<4:18:54,  2.73s/it]                                                    {'loss': 2.814, 'grad_norm': 4.902693271636963, 'learning_rate': 4.8186440677966105e-05, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:55<4:18:54,  2.73s/it]  5%|â–Œ         | 315/6000 [14:58<4:17:26,  2.72s/it]                                                    {'loss': 2.7724, 'grad_norm': 2.8070178031921387, 'learning_rate': 4.817796610169492e-05, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:58<4:17:26,  2.72s/it]  5%|â–Œ         | 316/6000 [15:01<4:16:54,  2.71s/it]                                                    {'loss': 2.7861, 'grad_norm': 2.0792315006256104, 'learning_rate': 4.8169491525423735e-05, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [15:01<4:16:54,  2.71s/it]  5%|â–Œ         | 317/6000 [15:03<4:21:08,  2.76s/it]                                                    {'loss': 2.8414, 'grad_norm': 2.3424232006073, 'learning_rate': 4.8161016949152546e-05, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [15:03<4:21:08,  2.76s/it]  5%|â–Œ         | 318/6000 [15:06<4:29:11,  2.84s/it]                                                    {'loss': 2.7653, 'grad_norm': 2.09177827835083, 'learning_rate': 4.815254237288136e-05, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [15:06<4:29:11,  2.84s/it]  5%|â–Œ         | 319/6000 [15:09<4:25:21,  2.80s/it]                                                    {'loss': 2.7634, 'grad_norm': 2.3135547637939453, 'learning_rate': 4.814406779661017e-05, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [15:09<4:25:21,  2.80s/it]  5%|â–Œ         | 320/6000 [15:12<4:24:34,  2.79s/it]                                                    {'loss': 2.765, 'grad_norm': 9.830443382263184, 'learning_rate': 4.813559322033899e-05, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:12<4:24:34,  2.79s/it]  5%|â–Œ         | 321/6000 [15:15<4:33:50,  2.89s/it]                                                    {'loss': 2.7921, 'grad_norm': 2.499556541442871, 'learning_rate': 4.81271186440678e-05, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:15<4:33:50,  2.89s/it]  5%|â–Œ         | 322/6000 [15:18<4:28:42,  2.84s/it]                                                    {'loss': 2.7916, 'grad_norm': 3.4417381286621094, 'learning_rate': 4.8118644067796616e-05, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:18<4:28:42,  2.84s/it]  5%|â–Œ         | 323/6000 [15:21<4:41:48,  2.98s/it]                                                    {'loss': 2.7614, 'grad_norm': 2.981611490249634, 'learning_rate': 4.811016949152543e-05, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:21<4:41:48,  2.98s/it]  5%|â–Œ         | 324/6000 [15:24<4:31:58,  2.88s/it]                                                    {'loss': 2.8041, 'grad_norm': 1.0946050882339478, 'learning_rate': 4.810169491525424e-05, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:24<4:31:58,  2.88s/it]  5%|â–Œ         | 325/6000 [15:26<4:26:36,  2.82s/it]                                                    {'loss': 2.7809, 'grad_norm': 1.642179012298584, 'learning_rate': 4.809322033898305e-05, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:26<4:26:36,  2.82s/it]  5%|â–Œ         | 326/6000 [15:29<4:23:03,  2.78s/it]                                                    {'loss': 2.7863, 'grad_norm': 2.7259867191314697, 'learning_rate': 4.808474576271187e-05, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:29<4:23:03,  2.78s/it]  5%|â–Œ         | 327/6000 [15:32<4:18:50,  2.74s/it]                                                    {'loss': 2.7732, 'grad_norm': 1.8318955898284912, 'learning_rate': 4.807627118644068e-05, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:32<4:18:50,  2.74s/it]