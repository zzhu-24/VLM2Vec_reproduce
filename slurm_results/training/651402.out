==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 17:22:22.291000 124238778660672 torch/distributed/run.py:779] 
W1103 17:22:22.291000 124238778660672 torch/distributed/run.py:779] *****************************************
W1103 17:22:22.291000 124238778660672 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 17:22:22.291000 124238778660672 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 17:22:32,748] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.98it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.68it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_172233-c07hbl7p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/c07hbl7p
[2025-11-03 17:22:34,267] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.21it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.98it/s]
[2025-11-03 17:22:34,894] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 17:22:44,040] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 17:22:45,173] INFO [src.utils:19] PeftModel(
  (base_model): LoraModel(
    (model): Qwen2VLForConditionalGeneration(
      (visual): Qwen2VisionTransformerPretrainedModel(
        (patch_embed): PatchEmbed(
          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
        )
        (rotary_pos_emb): VisionRotaryEmbedding()
        (blocks): ModuleList(
          (0-31): 32 x Qwen2VLVisionBlock(
            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (attn): VisionFlashAttention2(
              (qkv): Linear(in_features=1280, out_features=3840, bias=True)
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (mlp): VisionMlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): QuickGELUActivation()
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (merger): PatchMerger(
          (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (mlp): Sequential(
            (0): Linear(in_features=5120, out_features=5120, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=5120, out_features=1536, bias=True)
          )
        )
      )
      (model): Qwen2VLModel(
        (embed_tokens): Embedding(151936, 1536)
        (layers): ModuleList(
          (0-27): 28 x Qwen2VLDecoderLayer(
            (self_attn): Qwen2VLFlashAttention2(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (rotary_emb): Qwen2VLRotaryEmbedding()
            )
            (mlp): Qwen2MLP(
              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8960, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (act_fn): SiLU()
            )
            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          )
        )
        (norm): Qwen2RMSNorm((1536,), eps=1e-06)
        (rotary_emb): Qwen2VLRotaryEmbedding()
      )
      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
    )
  )
)
[2025-11-03 17:22:45,182] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 17:22:45,183] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 17:22:49,666] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 17:22:49,667] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 17:22:50,515] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 17:22:50,516] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 17:22:50,516] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 17:22:50,518] INFO [src.utils:19] ==================================================
[2025-11-03 17:22:50,518] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 17:22:50,519] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 17:22:50,520] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 17:22:50,520] INFO [src.utils:19] ==================================================
[2025-11-03 17:22:52,292] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 17:22:52,293] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 17:22:52,294] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 17:22:52,294] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 17:22:52,294] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 17:22:52,294] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 17:22:52,295] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 17:22:52,306] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 17:22:52,308] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 17:22:52,308] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 17:22:52,308] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 17:22:52,308] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 17:22:52,308] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 17:22:52,308] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 17:22:52,308] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 17:22:52,313] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 17:22:52,317] INFO [src.trainer:351]   Number of trainable parameters = 9,205,248
[2025-11-03 17:22:52,328] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1103 17:22:55.089309489 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1103 17:22:55.115632690 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:50:54,  4.11s/it]                                                  {'loss': 11.1348, 'grad_norm': 1790.6873779296875, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:50:54,  4.11s/it]  0%|          | 2/6000 [00:06<5:25:56,  3.26s/it]                                                  {'loss': 8.7174, 'grad_norm': 1515.6126708984375, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:25:56,  3.26s/it]  0%|          | 3/6000 [00:09<5:04:24,  3.05s/it]                                                  {'loss': 9.1776, 'grad_norm': 1554.2205810546875, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:04:24,  3.05s/it]  0%|          | 4/6000 [00:12<4:50:49,  2.91s/it]                                                  {'loss': 8.8576, 'grad_norm': 1541.6761474609375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:50:49,  2.91s/it]  0%|          | 5/6000 [00:14<4:42:30,  2.83s/it]                                                  {'loss': 9.2765, 'grad_norm': 1625.68798828125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:42:30,  2.83s/it]  0%|          | 6/6000 [00:17<4:37:23,  2.78s/it]                                                  {'loss': 9.2445, 'grad_norm': 1614.6734619140625, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:37:23,  2.78s/it]  0%|          | 7/6000 [00:20<4:32:57,  2.73s/it]                                                  {'loss': 9.959, 'grad_norm': 1599.92138671875, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:32:57,  2.73s/it]  0%|          | 8/6000 [00:22<4:28:40,  2.69s/it]                                                  {'loss': 9.9377, 'grad_norm': 1495.74169921875, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:28:40,  2.69s/it]  0%|          | 9/6000 [00:25<4:30:36,  2.71s/it]                                                  {'loss': 7.1755, 'grad_norm': 1020.5604858398438, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:30:36,  2.71s/it]  0%|          | 10/6000 [00:28<4:28:20,  2.69s/it]                                                   {'loss': 8.7551, 'grad_norm': 1267.7630615234375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:28:20,  2.69s/it]  0%|          | 11/6000 [00:31<4:38:31,  2.79s/it]                                                   {'loss': 9.6309, 'grad_norm': 1559.672607421875, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:38:31,  2.79s/it]  0%|          | 12/6000 [00:34<4:41:20,  2.82s/it]                                                   {'loss': 8.032, 'grad_norm': 1279.9541015625, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:41:20,  2.82s/it]  0%|          | 13/6000 [00:36<4:40:10,  2.81s/it]                                                   {'loss': 8.0248, 'grad_norm': 1043.7833251953125, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:40:10,  2.81s/it]  0%|          | 14/6000 [00:39<4:39:50,  2.80s/it]                                                   {'loss': 7.3123, 'grad_norm': 1084.34228515625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:39<4:39:50,  2.80s/it]  0%|          | 15/6000 [00:42<4:36:38,  2.77s/it]                                                   {'loss': 6.116, 'grad_norm': 694.2456665039062, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:36:38,  2.77s/it]  0%|          | 16/6000 [00:45<4:33:34,  2.74s/it]                                                   {'loss': 6.3087, 'grad_norm': 596.2648315429688, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:33:34,  2.74s/it]  0%|          | 17/6000 [00:47<4:33:31,  2.74s/it]                                                   {'loss': 5.4679, 'grad_norm': 410.58056640625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:47<4:33:31,  2.74s/it]  0%|          | 18/6000 [00:50<4:33:24,  2.74s/it]                                                   {'loss': 5.0075, 'grad_norm': 337.3418273925781, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:33:24,  2.74s/it]  0%|          | 19/6000 [00:53<4:30:59,  2.72s/it]                                                   {'loss': 5.5038, 'grad_norm': 431.2203369140625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:30:59,  2.72s/it]  0%|          | 20/6000 [00:55<4:30:04,  2.71s/it]                                                   {'loss': 5.2415, 'grad_norm': 533.6963500976562, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:55<4:30:04,  2.71s/it]  0%|          | 21/6000 [00:58<4:34:47,  2.76s/it]                                                   {'loss': 4.3013, 'grad_norm': 495.9232482910156, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:58<4:34:47,  2.76s/it]  0%|          | 22/6000 [01:01<4:36:23,  2.77s/it]                                                   {'loss': 5.1261, 'grad_norm': 508.47430419921875, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:36:23,  2.77s/it]  0%|          | 23/6000 [01:04<4:35:24,  2.76s/it]                                                   {'loss': 4.3179, 'grad_norm': 439.6645812988281, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:35:24,  2.76s/it]  0%|          | 24/6000 [01:07<4:35:26,  2.77s/it]                                                   {'loss': 3.6114, 'grad_norm': 234.62791442871094, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:35:26,  2.77s/it]  0%|          | 25/6000 [01:09<4:33:34,  2.75s/it]                                                   {'loss': 3.7452, 'grad_norm': 418.5331115722656, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:09<4:33:34,  2.75s/it]  0%|          | 26/6000 [01:12<4:34:46,  2.76s/it]                                                   {'loss': 3.5288, 'grad_norm': 299.0206298828125, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:12<4:34:46,  2.76s/it]  0%|          | 27/6000 [01:15<4:34:40,  2.76s/it]                                                   {'loss': 3.7005, 'grad_norm': 147.68789672851562, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:34:40,  2.76s/it]  0%|          | 28/6000 [01:19<5:01:07,  3.03s/it]                                                   {'loss': 3.3205, 'grad_norm': 128.17027282714844, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:19<5:01:07,  3.03s/it]  0%|          | 29/6000 [01:21<4:51:03,  2.92s/it]                                                   {'loss': 3.3582, 'grad_norm': 129.19996643066406, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:21<4:51:03,  2.92s/it]  0%|          | 30/6000 [01:24<4:46:07,  2.88s/it]                                                   {'loss': 3.2047, 'grad_norm': 87.35380554199219, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:46:07,  2.88s/it]  1%|          | 31/6000 [01:27<4:41:09,  2.83s/it]                                                   {'loss': 3.1364, 'grad_norm': 88.29024505615234, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:27<4:41:09,  2.83s/it]  1%|          | 32/6000 [01:29<4:38:38,  2.80s/it]                                                   {'loss': 3.177, 'grad_norm': 106.21746063232422, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:29<4:38:38,  2.80s/it]  1%|          | 33/6000 [01:32<4:36:59,  2.79s/it]                                                   {'loss': 3.0367, 'grad_norm': 84.1380615234375, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:32<4:36:59,  2.79s/it]  1%|          | 34/6000 [01:35<4:34:24,  2.76s/it]                                                   {'loss': 3.4073, 'grad_norm': 116.67500305175781, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:34:24,  2.76s/it]  1%|          | 35/6000 [01:38<4:34:36,  2.76s/it]                                                   {'loss': 3.2642, 'grad_norm': 132.3817901611328, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:38<4:34:36,  2.76s/it]  1%|          | 36/6000 [01:40<4:31:35,  2.73s/it]                                                   {'loss': 3.1842, 'grad_norm': 76.4229965209961, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:40<4:31:35,  2.73s/it]  1%|          | 37/6000 [01:43<4:32:30,  2.74s/it]                                                   {'loss': 2.9305, 'grad_norm': 57.66641616821289, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:32:30,  2.74s/it]  1%|          | 38/6000 [01:46<4:30:24,  2.72s/it]                                                   {'loss': 3.0598, 'grad_norm': 59.3463249206543, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:46<4:30:24,  2.72s/it]  1%|          | 39/6000 [01:48<4:30:31,  2.72s/it]                                                   {'loss': 2.9854, 'grad_norm': 49.41105270385742, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:48<4:30:31,  2.72s/it]  1%|          | 40/6000 [01:51<4:30:10,  2.72s/it]                                                   {'loss': 3.1328, 'grad_norm': 64.24708557128906, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:51<4:30:10,  2.72s/it]  1%|          | 41/6000 [01:54<4:30:06,  2.72s/it]                                                   {'loss': 2.9116, 'grad_norm': 57.879310607910156, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:54<4:30:06,  2.72s/it]  1%|          | 42/6000 [01:57<4:30:39,  2.73s/it]                                                   {'loss': 3.1407, 'grad_norm': 81.9634780883789, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:57<4:30:39,  2.73s/it]  1%|          | 43/6000 [02:00<5:01:38,  3.04s/it]                                                   {'loss': 2.8687, 'grad_norm': 25.97345733642578, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:00<5:01:38,  3.04s/it]  1%|          | 44/6000 [02:04<5:07:00,  3.09s/it]                                                   {'loss': 2.8755, 'grad_norm': 36.04441452026367, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:04<5:07:00,  3.09s/it]  1%|          | 45/6000 [02:06<4:57:03,  2.99s/it]                                                   {'loss': 3.1313, 'grad_norm': 55.446205139160156, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:06<4:57:03,  2.99s/it]  1%|          | 46/6000 [02:09<4:53:58,  2.96s/it]                                                   {'loss': 2.8697, 'grad_norm': 27.553104400634766, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:09<4:53:58,  2.96s/it]  1%|          | 47/6000 [02:12<4:47:34,  2.90s/it]                                                   {'loss': 2.92, 'grad_norm': 50.34490203857422, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:12<4:47:34,  2.90s/it]  1%|          | 48/6000 [02:15<4:45:57,  2.88s/it]                                                   {'loss': 2.9449, 'grad_norm': 42.72852325439453, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:15<4:45:57,  2.88s/it]  1%|          | 49/6000 [02:18<4:39:25,  2.82s/it]                                                   {'loss': 2.913, 'grad_norm': 38.45360565185547, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:18<4:39:25,  2.82s/it]  1%|          | 50/6000 [02:20<4:42:46,  2.85s/it]                                                   {'loss': 2.9173, 'grad_norm': 37.96518325805664, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:21<4:42:46,  2.85s/it]  1%|          | 51/6000 [02:23<4:38:36,  2.81s/it]                                                   {'loss': 2.9242, 'grad_norm': 55.81700134277344, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:23<4:38:36,  2.81s/it]  1%|          | 52/6000 [02:26<4:34:34,  2.77s/it]                                                   {'loss': 2.9289, 'grad_norm': 36.66206359863281, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:26<4:34:34,  2.77s/it]  1%|          | 53/6000 [02:29<4:34:51,  2.77s/it]                                                   {'loss': 3.1986, 'grad_norm': 69.60537719726562, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:29<4:34:51,  2.77s/it]  1%|          | 54/6000 [02:31<4:31:37,  2.74s/it]                                                   {'loss': 2.9483, 'grad_norm': 32.6512336730957, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:31<4:31:37,  2.74s/it]  1%|          | 55/6000 [02:34<4:31:30,  2.74s/it]                                                   {'loss': 2.8855, 'grad_norm': 31.47579002380371, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:34<4:31:30,  2.74s/it]  1%|          | 56/6000 [02:37<4:32:09,  2.75s/it]                                                   {'loss': 2.8927, 'grad_norm': 31.916292190551758, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:37<4:32:09,  2.75s/it]  1%|          | 57/6000 [02:40<4:30:35,  2.73s/it]                                                   {'loss': 2.854, 'grad_norm': 27.550024032592773, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:40<4:30:35,  2.73s/it]  1%|          | 58/6000 [02:42<4:32:02,  2.75s/it]                                                   {'loss': 2.931, 'grad_norm': 50.63734436035156, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:42<4:32:02,  2.75s/it]  1%|          | 59/6000 [02:45<4:29:12,  2.72s/it]                                                   {'loss': 2.8758, 'grad_norm': 29.909379959106445, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [02:45<4:29:12,  2.72s/it]  1%|          | 60/6000 [02:48<4:28:35,  2.71s/it]                                                   {'loss': 2.8275, 'grad_norm': 21.824298858642578, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:48<4:28:35,  2.71s/it]  1%|          | 61/6000 [02:50<4:28:12,  2.71s/it]                                                   {'loss': 2.8488, 'grad_norm': 20.083110809326172, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:50<4:28:12,  2.71s/it]  1%|          | 62/6000 [02:53<4:30:36,  2.73s/it]                                                   {'loss': 2.868, 'grad_norm': 25.674274444580078, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:53<4:30:36,  2.73s/it]  1%|          | 63/6000 [02:56<4:29:13,  2.72s/it]                                                   {'loss': 2.8198, 'grad_norm': 17.177806854248047, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:56<4:29:13,  2.72s/it]  1%|          | 64/6000 [02:59<4:27:24,  2.70s/it]                                                   {'loss': 2.7936, 'grad_norm': 14.984057426452637, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:59<4:27:24,  2.70s/it]  1%|          | 65/6000 [03:01<4:26:28,  2.69s/it]                                                   {'loss': 2.8268, 'grad_norm': 12.436415672302246, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:01<4:26:28,  2.69s/it]  1%|          | 66/6000 [03:04<4:40:28,  2.84s/it]                                                   {'loss': 2.8225, 'grad_norm': 17.038806915283203, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:04<4:40:28,  2.84s/it]  1%|          | 67/6000 [03:07<4:35:47,  2.79s/it]                                                   {'loss': 2.8481, 'grad_norm': 21.38487434387207, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:07<4:35:47,  2.79s/it]  1%|          | 68/6000 [03:10<4:31:43,  2.75s/it]                                                   {'loss': 2.8123, 'grad_norm': 19.13060760498047, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:10<4:31:43,  2.75s/it]  1%|          | 69/6000 [03:12<4:29:50,  2.73s/it]                                                   {'loss': 2.8494, 'grad_norm': 19.352258682250977, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:12<4:29:50,  2.73s/it]  1%|          | 70/6000 [03:15<4:32:20,  2.76s/it]                                                   {'loss': 2.8497, 'grad_norm': 21.86101722717285, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:15<4:32:20,  2.76s/it]  1%|          | 71/6000 [03:18<4:29:08,  2.72s/it]                                                   {'loss': 2.8104, 'grad_norm': 24.511091232299805, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:18<4:29:08,  2.72s/it]  1%|          | 72/6000 [03:21<4:40:18,  2.84s/it]                                                   {'loss': 2.8332, 'grad_norm': 24.576988220214844, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:21<4:40:18,  2.84s/it]  1%|          | 73/6000 [03:24<4:39:20,  2.83s/it]                                                   {'loss': 2.8453, 'grad_norm': 12.961730003356934, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:24<4:39:20,  2.83s/it]  1%|          | 74/6000 [03:26<4:34:50,  2.78s/it]                                                   {'loss': 2.8255, 'grad_norm': 17.14335060119629, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:26<4:34:50,  2.78s/it]  1%|â–         | 75/6000 [03:29<4:39:52,  2.83s/it]                                                   {'loss': 2.8143, 'grad_norm': 13.196236610412598, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:29<4:39:52,  2.83s/it]  1%|â–         | 76/6000 [03:32<4:40:37,  2.84s/it]                                                   {'loss': 2.8215, 'grad_norm': 16.78017807006836, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:32<4:40:37,  2.84s/it]  1%|â–         | 77/6000 [03:35<4:39:14,  2.83s/it]                                                   {'loss': 3.1065, 'grad_norm': 38.28691864013672, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:35<4:39:14,  2.83s/it]  1%|â–         | 78/6000 [03:38<4:48:12,  2.92s/it]                                                   {'loss': 2.8119, 'grad_norm': 16.12317657470703, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:38<4:48:12,  2.92s/it]  1%|â–         | 79/6000 [03:41<4:49:44,  2.94s/it]                                                   {'loss': 2.7712, 'grad_norm': 9.965381622314453, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:41<4:49:44,  2.94s/it]  1%|â–         | 80/6000 [03:44<4:42:37,  2.86s/it]                                                   {'loss': 2.7941, 'grad_norm': 10.484358787536621, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:44<4:42:37,  2.86s/it]  1%|â–         | 81/6000 [03:47<4:37:15,  2.81s/it]                                                   {'loss': 2.7981, 'grad_norm': 7.350918292999268, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:47<4:37:15,  2.81s/it]  1%|â–         | 82/6000 [03:49<4:35:25,  2.79s/it]                                                   {'loss': 2.7803, 'grad_norm': 9.800503730773926, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:49<4:35:25,  2.79s/it]  1%|â–         | 83/6000 [03:52<4:33:02,  2.77s/it]                                                   {'loss': 2.8159, 'grad_norm': 8.292193412780762, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:52<4:33:02,  2.77s/it]  1%|â–         | 84/6000 [03:55<4:35:07,  2.79s/it]                                                   {'loss': 2.8114, 'grad_norm': 10.841299057006836, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:55<4:35:07,  2.79s/it]  1%|â–         | 85/6000 [03:58<4:44:41,  2.89s/it]                                                   {'loss': 2.8008, 'grad_norm': 21.376192092895508, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:58<4:44:41,  2.89s/it]  1%|â–         | 86/6000 [04:01<4:38:58,  2.83s/it]                                                   {'loss': 2.8076, 'grad_norm': 8.905410766601562, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:01<4:38:58,  2.83s/it]  1%|â–         | 87/6000 [04:03<4:33:36,  2.78s/it]                                                   {'loss': 2.8025, 'grad_norm': 11.421062469482422, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:03<4:33:36,  2.78s/it]  1%|â–         | 88/6000 [04:06<4:32:39,  2.77s/it]                                                   {'loss': 2.8312, 'grad_norm': 17.696653366088867, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:06<4:32:39,  2.77s/it]  1%|â–         | 89/6000 [04:09<4:30:55,  2.75s/it]                                                   {'loss': 2.7856, 'grad_norm': 7.653225421905518, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:09<4:30:55,  2.75s/it]  2%|â–         | 90/6000 [04:12<4:32:50,  2.77s/it]                                                   {'loss': 2.8674, 'grad_norm': 15.095324516296387, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:12<4:32:50,  2.77s/it]  2%|â–         | 91/6000 [04:14<4:32:33,  2.77s/it]                                                   {'loss': 2.7787, 'grad_norm': 8.747773170471191, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:14<4:32:33,  2.77s/it]  2%|â–         | 92/6000 [04:17<4:43:19,  2.88s/it]                                                   {'loss': 2.7818, 'grad_norm': 12.200291633605957, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:17<4:43:19,  2.88s/it]  2%|â–         | 93/6000 [04:20<4:41:52,  2.86s/it]                                                   {'loss': 2.7806, 'grad_norm': 13.756909370422363, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:20<4:41:52,  2.86s/it]  2%|â–         | 94/6000 [04:23<4:37:41,  2.82s/it]                                                   {'loss': 2.8191, 'grad_norm': 11.572941780090332, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:23<4:37:41,  2.82s/it]  2%|â–         | 95/6000 [04:26<4:35:45,  2.80s/it]                                                   {'loss': 2.7784, 'grad_norm': 4.9353132247924805, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:26<4:35:45,  2.80s/it]  2%|â–         | 96/6000 [04:29<4:34:38,  2.79s/it]                                                   {'loss': 2.7736, 'grad_norm': 6.611154556274414, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:29<4:34:38,  2.79s/it]  2%|â–         | 97/6000 [04:31<4:31:26,  2.76s/it]                                                   {'loss': 2.8068, 'grad_norm': 18.796768188476562, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:31<4:31:26,  2.76s/it]  2%|â–         | 98/6000 [04:34<4:28:25,  2.73s/it]                                                   {'loss': 2.7812, 'grad_norm': 6.869228363037109, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:34<4:28:25,  2.73s/it]  2%|â–         | 99/6000 [04:37<4:26:58,  2.71s/it]                                                   {'loss': 2.783, 'grad_norm': 6.052299499511719, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:37<4:26:58,  2.71s/it]  2%|â–         | 100/6000 [04:39<4:26:06,  2.71s/it]                                                    {'loss': 2.7653, 'grad_norm': 4.869085788726807, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:39<4:26:06,  2.71s/it]  2%|â–         | 101/6000 [04:43<4:57:22,  3.02s/it]                                                    {'loss': 2.7719, 'grad_norm': 8.920279502868652, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:43<4:57:22,  3.02s/it]  2%|â–         | 102/6000 [04:46<4:48:26,  2.93s/it]                                                    {'loss': 2.7976, 'grad_norm': 5.766716480255127, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:46<4:48:26,  2.93s/it]  2%|â–         | 103/6000 [04:49<4:44:17,  2.89s/it]                                                    {'loss': 2.7804, 'grad_norm': 5.970633506774902, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:49<4:44:17,  2.89s/it]  2%|â–         | 104/6000 [04:51<4:45:47,  2.91s/it]                                                    {'loss': 2.7822, 'grad_norm': 6.2213263511657715, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:51<4:45:47,  2.91s/it]  2%|â–         | 105/6000 [04:54<4:43:34,  2.89s/it]                                                    {'loss': 2.7823, 'grad_norm': 6.086118221282959, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:54<4:43:34,  2.89s/it]  2%|â–         | 106/6000 [04:57<4:43:59,  2.89s/it]                                                    {'loss': 2.7809, 'grad_norm': 4.481115341186523, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:57<4:43:59,  2.89s/it]  2%|â–         | 107/6000 [05:00<4:38:25,  2.83s/it]                                                    {'loss': 2.7957, 'grad_norm': 5.026888370513916, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:00<4:38:25,  2.83s/it]  2%|â–         | 108/6000 [05:03<4:38:47,  2.84s/it]                                                    {'loss': 2.7915, 'grad_norm': 10.868331909179688, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:03<4:38:47,  2.84s/it]  2%|â–         | 109/6000 [05:06<4:48:27,  2.94s/it]                                                    {'loss': 2.8273, 'grad_norm': 13.069425582885742, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:06<4:48:27,  2.94s/it]  2%|â–         | 110/6000 [05:09<4:43:56,  2.89s/it]                                                    {'loss': 2.853, 'grad_norm': 11.796709060668945, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:09<4:43:56,  2.89s/it]  2%|â–         | 111/6000 [05:12<4:44:41,  2.90s/it]                                                    {'loss': 2.7715, 'grad_norm': 6.806140422821045, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:12<4:44:41,  2.90s/it]  2%|â–         | 112/6000 [05:15<4:56:32,  3.02s/it]                                                    {'loss': 2.7705, 'grad_norm': 13.605896949768066, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:15<4:56:32,  3.02s/it]  2%|â–         | 113/6000 [05:18<4:47:54,  2.93s/it]                                                    {'loss': 2.7778, 'grad_norm': 5.923957347869873, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:18<4:47:54,  2.93s/it]  2%|â–         | 114/6000 [05:21<4:44:52,  2.90s/it]                                                    {'loss': 2.7703, 'grad_norm': 6.081880569458008, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:21<4:44:52,  2.90s/it]  2%|â–         | 115/6000 [05:23<4:40:42,  2.86s/it]                                                    {'loss': 2.8789, 'grad_norm': 15.617477416992188, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:23<4:40:42,  2.86s/it]  2%|â–         | 116/6000 [05:26<4:37:07,  2.83s/it]                                                    {'loss': 2.8026, 'grad_norm': 8.029642105102539, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:26<4:37:07,  2.83s/it]  2%|â–         | 117/6000 [05:29<4:36:02,  2.82s/it]                                                    {'loss': 2.9091, 'grad_norm': 5.498910427093506, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:29<4:36:02,  2.82s/it]  2%|â–         | 118/6000 [05:32<4:51:49,  2.98s/it]                                                    {'loss': 2.7798, 'grad_norm': 5.743808746337891, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:32<4:51:49,  2.98s/it]  2%|â–         | 119/6000 [05:35<4:44:13,  2.90s/it]                                                    {'loss': 2.7748, 'grad_norm': 6.558351039886475, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:35<4:44:13,  2.90s/it]  2%|â–         | 120/6000 [05:38<4:39:25,  2.85s/it]                                                    {'loss': 2.7774, 'grad_norm': 10.71639633178711, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:38<4:39:25,  2.85s/it]  2%|â–         | 121/6000 [05:40<4:37:09,  2.83s/it]                                                    {'loss': 2.7793, 'grad_norm': 9.458106994628906, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:40<4:37:09,  2.83s/it]  2%|â–         | 122/6000 [05:43<4:38:43,  2.85s/it]                                                    {'loss': 2.763, 'grad_norm': 7.382683277130127, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:43<4:38:43,  2.85s/it]  2%|â–         | 123/6000 [05:46<4:33:19,  2.79s/it]                                                    {'loss': 2.788, 'grad_norm': 7.623977184295654, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:46<4:33:19,  2.79s/it]  2%|â–         | 124/6000 [05:49<4:29:24,  2.75s/it]                                                    {'loss': 2.8246, 'grad_norm': 5.596255302429199, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:49<4:29:24,  2.75s/it]  2%|â–         | 125/6000 [05:52<4:36:10,  2.82s/it]                                                    {'loss': 2.9425, 'grad_norm': 6.393355846405029, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:52<4:36:10,  2.82s/it]  2%|â–         | 126/6000 [05:54<4:35:04,  2.81s/it]                                                    {'loss': 2.7764, 'grad_norm': 7.354246139526367, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:54<4:35:04,  2.81s/it]  2%|â–         | 127/6000 [05:57<4:37:23,  2.83s/it]                                                    {'loss': 2.7828, 'grad_norm': 6.186102867126465, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:57<4:37:23,  2.83s/it]  2%|â–         | 128/6000 [06:00<4:32:40,  2.79s/it]                                                    {'loss': 2.7884, 'grad_norm': 6.752124786376953, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:00<4:32:40,  2.79s/it]  2%|â–         | 129/6000 [06:03<4:31:13,  2.77s/it]                                                    {'loss': 2.856, 'grad_norm': 7.387349605560303, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:03<4:31:13,  2.77s/it]  2%|â–         | 130/6000 [06:05<4:31:59,  2.78s/it]                                                    {'loss': 2.7678, 'grad_norm': 4.782750606536865, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:05<4:31:59,  2.78s/it]  2%|â–         | 131/6000 [06:08<4:31:13,  2.77s/it]                                                    {'loss': 2.7844, 'grad_norm': 6.644246578216553, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:08<4:31:13,  2.77s/it]  2%|â–         | 132/6000 [06:11<4:28:53,  2.75s/it]                                                    {'loss': 2.7882, 'grad_norm': 6.079935550689697, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:11<4:28:53,  2.75s/it]  2%|â–         | 133/6000 [06:14<4:29:31,  2.76s/it]                                                    {'loss': 2.7817, 'grad_norm': 5.797173976898193, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:14<4:29:31,  2.76s/it]  2%|â–         | 134/6000 [06:17<4:33:31,  2.80s/it]                                                    {'loss': 2.7787, 'grad_norm': 6.559779644012451, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:17<4:33:31,  2.80s/it]  2%|â–         | 135/6000 [06:19<4:33:42,  2.80s/it]                                                    {'loss': 2.7827, 'grad_norm': 5.888107776641846, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:19<4:33:42,  2.80s/it]  2%|â–         | 136/6000 [06:22<4:31:31,  2.78s/it]                                                    {'loss': 2.7835, 'grad_norm': 8.304728507995605, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:22<4:31:31,  2.78s/it]  2%|â–         | 137/6000 [06:25<4:46:13,  2.93s/it]                                                    {'loss': 2.7815, 'grad_norm': 7.547821998596191, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:25<4:46:13,  2.93s/it]  2%|â–         | 138/6000 [06:28<4:38:57,  2.86s/it]                                                    {'loss': 2.783, 'grad_norm': 4.5560784339904785, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:28<4:38:57,  2.86s/it]  2%|â–         | 139/6000 [06:31<4:39:56,  2.87s/it]                                                    {'loss': 2.8159, 'grad_norm': 4.075457572937012, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:31<4:39:56,  2.87s/it]  2%|â–         | 140/6000 [06:34<4:46:59,  2.94s/it]                                                    {'loss': 2.7672, 'grad_norm': 6.459346771240234, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:34<4:46:59,  2.94s/it]  2%|â–         | 141/6000 [06:37<4:42:01,  2.89s/it]                                                    {'loss': 2.7811, 'grad_norm': 5.423235893249512, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:37<4:42:01,  2.89s/it]  2%|â–         | 142/6000 [06:40<4:35:52,  2.83s/it]                                                    {'loss': 2.7738, 'grad_norm': 4.738914489746094, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:40<4:35:52,  2.83s/it]  2%|â–         | 143/6000 [06:42<4:32:00,  2.79s/it]                                                    {'loss': 2.7682, 'grad_norm': 5.368412971496582, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:42<4:32:00,  2.79s/it]  2%|â–         | 144/6000 [06:45<4:30:39,  2.77s/it]                                                    {'loss': 2.8023, 'grad_norm': 7.582935333251953, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:45<4:30:39,  2.77s/it]  2%|â–         | 145/6000 [06:48<4:27:16,  2.74s/it]                                                    {'loss': 2.8059, 'grad_norm': 5.266842365264893, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:48<4:27:16,  2.74s/it]  2%|â–         | 146/6000 [06:50<4:26:39,  2.73s/it]                                                    {'loss': 2.8161, 'grad_norm': 6.658926486968994, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:50<4:26:39,  2.73s/it]  2%|â–         | 147/6000 [06:53<4:24:27,  2.71s/it]                                                    {'loss': 2.7719, 'grad_norm': 5.593555450439453, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:53<4:24:27,  2.71s/it]  2%|â–         | 148/6000 [06:56<4:25:44,  2.72s/it]                                                    {'loss': 2.7872, 'grad_norm': 6.304561614990234, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:56<4:25:44,  2.72s/it]  2%|â–         | 149/6000 [06:58<4:24:25,  2.71s/it]                                                    {'loss': 2.7731, 'grad_norm': 7.259800434112549, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [06:58<4:24:25,  2.71s/it]  2%|â–Ž         | 150/6000 [07:01<4:23:16,  2.70s/it]                                                    {'loss': 2.7951, 'grad_norm': 10.421496391296387, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:01<4:23:16,  2.70s/it]  3%|â–Ž         | 151/6000 [07:04<4:25:31,  2.72s/it]                                                    {'loss': 2.7852, 'grad_norm': 9.009394645690918, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:04<4:25:31,  2.72s/it]  3%|â–Ž         | 152/6000 [07:07<4:26:43,  2.74s/it]                                                    {'loss': 2.7862, 'grad_norm': 5.724083423614502, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:07<4:26:43,  2.74s/it]  3%|â–Ž         | 153/6000 [07:09<4:25:14,  2.72s/it]                                                    {'loss': 2.7776, 'grad_norm': 6.096724987030029, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:09<4:25:14,  2.72s/it]  3%|â–Ž         | 154/6000 [07:12<4:29:04,  2.76s/it]                                                    {'loss': 2.7678, 'grad_norm': 5.449110984802246, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:12<4:29:04,  2.76s/it]  3%|â–Ž         | 155/6000 [07:15<4:28:35,  2.76s/it]                                                    {'loss': 2.8034, 'grad_norm': 7.496960163116455, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:15<4:28:35,  2.76s/it]  3%|â–Ž         | 156/6000 [07:18<4:36:40,  2.84s/it]                                                    {'loss': 2.8464, 'grad_norm': 7.256650447845459, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:18<4:36:40,  2.84s/it]  3%|â–Ž         | 157/6000 [07:21<4:47:20,  2.95s/it]                                                    {'loss': 2.7668, 'grad_norm': 9.159280776977539, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:21<4:47:20,  2.95s/it]  3%|â–Ž         | 158/6000 [07:24<4:39:41,  2.87s/it]                                                    {'loss': 2.7888, 'grad_norm': 6.396533012390137, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:24<4:39:41,  2.87s/it]  3%|â–Ž         | 159/6000 [07:27<4:35:47,  2.83s/it]                                                    {'loss': 2.7681, 'grad_norm': 7.658471584320068, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:27<4:35:47,  2.83s/it]  3%|â–Ž         | 160/6000 [07:30<4:50:01,  2.98s/it]                                                    {'loss': 2.8106, 'grad_norm': 8.361092567443848, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:30<4:50:01,  2.98s/it]  3%|â–Ž         | 161/6000 [07:33<4:46:53,  2.95s/it]                                                    {'loss': 2.7731, 'grad_norm': 7.499653339385986, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:33<4:46:53,  2.95s/it]  3%|â–Ž         | 162/6000 [07:35<4:38:52,  2.87s/it]                                                    {'loss': 2.8014, 'grad_norm': 6.332521438598633, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:36<4:38:52,  2.87s/it]  3%|â–Ž         | 163/6000 [07:39<4:51:47,  3.00s/it]                                                    {'loss': 2.787, 'grad_norm': 6.789314270019531, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:39<4:51:47,  3.00s/it]  3%|â–Ž         | 164/6000 [07:42<4:43:23,  2.91s/it]                                                    {'loss': 2.8943, 'grad_norm': 9.034104347229004, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:42<4:43:23,  2.91s/it]  3%|â–Ž         | 165/6000 [07:44<4:40:27,  2.88s/it]                                                    {'loss': 2.7853, 'grad_norm': 11.096445083618164, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:44<4:40:27,  2.88s/it]  3%|â–Ž         | 166/6000 [07:47<4:34:44,  2.83s/it]                                                    {'loss': 2.7867, 'grad_norm': 6.910373210906982, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:47<4:34:44,  2.83s/it]  3%|â–Ž         | 167/6000 [07:50<4:33:21,  2.81s/it]                                                    {'loss': 2.7942, 'grad_norm': 8.973045349121094, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:50<4:33:21,  2.81s/it]  3%|â–Ž         | 168/6000 [07:53<4:33:21,  2.81s/it]                                                    {'loss': 2.8184, 'grad_norm': 5.380213737487793, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:53<4:33:21,  2.81s/it]  3%|â–Ž         | 169/6000 [07:56<4:44:17,  2.93s/it]                                                    {'loss': 2.8077, 'grad_norm': 7.816074848175049, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:56<4:44:17,  2.93s/it]  3%|â–Ž         | 170/6000 [07:58<4:37:01,  2.85s/it]                                                    {'loss': 2.7746, 'grad_norm': 5.598034381866455, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [07:58<4:37:01,  2.85s/it]  3%|â–Ž         | 171/6000 [08:01<4:32:38,  2.81s/it]                                                    {'loss': 2.7783, 'grad_norm': 4.75199031829834, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:01<4:32:38,  2.81s/it]  3%|â–Ž         | 172/6000 [08:04<4:34:12,  2.82s/it]                                                    {'loss': 2.8016, 'grad_norm': 5.794015884399414, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:04<4:34:12,  2.82s/it]  3%|â–Ž         | 173/6000 [08:07<4:32:04,  2.80s/it]                                                    {'loss': 2.7884, 'grad_norm': 6.329308032989502, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:07<4:32:04,  2.80s/it]  3%|â–Ž         | 174/6000 [08:10<4:50:50,  3.00s/it]                                                    {'loss': 2.7602, 'grad_norm': 8.134172439575195, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:10<4:50:50,  3.00s/it]  3%|â–Ž         | 175/6000 [08:13<4:44:16,  2.93s/it]                                                    {'loss': 2.7817, 'grad_norm': 7.445488452911377, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:13<4:44:16,  2.93s/it]  3%|â–Ž         | 176/6000 [08:16<4:46:42,  2.95s/it]                                                    {'loss': 2.7851, 'grad_norm': 9.790181159973145, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:16<4:46:42,  2.95s/it]  3%|â–Ž         | 177/6000 [08:19<4:39:24,  2.88s/it]                                                    {'loss': 2.7921, 'grad_norm': 5.298447608947754, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:19<4:39:24,  2.88s/it]  3%|â–Ž         | 178/6000 [08:22<4:36:22,  2.85s/it]                                                    {'loss': 2.7757, 'grad_norm': 5.331990718841553, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:22<4:36:22,  2.85s/it]  3%|â–Ž         | 179/6000 [08:24<4:30:48,  2.79s/it]                                                    {'loss': 2.8053, 'grad_norm': 5.447779655456543, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:24<4:30:48,  2.79s/it]  3%|â–Ž         | 180/6000 [08:27<4:32:47,  2.81s/it]                                                    {'loss': 2.7925, 'grad_norm': 6.201914310455322, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:27<4:32:47,  2.81s/it]  3%|â–Ž         | 181/6000 [08:30<4:31:12,  2.80s/it]                                                    {'loss': 2.8125, 'grad_norm': 5.674059867858887, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:30<4:31:12,  2.80s/it]  3%|â–Ž         | 182/6000 [08:33<4:29:25,  2.78s/it]                                                    {'loss': 2.8172, 'grad_norm': 4.739638328552246, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:33<4:29:25,  2.78s/it]  3%|â–Ž         | 183/6000 [08:35<4:27:38,  2.76s/it]                                                    {'loss': 2.7737, 'grad_norm': 4.4680609703063965, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:35<4:27:38,  2.76s/it]  3%|â–Ž         | 184/6000 [08:38<4:26:00,  2.74s/it]                                                    {'loss': 2.7706, 'grad_norm': 5.752053260803223, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:38<4:26:00,  2.74s/it]  3%|â–Ž         | 185/6000 [08:41<4:26:49,  2.75s/it]                                                    {'loss': 2.7904, 'grad_norm': 5.001781463623047, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:41<4:26:49,  2.75s/it]  3%|â–Ž         | 186/6000 [08:43<4:26:52,  2.75s/it]                                                    {'loss': 2.7878, 'grad_norm': 6.011643409729004, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:43<4:26:52,  2.75s/it]  3%|â–Ž         | 187/6000 [08:46<4:27:47,  2.76s/it]                                                    {'loss': 2.7752, 'grad_norm': 8.084795951843262, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:46<4:27:47,  2.76s/it]  3%|â–Ž         | 188/6000 [08:49<4:26:48,  2.75s/it]                                                    {'loss': 2.7864, 'grad_norm': 4.9200639724731445, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:49<4:26:48,  2.75s/it]  3%|â–Ž         | 189/6000 [08:52<4:27:29,  2.76s/it]                                                    {'loss': 2.7835, 'grad_norm': 4.815948009490967, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:52<4:27:29,  2.76s/it]  3%|â–Ž         | 190/6000 [08:55<4:26:20,  2.75s/it]                                                    {'loss': 2.7867, 'grad_norm': 6.608051300048828, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:55<4:26:20,  2.75s/it]  3%|â–Ž         | 191/6000 [08:57<4:28:25,  2.77s/it]                                                    {'loss': 2.7834, 'grad_norm': 8.465780258178711, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [08:57<4:28:25,  2.77s/it]  3%|â–Ž         | 192/6000 [09:01<4:45:25,  2.95s/it]                                                    {'loss': 2.869, 'grad_norm': 5.37948751449585, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:01<4:45:25,  2.95s/it]  3%|â–Ž         | 193/6000 [09:04<4:48:37,  2.98s/it]                                                    {'loss': 2.7635, 'grad_norm': 6.049805641174316, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:04<4:48:37,  2.98s/it]  3%|â–Ž         | 194/6000 [09:06<4:38:23,  2.88s/it]                                                    {'loss': 2.7834, 'grad_norm': 7.294337272644043, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:06<4:38:23,  2.88s/it]  3%|â–Ž         | 195/6000 [09:09<4:35:12,  2.84s/it]                                                    {'loss': 2.8196, 'grad_norm': 6.192533493041992, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:09<4:35:12,  2.84s/it]  3%|â–Ž         | 196/6000 [09:12<4:33:10,  2.82s/it]                                                    {'loss': 2.7852, 'grad_norm': 6.802428245544434, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:12<4:33:10,  2.82s/it]  3%|â–Ž         | 197/6000 [09:15<4:35:28,  2.85s/it]                                                    {'loss': 2.8005, 'grad_norm': 7.24944543838501, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:15<4:35:28,  2.85s/it]  3%|â–Ž         | 198/6000 [09:18<4:35:44,  2.85s/it]                                                    {'loss': 2.7886, 'grad_norm': 6.803271770477295, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:18<4:35:44,  2.85s/it]  3%|â–Ž         | 199/6000 [09:20<4:31:50,  2.81s/it]                                                    {'loss': 2.7725, 'grad_norm': 4.185065269470215, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:20<4:31:50,  2.81s/it]  3%|â–Ž         | 200/6000 [09:24<4:44:55,  2.95s/it]                                                    {'loss': 2.7784, 'grad_norm': 8.991521835327148, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:24<4:44:55,  2.95s/it]  3%|â–Ž         | 201/6000 [09:26<4:39:47,  2.89s/it]                                                    {'loss': 2.7788, 'grad_norm': 7.001749515533447, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:26<4:39:47,  2.89s/it]  3%|â–Ž         | 202/6000 [09:29<4:34:40,  2.84s/it]                                                    {'loss': 2.7776, 'grad_norm': 4.765707015991211, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:29<4:34:40,  2.84s/it]  3%|â–Ž         | 203/6000 [09:32<4:32:11,  2.82s/it]                                                    {'loss': 2.7779, 'grad_norm': 5.568476676940918, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:32<4:32:11,  2.82s/it]  3%|â–Ž         | 204/6000 [09:35<4:33:21,  2.83s/it]                                                    {'loss': 2.7799, 'grad_norm': 6.21299934387207, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:35<4:33:21,  2.83s/it]  3%|â–Ž         | 205/6000 [09:37<4:29:06,  2.79s/it]                                                    {'loss': 2.7571, 'grad_norm': 4.703311443328857, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:37<4:29:06,  2.79s/it]  3%|â–Ž         | 206/6000 [09:40<4:25:32,  2.75s/it]                                                    {'loss': 2.8274, 'grad_norm': 6.5944504737854, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:40<4:25:32,  2.75s/it]  3%|â–Ž         | 207/6000 [09:43<4:22:41,  2.72s/it]                                                    {'loss': 2.8727, 'grad_norm': 5.656782150268555, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:43<4:22:41,  2.72s/it]  3%|â–Ž         | 208/6000 [09:45<4:21:49,  2.71s/it]                                                    {'loss': 2.8029, 'grad_norm': 7.5634918212890625, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:45<4:21:49,  2.71s/it]  3%|â–Ž         | 209/6000 [09:48<4:21:56,  2.71s/it]                                                    {'loss': 2.7701, 'grad_norm': 5.380861282348633, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:48<4:21:56,  2.71s/it]  4%|â–Ž         | 210/6000 [09:51<4:34:36,  2.85s/it]                                                    {'loss': 2.7745, 'grad_norm': 5.067676067352295, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:51<4:34:36,  2.85s/it]  4%|â–Ž         | 211/6000 [09:54<4:36:13,  2.86s/it]                                                    {'loss': 2.8089, 'grad_norm': 7.457039833068848, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [09:54<4:36:13,  2.86s/it]  4%|â–Ž         | 212/6000 [09:57<4:44:48,  2.95s/it]                                                    {'loss': 2.7792, 'grad_norm': 8.61559772491455, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [09:57<4:44:48,  2.95s/it]  4%|â–Ž         | 213/6000 [10:00<4:40:50,  2.91s/it]                                                    {'loss': 2.7717, 'grad_norm': 10.376474380493164, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:00<4:40:50,  2.91s/it]  4%|â–Ž         | 214/6000 [10:03<4:33:39,  2.84s/it]                                                    {'loss': 2.7699, 'grad_norm': 5.1225996017456055, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:03<4:33:39,  2.84s/it]  4%|â–Ž         | 215/6000 [10:06<4:29:44,  2.80s/it]                                                    {'loss': 2.8087, 'grad_norm': 7.3459296226501465, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:06<4:29:44,  2.80s/it]  4%|â–Ž         | 216/6000 [10:08<4:29:10,  2.79s/it]                                                    {'loss': 2.7736, 'grad_norm': 7.75317907333374, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:08<4:29:10,  2.79s/it]  4%|â–Ž         | 217/6000 [10:11<4:31:04,  2.81s/it]                                                    {'loss': 2.8179, 'grad_norm': 4.772798538208008, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:11<4:31:04,  2.81s/it]  4%|â–Ž         | 218/6000 [10:14<4:31:40,  2.82s/it]                                                    {'loss': 2.8173, 'grad_norm': 7.4708943367004395, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:14<4:31:40,  2.82s/it]  4%|â–Ž         | 219/6000 [10:17<4:27:45,  2.78s/it]                                                    {'loss': 2.8211, 'grad_norm': 4.75958251953125, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:17<4:27:45,  2.78s/it]  4%|â–Ž         | 220/6000 [10:19<4:26:00,  2.76s/it]                                                    {'loss': 2.7761, 'grad_norm': 4.591399669647217, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:19<4:26:00,  2.76s/it]  4%|â–Ž         | 221/6000 [10:22<4:25:00,  2.75s/it]                                                    {'loss': 2.7941, 'grad_norm': 5.395907402038574, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:22<4:25:00,  2.75s/it]  4%|â–Ž         | 222/6000 [10:25<4:23:06,  2.73s/it]                                                    {'loss': 2.78, 'grad_norm': 4.8415069580078125, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:25<4:23:06,  2.73s/it]  4%|â–Ž         | 223/6000 [10:28<4:23:04,  2.73s/it]                                                    {'loss': 2.7675, 'grad_norm': 4.741296291351318, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:28<4:23:04,  2.73s/it]  4%|â–Ž         | 224/6000 [10:30<4:22:39,  2.73s/it]                                                    {'loss': 2.8251, 'grad_norm': 4.809056282043457, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:30<4:22:39,  2.73s/it]  4%|â–         | 225/6000 [10:33<4:25:05,  2.75s/it]                                                    {'loss': 2.7801, 'grad_norm': 4.3524861335754395, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:33<4:25:05,  2.75s/it]  4%|â–         | 226/6000 [10:36<4:39:03,  2.90s/it]                                                    {'loss': 2.77, 'grad_norm': 6.282362461090088, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:36<4:39:03,  2.90s/it]  4%|â–         | 227/6000 [10:39<4:33:29,  2.84s/it]                                                    {'loss': 2.8166, 'grad_norm': 3.587846517562866, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:39<4:33:29,  2.84s/it]  4%|â–         | 228/6000 [10:42<4:32:27,  2.83s/it]                                                    {'loss': 2.7685, 'grad_norm': 5.085143089294434, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:42<4:32:27,  2.83s/it]  4%|â–         | 229/6000 [10:45<4:27:45,  2.78s/it]                                                    {'loss': 2.8062, 'grad_norm': 5.665733337402344, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:45<4:27:45,  2.78s/it]  4%|â–         | 230/6000 [10:47<4:28:47,  2.80s/it]                                                    {'loss': 2.7961, 'grad_norm': 9.153617858886719, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:47<4:28:47,  2.80s/it]  4%|â–         | 231/6000 [10:50<4:26:26,  2.77s/it]                                                    {'loss': 2.7836, 'grad_norm': 3.9507458209991455, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:50<4:26:26,  2.77s/it]  4%|â–         | 232/6000 [10:53<4:25:28,  2.76s/it]                                                    {'loss': 2.7693, 'grad_norm': 5.528418064117432, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [10:53<4:25:28,  2.76s/it]  4%|â–         | 233/6000 [10:56<4:36:01,  2.87s/it]                                                    {'loss': 2.7712, 'grad_norm': 5.068345546722412, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [10:56<4:36:01,  2.87s/it]  4%|â–         | 234/6000 [10:59<4:40:39,  2.92s/it]                                                    {'loss': 2.7797, 'grad_norm': 6.308037281036377, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [10:59<4:40:39,  2.92s/it]  4%|â–         | 235/6000 [11:02<4:34:40,  2.86s/it]                                                    {'loss': 2.8471, 'grad_norm': 3.744446277618408, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:02<4:34:40,  2.86s/it]  4%|â–         | 236/6000 [11:05<4:32:22,  2.84s/it]                                                    {'loss': 2.8025, 'grad_norm': 5.203391075134277, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:05<4:32:22,  2.84s/it]  4%|â–         | 237/6000 [11:07<4:35:06,  2.86s/it]                                                    {'loss': 2.7959, 'grad_norm': 6.256959438323975, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:07<4:35:06,  2.86s/it]  4%|â–         | 238/6000 [11:11<4:45:48,  2.98s/it]                                                    {'loss': 2.8435, 'grad_norm': 5.705676078796387, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:11<4:45:48,  2.98s/it]  4%|â–         | 239/6000 [11:13<4:40:58,  2.93s/it]                                                    {'loss': 2.7619, 'grad_norm': 6.071324825286865, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:14<4:40:58,  2.93s/it]  4%|â–         | 240/6000 [11:16<4:35:32,  2.87s/it]                                                    {'loss': 2.7926, 'grad_norm': 5.589595317840576, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:16<4:35:32,  2.87s/it]  4%|â–         | 241/6000 [11:19<4:32:13,  2.84s/it]                                                    {'loss': 2.7825, 'grad_norm': 8.548978805541992, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:19<4:32:13,  2.84s/it]  4%|â–         | 242/6000 [11:22<4:27:17,  2.79s/it]                                                    {'loss': 2.8014, 'grad_norm': 4.442023277282715, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:22<4:27:17,  2.79s/it]  4%|â–         | 243/6000 [11:25<4:34:56,  2.87s/it]                                                    {'loss': 2.7717, 'grad_norm': 5.1761274337768555, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:25<4:34:56,  2.87s/it]  4%|â–         | 244/6000 [11:27<4:31:21,  2.83s/it]                                                    {'loss': 2.7806, 'grad_norm': 6.063473224639893, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:27<4:31:21,  2.83s/it]  4%|â–         | 245/6000 [11:30<4:30:09,  2.82s/it]                                                    {'loss': 2.7593, 'grad_norm': 4.391274452209473, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:30<4:30:09,  2.82s/it]  4%|â–         | 246/6000 [11:33<4:31:39,  2.83s/it]                                                    {'loss': 2.781, 'grad_norm': 5.504738807678223, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:33<4:31:39,  2.83s/it]  4%|â–         | 247/6000 [11:36<4:32:02,  2.84s/it]                                                    {'loss': 2.776, 'grad_norm': 3.0998759269714355, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:36<4:32:02,  2.84s/it]  4%|â–         | 248/6000 [11:39<4:32:11,  2.84s/it]                                                    {'loss': 2.7669, 'grad_norm': 4.636697292327881, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:39<4:32:11,  2.84s/it]  4%|â–         | 249/6000 [11:42<4:29:09,  2.81s/it]                                                    {'loss': 2.7841, 'grad_norm': 6.182101249694824, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:42<4:29:09,  2.81s/it]  4%|â–         | 250/6000 [11:44<4:27:58,  2.80s/it]                                                    {'loss': 2.8072, 'grad_norm': 5.442046642303467, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:44<4:27:58,  2.80s/it]  4%|â–         | 251/6000 [11:47<4:26:01,  2.78s/it]                                                    {'loss': 2.816, 'grad_norm': 5.145337104797363, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:47<4:26:01,  2.78s/it]  4%|â–         | 252/6000 [11:50<4:30:53,  2.83s/it]                                                    {'loss': 2.8048, 'grad_norm': 5.202995777130127, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [11:50<4:30:53,  2.83s/it]  4%|â–         | 253/6000 [11:53<4:28:17,  2.80s/it]                                                    {'loss': 2.8098, 'grad_norm': 4.185875415802002, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [11:53<4:28:17,  2.80s/it]  4%|â–         | 254/6000 [11:56<4:27:39,  2.79s/it]                                                    {'loss': 2.7611, 'grad_norm': 5.233146667480469, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [11:56<4:27:39,  2.79s/it]  4%|â–         | 255/6000 [11:58<4:25:07,  2.77s/it]                                                    {'loss': 2.7692, 'grad_norm': 4.192209243774414, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [11:58<4:25:07,  2.77s/it]  4%|â–         | 256/6000 [12:01<4:35:51,  2.88s/it]                                                    {'loss': 2.777, 'grad_norm': 5.069677829742432, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:01<4:35:51,  2.88s/it]  4%|â–         | 257/6000 [12:04<4:30:10,  2.82s/it]                                                    {'loss': 2.7714, 'grad_norm': 3.3455398082733154, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:04<4:30:10,  2.82s/it]  4%|â–         | 258/6000 [12:07<4:26:09,  2.78s/it]                                                    {'loss': 2.7715, 'grad_norm': 3.414578914642334, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:07<4:26:09,  2.78s/it]  4%|â–         | 259/6000 [12:09<4:22:05,  2.74s/it]                                                    {'loss': 2.775, 'grad_norm': 5.460013389587402, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:09<4:22:05,  2.74s/it]  4%|â–         | 260/6000 [12:12<4:20:42,  2.73s/it]                                                    {'loss': 2.7898, 'grad_norm': 5.171324253082275, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:12<4:20:42,  2.73s/it]  4%|â–         | 261/6000 [12:15<4:26:00,  2.78s/it]                                                    {'loss': 2.7951, 'grad_norm': 4.255960464477539, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:15<4:26:00,  2.78s/it]  4%|â–         | 262/6000 [12:18<4:26:19,  2.78s/it]                                                    {'loss': 2.7843, 'grad_norm': 4.7608513832092285, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:18<4:26:19,  2.78s/it]  4%|â–         | 263/6000 [12:21<4:35:08,  2.88s/it]                                                    {'loss': 2.8421, 'grad_norm': 5.498937606811523, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:21<4:35:08,  2.88s/it]  4%|â–         | 264/6000 [12:24<4:29:34,  2.82s/it]                                                    {'loss': 2.7802, 'grad_norm': 5.998965740203857, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:24<4:29:34,  2.82s/it]  4%|â–         | 265/6000 [12:26<4:32:09,  2.85s/it]                                                    {'loss': 2.7783, 'grad_norm': 4.69892692565918, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:26<4:32:09,  2.85s/it]  4%|â–         | 266/6000 [12:29<4:34:15,  2.87s/it]                                                    {'loss': 2.8084, 'grad_norm': 5.5290632247924805, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:29<4:34:15,  2.87s/it]  4%|â–         | 267/6000 [12:32<4:28:56,  2.81s/it]                                                    {'loss': 2.7658, 'grad_norm': 3.9168620109558105, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:32<4:28:56,  2.81s/it]  4%|â–         | 268/6000 [12:35<4:26:40,  2.79s/it]                                                    {'loss': 2.8465, 'grad_norm': 3.830348253250122, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:35<4:26:40,  2.79s/it]  4%|â–         | 269/6000 [12:38<4:28:45,  2.81s/it]                                                    {'loss': 2.7792, 'grad_norm': 3.990687608718872, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:38<4:28:45,  2.81s/it]  4%|â–         | 270/6000 [12:40<4:26:28,  2.79s/it]                                                    {'loss': 2.772, 'grad_norm': 3.9826762676239014, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:40<4:26:28,  2.79s/it]  5%|â–         | 271/6000 [12:43<4:23:13,  2.76s/it]                                                    {'loss': 2.7895, 'grad_norm': 3.9058115482330322, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:43<4:23:13,  2.76s/it]  5%|â–         | 272/6000 [12:46<4:22:28,  2.75s/it]                                                    {'loss': 2.7647, 'grad_norm': 6.0074872970581055, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:46<4:22:28,  2.75s/it]  5%|â–         | 273/6000 [12:49<4:22:02,  2.75s/it]                                                    {'loss': 2.7986, 'grad_norm': 4.882095813751221, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:49<4:22:02,  2.75s/it]  5%|â–         | 274/6000 [12:52<4:33:26,  2.87s/it]                                                    {'loss': 2.7674, 'grad_norm': 5.237237453460693, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [12:52<4:33:26,  2.87s/it]  5%|â–         | 275/6000 [12:54<4:31:12,  2.84s/it]                                                    {'loss': 2.7988, 'grad_norm': 6.274049758911133, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [12:54<4:31:12,  2.84s/it]  5%|â–         | 276/6000 [12:57<4:29:50,  2.83s/it]                                                    {'loss': 2.8507, 'grad_norm': 4.828042030334473, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [12:57<4:29:50,  2.83s/it]  5%|â–         | 277/6000 [13:00<4:25:12,  2.78s/it]                                                    {'loss': 2.7777, 'grad_norm': 4.594925403594971, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:00<4:25:12,  2.78s/it]  5%|â–         | 278/6000 [13:03<4:21:48,  2.75s/it]                                                    {'loss': 2.7751, 'grad_norm': 6.5852885246276855, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:03<4:21:48,  2.75s/it]  5%|â–         | 279/6000 [13:05<4:23:03,  2.76s/it]                                                    {'loss': 2.7811, 'grad_norm': 5.812668323516846, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:05<4:23:03,  2.76s/it]  5%|â–         | 280/6000 [13:08<4:21:40,  2.74s/it]                                                    {'loss': 2.7746, 'grad_norm': 4.720823764801025, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:08<4:21:40,  2.74s/it]  5%|â–         | 281/6000 [13:11<4:20:53,  2.74s/it]                                                    {'loss': 2.7717, 'grad_norm': 5.3357253074646, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:11<4:20:53,  2.74s/it]  5%|â–         | 282/6000 [13:14<4:32:21,  2.86s/it]                                                    {'loss': 2.7778, 'grad_norm': 4.712026119232178, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:14<4:32:21,  2.86s/it]  5%|â–         | 283/6000 [13:17<4:36:39,  2.90s/it]                                                    {'loss': 2.8065, 'grad_norm': 6.974636554718018, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:17<4:36:39,  2.90s/it]  5%|â–         | 284/6000 [13:20<4:51:54,  3.06s/it]                                                    {'loss': 2.7925, 'grad_norm': 5.906268119812012, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:20<4:51:54,  3.06s/it]  5%|â–         | 285/6000 [13:23<4:46:39,  3.01s/it]                                                    {'loss': 2.7748, 'grad_norm': 6.998355865478516, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:23<4:46:39,  3.01s/it]  5%|â–         | 286/6000 [13:26<4:35:08,  2.89s/it]                                                    {'loss': 2.7759, 'grad_norm': 6.478199481964111, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:26<4:35:08,  2.89s/it]  5%|â–         | 287/6000 [13:29<4:31:47,  2.85s/it]                                                    {'loss': 2.8042, 'grad_norm': 5.709052562713623, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:29<4:31:47,  2.85s/it]  5%|â–         | 288/6000 [13:31<4:27:30,  2.81s/it]                                                    {'loss': 2.7638, 'grad_norm': 8.72385025024414, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:31<4:27:30,  2.81s/it]  5%|â–         | 289/6000 [13:34<4:30:23,  2.84s/it]                                                    {'loss': 2.7693, 'grad_norm': 6.421313762664795, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:34<4:30:23,  2.84s/it]  5%|â–         | 290/6000 [13:37<4:27:13,  2.81s/it]                                                    {'loss': 2.803, 'grad_norm': 7.162247657775879, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:37<4:27:13,  2.81s/it]  5%|â–         | 291/6000 [13:40<4:23:32,  2.77s/it]                                                    {'loss': 2.7687, 'grad_norm': 12.380966186523438, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:40<4:23:32,  2.77s/it]  5%|â–         | 292/6000 [13:42<4:23:07,  2.77s/it]                                                    {'loss': 2.7685, 'grad_norm': 5.10884952545166, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:42<4:23:07,  2.77s/it]  5%|â–         | 293/6000 [13:45<4:20:50,  2.74s/it]                                                    {'loss': 2.8294, 'grad_norm': 7.18787145614624, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:45<4:20:50,  2.74s/it]  5%|â–         | 294/6000 [13:48<4:20:18,  2.74s/it]                                                    {'loss': 2.7986, 'grad_norm': 7.256424427032471, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:48<4:20:18,  2.74s/it]  5%|â–         | 295/6000 [13:51<4:19:04,  2.72s/it]                                                    {'loss': 2.7845, 'grad_norm': 4.225552082061768, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [13:51<4:19:04,  2.72s/it]  5%|â–         | 296/6000 [13:53<4:22:25,  2.76s/it]                                                    {'loss': 2.7664, 'grad_norm': 13.535696983337402, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [13:53<4:22:25,  2.76s/it]  5%|â–         | 297/6000 [13:56<4:21:32,  2.75s/it]                                                    {'loss': 2.7799, 'grad_norm': 6.508237361907959, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [13:56<4:21:32,  2.75s/it]  5%|â–         | 298/6000 [13:59<4:26:13,  2.80s/it]                                                    {'loss': 2.7824, 'grad_norm': 5.286401748657227, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [13:59<4:26:13,  2.80s/it]  5%|â–         | 299/6000 [14:03<4:48:16,  3.03s/it]                                                    {'loss': 2.8027, 'grad_norm': 6.654472827911377, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:03<4:48:16,  3.03s/it]  5%|â–Œ         | 300/6000 [14:05<4:40:21,  2.95s/it]                                                    {'loss': 2.7676, 'grad_norm': 6.409305095672607, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:05<4:40:21,  2.95s/it]  5%|â–Œ         | 301/6000 [14:08<4:36:45,  2.91s/it]                                                    {'loss': 2.7869, 'grad_norm': 8.061295509338379, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:08<4:36:45,  2.91s/it]  5%|â–Œ         | 302/6000 [14:11<4:33:07,  2.88s/it]                                                    {'loss': 2.7792, 'grad_norm': 5.008968353271484, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:11<4:33:07,  2.88s/it]  5%|â–Œ         | 303/6000 [14:14<4:31:10,  2.86s/it]                                                    {'loss': 2.7751, 'grad_norm': 9.037776947021484, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:14<4:31:10,  2.86s/it]  5%|â–Œ         | 304/6000 [14:17<4:27:58,  2.82s/it]                                                    {'loss': 2.8005, 'grad_norm': 8.891810417175293, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:17<4:27:58,  2.82s/it]  5%|â–Œ         | 305/6000 [14:19<4:24:08,  2.78s/it]                                                    {'loss': 2.7949, 'grad_norm': 6.06981897354126, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:19<4:24:08,  2.78s/it]  5%|â–Œ         | 306/6000 [14:22<4:24:29,  2.79s/it]                                                    {'loss': 2.7755, 'grad_norm': 5.960319519042969, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:22<4:24:29,  2.79s/it]  5%|â–Œ         | 307/6000 [14:25<4:22:13,  2.76s/it]                                                    {'loss': 2.7831, 'grad_norm': 8.07986068725586, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:25<4:22:13,  2.76s/it]  5%|â–Œ         | 308/6000 [14:27<4:20:08,  2.74s/it]                                                    {'loss': 2.7872, 'grad_norm': 6.052028656005859, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:27<4:20:08,  2.74s/it]  5%|â–Œ         | 309/6000 [14:30<4:22:42,  2.77s/it]                                                    {'loss': 2.7813, 'grad_norm': 5.426515579223633, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:30<4:22:42,  2.77s/it]  5%|â–Œ         | 310/6000 [14:33<4:21:48,  2.76s/it]                                                    {'loss': 2.8064, 'grad_norm': 6.791080474853516, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:33<4:21:48,  2.76s/it]  5%|â–Œ         | 311/6000 [14:36<4:21:02,  2.75s/it]                                                    {'loss': 2.7825, 'grad_norm': 5.895778656005859, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:36<4:21:02,  2.75s/it]  5%|â–Œ         | 312/6000 [14:39<4:20:44,  2.75s/it]                                                    {'loss': 2.777, 'grad_norm': 5.71501350402832, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:39<4:20:44,  2.75s/it]  5%|â–Œ         | 313/6000 [14:41<4:21:49,  2.76s/it]                                                    {'loss': 2.8315, 'grad_norm': 4.762999534606934, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:41<4:21:49,  2.76s/it]  5%|â–Œ         | 314/6000 [14:44<4:22:47,  2.77s/it]                                                    {'loss': 2.7773, 'grad_norm': 5.061707019805908, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:44<4:22:47,  2.77s/it]  5%|â–Œ         | 315/6000 [14:47<4:21:10,  2.76s/it]                                                    {'loss': 2.7742, 'grad_norm': 5.0365986824035645, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:47<4:21:10,  2.76s/it]  5%|â–Œ         | 316/6000 [14:50<4:21:10,  2.76s/it]                                                    {'loss': 2.7802, 'grad_norm': 3.8402931690216064, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [14:50<4:21:10,  2.76s/it]  5%|â–Œ         | 317/6000 [14:52<4:25:14,  2.80s/it]                                                    {'loss': 2.8392, 'grad_norm': 8.770588874816895, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [14:52<4:25:14,  2.80s/it]  5%|â–Œ         | 318/6000 [14:56<4:32:06,  2.87s/it]                                                    {'loss': 2.7698, 'grad_norm': 5.740539073944092, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [14:56<4:32:06,  2.87s/it]  5%|â–Œ         | 319/6000 [14:58<4:32:56,  2.88s/it]                                                    {'loss': 2.7738, 'grad_norm': 5.742650032043457, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [14:58<4:32:56,  2.88s/it]  5%|â–Œ         | 320/6000 [15:01<4:30:59,  2.86s/it]                                                    {'loss': 2.7384, 'grad_norm': 8.539236068725586, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:01<4:30:59,  2.86s/it]  5%|â–Œ         | 321/6000 [15:04<4:39:17,  2.95s/it]                                                    {'loss': 2.7771, 'grad_norm': 9.393467903137207, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:04<4:39:17,  2.95s/it]  5%|â–Œ         | 322/6000 [15:07<4:33:07,  2.89s/it]                                                    {'loss': 2.7559, 'grad_norm': 9.265080451965332, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:07<4:33:07,  2.89s/it]  5%|â–Œ         | 323/6000 [15:10<4:45:14,  3.01s/it]                                                    {'loss': 2.7632, 'grad_norm': 10.068340301513672, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:10<4:45:14,  3.01s/it]  5%|â–Œ         | 324/6000 [15:13<4:35:26,  2.91s/it]                                                    {'loss': 2.7839, 'grad_norm': 7.658436298370361, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:13<4:35:26,  2.91s/it]  5%|â–Œ         | 325/6000 [15:16<4:30:02,  2.86s/it]                                                    {'loss': 2.7701, 'grad_norm': 8.024545669555664, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:16<4:30:02,  2.86s/it]  5%|â–Œ         | 326/6000 [15:19<4:25:37,  2.81s/it]                                                    {'loss': 2.7944, 'grad_norm': 6.214707851409912, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:19<4:25:37,  2.81s/it]  5%|â–Œ         | 327/6000 [15:21<4:21:26,  2.77s/it]                                                    {'loss': 2.7639, 'grad_norm': 6.51235294342041, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:21<4:21:26,  2.77s/it]  5%|â–Œ         | 328/6000 [15:24<4:21:03,  2.76s/it]                                                    {'loss': 2.7178, 'grad_norm': 9.730356216430664, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:24<4:21:03,  2.76s/it]  5%|â–Œ         | 329/6000 [15:27<4:20:42,  2.76s/it]                                                    {'loss': 2.7774, 'grad_norm': 6.388922214508057, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:27<4:20:42,  2.76s/it]  6%|â–Œ         | 330/6000 [15:29<4:19:13,  2.74s/it]                                                    {'loss': 2.7911, 'grad_norm': 7.589465141296387, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:29<4:19:13,  2.74s/it]  6%|â–Œ         | 331/6000 [15:32<4:23:25,  2.79s/it]                                                    {'loss': 2.7665, 'grad_norm': 6.324270725250244, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:32<4:23:25,  2.79s/it]  6%|â–Œ         | 332/6000 [15:35<4:20:48,  2.76s/it]                                                    {'loss': 2.7456, 'grad_norm': 5.957969665527344, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:35<4:20:48,  2.76s/it]  6%|â–Œ         | 333/6000 [15:38<4:18:50,  2.74s/it]                                                    {'loss': 2.7761, 'grad_norm': 9.969199180603027, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:38<4:18:50,  2.74s/it]  6%|â–Œ         | 334/6000 [15:40<4:18:29,  2.74s/it]                                                    {'loss': 2.7922, 'grad_norm': 11.621465682983398, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:40<4:18:29,  2.74s/it]  6%|â–Œ         | 335/6000 [15:43<4:19:54,  2.75s/it]                                                    {'loss': 2.8457, 'grad_norm': 12.76364517211914, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [15:43<4:19:54,  2.75s/it]  6%|â–Œ         | 336/6000 [15:46<4:17:56,  2.73s/it]                                                    {'loss': 2.7646, 'grad_norm': 6.917796611785889, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [15:46<4:17:56,  2.73s/it]  6%|â–Œ         | 337/6000 [15:49<4:19:56,  2.75s/it]                                                    {'loss': 2.7862, 'grad_norm': 7.658210754394531, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [15:49<4:19:56,  2.75s/it]  6%|â–Œ         | 338/6000 [15:51<4:16:29,  2.72s/it]                                                    {'loss': 2.757, 'grad_norm': 10.792262077331543, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [15:51<4:16:29,  2.72s/it]  6%|â–Œ         | 339/6000 [15:54<4:15:11,  2.70s/it]                                                    {'loss': 2.7941, 'grad_norm': 11.138026237487793, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [15:54<4:15:11,  2.70s/it]  6%|â–Œ         | 340/6000 [15:57<4:23:41,  2.80s/it]                                                    {'loss': 2.7683, 'grad_norm': 9.456912994384766, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [15:57<4:23:41,  2.80s/it]  6%|â–Œ         | 341/6000 [16:00<4:19:50,  2.75s/it]                                                    {'loss': 2.7651, 'grad_norm': 8.926840782165527, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [16:00<4:19:50,  2.75s/it]  6%|â–Œ         | 342/6000 [16:02<4:17:59,  2.74s/it]                                                    {'loss': 2.8288, 'grad_norm': 7.502130031585693, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [16:02<4:17:59,  2.74s/it]  6%|â–Œ         | 343/6000 [16:06<4:35:04,  2.92s/it]                                                    {'loss': 2.7872, 'grad_norm': 10.255024909973145, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [16:06<4:35:04,  2.92s/it]  6%|â–Œ         | 344/6000 [16:08<4:30:08,  2.87s/it]                                                    {'loss': 2.7676, 'grad_norm': 8.496387481689453, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [16:08<4:30:08,  2.87s/it]  6%|â–Œ         | 345/6000 [16:11<4:25:09,  2.81s/it]                                                    {'loss': 2.8142, 'grad_norm': 8.460600852966309, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [16:11<4:25:09,  2.81s/it]  6%|â–Œ         | 346/6000 [16:14<4:22:33,  2.79s/it]                                                    {'loss': 2.77, 'grad_norm': 13.194665908813477, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:14<4:22:33,  2.79s/it]  6%|â–Œ         | 347/6000 [16:17<4:20:25,  2.76s/it]                                                    {'loss': 2.7911, 'grad_norm': 12.510529518127441, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:17<4:20:25,  2.76s/it]  6%|â–Œ         | 348/6000 [16:19<4:21:43,  2.78s/it]                                                    {'loss': 2.7874, 'grad_norm': 8.51081657409668, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:19<4:21:43,  2.78s/it]  6%|â–Œ         | 349/6000 [16:22<4:19:47,  2.76s/it]                                                    {'loss': 2.7893, 'grad_norm': 15.116669654846191, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:22<4:19:47,  2.76s/it]  6%|â–Œ         | 350/6000 [16:25<4:19:05,  2.75s/it]                                                    {'loss': 2.7819, 'grad_norm': 8.978287696838379, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:25<4:19:05,  2.75s/it]  6%|â–Œ         | 351/6000 [16:28<4:27:39,  2.84s/it]                                                    {'loss': 2.7613, 'grad_norm': 28.38459014892578, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:28<4:27:39,  2.84s/it]  6%|â–Œ         | 352/6000 [16:31<4:22:46,  2.79s/it]                                                    {'loss': 2.7651, 'grad_norm': 14.871110916137695, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:31<4:22:46,  2.79s/it]  6%|â–Œ         | 353/6000 [16:33<4:18:27,  2.75s/it]                                                    {'loss': 2.8349, 'grad_norm': 9.200765609741211, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:33<4:18:27,  2.75s/it]  6%|â–Œ         | 354/6000 [16:36<4:19:19,  2.76s/it]                                                    {'loss': 2.7731, 'grad_norm': 15.987133026123047, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:36<4:19:19,  2.76s/it]  6%|â–Œ         | 355/6000 [16:39<4:17:48,  2.74s/it]                                                    {'loss': 2.8167, 'grad_norm': 15.587571144104004, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:39<4:17:48,  2.74s/it]  6%|â–Œ         | 356/6000 [16:42<4:25:08,  2.82s/it]                                                    {'loss': 2.7448, 'grad_norm': 16.74190902709961, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [16:42<4:25:08,  2.82s/it]  6%|â–Œ         | 357/6000 [16:44<4:22:47,  2.79s/it]                                                    {'loss': 2.7642, 'grad_norm': 14.90635871887207, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [16:44<4:22:47,  2.79s/it]  6%|â–Œ         | 358/6000 [16:47<4:19:54,  2.76s/it]                                                    {'loss': 2.7778, 'grad_norm': 9.93155288696289, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [16:47<4:19:54,  2.76s/it]  6%|â–Œ         | 359/6000 [16:50<4:20:24,  2.77s/it]                                                    {'loss': 2.7852, 'grad_norm': 12.637974739074707, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [16:50<4:20:24,  2.77s/it]  6%|â–Œ         | 360/6000 [16:53<4:20:33,  2.77s/it]                                                    {'loss': 2.8259, 'grad_norm': 12.010722160339355, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [16:53<4:20:33,  2.77s/it]