==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 03Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-6 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1103 15:02:37.999000 134269288150848 torch/distributed/run.py:779] 
W1103 15:02:37.999000 134269288150848 torch/distributed/run.py:779] *****************************************
W1103 15:02:37.999000 134269288150848 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1103 15:02:37.999000 134269288150848 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-03 15:02:48,531] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  3.99it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.70it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251103_150248-nya1do9r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 03Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/nya1do9r
[2025-11-03 15:02:50,030] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.97it/s]
[2025-11-03 15:02:50,663] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-03 15:02:59,617] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-03 15:03:00,805] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-03 15:03:00,806] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-03 15:03:05,174] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-03 15:03:05,175] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-03 15:03:06,003] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-03 15:03:06,004] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-03 15:03:06,004] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-03 15:03:06,006] INFO [src.utils:19] ==================================================
[2025-11-03 15:03:06,006] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-03 15:03:06,007] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:03:06,008] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-03 15:03:06,008] INFO [src.utils:19] ==================================================
[2025-11-03 15:03:07,760] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:03:07,760] INFO [src.trainer:342] ***** Running training *****
[2025-11-03 15:03:07,760] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:03:07,760] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:03:07,760] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:03:07,761] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:03:07,761] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:03:07,761] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:03:07,761] INFO [src.trainer:343]   Num examples = 192,000
[2025-11-03 15:03:07,761] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-03 15:03:07,762] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-11-03 15:03:07,762] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-03 15:03:07,762] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-11-03 15:03:07,763] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-11-03 15:03:07,772] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 15:03:07,775] INFO [src.trainer:351]   Number of trainable parameters = 242,578,944
[2025-11-03 15:03:07,782] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-03 15:03:07,785] INFO [src.trainer:352]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.embed_tokens.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1103 15:03:10.501501421 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1103 15:03:10.537694437 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:57:33,  4.18s/it]                                                  {'loss': 26.3553, 'grad_norm': 604.57470703125, 'learning_rate': 5.0000000000000004e-08, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:57:33,  4.18s/it]  0%|          | 2/6000 [00:06<5:33:32,  3.34s/it]                                                  {'loss': 20.1376, 'grad_norm': 453.0086669921875, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:33:32,  3.34s/it]  0%|          | 3/6000 [00:09<5:10:19,  3.10s/it]                                                  {'loss': 19.7094, 'grad_norm': 409.0905456542969, 'learning_rate': 1.5000000000000002e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:10:19,  3.10s/it]  0%|          | 4/6000 [00:12<4:59:41,  3.00s/it]                                                  {'loss': 20.0114, 'grad_norm': 447.3033447265625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:59:41,  3.00s/it]  0%|          | 5/6000 [00:15<4:49:51,  2.90s/it]                                                  {'loss': 22.7741, 'grad_norm': 512.9996337890625, 'learning_rate': 2.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:15<4:49:51,  2.90s/it]  0%|          | 6/6000 [00:18<4:46:33,  2.87s/it]                                                  {'loss': 23.5592, 'grad_norm': 534.0322265625, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:18<4:46:33,  2.87s/it]  0%|          | 7/6000 [00:20<4:42:18,  2.83s/it]                                                  {'loss': 23.0571, 'grad_norm': 499.4904479980469, 'learning_rate': 3.5000000000000004e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:42:18,  2.83s/it]  0%|          | 8/6000 [00:23<4:38:59,  2.79s/it]                                                  {'loss': 24.6412, 'grad_norm': 560.0113525390625, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:23<4:38:59,  2.79s/it]  0%|          | 9/6000 [00:26<4:39:14,  2.80s/it]                                                  {'loss': 15.695, 'grad_norm': 322.0595397949219, 'learning_rate': 4.5000000000000003e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:26<4:39:14,  2.80s/it]  0%|          | 10/6000 [00:29<4:36:04,  2.77s/it]                                                   {'loss': 22.6761, 'grad_norm': 500.980712890625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 10/6000 [00:29<4:36:04,  2.77s/it]  0%|          | 11/6000 [00:32<4:46:31,  2.87s/it]                                                   {'loss': 28.6195, 'grad_norm': 666.7045288085938, 'learning_rate': 5.5e-07, 'epoch': 0.0}
  0%|          | 11/6000 [00:32<4:46:31,  2.87s/it]  0%|          | 12/6000 [00:35<4:48:41,  2.89s/it]                                                   {'loss': 23.2365, 'grad_norm': 555.7964477539062, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 12/6000 [00:35<4:48:41,  2.89s/it]  0%|          | 13/6000 [00:37<4:47:26,  2.88s/it]                                                   {'loss': 23.6345, 'grad_norm': 558.4830932617188, 'learning_rate': 6.5e-07, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:47:26,  2.88s/it]  0%|          | 14/6000 [00:40<4:48:40,  2.89s/it]                                                   {'loss': 25.4173, 'grad_norm': 638.93359375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 14/6000 [00:40<4:48:40,  2.89s/it]  0%|          | 15/6000 [00:43<4:43:52,  2.85s/it]                                                   {'loss': 20.4059, 'grad_norm': 499.7782287597656, 'learning_rate': 7.5e-07, 'epoch': 0.0}
  0%|          | 15/6000 [00:43<4:43:52,  2.85s/it]  0%|          | 16/6000 [00:46<4:41:30,  2.82s/it]                                                   {'loss': 23.3844, 'grad_norm': 615.2567749023438, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 16/6000 [00:46<4:41:30,  2.82s/it]  0%|          | 17/6000 [00:49<4:41:58,  2.83s/it]                                                   {'loss': 21.0599, 'grad_norm': 549.708984375, 'learning_rate': 8.500000000000001e-07, 'epoch': 0.0}
  0%|          | 17/6000 [00:49<4:41:58,  2.83s/it]  0%|          | 18/6000 [00:52<4:41:56,  2.83s/it]                                                   {'loss': 17.1852, 'grad_norm': 460.0498352050781, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 18/6000 [00:52<4:41:56,  2.83s/it]  0%|          | 19/6000 [00:54<4:40:34,  2.81s/it]                                                   {'loss': 22.99, 'grad_norm': 618.7460327148438, 'learning_rate': 9.500000000000001e-07, 'epoch': 0.0}
  0%|          | 19/6000 [00:54<4:40:34,  2.81s/it]  0%|          | 20/6000 [00:57<4:39:13,  2.80s/it]                                                   {'loss': 23.3971, 'grad_norm': 713.5499267578125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:57<4:39:13,  2.80s/it]  0%|          | 21/6000 [01:00<4:42:37,  2.84s/it]                                                   {'loss': 19.445, 'grad_norm': 582.7927856445312, 'learning_rate': 1.0500000000000001e-06, 'epoch': 0.0}
  0%|          | 21/6000 [01:00<4:42:37,  2.84s/it]  0%|          | 22/6000 [01:03<4:43:39,  2.85s/it]                                                   {'loss': 19.7337, 'grad_norm': 535.4307861328125, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:03<4:43:39,  2.85s/it]  0%|          | 23/6000 [01:06<4:42:33,  2.84s/it]                                                   {'loss': 18.8034, 'grad_norm': 546.1770629882812, 'learning_rate': 1.1500000000000002e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:06<4:42:33,  2.84s/it]  0%|          | 24/6000 [01:09<4:44:13,  2.85s/it]                                                   {'loss': 11.3031, 'grad_norm': 307.2600402832031, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:09<4:44:13,  2.85s/it]  0%|          | 25/6000 [01:11<4:42:31,  2.84s/it]                                                   {'loss': 19.5094, 'grad_norm': 608.3035278320312, 'learning_rate': 1.25e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:11<4:42:31,  2.84s/it]  0%|          | 26/6000 [01:14<4:44:01,  2.85s/it]                                                   {'loss': 21.8607, 'grad_norm': 764.7496948242188, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:14<4:44:01,  2.85s/it]  0%|          | 27/6000 [01:17<4:44:23,  2.86s/it]                                                   {'loss': 21.9294, 'grad_norm': 789.3915405273438, 'learning_rate': 1.3500000000000002e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:17<4:44:23,  2.86s/it]  0%|          | 28/6000 [01:21<5:09:31,  3.11s/it]                                                   {'loss': 17.0953, 'grad_norm': 619.3863525390625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:21<5:09:31,  3.11s/it]  0%|          | 29/6000 [01:24<4:59:52,  3.01s/it]                                                   {'loss': 15.8878, 'grad_norm': 623.7965087890625, 'learning_rate': 1.45e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:24<4:59:52,  3.01s/it]  0%|          | 30/6000 [01:26<4:53:52,  2.95s/it]                                                   {'loss': 17.7632, 'grad_norm': 735.877685546875, 'learning_rate': 1.5e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:26<4:53:52,  2.95s/it]  1%|          | 31/6000 [01:29<4:49:09,  2.91s/it]                                                   {'loss': 14.3189, 'grad_norm': 653.8275756835938, 'learning_rate': 1.5500000000000002e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:29<4:49:09,  2.91s/it]  1%|          | 32/6000 [01:32<4:45:59,  2.88s/it]                                                   {'loss': 15.9161, 'grad_norm': 702.668212890625, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:32<4:45:59,  2.88s/it]  1%|          | 33/6000 [01:35<4:45:22,  2.87s/it]                                                   {'loss': 15.3643, 'grad_norm': 691.396484375, 'learning_rate': 1.6500000000000003e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:35<4:45:22,  2.87s/it]  1%|          | 34/6000 [01:38<4:43:40,  2.85s/it]                                                   {'loss': 18.0406, 'grad_norm': 1010.8543701171875, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:38<4:43:40,  2.85s/it]  1%|          | 35/6000 [01:41<4:44:04,  2.86s/it]                                                   {'loss': 12.2303, 'grad_norm': 621.775390625, 'learning_rate': 1.75e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:41<4:44:04,  2.86s/it]  1%|          | 36/6000 [01:43<4:41:14,  2.83s/it]                                                   {'loss': 15.8751, 'grad_norm': 973.127685546875, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:43<4:41:14,  2.83s/it]  1%|          | 37/6000 [01:46<4:39:56,  2.82s/it]                                                   {'loss': 11.4506, 'grad_norm': 659.9071655273438, 'learning_rate': 1.85e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:46<4:39:56,  2.82s/it]  1%|          | 38/6000 [01:49<4:37:17,  2.79s/it]                                                   {'loss': 11.8779, 'grad_norm': 848.87841796875, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:49<4:37:17,  2.79s/it]  1%|          | 39/6000 [01:52<4:37:03,  2.79s/it]                                                   {'loss': 8.3956, 'grad_norm': 525.6376342773438, 'learning_rate': 1.9500000000000004e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:52<4:37:03,  2.79s/it]  1%|          | 40/6000 [01:55<4:37:55,  2.80s/it]                                                   {'loss': 11.0731, 'grad_norm': 822.60400390625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:55<4:37:55,  2.80s/it]  1%|          | 41/6000 [01:57<4:37:00,  2.79s/it]                                                   {'loss': 9.5222, 'grad_norm': 781.8294067382812, 'learning_rate': 2.05e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:57<4:37:00,  2.79s/it]  1%|          | 42/6000 [02:00<4:36:09,  2.78s/it]                                                   {'loss': 9.4572, 'grad_norm': 904.519775390625, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.01}
  1%|          | 42/6000 [02:00<4:36:09,  2.78s/it]  1%|          | 43/6000 [02:04<5:08:42,  3.11s/it]                                                   {'loss': 7.213, 'grad_norm': 678.281494140625, 'learning_rate': 2.15e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:04<5:08:42,  3.11s/it]  1%|          | 44/6000 [02:07<5:13:08,  3.15s/it]                                                   {'loss': 6.502, 'grad_norm': 864.67138671875, 'learning_rate': 2.2e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:07<5:13:08,  3.15s/it]  1%|          | 45/6000 [02:10<5:03:33,  3.06s/it]                                                   {'loss': 5.5722, 'grad_norm': 978.8134155273438, 'learning_rate': 2.25e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:10<5:03:33,  3.06s/it]  1%|          | 46/6000 [02:13<4:59:18,  3.02s/it]                                                   {'loss': 4.4645, 'grad_norm': 682.5322875976562, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:13<4:59:18,  3.02s/it]  1%|          | 47/6000 [02:16<4:54:13,  2.97s/it]                                                   {'loss': 4.004, 'grad_norm': 313.361572265625, 'learning_rate': 2.35e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:16<4:54:13,  2.97s/it]  1%|          | 48/6000 [02:19<4:53:12,  2.96s/it]                                                   {'loss': 3.7554, 'grad_norm': 113.23323822021484, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:19<4:53:12,  2.96s/it]  1%|          | 49/6000 [02:21<4:46:46,  2.89s/it]                                                   {'loss': 3.4854, 'grad_norm': 95.20929718017578, 'learning_rate': 2.4500000000000003e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:21<4:46:46,  2.89s/it]  1%|          | 50/6000 [02:25<4:51:21,  2.94s/it]                                                   {'loss': 3.7323, 'grad_norm': 96.03394317626953, 'learning_rate': 2.5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:25<4:51:21,  2.94s/it][2025-11-03 15:05:32,974] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-03 15:05:32,987] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:05:33,613] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:38<10:13:30,  6.19s/it]                                                    {'loss': 4.3664, 'grad_norm': 160.88841247558594, 'learning_rate': 2.55e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:38<10:13:30,  6.19s/it]  1%|          | 52/6000 [02:41<8:29:44,  5.14s/it]                                                    {'loss': 3.6838, 'grad_norm': 83.57176208496094, 'learning_rate': 2.6e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:41<8:29:44,  5.14s/it]  1%|          | 53/6000 [02:44<7:22:23,  4.46s/it]                                                   {'loss': 5.7839, 'grad_norm': 292.3042907714844, 'learning_rate': 2.6500000000000005e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:44<7:22:23,  4.46s/it]  1%|          | 54/6000 [02:47<6:32:05,  3.96s/it]                                                   {'loss': 4.4176, 'grad_norm': 114.37214660644531, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:47<6:32:05,  3.96s/it]  1%|          | 55/6000 [02:49<5:57:27,  3.61s/it]                                                   {'loss': 4.5297, 'grad_norm': 148.70828247070312, 'learning_rate': 2.7500000000000004e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:49<5:57:27,  3.61s/it]  1%|          | 56/6000 [02:52<5:33:30,  3.37s/it]                                                   {'loss': 4.0912, 'grad_norm': 95.80776977539062, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:52<5:33:30,  3.37s/it]  1%|          | 57/6000 [02:55<5:16:23,  3.19s/it]                                                   {'loss': 3.6936, 'grad_norm': 103.57036590576172, 'learning_rate': 2.85e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:55<5:16:23,  3.19s/it]  1%|          | 58/6000 [02:58<5:03:45,  3.07s/it]                                                   {'loss': 3.8182, 'grad_norm': 90.2112045288086, 'learning_rate': 2.9e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:58<5:03:45,  3.07s/it]  1%|          | 59/6000 [03:01<4:53:55,  2.97s/it]                                                   {'loss': 3.8055, 'grad_norm': 99.33110046386719, 'learning_rate': 2.95e-06, 'epoch': 0.01}
  1%|          | 59/6000 [03:01<4:53:55,  2.97s/it]  1%|          | 60/6000 [03:03<4:47:49,  2.91s/it]                                                   {'loss': 3.3835, 'grad_norm': 89.89637756347656, 'learning_rate': 3e-06, 'epoch': 0.01}
  1%|          | 60/6000 [03:03<4:47:49,  2.91s/it]  1%|          | 61/6000 [03:06<4:42:13,  2.85s/it]                                                   {'loss': 3.3393, 'grad_norm': 75.62745666503906, 'learning_rate': 3.05e-06, 'epoch': 0.01}
  1%|          | 61/6000 [03:06<4:42:13,  2.85s/it]  1%|          | 62/6000 [03:09<4:40:44,  2.84s/it]                                                   {'loss': 3.5429, 'grad_norm': 93.92011260986328, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 62/6000 [03:09<4:40:44,  2.84s/it]  1%|          | 63/6000 [03:12<4:40:08,  2.83s/it]                                                   {'loss': 3.3358, 'grad_norm': 70.25078582763672, 'learning_rate': 3.1500000000000003e-06, 'epoch': 0.01}
  1%|          | 63/6000 [03:12<4:40:08,  2.83s/it]  1%|          | 64/6000 [03:14<4:37:45,  2.81s/it]                                                   {'loss': 3.0657, 'grad_norm': 49.12055206298828, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 64/6000 [03:14<4:37:45,  2.81s/it]  1%|          | 65/6000 [03:17<4:34:37,  2.78s/it]                                                   {'loss': 3.4407, 'grad_norm': 47.067596435546875, 'learning_rate': 3.2500000000000002e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:17<4:34:37,  2.78s/it]  1%|          | 66/6000 [03:20<4:48:25,  2.92s/it]                                                   {'loss': 3.1865, 'grad_norm': 74.30858612060547, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:20<4:48:25,  2.92s/it]  1%|          | 67/6000 [03:23<4:44:59,  2.88s/it]                                                   {'loss': 3.52, 'grad_norm': 68.31439208984375, 'learning_rate': 3.3500000000000005e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:23<4:44:59,  2.88s/it]  1%|          | 68/6000 [03:26<4:39:38,  2.83s/it]                                                   {'loss': 3.1344, 'grad_norm': 49.19506072998047, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:26<4:39:38,  2.83s/it]  1%|          | 69/6000 [03:29<4:37:51,  2.81s/it]                                                   {'loss': 3.7389, 'grad_norm': 422.1791076660156, 'learning_rate': 3.45e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:29<4:37:51,  2.81s/it]  1%|          | 70/6000 [03:32<4:41:06,  2.84s/it]                                                   {'loss': 3.022, 'grad_norm': 40.75847625732422, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:32<4:41:06,  2.84s/it]  1%|          | 71/6000 [03:34<4:38:09,  2.81s/it]                                                   {'loss': 3.3996, 'grad_norm': 95.15216827392578, 'learning_rate': 3.5500000000000003e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:34<4:38:09,  2.81s/it]  1%|          | 72/6000 [03:37<4:49:29,  2.93s/it]                                                   {'loss': 3.2017, 'grad_norm': 59.57576370239258, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:37<4:49:29,  2.93s/it]  1%|          | 73/6000 [03:41<4:54:50,  2.98s/it]                                                   {'loss': 3.3372, 'grad_norm': 50.622886657714844, 'learning_rate': 3.65e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:41<4:54:50,  2.98s/it]  1%|          | 74/6000 [03:43<4:46:53,  2.90s/it]                                                   {'loss': 3.1437, 'grad_norm': 49.174407958984375, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:43<4:46:53,  2.90s/it]  1%|â–         | 75/6000 [03:46<4:40:57,  2.85s/it]                                                   {'loss': 3.0178, 'grad_norm': 30.242759704589844, 'learning_rate': 3.7500000000000005e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:46<4:40:57,  2.85s/it]  1%|â–         | 76/6000 [03:49<4:42:32,  2.86s/it]                                                   {'loss': 3.0814, 'grad_norm': 39.97257995605469, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:49<4:42:32,  2.86s/it]  1%|â–         | 77/6000 [03:52<4:42:04,  2.86s/it]                                                   {'loss': 3.4205, 'grad_norm': 61.44889450073242, 'learning_rate': 3.85e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:52<4:42:04,  2.86s/it]  1%|â–         | 78/6000 [03:55<4:52:46,  2.97s/it]                                                   {'loss': 3.2124, 'grad_norm': 38.91362380981445, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:55<4:52:46,  2.97s/it]  1%|â–         | 79/6000 [03:58<4:48:50,  2.93s/it]                                                   {'loss': 2.9641, 'grad_norm': 34.801937103271484, 'learning_rate': 3.95e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:58<4:48:50,  2.93s/it]  1%|â–         | 80/6000 [04:01<4:52:52,  2.97s/it]                                                   {'loss': 3.3317, 'grad_norm': 58.66325378417969, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [04:01<4:52:52,  2.97s/it]  1%|â–         | 81/6000 [04:04<4:48:07,  2.92s/it]                                                   {'loss': 3.1688, 'grad_norm': 45.539974212646484, 'learning_rate': 4.05e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [04:04<4:48:07,  2.92s/it]  1%|â–         | 82/6000 [04:07<4:45:04,  2.89s/it]                                                   {'loss': 2.9993, 'grad_norm': 37.04005813598633, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [04:07<4:45:04,  2.89s/it]  1%|â–         | 83/6000 [04:09<4:42:27,  2.86s/it]                                                   {'loss': 3.568, 'grad_norm': 86.56645965576172, 'learning_rate': 4.15e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [04:09<4:42:27,  2.86s/it]  1%|â–         | 84/6000 [04:12<4:44:18,  2.88s/it]                                                   {'loss': 3.0104, 'grad_norm': 43.331356048583984, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [04:12<4:44:18,  2.88s/it]  1%|â–         | 85/6000 [04:15<4:53:24,  2.98s/it]                                                   {'loss': 3.1061, 'grad_norm': 52.34645080566406, 'learning_rate': 4.25e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [04:15<4:53:24,  2.98s/it]  1%|â–         | 86/6000 [04:18<4:47:50,  2.92s/it]                                                   {'loss': 3.0087, 'grad_norm': 47.02388000488281, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:18<4:47:50,  2.92s/it]  1%|â–         | 87/6000 [04:21<4:42:19,  2.86s/it]                                                   {'loss': 2.9997, 'grad_norm': 22.259126663208008, 'learning_rate': 4.350000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:21<4:42:19,  2.86s/it]  1%|â–         | 88/6000 [04:24<4:38:47,  2.83s/it]                                                   {'loss': 3.8752, 'grad_norm': 63.73691177368164, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:24<4:38:47,  2.83s/it]  1%|â–         | 89/6000 [04:26<4:35:25,  2.80s/it]                                                   {'loss': 3.1772, 'grad_norm': 42.197574615478516, 'learning_rate': 4.450000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:26<4:35:25,  2.80s/it]  2%|â–         | 90/6000 [04:29<4:36:06,  2.80s/it]                                                   {'loss': 3.0214, 'grad_norm': 40.4395637512207, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:29<4:36:06,  2.80s/it]  2%|â–         | 91/6000 [04:32<4:36:48,  2.81s/it]                                                   {'loss': 2.985, 'grad_norm': 28.581138610839844, 'learning_rate': 4.5500000000000005e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:32<4:36:48,  2.81s/it]  2%|â–         | 92/6000 [04:35<4:48:39,  2.93s/it]                                                   {'loss': 3.3114, 'grad_norm': 70.44659423828125, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:35<4:48:39,  2.93s/it]  2%|â–         | 93/6000 [04:38<4:50:03,  2.95s/it]                                                   {'loss': 3.1146, 'grad_norm': 38.77046585083008, 'learning_rate': 4.65e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:38<4:50:03,  2.95s/it]  2%|â–         | 94/6000 [04:41<4:45:44,  2.90s/it]                                                   {'loss': 3.4097, 'grad_norm': 57.849639892578125, 'learning_rate': 4.7e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:41<4:45:44,  2.90s/it]  2%|â–         | 95/6000 [04:44<4:42:50,  2.87s/it]                                                   {'loss': 2.8911, 'grad_norm': 33.64239501953125, 'learning_rate': 4.75e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:44<4:42:50,  2.87s/it]  2%|â–         | 96/6000 [04:47<4:40:51,  2.85s/it]                                                   {'loss': 2.9958, 'grad_norm': 45.22248458862305, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:47<4:40:51,  2.85s/it]  2%|â–         | 97/6000 [04:49<4:38:47,  2.83s/it]                                                   {'loss': 3.1676, 'grad_norm': 56.534339904785156, 'learning_rate': 4.85e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:49<4:38:47,  2.83s/it]  2%|â–         | 98/6000 [04:52<4:37:38,  2.82s/it]                                                   {'loss': 3.0848, 'grad_norm': 45.43878173828125, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:52<4:37:38,  2.82s/it]  2%|â–         | 99/6000 [04:55<4:35:39,  2.80s/it]                                                   {'loss': 2.9209, 'grad_norm': 24.58292007446289, 'learning_rate': 4.95e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:55<4:35:39,  2.80s/it]  2%|â–         | 100/6000 [04:58<4:33:15,  2.78s/it]                                                    {'loss': 3.0289, 'grad_norm': 58.10688018798828, 'learning_rate': 5e-06, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:58<4:33:15,  2.78s/it][2025-11-03 15:08:06,216] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-03 15:08:06,231] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:08:06,824] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [05:12<10:23:20,  6.34s/it]                                                     {'loss': 3.1393, 'grad_norm': 230.39944458007812, 'learning_rate': 4.999152542372881e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [05:12<10:23:20,  6.34s/it]  2%|â–         | 102/6000 [05:15<8:38:53,  5.28s/it]                                                     {'loss': 2.9353, 'grad_norm': 27.09733772277832, 'learning_rate': 4.998305084745763e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [05:15<8:38:53,  5.28s/it]  2%|â–         | 103/6000 [05:18<7:28:09,  4.56s/it]                                                    {'loss': 3.1125, 'grad_norm': 42.15174865722656, 'learning_rate': 4.9974576271186445e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [05:18<7:28:09,  4.56s/it]  2%|â–         | 104/6000 [05:21<6:43:47,  4.11s/it]                                                    {'loss': 2.952, 'grad_norm': 25.058570861816406, 'learning_rate': 4.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [05:21<6:43:47,  4.11s/it]  2%|â–         | 105/6000 [05:24<6:07:33,  3.74s/it]                                                    {'loss': 2.9296, 'grad_norm': 22.319494247436523, 'learning_rate': 4.995762711864407e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [05:24<6:07:33,  3.74s/it]  2%|â–         | 106/6000 [05:27<5:44:30,  3.51s/it]                                                    {'loss': 2.9661, 'grad_norm': 31.638904571533203, 'learning_rate': 4.9949152542372885e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [05:27<5:44:30,  3.51s/it]  2%|â–         | 107/6000 [05:30<5:21:09,  3.27s/it]                                                    {'loss': 2.9754, 'grad_norm': 37.617679595947266, 'learning_rate': 4.994067796610169e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:30<5:21:09,  3.27s/it]  2%|â–         | 108/6000 [05:33<5:10:27,  3.16s/it]                                                    {'loss': 2.9082, 'grad_norm': 26.372318267822266, 'learning_rate': 4.993220338983051e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:33<5:10:27,  3.16s/it]  2%|â–         | 109/6000 [05:36<5:12:42,  3.18s/it]                                                    {'loss': 3.0553, 'grad_norm': 27.985774993896484, 'learning_rate': 4.992372881355933e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:36<5:12:42,  3.18s/it]  2%|â–         | 110/6000 [05:39<5:02:19,  3.08s/it]                                                    {'loss': 3.068, 'grad_norm': 32.5583610534668, 'learning_rate': 4.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:39<5:02:19,  3.08s/it]  2%|â–         | 111/6000 [05:42<4:58:04,  3.04s/it]                                                    {'loss': 2.8568, 'grad_norm': 16.28952980041504, 'learning_rate': 4.990677966101695e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:42<4:58:04,  3.04s/it]  2%|â–         | 112/6000 [05:45<5:08:57,  3.15s/it]                                                    {'loss': 3.1151, 'grad_norm': 40.82276153564453, 'learning_rate': 4.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:45<5:08:57,  3.15s/it]  2%|â–         | 113/6000 [05:48<4:59:26,  3.05s/it]                                                    {'loss': 2.9661, 'grad_norm': 34.04541778564453, 'learning_rate': 4.988983050847458e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:48<4:59:26,  3.05s/it]  2%|â–         | 114/6000 [05:51<4:54:14,  3.00s/it]                                                    {'loss': 2.8687, 'grad_norm': 16.4927921295166, 'learning_rate': 4.98813559322034e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:51<4:54:14,  3.00s/it]  2%|â–         | 115/6000 [05:54<4:49:20,  2.95s/it]                                                    {'loss': 3.22, 'grad_norm': 25.784780502319336, 'learning_rate': 4.987288135593221e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:54<4:49:20,  2.95s/it]  2%|â–         | 116/6000 [05:56<4:46:17,  2.92s/it]                                                    {'loss': 3.2485, 'grad_norm': 308.87042236328125, 'learning_rate': 4.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:56<4:46:17,  2.92s/it]  2%|â–         | 117/6000 [05:59<4:44:19,  2.90s/it]                                                    {'loss': 3.3545, 'grad_norm': 35.00901794433594, 'learning_rate': 4.985593220338983e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:59<4:44:19,  2.90s/it]  2%|â–         | 118/6000 [06:03<4:59:51,  3.06s/it]                                                    {'loss': 2.9565, 'grad_norm': 29.651926040649414, 'learning_rate': 4.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [06:03<4:59:51,  3.06s/it]  2%|â–         | 119/6000 [06:06<4:53:13,  2.99s/it]                                                    {'loss': 2.8593, 'grad_norm': 24.252031326293945, 'learning_rate': 4.9838983050847464e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [06:06<4:53:13,  2.99s/it]  2%|â–         | 120/6000 [06:08<4:48:05,  2.94s/it]                                                    {'loss': 3.1372, 'grad_norm': 148.5762939453125, 'learning_rate': 4.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [06:08<4:48:05,  2.94s/it]  2%|â–         | 121/6000 [06:11<4:45:31,  2.91s/it]                                                    {'loss': 2.8713, 'grad_norm': 32.790321350097656, 'learning_rate': 4.982203389830509e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [06:11<4:45:31,  2.91s/it]  2%|â–         | 122/6000 [06:14<4:46:43,  2.93s/it]                                                    {'loss': 3.1137, 'grad_norm': 72.44821166992188, 'learning_rate': 4.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [06:14<4:46:43,  2.93s/it]  2%|â–         | 123/6000 [06:17<4:42:59,  2.89s/it]                                                    {'loss': 2.8862, 'grad_norm': 31.517366409301758, 'learning_rate': 4.980508474576271e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [06:17<4:42:59,  2.89s/it]  2%|â–         | 124/6000 [06:20<4:39:17,  2.85s/it]                                                    {'loss': 2.9619, 'grad_norm': 27.975799560546875, 'learning_rate': 4.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [06:20<4:39:17,  2.85s/it]  2%|â–         | 125/6000 [06:23<4:45:37,  2.92s/it]                                                    {'loss': 2.9874, 'grad_norm': 14.057241439819336, 'learning_rate': 4.9788135593220346e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [06:23<4:45:37,  2.92s/it]  2%|â–         | 126/6000 [06:26<4:46:08,  2.92s/it]                                                    {'loss': 2.9377, 'grad_norm': 32.50300216674805, 'learning_rate': 4.977966101694915e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [06:26<4:46:08,  2.92s/it]  2%|â–         | 127/6000 [06:29<4:49:01,  2.95s/it]                                                    {'loss': 2.8479, 'grad_norm': 15.71965217590332, 'learning_rate': 4.977118644067797e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [06:29<4:49:01,  2.95s/it]  2%|â–         | 128/6000 [06:32<4:43:05,  2.89s/it]                                                    {'loss': 3.0311, 'grad_norm': 84.79143524169922, 'learning_rate': 4.976271186440678e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:32<4:43:05,  2.89s/it]  2%|â–         | 129/6000 [06:34<4:41:20,  2.88s/it]                                                    {'loss': 3.2007, 'grad_norm': 31.375316619873047, 'learning_rate': 4.97542372881356e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:34<4:41:20,  2.88s/it]  2%|â–         | 130/6000 [06:37<4:39:38,  2.86s/it]                                                    {'loss': 2.8494, 'grad_norm': 20.417495727539062, 'learning_rate': 4.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:37<4:39:38,  2.86s/it]  2%|â–         | 131/6000 [06:40<4:38:34,  2.85s/it]                                                    {'loss': 3.1996, 'grad_norm': 62.05324935913086, 'learning_rate': 4.973728813559323e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:40<4:38:34,  2.85s/it]  2%|â–         | 132/6000 [06:43<4:35:57,  2.82s/it]                                                    {'loss': 2.8425, 'grad_norm': 14.90786075592041, 'learning_rate': 4.9728813559322035e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:43<4:35:57,  2.82s/it]  2%|â–         | 133/6000 [06:46<4:35:46,  2.82s/it]                                                    {'loss': 2.8869, 'grad_norm': 21.89874839782715, 'learning_rate': 4.972033898305085e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:46<4:35:46,  2.82s/it]  2%|â–         | 134/6000 [06:49<4:41:27,  2.88s/it]                                                    {'loss': 2.8789, 'grad_norm': 21.414962768554688, 'learning_rate': 4.971186440677967e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:49<4:41:27,  2.88s/it]  2%|â–         | 135/6000 [06:51<4:40:09,  2.87s/it]                                                    {'loss': 3.0388, 'grad_norm': 28.985334396362305, 'learning_rate': 4.970338983050848e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:51<4:40:09,  2.87s/it]  2%|â–         | 136/6000 [06:54<4:40:22,  2.87s/it]                                                    {'loss': 2.9057, 'grad_norm': 27.12099266052246, 'learning_rate': 4.969491525423729e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:54<4:40:22,  2.87s/it]  2%|â–         | 137/6000 [06:58<4:54:18,  3.01s/it]                                                    {'loss': 2.882, 'grad_norm': 33.55241394042969, 'learning_rate': 4.968644067796611e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:58<4:54:18,  3.01s/it]  2%|â–         | 138/6000 [07:00<4:46:36,  2.93s/it]                                                    {'loss': 2.7957, 'grad_norm': 13.354255676269531, 'learning_rate': 4.967796610169492e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [07:00<4:46:36,  2.93s/it]  2%|â–         | 139/6000 [07:03<4:47:15,  2.94s/it]                                                    {'loss': 2.8624, 'grad_norm': 26.584163665771484, 'learning_rate': 4.966949152542373e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [07:03<4:47:15,  2.94s/it]  2%|â–         | 140/6000 [07:07<4:55:15,  3.02s/it]                                                    {'loss': 2.8712, 'grad_norm': 15.344489097595215, 'learning_rate': 4.966101694915255e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [07:07<4:55:15,  3.02s/it]  2%|â–         | 141/6000 [07:09<4:49:46,  2.97s/it]                                                    {'loss': 2.8361, 'grad_norm': 16.507186889648438, 'learning_rate': 4.9652542372881365e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [07:09<4:49:46,  2.97s/it]  2%|â–         | 142/6000 [07:12<4:43:55,  2.91s/it]                                                    {'loss': 2.9427, 'grad_norm': 18.736339569091797, 'learning_rate': 4.964406779661017e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [07:12<4:43:55,  2.91s/it]  2%|â–         | 143/6000 [07:15<4:40:23,  2.87s/it]                                                    {'loss': 2.8489, 'grad_norm': 19.22795295715332, 'learning_rate': 4.963559322033898e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [07:15<4:40:23,  2.87s/it]  2%|â–         | 144/6000 [07:18<4:40:41,  2.88s/it]                                                    {'loss': 2.9678, 'grad_norm': 48.9844856262207, 'learning_rate': 4.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [07:18<4:40:41,  2.88s/it]  2%|â–         | 145/6000 [07:21<4:37:48,  2.85s/it]                                                    {'loss': 2.9525, 'grad_norm': 21.554340362548828, 'learning_rate': 4.961864406779661e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [07:21<4:37:48,  2.85s/it]  2%|â–         | 146/6000 [07:23<4:38:50,  2.86s/it]                                                    {'loss': 2.865, 'grad_norm': 13.260826110839844, 'learning_rate': 4.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [07:23<4:38:50,  2.86s/it]  2%|â–         | 147/6000 [07:26<4:36:10,  2.83s/it]                                                    {'loss': 2.8724, 'grad_norm': 20.774179458618164, 'learning_rate': 4.960169491525424e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [07:26<4:36:10,  2.83s/it]  2%|â–         | 148/6000 [07:29<4:36:00,  2.83s/it]                                                    {'loss': 2.8729, 'grad_norm': 24.900598526000977, 'learning_rate': 4.9593220338983054e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [07:29<4:36:00,  2.83s/it]  2%|â–         | 149/6000 [07:32<4:35:14,  2.82s/it]                                                    {'loss': 2.8269, 'grad_norm': 14.33387279510498, 'learning_rate': 4.958474576271187e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:32<4:35:14,  2.82s/it]  2%|â–Ž         | 150/6000 [07:35<4:35:07,  2.82s/it]                                                    {'loss': 3.0306, 'grad_norm': 35.25452423095703, 'learning_rate': 4.957627118644069e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:35<4:35:07,  2.82s/it][2025-11-03 15:10:43,189] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-03 15:10:43,200] INFO [src.utils:19] Detected wrapper â€” saving only LoRA model (encoder.base).
[2025-11-03 15:10:43,856] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/03Nov-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:49<10:02:15,  6.18s/it]                                                     {'loss': 2.8761, 'grad_norm': 19.118602752685547, 'learning_rate': 4.9567796610169495e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:49<10:02:15,  6.18s/it]  3%|â–Ž         | 152/6000 [07:52<8:23:02,  5.16s/it]                                                     {'loss': 2.8326, 'grad_norm': 10.947179794311523, 'learning_rate': 4.955932203389831e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:52<8:23:02,  5.16s/it]  3%|â–Ž         | 153/6000 [07:54<7:11:54,  4.43s/it]                                                    {'loss': 2.8499, 'grad_norm': 17.03490447998047, 'learning_rate': 4.955084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:54<7:11:54,  4.43s/it]  3%|â–Ž         | 154/6000 [07:57<6:27:51,  3.98s/it]                                                    {'loss': 2.9423, 'grad_norm': 19.61714744567871, 'learning_rate': 4.9542372881355936e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:57<6:27:51,  3.98s/it]  3%|â–Ž         | 155/6000 [08:00<5:55:20,  3.65s/it]                                                    {'loss': 2.935, 'grad_norm': 27.010391235351562, 'learning_rate': 4.953389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [08:00<5:55:20,  3.65s/it]  3%|â–Ž         | 156/6000 [08:03<5:34:15,  3.43s/it]                                                    {'loss': 2.9121, 'grad_norm': 12.005714416503906, 'learning_rate': 4.952542372881357e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [08:03<5:34:15,  3.43s/it]  3%|â–Ž         | 157/6000 [08:06<5:31:05,  3.40s/it]                                                    {'loss': 2.8351, 'grad_norm': 17.1306209564209, 'learning_rate': 4.951694915254238e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [08:06<5:31:05,  3.40s/it]  3%|â–Ž         | 158/6000 [08:09<5:12:13,  3.21s/it]                                                    {'loss': 2.8436, 'grad_norm': 12.266667366027832, 'learning_rate': 4.950847457627119e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [08:09<5:12:13,  3.21s/it]  3%|â–Ž         | 159/6000 [08:12<4:58:51,  3.07s/it]                                                    {'loss': 2.8605, 'grad_norm': 12.12592601776123, 'learning_rate': 4.95e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [08:12<4:58:51,  3.07s/it]  3%|â–Ž         | 160/6000 [08:15<5:08:42,  3.17s/it]                                                    {'loss': 2.8607, 'grad_norm': 12.367480278015137, 'learning_rate': 4.949152542372882e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [08:15<5:08:42,  3.17s/it]  3%|â–Ž         | 161/6000 [08:18<5:02:51,  3.11s/it]                                                    {'loss': 2.8593, 'grad_norm': 16.308650970458984, 'learning_rate': 4.948305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [08:18<5:02:51,  3.11s/it]  3%|â–Ž         | 162/6000 [08:21<4:52:56,  3.01s/it]                                                    {'loss': 2.8509, 'grad_norm': 14.921860694885254, 'learning_rate': 4.947457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [08:21<4:52:56,  3.01s/it]  3%|â–Ž         | 163/6000 [08:24<5:04:29,  3.13s/it]                                                    {'loss': 2.8764, 'grad_norm': 16.236690521240234, 'learning_rate': 4.946610169491526e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [08:24<5:04:29,  3.13s/it]  3%|â–Ž         | 164/6000 [08:27<4:53:46,  3.02s/it]                                                    {'loss': 2.921, 'grad_norm': 6.5454840660095215, 'learning_rate': 4.9457627118644065e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [08:27<4:53:46,  3.02s/it]  3%|â–Ž         | 165/6000 [08:30<4:51:07,  2.99s/it]                                                    {'loss': 2.8362, 'grad_norm': 11.442021369934082, 'learning_rate': 4.944915254237288e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [08:30<4:51:07,  2.99s/it]  3%|â–Ž         | 166/6000 [08:33<4:43:50,  2.92s/it]                                                    {'loss': 2.8396, 'grad_norm': 10.163270950317383, 'learning_rate': 4.94406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [08:33<4:43:50,  2.92s/it]  3%|â–Ž         | 167/6000 [08:36<4:42:31,  2.91s/it]                                                    {'loss': 2.9154, 'grad_norm': 18.702909469604492, 'learning_rate': 4.9432203389830514e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [08:36<4:42:31,  2.91s/it]  3%|â–Ž         | 168/6000 [08:39<4:40:59,  2.89s/it]                                                    {'loss': 2.862, 'grad_norm': 17.67929458618164, 'learning_rate': 4.942372881355932e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [08:39<4:40:59,  2.89s/it]  3%|â–Ž         | 169/6000 [08:42<4:51:53,  3.00s/it]                                                    {'loss': 2.8259, 'grad_norm': 6.766461372375488, 'learning_rate': 4.941525423728814e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [08:42<4:51:53,  3.00s/it]  3%|â–Ž         | 170/6000 [08:45<4:43:47,  2.92s/it]                                                    {'loss': 2.8314, 'grad_norm': 20.960681915283203, 'learning_rate': 4.9406779661016955e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:45<4:43:47,  2.92s/it]  3%|â–Ž         | 171/6000 [08:47<4:38:22,  2.87s/it]                                                    {'loss': 2.7994, 'grad_norm': 9.566179275512695, 'learning_rate': 4.939830508474577e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:47<4:38:22,  2.87s/it]  3%|â–Ž         | 172/6000 [08:50<4:40:39,  2.89s/it]                                                    {'loss': 2.8847, 'grad_norm': 20.136474609375, 'learning_rate': 4.938983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:50<4:40:39,  2.89s/it]  3%|â–Ž         | 173/6000 [08:53<4:38:38,  2.87s/it]                                                    {'loss': 2.9005, 'grad_norm': 14.016840934753418, 'learning_rate': 4.9381355932203396e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:53<4:38:38,  2.87s/it]