==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/train.log
W1021 00:31:09.214000 132236374091584 torch/distributed/run.py:779] 
W1021 00:31:09.214000 132236374091584 torch/distributed/run.py:779] *****************************************
W1021 00:31:09.214000 132236374091584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1021 00:31:09.214000 132236374091584 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-21 00:31:18,919] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.06it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.81it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251021_003119-kkb7nctu
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/kkb7nctu
[2025-10-21 00:31:20,434] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.23it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.12it/s]
[2025-10-21 00:31:21,052] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-21 00:31:21,056] INFO [src.utils:19] Loading lora adapter from TailTokenDetachPrefixWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-21 00:31:30,001] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-21 00:31:31,294] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-21 00:31:31,295] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-21 00:31:35,542] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-21 00:31:35,542] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-21 00:31:36,521] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-21 00:31:36,522] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-21 00:31:36,522] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-21 00:31:36,524] INFO [src.utils:19] ==================================================
[2025-10-21 00:31:36,524] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-21 00:31:36,525] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:31:36,526] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:31:36,526] INFO [src.utils:19] ==================================================
[2025-10-21 00:31:38,288] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:31:38,288] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:31:38,288] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:31:38,288] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:31:38,288] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:31:38,288] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:31:38,288] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:31:38,288] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:31:38,289] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:31:38,289] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:31:38,290] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:31:38,290] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:31:38,291] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:31:38,291] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:31:38,297] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-21 00:31:38,299] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1021 00:31:41.318523716 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1021 00:31:41.356700805 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:43:53,  4.04s/it]                                                  {'loss': 15.8173, 'grad_norm': 1238.916259765625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:43:53,  4.04s/it]  0%|          | 2/6000 [00:06<5:18:45,  3.19s/it]                                                  {'loss': 13.808, 'grad_norm': 1147.3233642578125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:18:45,  3.19s/it]  0%|          | 3/6000 [00:09<4:55:44,  2.96s/it]                                                  {'loss': 12.8007, 'grad_norm': 1264.4296875, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:55:44,  2.96s/it]  0%|          | 4/6000 [00:11<4:43:36,  2.84s/it]                                                  {'loss': 13.2278, 'grad_norm': 1255.15234375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:11<4:43:36,  2.84s/it]  0%|          | 5/6000 [00:14<4:36:48,  2.77s/it]                                                  {'loss': 13.6448, 'grad_norm': 1101.8564453125, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:36:48,  2.77s/it]  0%|          | 6/6000 [00:17<4:33:05,  2.73s/it]                                                  {'loss': 13.5561, 'grad_norm': 1202.6602783203125, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:33:05,  2.73s/it]  0%|          | 7/6000 [00:19<4:29:07,  2.69s/it]                                                  {'loss': 11.9133, 'grad_norm': 935.900146484375, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:19<4:29:07,  2.69s/it]  0%|          | 8/6000 [00:22<4:26:50,  2.67s/it]                                                  {'loss': 10.7917, 'grad_norm': 752.428955078125, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:26:50,  2.67s/it]  0%|          | 9/6000 [00:25<4:27:34,  2.68s/it]                                                  {'loss': 7.4094, 'grad_norm': 383.9624938964844, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:27:34,  2.68s/it]  0%|          | 10/6000 [00:27<4:23:37,  2.64s/it]                                                   {'loss': 8.6821, 'grad_norm': 501.0040283203125, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:27<4:23:37,  2.64s/it]  0%|          | 11/6000 [00:30<4:32:05,  2.73s/it]                                                   {'loss': 9.0686, 'grad_norm': 478.29254150390625, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:30<4:32:05,  2.73s/it]  0%|          | 12/6000 [00:33<4:35:31,  2.76s/it]                                                   {'loss': 7.3679, 'grad_norm': 378.54779052734375, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:33<4:35:31,  2.76s/it]  0%|          | 13/6000 [00:36<4:32:35,  2.73s/it]                                                   {'loss': 7.0078, 'grad_norm': 327.0489196777344, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:32:35,  2.73s/it]  0%|          | 14/6000 [00:38<4:32:00,  2.73s/it]                                                   {'loss': 5.8342, 'grad_norm': 288.40057373046875, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:38<4:32:00,  2.73s/it]  0%|          | 15/6000 [00:41<4:29:02,  2.70s/it]                                                   {'loss': 5.5349, 'grad_norm': 254.36782836914062, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:41<4:29:02,  2.70s/it]  0%|          | 16/6000 [00:44<4:25:54,  2.67s/it]                                                   {'loss': 5.6174, 'grad_norm': 277.89459228515625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:44<4:25:54,  2.67s/it]  0%|          | 17/6000 [00:46<4:25:50,  2.67s/it]                                                   {'loss': 5.0419, 'grad_norm': 267.8714294433594, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:46<4:25:50,  2.67s/it]  0%|          | 18/6000 [00:49<4:27:34,  2.68s/it]                                                   {'loss': 3.8553, 'grad_norm': 232.15956115722656, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:49<4:27:34,  2.68s/it]  0%|          | 19/6000 [00:52<4:27:00,  2.68s/it]                                                   {'loss': 3.531, 'grad_norm': 204.13333129882812, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:52<4:27:00,  2.68s/it]  0%|          | 20/6000 [00:54<4:26:52,  2.68s/it]                                                   {'loss': 3.269, 'grad_norm': 83.97966766357422, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:54<4:26:52,  2.68s/it]  0%|          | 21/6000 [00:57<4:30:40,  2.72s/it]                                                   {'loss': 2.9108, 'grad_norm': 46.651710510253906, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:57<4:30:40,  2.72s/it]  0%|          | 22/6000 [01:00<4:32:42,  2.74s/it]                                                   {'loss': 3.0555, 'grad_norm': 60.5523681640625, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:00<4:32:42,  2.74s/it]  0%|          | 23/6000 [01:03<4:29:42,  2.71s/it]                                                   {'loss': 3.0219, 'grad_norm': 63.217247009277344, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:03<4:29:42,  2.71s/it]  0%|          | 24/6000 [01:05<4:29:52,  2.71s/it]                                                   {'loss': 2.987, 'grad_norm': 59.11201477050781, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:05<4:29:52,  2.71s/it]  0%|          | 25/6000 [01:08<4:28:43,  2.70s/it]                                                   {'loss': 2.9675, 'grad_norm': 43.902442932128906, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:08<4:28:43,  2.70s/it]  0%|          | 26/6000 [01:11<4:29:21,  2.71s/it]                                                   {'loss': 2.8521, 'grad_norm': 20.166872024536133, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:11<4:29:21,  2.71s/it]  0%|          | 27/6000 [01:13<4:27:41,  2.69s/it]                                                   {'loss': 2.9356, 'grad_norm': 35.26981735229492, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:13<4:27:41,  2.69s/it]  0%|          | 28/6000 [01:17<4:53:43,  2.95s/it]                                                   {'loss': 2.8894, 'grad_norm': 40.64682388305664, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:17<4:53:43,  2.95s/it]  0%|          | 29/6000 [01:20<4:43:01,  2.84s/it]                                                   {'loss': 2.8245, 'grad_norm': 15.832002639770508, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:20<4:43:01,  2.84s/it]  0%|          | 30/6000 [01:22<4:38:06,  2.80s/it]                                                   {'loss': 2.8398, 'grad_norm': 18.003877639770508, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:22<4:38:06,  2.80s/it]  1%|          | 31/6000 [01:25<4:35:06,  2.77s/it]                                                   {'loss': 2.8282, 'grad_norm': 18.326969146728516, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:25<4:35:06,  2.77s/it]  1%|          | 32/6000 [01:28<4:31:32,  2.73s/it]                                                   {'loss': 2.801, 'grad_norm': 12.150885581970215, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:28<4:31:32,  2.73s/it]  1%|          | 33/6000 [01:30<4:30:25,  2.72s/it]                                                   {'loss': 2.7958, 'grad_norm': 11.499507904052734, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:30<4:30:25,  2.72s/it]  1%|          | 34/6000 [01:33<4:30:51,  2.72s/it]                                                   {'loss': 2.848, 'grad_norm': 7.887072563171387, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:33<4:30:51,  2.72s/it]  1%|          | 35/6000 [01:36<4:31:26,  2.73s/it]                                                   {'loss': 2.8068, 'grad_norm': 8.739243507385254, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:36<4:31:26,  2.73s/it]  1%|          | 36/6000 [01:38<4:28:35,  2.70s/it]                                                   {'loss': 2.8016, 'grad_norm': 8.788897514343262, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:38<4:28:35,  2.70s/it]  1%|          | 37/6000 [01:41<4:26:13,  2.68s/it]                                                   {'loss': 2.8041, 'grad_norm': 5.842103958129883, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:41<4:26:13,  2.68s/it]  1%|          | 38/6000 [01:44<4:24:34,  2.66s/it]                                                   {'loss': 2.7898, 'grad_norm': 7.3666839599609375, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:44<4:24:34,  2.66s/it]  1%|          | 39/6000 [01:46<4:23:32,  2.65s/it]                                                   {'loss': 2.8552, 'grad_norm': 7.499634265899658, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:46<4:23:32,  2.65s/it]  1%|          | 40/6000 [01:49<4:23:21,  2.65s/it]                                                   {'loss': 2.8383, 'grad_norm': 14.96182918548584, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:49<4:23:21,  2.65s/it]  1%|          | 41/6000 [01:52<4:22:56,  2.65s/it]                                                   {'loss': 2.7847, 'grad_norm': 5.85023307800293, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:52<4:22:56,  2.65s/it]  1%|          | 42/6000 [01:54<4:23:01,  2.65s/it]                                                   {'loss': 2.7805, 'grad_norm': 4.123748302459717, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:54<4:23:01,  2.65s/it]  1%|          | 43/6000 [01:58<4:52:54,  2.95s/it]                                                   {'loss': 2.8486, 'grad_norm': 32.55035400390625, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [01:58<4:52:54,  2.95s/it]  1%|          | 44/6000 [02:01<4:58:07,  3.00s/it]                                                   {'loss': 2.7788, 'grad_norm': 6.395904541015625, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:01<4:58:07,  3.00s/it]  1%|          | 45/6000 [02:04<4:48:37,  2.91s/it]                                                   {'loss': 2.801, 'grad_norm': 4.262766361236572, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:04<4:48:37,  2.91s/it]  1%|          | 46/6000 [02:06<4:43:48,  2.86s/it]                                                   {'loss': 2.78, 'grad_norm': 4.143752098083496, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:06<4:43:48,  2.86s/it]  1%|          | 47/6000 [02:09<4:38:38,  2.81s/it]                                                   {'loss': 2.7812, 'grad_norm': 4.640450954437256, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:09<4:38:38,  2.81s/it]  1%|          | 48/6000 [02:12<4:37:24,  2.80s/it]                                                   {'loss': 2.7811, 'grad_norm': 5.256428241729736, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:12<4:37:24,  2.80s/it]  1%|          | 49/6000 [02:14<4:32:33,  2.75s/it]                                                   {'loss': 2.7854, 'grad_norm': 4.479612827301025, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:14<4:32:33,  2.75s/it]  1%|          | 50/6000 [02:17<4:34:54,  2.77s/it]                                                   {'loss': 2.7681, 'grad_norm': 4.116950511932373, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:17<4:34:54,  2.77s/it][2025-10-21 00:33:56,301] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:22<5:28:21,  3.31s/it]                                                   {'loss': 2.7973, 'grad_norm': 5.026512145996094, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:22<5:28:21,  3.31s/it]  1%|          | 52/6000 [02:25<5:08:11,  3.11s/it]                                                   {'loss': 2.7919, 'grad_norm': 4.140687465667725, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:25<5:08:11,  3.11s/it]  1%|          | 53/6000 [02:27<4:57:04,  3.00s/it]                                                   {'loss': 2.8494, 'grad_norm': 3.932699203491211, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:27<4:57:04,  3.00s/it]  1%|          | 54/6000 [02:30<4:46:22,  2.89s/it]                                                   {'loss': 2.7827, 'grad_norm': 3.4760842323303223, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:30<4:46:22,  2.89s/it]  1%|          | 55/6000 [02:33<4:39:58,  2.83s/it]                                                   {'loss': 2.7838, 'grad_norm': 3.8382556438446045, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:33<4:39:58,  2.83s/it]  1%|          | 56/6000 [02:35<4:34:45,  2.77s/it]                                                   {'loss': 2.7864, 'grad_norm': 3.9723832607269287, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:35<4:34:45,  2.77s/it]  1%|          | 57/6000 [02:38<4:30:48,  2.73s/it]                                                   {'loss': 2.7799, 'grad_norm': 3.5946061611175537, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:38<4:30:48,  2.73s/it]  1%|          | 58/6000 [02:41<4:28:36,  2.71s/it]                                                   {'loss': 2.773, 'grad_norm': 4.2731099128723145, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:41<4:28:36,  2.71s/it]  1%|          | 59/6000 [02:43<4:24:57,  2.68s/it]                                                   {'loss': 2.7773, 'grad_norm': 2.8274989128112793, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:43<4:24:57,  2.68s/it]  1%|          | 60/6000 [02:46<4:23:25,  2.66s/it]                                                   {'loss': 2.775, 'grad_norm': 4.275331974029541, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:46<4:23:25,  2.66s/it]  1%|          | 61/6000 [02:48<4:22:53,  2.66s/it]                                                   {'loss': 2.7977, 'grad_norm': 4.9144792556762695, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:48<4:22:53,  2.66s/it]  1%|          | 62/6000 [02:51<4:23:11,  2.66s/it]                                                   {'loss': 2.7904, 'grad_norm': 8.750885963439941, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:51<4:23:11,  2.66s/it]  1%|          | 63/6000 [02:54<4:22:50,  2.66s/it]                                                   {'loss': 2.7739, 'grad_norm': 5.677565097808838, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:54<4:22:50,  2.66s/it]  1%|          | 64/6000 [02:56<4:21:13,  2.64s/it]                                                   {'loss': 2.7919, 'grad_norm': 5.421158313751221, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [02:56<4:21:13,  2.64s/it]  1%|          | 65/6000 [02:59<4:20:05,  2.63s/it]                                                   {'loss': 2.782, 'grad_norm': 6.952876091003418, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [02:59<4:20:05,  2.63s/it]  1%|          | 66/6000 [03:02<4:33:03,  2.76s/it]                                                   {'loss': 2.7601, 'grad_norm': 9.004218101501465, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:02<4:33:03,  2.76s/it]  1%|          | 67/6000 [03:05<4:30:20,  2.73s/it]                                                   {'loss': 2.7832, 'grad_norm': 6.603579998016357, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:05<4:30:20,  2.73s/it]  1%|          | 68/6000 [03:07<4:28:12,  2.71s/it]                                                   {'loss': 2.8012, 'grad_norm': 7.545442581176758, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:07<4:28:12,  2.71s/it]  1%|          | 69/6000 [03:10<4:25:54,  2.69s/it]                                                   {'loss': 2.773, 'grad_norm': 6.036355018615723, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:10<4:25:54,  2.69s/it]  1%|          | 70/6000 [03:13<4:27:37,  2.71s/it]                                                   {'loss': 2.8272, 'grad_norm': 13.236098289489746, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:13<4:27:37,  2.71s/it]  1%|          | 71/6000 [03:15<4:25:45,  2.69s/it]                                                   {'loss': 2.7926, 'grad_norm': 8.954289436340332, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:15<4:25:45,  2.69s/it]  1%|          | 72/6000 [03:18<4:36:03,  2.79s/it]                                                   {'loss': 2.7941, 'grad_norm': 6.958523273468018, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:18<4:36:03,  2.79s/it]  1%|          | 73/6000 [03:21<4:41:04,  2.85s/it]                                                   {'loss': 2.8319, 'grad_norm': 6.290162086486816, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:21<4:41:04,  2.85s/it]  1%|          | 74/6000 [03:24<4:35:05,  2.79s/it]                                                   {'loss': 2.771, 'grad_norm': 4.034420967102051, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:24<4:35:05,  2.79s/it]  1%|â–         | 75/6000 [03:27<4:32:05,  2.76s/it]                                                   {'loss': 2.7975, 'grad_norm': 4.68225622177124, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:27<4:32:05,  2.76s/it]  1%|â–         | 76/6000 [03:29<4:32:47,  2.76s/it]                                                   {'loss': 2.8075, 'grad_norm': 3.5802383422851562, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:29<4:32:47,  2.76s/it]  1%|â–         | 77/6000 [03:32<4:30:27,  2.74s/it]                                                   {'loss': 2.8401, 'grad_norm': 3.3165502548217773, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:32<4:30:27,  2.74s/it]  1%|â–         | 78/6000 [03:35<4:39:15,  2.83s/it]                                                   {'loss': 2.7788, 'grad_norm': 4.138056755065918, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:35<4:39:15,  2.83s/it]  1%|â–         | 79/6000 [03:38<4:34:23,  2.78s/it]                                                   {'loss': 2.7879, 'grad_norm': 4.370080471038818, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:38<4:34:23,  2.78s/it]  1%|â–         | 80/6000 [03:41<4:38:30,  2.82s/it]                                                   {'loss': 2.7893, 'grad_norm': 3.688669204711914, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:41<4:38:30,  2.82s/it]  1%|â–         | 81/6000 [03:43<4:34:19,  2.78s/it]                                                   {'loss': 2.7652, 'grad_norm': 3.5745675563812256, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:43<4:34:19,  2.78s/it]  1%|â–         | 82/6000 [03:46<4:33:13,  2.77s/it]                                                   {'loss': 2.7772, 'grad_norm': 3.914776563644409, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:46<4:33:13,  2.77s/it]  1%|â–         | 83/6000 [03:49<4:28:56,  2.73s/it]                                                   {'loss': 2.794, 'grad_norm': 4.108569145202637, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:49<4:28:56,  2.73s/it]  1%|â–         | 84/6000 [03:52<4:30:26,  2.74s/it]                                                   {'loss': 2.776, 'grad_norm': 5.151379108428955, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:52<4:30:26,  2.74s/it]  1%|â–         | 85/6000 [03:55<4:38:02,  2.82s/it]                                                   {'loss': 2.7842, 'grad_norm': 5.5138726234436035, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:55<4:38:02,  2.82s/it]  1%|â–         | 86/6000 [03:57<4:32:28,  2.76s/it]                                                   {'loss': 2.7877, 'grad_norm': 4.092667102813721, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [03:57<4:32:28,  2.76s/it]  1%|â–         | 87/6000 [04:00<4:28:52,  2.73s/it]                                                   {'loss': 2.7884, 'grad_norm': 3.718149423599243, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:00<4:28:52,  2.73s/it]  1%|â–         | 88/6000 [04:03<4:28:34,  2.73s/it]                                                   {'loss': 2.7839, 'grad_norm': 5.120832443237305, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:03<4:28:34,  2.73s/it]  1%|â–         | 89/6000 [04:05<4:24:09,  2.68s/it]                                                   {'loss': 2.7745, 'grad_norm': 4.058126926422119, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:05<4:24:09,  2.68s/it]  2%|â–         | 90/6000 [04:08<4:24:25,  2.68s/it]                                                   {'loss': 2.8198, 'grad_norm': 11.951772689819336, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:08<4:24:25,  2.68s/it]  2%|â–         | 91/6000 [04:11<4:23:07,  2.67s/it]                                                   {'loss': 2.7898, 'grad_norm': 7.236229419708252, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:11<4:23:07,  2.67s/it]  2%|â–         | 92/6000 [04:14<4:33:35,  2.78s/it]                                                   {'loss': 2.786, 'grad_norm': 5.993050575256348, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:14<4:33:35,  2.78s/it]  2%|â–         | 93/6000 [04:16<4:32:45,  2.77s/it]                                                   {'loss': 2.7755, 'grad_norm': 5.084987640380859, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:16<4:32:45,  2.77s/it]  2%|â–         | 94/6000 [04:19<4:29:01,  2.73s/it]                                                   {'loss': 2.7976, 'grad_norm': 5.41937255859375, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:19<4:29:01,  2.73s/it]  2%|â–         | 95/6000 [04:22<4:26:48,  2.71s/it]                                                   {'loss': 2.7859, 'grad_norm': 6.312000274658203, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:22<4:26:48,  2.71s/it]  2%|â–         | 96/6000 [04:24<4:24:26,  2.69s/it]                                                   {'loss': 2.768, 'grad_norm': 5.680116176605225, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:24<4:24:26,  2.69s/it]  2%|â–         | 97/6000 [04:27<4:23:03,  2.67s/it]                                                   {'loss': 2.7937, 'grad_norm': 6.945690155029297, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:27<4:23:03,  2.67s/it]  2%|â–         | 98/6000 [04:30<4:22:11,  2.67s/it]                                                   {'loss': 2.7606, 'grad_norm': 6.0454020500183105, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:30<4:22:11,  2.67s/it]  2%|â–         | 99/6000 [04:32<4:21:25,  2.66s/it]                                                   {'loss': 2.7796, 'grad_norm': 4.42240047454834, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:32<4:21:25,  2.66s/it]  2%|â–         | 100/6000 [04:35<4:19:09,  2.64s/it]                                                    {'loss': 2.7741, 'grad_norm': 5.055241107940674, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:35<4:19:09,  2.64s/it][2025-10-21 00:36:13,739] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:40<5:43:56,  3.50s/it]                                                    {'loss': 2.7824, 'grad_norm': 6.0748724937438965, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:40<5:43:56,  3.50s/it]  2%|â–         | 102/6000 [04:43<5:18:34,  3.24s/it]                                                    {'loss': 2.7985, 'grad_norm': 4.828510761260986, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:43<5:18:34,  3.24s/it]  2%|â–         | 103/6000 [04:46<5:03:00,  3.08s/it]                                                    {'loss': 2.7817, 'grad_norm': 7.488555908203125, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:46<5:03:00,  3.08s/it]  2%|â–         | 104/6000 [04:48<4:57:04,  3.02s/it]                                                    {'loss': 2.7775, 'grad_norm': 4.841434001922607, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:48<4:57:04,  3.02s/it]  2%|â–         | 105/6000 [04:51<4:48:40,  2.94s/it]                                                    {'loss': 2.793, 'grad_norm': 4.1313700675964355, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:51<4:48:40,  2.94s/it]  2%|â–         | 106/6000 [04:54<4:45:09,  2.90s/it]                                                    {'loss': 2.778, 'grad_norm': 3.289112091064453, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:54<4:45:09,  2.90s/it]  2%|â–         | 107/6000 [04:57<4:37:21,  2.82s/it]                                                    {'loss': 2.7994, 'grad_norm': 3.221935987472534, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [04:57<4:37:21,  2.82s/it]  2%|â–         | 108/6000 [04:59<4:36:25,  2.81s/it]                                                    {'loss': 2.7764, 'grad_norm': 2.2475361824035645, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [04:59<4:36:25,  2.81s/it]  2%|â–         | 109/6000 [05:03<4:44:18,  2.90s/it]                                                    {'loss': 2.818, 'grad_norm': 2.435429811477661, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:03<4:44:18,  2.90s/it]  2%|â–         | 110/6000 [05:05<4:38:17,  2.83s/it]                                                    {'loss': 2.8466, 'grad_norm': 2.1659114360809326, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:05<4:38:17,  2.83s/it]  2%|â–         | 111/6000 [05:08<4:37:35,  2.83s/it]                                                    {'loss': 2.7785, 'grad_norm': 1.864357590675354, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:08<4:37:35,  2.83s/it]  2%|â–         | 112/6000 [05:11<4:49:12,  2.95s/it]                                                    {'loss': 2.7847, 'grad_norm': 2.761521816253662, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:11<4:49:12,  2.95s/it]  2%|â–         | 113/6000 [05:14<4:40:56,  2.86s/it]                                                    {'loss': 2.7718, 'grad_norm': 2.035782814025879, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:14<4:40:56,  2.86s/it]  2%|â–         | 114/6000 [05:17<4:36:32,  2.82s/it]                                                    {'loss': 2.7653, 'grad_norm': 1.9146844148635864, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:17<4:36:32,  2.82s/it]  2%|â–         | 115/6000 [05:19<4:32:31,  2.78s/it]                                                    {'loss': 2.843, 'grad_norm': 2.71336030960083, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:19<4:32:31,  2.78s/it]  2%|â–         | 116/6000 [05:22<4:29:53,  2.75s/it]                                                    {'loss': 2.7998, 'grad_norm': 4.231805324554443, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:22<4:29:53,  2.75s/it]  2%|â–         | 117/6000 [05:25<4:28:55,  2.74s/it]                                                    {'loss': 2.9206, 'grad_norm': 6.433759689331055, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:25<4:28:55,  2.74s/it]  2%|â–         | 118/6000 [05:28<4:43:12,  2.89s/it]                                                    {'loss': 2.7818, 'grad_norm': 3.309501886367798, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:28<4:43:12,  2.89s/it]  2%|â–         | 119/6000 [05:31<4:36:34,  2.82s/it]                                                    {'loss': 2.7811, 'grad_norm': 2.8833513259887695, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:31<4:36:34,  2.82s/it]  2%|â–         | 120/6000 [05:33<4:33:03,  2.79s/it]                                                    {'loss': 2.779, 'grad_norm': 4.471922397613525, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:33<4:33:03,  2.79s/it]  2%|â–         | 121/6000 [05:36<4:31:12,  2.77s/it]                                                    {'loss': 2.7767, 'grad_norm': 2.5731353759765625, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:36<4:31:12,  2.77s/it]  2%|â–         | 122/6000 [05:39<4:33:53,  2.80s/it]                                                    {'loss': 2.7727, 'grad_norm': 2.184136152267456, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:39<4:33:53,  2.80s/it]  2%|â–         | 123/6000 [05:42<4:30:25,  2.76s/it]                                                    {'loss': 2.7878, 'grad_norm': 1.8929333686828613, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:42<4:30:25,  2.76s/it]  2%|â–         | 124/6000 [05:44<4:26:20,  2.72s/it]                                                    {'loss': 2.8203, 'grad_norm': 2.201618194580078, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:44<4:26:20,  2.72s/it]  2%|â–         | 125/6000 [05:47<4:32:12,  2.78s/it]                                                    {'loss': 2.9319, 'grad_norm': 2.5441694259643555, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:47<4:32:12,  2.78s/it]  2%|â–         | 126/6000 [05:50<4:31:37,  2.77s/it]                                                    {'loss': 2.7744, 'grad_norm': 1.6392743587493896, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:50<4:31:37,  2.77s/it]  2%|â–         | 127/6000 [05:53<4:31:32,  2.77s/it]                                                    {'loss': 2.772, 'grad_norm': 1.6808267831802368, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:53<4:31:32,  2.77s/it]  2%|â–         | 128/6000 [05:55<4:28:21,  2.74s/it]                                                    {'loss': 2.8012, 'grad_norm': 2.1769144535064697, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [05:55<4:28:21,  2.74s/it]  2%|â–         | 129/6000 [05:58<4:25:55,  2.72s/it]                                                    {'loss': 2.8428, 'grad_norm': 2.200206995010376, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [05:58<4:25:55,  2.72s/it]  2%|â–         | 130/6000 [06:01<4:24:01,  2.70s/it]                                                    {'loss': 2.7744, 'grad_norm': 2.26839017868042, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:01<4:24:01,  2.70s/it]  2%|â–         | 131/6000 [06:03<4:23:50,  2.70s/it]                                                    {'loss': 2.7763, 'grad_norm': 2.1270415782928467, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:03<4:23:50,  2.70s/it]  2%|â–         | 132/6000 [06:06<4:21:49,  2.68s/it]                                                    {'loss': 2.7997, 'grad_norm': 8.894871711730957, 'learning_rate': 4.972881355932204e-05, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:06<4:21:49,  2.68s/it]  2%|â–         | 133/6000 [06:09<4:20:48,  2.67s/it]                                                    {'loss': 2.7687, 'grad_norm': 1.8562352657318115, 'learning_rate': 4.972033898305085e-05, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:09<4:20:48,  2.67s/it]  2%|â–         | 134/6000 [06:12<4:27:07,  2.73s/it]                                                    {'loss': 2.7735, 'grad_norm': 2.918765068054199, 'learning_rate': 4.971186440677966e-05, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:12<4:27:07,  2.73s/it]  2%|â–         | 135/6000 [06:14<4:26:03,  2.72s/it]                                                    {'loss': 2.7849, 'grad_norm': 2.4032211303710938, 'learning_rate': 4.970338983050848e-05, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:14<4:26:03,  2.72s/it]  2%|â–         | 136/6000 [06:17<4:25:04,  2.71s/it]                                                    {'loss': 2.7883, 'grad_norm': 2.598458766937256, 'learning_rate': 4.969491525423729e-05, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:17<4:25:04,  2.71s/it]  2%|â–         | 137/6000 [06:20<4:40:03,  2.87s/it]                                                    {'loss': 2.762, 'grad_norm': 2.9350974559783936, 'learning_rate': 4.968644067796611e-05, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:20<4:40:03,  2.87s/it]  2%|â–         | 138/6000 [06:23<4:33:04,  2.80s/it]                                                    {'loss': 2.7711, 'grad_norm': 2.437509536743164, 'learning_rate': 4.967796610169492e-05, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:23<4:33:04,  2.80s/it]  2%|â–         | 139/6000 [06:26<4:33:17,  2.80s/it]                                                    {'loss': 2.8306, 'grad_norm': 3.5869898796081543, 'learning_rate': 4.966949152542373e-05, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:26<4:33:17,  2.80s/it]  2%|â–         | 140/6000 [06:29<4:39:43,  2.86s/it]                                                    {'loss': 2.7794, 'grad_norm': 3.602952718734741, 'learning_rate': 4.966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:29<4:39:43,  2.86s/it]  2%|â–         | 141/6000 [06:31<4:35:14,  2.82s/it]                                                    {'loss': 2.7725, 'grad_norm': 2.3051464557647705, 'learning_rate': 4.965254237288136e-05, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:31<4:35:14,  2.82s/it]  2%|â–         | 142/6000 [06:34<4:28:58,  2.76s/it]                                                    {'loss': 2.7781, 'grad_norm': 2.0505690574645996, 'learning_rate': 4.964406779661017e-05, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:34<4:28:58,  2.76s/it]  2%|â–         | 143/6000 [06:37<4:26:25,  2.73s/it]                                                    {'loss': 2.7593, 'grad_norm': 2.167635917663574, 'learning_rate': 4.963559322033898e-05, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:37<4:26:25,  2.73s/it]  2%|â–         | 144/6000 [06:39<4:25:03,  2.72s/it]                                                    {'loss': 2.7656, 'grad_norm': 3.6381139755249023, 'learning_rate': 4.96271186440678e-05, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:39<4:25:03,  2.72s/it]  2%|â–         | 145/6000 [06:42<4:23:34,  2.70s/it]                                                    {'loss': 2.8235, 'grad_norm': 18.596717834472656, 'learning_rate': 4.961864406779661e-05, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:42<4:23:34,  2.70s/it]  2%|â–         | 146/6000 [06:45<4:24:14,  2.71s/it]                                                    {'loss': 2.8196, 'grad_norm': 13.71226978302002, 'learning_rate': 4.961016949152543e-05, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:45<4:24:14,  2.71s/it]  2%|â–         | 147/6000 [06:47<4:22:57,  2.70s/it]                                                    {'loss': 2.8131, 'grad_norm': 28.15581512451172, 'learning_rate': 4.9601694915254234e-05, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:47<4:22:57,  2.70s/it]  2%|â–         | 148/6000 [06:50<4:23:14,  2.70s/it]                                                    {'loss': 2.8129, 'grad_norm': 4.14888334274292, 'learning_rate': 4.959322033898305e-05, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:50<4:23:14,  2.70s/it]  2%|â–         | 149/6000 [06:53<4:21:25,  2.68s/it]                                                    {'loss': 2.7688, 'grad_norm': 3.112623691558838, 'learning_rate': 4.9584745762711864e-05, 'epoch': 0.02}
  2%|â–         | 149/6000 [06:53<4:21:25,  2.68s/it]  2%|â–Ž         | 150/6000 [06:55<4:19:34,  2.66s/it]                                                    {'loss': 2.803, 'grad_norm': 3.321108818054199, 'learning_rate': 4.957627118644068e-05, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [06:55<4:19:34,  2.66s/it][2025-10-21 00:38:34,309] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [07:00<5:22:33,  3.31s/it]                                                    {'loss': 2.7859, 'grad_norm': 2.9630208015441895, 'learning_rate': 4.956779661016949e-05, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:00<5:22:33,  3.31s/it]  3%|â–Ž         | 152/6000 [07:03<5:04:14,  3.12s/it]                                                    {'loss': 2.7847, 'grad_norm': 3.829665184020996, 'learning_rate': 4.955932203389831e-05, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:03<5:04:14,  3.12s/it]  3%|â–Ž         | 153/6000 [07:05<4:50:06,  2.98s/it]                                                    {'loss': 2.7563, 'grad_norm': 3.514068365097046, 'learning_rate': 4.955084745762712e-05, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:05<4:50:06,  2.98s/it]  3%|â–Ž         | 154/6000 [07:08<4:43:08,  2.91s/it]                                                    {'loss': 2.7665, 'grad_norm': 3.3251185417175293, 'learning_rate': 4.9542372881355934e-05, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:08<4:43:08,  2.91s/it]  3%|â–Ž         | 155/6000 [07:11<4:37:29,  2.85s/it]                                                    {'loss': 2.8143, 'grad_norm': 8.295954704284668, 'learning_rate': 4.9533898305084745e-05, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:11<4:37:29,  2.85s/it]  3%|â–Ž         | 156/6000 [07:14<4:35:38,  2.83s/it]                                                    {'loss': 2.8521, 'grad_norm': 5.747308731079102, 'learning_rate': 4.952542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:14<4:35:38,  2.83s/it]  3%|â–Ž         | 157/6000 [07:17<4:44:57,  2.93s/it]                                                    {'loss': 2.7585, 'grad_norm': 6.379432678222656, 'learning_rate': 4.9516949152542374e-05, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:17<4:44:57,  2.93s/it]  3%|â–Ž         | 158/6000 [07:20<4:37:06,  2.85s/it]                                                    {'loss': 2.781, 'grad_norm': 4.461342811584473, 'learning_rate': 4.950847457627119e-05, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:20<4:37:06,  2.85s/it]  3%|â–Ž         | 159/6000 [07:22<4:32:39,  2.80s/it]                                                    {'loss': 2.7689, 'grad_norm': 3.895707607269287, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:22<4:32:39,  2.80s/it]  3%|â–Ž         | 160/6000 [07:25<4:46:36,  2.94s/it]                                                    {'loss': 2.7332, 'grad_norm': 6.419989585876465, 'learning_rate': 4.9491525423728815e-05, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:25<4:46:36,  2.94s/it]  3%|â–Ž         | 161/6000 [07:28<4:42:16,  2.90s/it]                                                    {'loss': 2.7641, 'grad_norm': 6.284256935119629, 'learning_rate': 4.9483050847457626e-05, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:28<4:42:16,  2.90s/it]  3%|â–Ž         | 162/6000 [07:31<4:34:28,  2.82s/it]                                                    {'loss': 2.8019, 'grad_norm': 4.689355850219727, 'learning_rate': 4.9474576271186444e-05, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:31<4:34:28,  2.82s/it]  3%|â–Ž         | 163/6000 [07:34<4:46:34,  2.95s/it]                                                    {'loss': 2.784, 'grad_norm': 6.690652370452881, 'learning_rate': 4.9466101694915256e-05, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:34<4:46:34,  2.95s/it]  3%|â–Ž         | 164/6000 [07:37<4:37:26,  2.85s/it]                                                    {'loss': 2.9141, 'grad_norm': 6.516364097595215, 'learning_rate': 4.945762711864407e-05, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:37<4:37:26,  2.85s/it]  3%|â–Ž         | 165/6000 [07:40<4:34:48,  2.83s/it]                                                    {'loss': 2.8416, 'grad_norm': 7.776507377624512, 'learning_rate': 4.9449152542372885e-05, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:40<4:34:48,  2.83s/it]  3%|â–Ž         | 166/6000 [07:42<4:30:17,  2.78s/it]                                                    {'loss': 2.7763, 'grad_norm': 5.512590408325195, 'learning_rate': 4.9440677966101696e-05, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:42<4:30:17,  2.78s/it]  3%|â–Ž         | 167/6000 [07:45<4:29:00,  2.77s/it]                                                    {'loss': 2.8432, 'grad_norm': 6.597500801086426, 'learning_rate': 4.9432203389830514e-05, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:45<4:29:00,  2.77s/it]  3%|â–Ž         | 168/6000 [07:48<4:27:31,  2.75s/it]                                                    {'loss': 2.8232, 'grad_norm': 4.7606611251831055, 'learning_rate': 4.9423728813559326e-05, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:48<4:27:31,  2.75s/it]  3%|â–Ž         | 169/6000 [07:51<4:37:38,  2.86s/it]                                                    {'loss': 2.81, 'grad_norm': 2.813091278076172, 'learning_rate': 4.941525423728814e-05, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:51<4:37:38,  2.86s/it]  3%|â–Ž         | 170/6000 [07:53<4:31:22,  2.79s/it]                                                    {'loss': 2.7712, 'grad_norm': 2.3097357749938965, 'learning_rate': 4.940677966101695e-05, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [07:53<4:31:22,  2.79s/it]  3%|â–Ž         | 171/6000 [07:56<4:27:31,  2.75s/it]                                                    {'loss': 2.7803, 'grad_norm': 4.569736480712891, 'learning_rate': 4.9398305084745766e-05, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [07:56<4:27:31,  2.75s/it]  3%|â–Ž         | 172/6000 [07:59<4:27:52,  2.76s/it]                                                    {'loss': 2.7841, 'grad_norm': 6.299350261688232, 'learning_rate': 4.938983050847458e-05, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [07:59<4:27:52,  2.76s/it]  3%|â–Ž         | 173/6000 [08:02<4:27:01,  2.75s/it]                                                    {'loss': 2.779, 'grad_norm': 12.740720748901367, 'learning_rate': 4.9381355932203396e-05, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:02<4:27:01,  2.75s/it]  3%|â–Ž         | 174/6000 [08:05<4:44:40,  2.93s/it]                                                    {'loss': 2.7924, 'grad_norm': 12.021575927734375, 'learning_rate': 4.937288135593221e-05, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:05<4:44:40,  2.93s/it]  3%|â–Ž         | 175/6000 [08:08<4:38:01,  2.86s/it]                                                    {'loss': 2.7811, 'grad_norm': 4.39193868637085, 'learning_rate': 4.936440677966102e-05, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:08<4:38:01,  2.86s/it]  3%|â–Ž         | 176/6000 [08:11<4:40:17,  2.89s/it]                                                    {'loss': 2.773, 'grad_norm': 5.116250514984131, 'learning_rate': 4.935593220338983e-05, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:11<4:40:17,  2.89s/it]  3%|â–Ž         | 177/6000 [08:13<4:35:09,  2.84s/it]                                                    {'loss': 2.7969, 'grad_norm': 1.822433590888977, 'learning_rate': 4.934745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:13<4:35:09,  2.84s/it]  3%|â–Ž         | 178/6000 [08:16<4:32:58,  2.81s/it]                                                    {'loss': 2.796, 'grad_norm': 2.366159439086914, 'learning_rate': 4.933898305084746e-05, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:16<4:32:58,  2.81s/it]  3%|â–Ž         | 179/6000 [08:19<4:27:58,  2.76s/it]                                                    {'loss': 2.7934, 'grad_norm': 1.2178314924240112, 'learning_rate': 4.933050847457628e-05, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:19<4:27:58,  2.76s/it]  3%|â–Ž         | 180/6000 [08:21<4:27:48,  2.76s/it]                                                    {'loss': 2.7894, 'grad_norm': 5.3074774742126465, 'learning_rate': 4.932203389830509e-05, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:21<4:27:48,  2.76s/it]  3%|â–Ž         | 181/6000 [08:24<4:25:12,  2.73s/it]                                                    {'loss': 2.8037, 'grad_norm': 1.595239520072937, 'learning_rate': 4.9313559322033906e-05, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:24<4:25:12,  2.73s/it]  3%|â–Ž         | 182/6000 [08:27<4:23:20,  2.72s/it]                                                    {'loss': 2.8165, 'grad_norm': 1.741163969039917, 'learning_rate': 4.930508474576271e-05, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:27<4:23:20,  2.72s/it]  3%|â–Ž         | 183/6000 [08:29<4:21:33,  2.70s/it]                                                    {'loss': 2.7878, 'grad_norm': 8.835147857666016, 'learning_rate': 4.929661016949153e-05, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:29<4:21:33,  2.70s/it]  3%|â–Ž         | 184/6000 [08:32<4:20:05,  2.68s/it]                                                    {'loss': 2.7718, 'grad_norm': 1.5683073997497559, 'learning_rate': 4.928813559322034e-05, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:32<4:20:05,  2.68s/it]  3%|â–Ž         | 185/6000 [08:35<4:20:09,  2.68s/it]                                                    {'loss': 2.7862, 'grad_norm': 1.5311124324798584, 'learning_rate': 4.927966101694915e-05, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:35<4:20:09,  2.68s/it]  3%|â–Ž         | 186/6000 [08:37<4:20:24,  2.69s/it]                                                    {'loss': 2.7736, 'grad_norm': 1.6520824432373047, 'learning_rate': 4.927118644067797e-05, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:38<4:20:24,  2.69s/it]  3%|â–Ž         | 187/6000 [08:40<4:20:29,  2.69s/it]                                                    {'loss': 2.7979, 'grad_norm': 5.238644599914551, 'learning_rate': 4.926271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:40<4:20:29,  2.69s/it]  3%|â–Ž         | 188/6000 [08:43<4:20:13,  2.69s/it]                                                    {'loss': 2.7851, 'grad_norm': 2.280557870864868, 'learning_rate': 4.92542372881356e-05, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:43<4:20:13,  2.69s/it]  3%|â–Ž         | 189/6000 [08:46<4:20:50,  2.69s/it]                                                    {'loss': 2.7842, 'grad_norm': 2.283649444580078, 'learning_rate': 4.924576271186441e-05, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:46<4:20:50,  2.69s/it]  3%|â–Ž         | 190/6000 [08:48<4:19:31,  2.68s/it]                                                    {'loss': 2.7827, 'grad_norm': 2.0791256427764893, 'learning_rate': 4.923728813559322e-05, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:48<4:19:31,  2.68s/it]  3%|â–Ž         | 191/6000 [08:51<4:22:01,  2.71s/it]                                                    {'loss': 2.7803, 'grad_norm': 2.32800555229187, 'learning_rate': 4.922881355932203e-05, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [08:51<4:22:01,  2.71s/it]  3%|â–Ž         | 192/6000 [08:54<4:37:30,  2.87s/it]                                                    {'loss': 2.8693, 'grad_norm': 2.378444194793701, 'learning_rate': 4.922033898305085e-05, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [08:54<4:37:30,  2.87s/it]  3%|â–Ž         | 193/6000 [08:57<4:40:39,  2.90s/it]                                                    {'loss': 2.7628, 'grad_norm': 2.6420536041259766, 'learning_rate': 4.921186440677966e-05, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [08:57<4:40:39,  2.90s/it]  3%|â–Ž         | 194/6000 [09:00<4:33:55,  2.83s/it]                                                    {'loss': 2.7703, 'grad_norm': 3.3430073261260986, 'learning_rate': 4.920338983050848e-05, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:00<4:33:55,  2.83s/it]  3%|â–Ž         | 195/6000 [09:03<4:31:20,  2.80s/it]                                                    {'loss': 2.826, 'grad_norm': 3.260838508605957, 'learning_rate': 4.919491525423729e-05, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:03<4:31:20,  2.80s/it]  3%|â–Ž         | 196/6000 [09:05<4:27:43,  2.77s/it]                                                    {'loss': 2.7929, 'grad_norm': 2.908034086227417, 'learning_rate': 4.91864406779661e-05, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:05<4:27:43,  2.77s/it]  3%|â–Ž         | 197/6000 [09:08<4:32:01,  2.81s/it]                                                    {'loss': 2.8031, 'grad_norm': 2.5468320846557617, 'learning_rate': 4.9177966101694914e-05, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:08<4:32:01,  2.81s/it]  3%|â–Ž         | 198/6000 [09:11<4:31:33,  2.81s/it]                                                    {'loss': 2.7876, 'grad_norm': 2.351226568222046, 'learning_rate': 4.916949152542373e-05, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:11<4:31:33,  2.81s/it]  3%|â–Ž         | 199/6000 [09:14<4:26:35,  2.76s/it]                                                    {'loss': 2.7729, 'grad_norm': 2.507765054702759, 'learning_rate': 4.916101694915254e-05, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:14<4:26:35,  2.76s/it]  3%|â–Ž         | 200/6000 [09:17<4:37:16,  2.87s/it]                                                    {'loss': 2.7707, 'grad_norm': 2.2402760982513428, 'learning_rate': 4.915254237288136e-05, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:17<4:37:16,  2.87s/it][2025-10-21 00:40:55,791] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [09:21<5:28:08,  3.40s/it]                                                    {'loss': 2.7793, 'grad_norm': 4.496793746948242, 'learning_rate': 4.914406779661017e-05, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:21<5:28:08,  3.40s/it]  3%|â–Ž         | 202/6000 [09:24<5:06:22,  3.17s/it]                                                    {'loss': 2.7777, 'grad_norm': 3.668224334716797, 'learning_rate': 4.913559322033899e-05, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:24<5:06:22,  3.17s/it]  3%|â–Ž         | 203/6000 [09:27<4:52:46,  3.03s/it]                                                    {'loss': 2.7837, 'grad_norm': 2.7711267471313477, 'learning_rate': 4.91271186440678e-05, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:27<4:52:46,  3.03s/it]  3%|â–Ž         | 204/6000 [09:29<4:44:10,  2.94s/it]                                                    {'loss': 2.7707, 'grad_norm': 2.5320446491241455, 'learning_rate': 4.9118644067796607e-05, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:29<4:44:10,  2.94s/it]  3%|â–Ž         | 205/6000 [09:32<4:33:43,  2.83s/it]                                                    {'loss': 2.78, 'grad_norm': 3.8010830879211426, 'learning_rate': 4.9110169491525425e-05, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:32<4:33:43,  2.83s/it]  3%|â–Ž         | 206/6000 [09:35<4:27:08,  2.77s/it]                                                    {'loss': 2.8304, 'grad_norm': 2.719251871109009, 'learning_rate': 4.9101694915254236e-05, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:35<4:27:08,  2.77s/it]  3%|â–Ž         | 207/6000 [09:37<4:23:27,  2.73s/it]                                                    {'loss': 2.9012, 'grad_norm': 4.6881890296936035, 'learning_rate': 4.9093220338983054e-05, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:37<4:23:27,  2.73s/it]  3%|â–Ž         | 208/6000 [09:40<4:20:45,  2.70s/it]                                                    {'loss': 2.8159, 'grad_norm': 4.542445182800293, 'learning_rate': 4.9084745762711865e-05, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:40<4:20:45,  2.70s/it]  3%|â–Ž         | 209/6000 [09:43<4:20:37,  2.70s/it]                                                    {'loss': 2.7707, 'grad_norm': 2.675487518310547, 'learning_rate': 4.907627118644068e-05, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:43<4:20:37,  2.70s/it]  4%|â–Ž         | 210/6000 [09:46<4:31:48,  2.82s/it]                                                    {'loss': 2.7767, 'grad_norm': 2.6053428649902344, 'learning_rate': 4.9067796610169495e-05, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:46<4:31:48,  2.82s/it]  4%|â–Ž         | 211/6000 [09:49<4:33:01,  2.83s/it]                                                    {'loss': 2.7914, 'grad_norm': 2.274618148803711, 'learning_rate': 4.9059322033898306e-05, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [09:49<4:33:01,  2.83s/it]  4%|â–Ž         | 212/6000 [09:52<4:39:44,  2.90s/it]                                                    {'loss': 2.7764, 'grad_norm': 1.837174415588379, 'learning_rate': 4.905084745762712e-05, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [09:52<4:39:44,  2.90s/it]  4%|â–Ž         | 213/6000 [09:54<4:34:50,  2.85s/it]                                                    {'loss': 2.7901, 'grad_norm': 2.1748340129852295, 'learning_rate': 4.9042372881355935e-05, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [09:54<4:34:50,  2.85s/it]  4%|â–Ž         | 214/6000 [09:57<4:28:00,  2.78s/it]                                                    {'loss': 2.7742, 'grad_norm': 1.4882770776748657, 'learning_rate': 4.9033898305084746e-05, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [09:57<4:28:00,  2.78s/it]  4%|â–Ž         | 215/6000 [10:00<4:25:43,  2.76s/it]                                                    {'loss': 2.8064, 'grad_norm': 1.6905903816223145, 'learning_rate': 4.9025423728813565e-05, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:00<4:25:43,  2.76s/it]  4%|â–Ž         | 216/6000 [10:02<4:24:42,  2.75s/it]                                                    {'loss': 2.7828, 'grad_norm': 1.5524479150772095, 'learning_rate': 4.9016949152542376e-05, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:02<4:24:42,  2.75s/it]  4%|â–Ž         | 217/6000 [10:05<4:26:42,  2.77s/it]                                                    {'loss': 2.8207, 'grad_norm': 1.2471280097961426, 'learning_rate': 4.9008474576271194e-05, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:05<4:26:42,  2.77s/it]  4%|â–Ž         | 218/6000 [10:08<4:28:04,  2.78s/it]                                                    {'loss': 2.7935, 'grad_norm': 1.4012277126312256, 'learning_rate': 4.9e-05, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:08<4:28:04,  2.78s/it]  4%|â–Ž         | 219/6000 [10:11<4:22:56,  2.73s/it]                                                    {'loss': 2.8306, 'grad_norm': 1.4065990447998047, 'learning_rate': 4.8991525423728816e-05, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:11<4:22:56,  2.73s/it]  4%|â–Ž         | 220/6000 [10:13<4:22:02,  2.72s/it]                                                    {'loss': 2.7676, 'grad_norm': 1.1711866855621338, 'learning_rate': 4.898305084745763e-05, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:13<4:22:02,  2.72s/it]  4%|â–Ž         | 221/6000 [10:16<4:21:06,  2.71s/it]                                                    {'loss': 2.8016, 'grad_norm': 1.5888365507125854, 'learning_rate': 4.8974576271186446e-05, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:16<4:21:06,  2.71s/it]  4%|â–Ž         | 222/6000 [10:19<4:18:35,  2.69s/it]                                                    {'loss': 2.7666, 'grad_norm': 1.6784086227416992, 'learning_rate': 4.896610169491526e-05, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:19<4:18:35,  2.69s/it]  4%|â–Ž         | 223/6000 [10:21<4:17:46,  2.68s/it]                                                    {'loss': 2.7634, 'grad_norm': 1.5340518951416016, 'learning_rate': 4.8957627118644075e-05, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:21<4:17:46,  2.68s/it]  4%|â–Ž         | 224/6000 [10:24<4:17:12,  2.67s/it]                                                    {'loss': 2.8331, 'grad_norm': 2.0504608154296875, 'learning_rate': 4.8949152542372886e-05, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:24<4:17:12,  2.67s/it]  4%|â–         | 225/6000 [10:27<4:19:31,  2.70s/it]                                                    {'loss': 2.7786, 'grad_norm': 1.484028697013855, 'learning_rate': 4.89406779661017e-05, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:27<4:19:31,  2.70s/it]  4%|â–         | 226/6000 [10:30<4:32:47,  2.83s/it]                                                    {'loss': 2.7679, 'grad_norm': 2.3301541805267334, 'learning_rate': 4.893220338983051e-05, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:30<4:32:47,  2.83s/it]  4%|â–         | 227/6000 [10:33<4:27:18,  2.78s/it]                                                    {'loss': 2.8216, 'grad_norm': 1.6040935516357422, 'learning_rate': 4.892372881355932e-05, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:33<4:27:18,  2.78s/it]  4%|â–         | 228/6000 [10:35<4:25:45,  2.76s/it]                                                    {'loss': 2.775, 'grad_norm': 1.8165634870529175, 'learning_rate': 4.891525423728814e-05, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:35<4:25:45,  2.76s/it]  4%|â–         | 229/6000 [10:38<4:26:43,  2.77s/it]                                                    {'loss': 2.8027, 'grad_norm': 2.237820625305176, 'learning_rate': 4.890677966101695e-05, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:38<4:26:43,  2.77s/it]  4%|â–         | 230/6000 [10:41<4:25:24,  2.76s/it]                                                    {'loss': 2.8116, 'grad_norm': 2.285836935043335, 'learning_rate': 4.889830508474577e-05, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:41<4:25:24,  2.76s/it]  4%|â–         | 231/6000 [10:43<4:21:41,  2.72s/it]                                                    {'loss': 2.7926, 'grad_norm': 1.7946698665618896, 'learning_rate': 4.888983050847458e-05, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:43<4:21:41,  2.72s/it]  4%|â–         | 232/6000 [10:46<4:20:21,  2.71s/it]                                                    {'loss': 2.7919, 'grad_norm': 2.241978645324707, 'learning_rate': 4.888135593220339e-05, 'epoch': 0.04}
  4%|â–         | 232/6000 [10:46<4:20:21,  2.71s/it]  4%|â–         | 233/6000 [10:49<4:29:28,  2.80s/it]                                                    {'loss': 2.7734, 'grad_norm': 1.6110084056854248, 'learning_rate': 4.88728813559322e-05, 'epoch': 0.04}
  4%|â–         | 233/6000 [10:49<4:29:28,  2.80s/it]  4%|â–         | 234/6000 [10:52<4:33:24,  2.84s/it]                                                    {'loss': 2.7781, 'grad_norm': 1.621185541152954, 'learning_rate': 4.886440677966102e-05, 'epoch': 0.04}
  4%|â–         | 234/6000 [10:52<4:33:24,  2.84s/it]  4%|â–         | 235/6000 [10:55<4:27:35,  2.78s/it]                                                    {'loss': 2.8485, 'grad_norm': 1.5946338176727295, 'learning_rate': 4.885593220338983e-05, 'epoch': 0.04}
  4%|â–         | 235/6000 [10:55<4:27:35,  2.78s/it]  4%|â–         | 236/6000 [10:57<4:26:06,  2.77s/it]                                                    {'loss': 2.8097, 'grad_norm': 1.4550905227661133, 'learning_rate': 4.884745762711865e-05, 'epoch': 0.04}
  4%|â–         | 236/6000 [10:57<4:26:06,  2.77s/it]  4%|â–         | 237/6000 [11:00<4:24:39,  2.76s/it]                                                    {'loss': 2.7845, 'grad_norm': 1.9040740728378296, 'learning_rate': 4.883898305084746e-05, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:00<4:24:39,  2.76s/it]  4%|â–         | 238/6000 [11:03<4:35:42,  2.87s/it]                                                    {'loss': 2.8396, 'grad_norm': 1.3489220142364502, 'learning_rate': 4.883050847457628e-05, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:03<4:35:42,  2.87s/it]  4%|â–         | 239/6000 [11:06<4:33:03,  2.84s/it]                                                    {'loss': 2.7736, 'grad_norm': 1.2353394031524658, 'learning_rate': 4.882203389830508e-05, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:06<4:33:03,  2.84s/it]  4%|â–         | 240/6000 [11:09<4:28:21,  2.80s/it]                                                    {'loss': 2.7938, 'grad_norm': 1.3847869634628296, 'learning_rate': 4.88135593220339e-05, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:09<4:28:21,  2.80s/it]