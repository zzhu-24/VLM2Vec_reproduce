==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 5e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/train.log
W1021 00:22:59.058000 138428064171840 torch/distributed/run.py:779] 
W1021 00:22:59.058000 138428064171840 torch/distributed/run.py:779] *****************************************
W1021 00:22:59.058000 138428064171840 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1021 00:22:59.058000 138428064171840 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-10-21 00:23:09,427] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.11it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.82it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251021_002309-gwls28v0
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/gwls28v0
[2025-10-21 00:23:11,040] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.23it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.12it/s]
[2025-10-21 00:23:11,666] INFO [src.utils:19] Enabling TailTokenWrapper (learnable tail token).
[2025-10-21 00:23:11,670] INFO [src.utils:19] Loading lora adapter from TailTokenDetachPrefixWrapper(
  (base): Qwen2VLForConditionalGeneration(
    (visual): Qwen2VisionTransformerPretrainedModel(
      (patch_embed): PatchEmbed(
        (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
      )
      (rotary_pos_emb): VisionRotaryEmbedding()
      (blocks): ModuleList(
        (0-31): 32 x Qwen2VLVisionBlock(
          (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (attn): VisionFlashAttention2(
            (qkv): Linear(in_features=1280, out_features=3840, bias=True)
            (proj): Linear(in_features=1280, out_features=1280, bias=True)
          )
          (mlp): VisionMlp(
            (fc1): Linear(in_features=1280, out_features=5120, bias=True)
            (act): QuickGELUActivation()
            (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          )
        )
      )
      (merger): PatchMerger(
        (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (mlp): Sequential(
          (0): Linear(in_features=5120, out_features=5120, bias=True)
          (1): GELU(approximate='none')
          (2): Linear(in_features=5120, out_features=1536, bias=True)
        )
      )
    )
    (model): Qwen2VLModel(
      (embed_tokens): Embedding(151936, 1536)
      (layers): ModuleList(
        (0-27): 28 x Qwen2VLDecoderLayer(
          (self_attn): Qwen2VLFlashAttention2(
            (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
            (k_proj): Linear(in_features=1536, out_features=256, bias=True)
            (v_proj): Linear(in_features=1536, out_features=256, bias=True)
            (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
            (rotary_emb): Qwen2VLRotaryEmbedding()
          )
          (mlp): Qwen2MLP(
            (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
            (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
            (act_fn): SiLU()
          )
          (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        )
      )
      (norm): Qwen2RMSNorm((1536,), eps=1e-06)
      (rotary_emb): Qwen2VLRotaryEmbedding()
    )
    (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
  )
)
[2025-10-21 00:23:20,531] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-10-21 00:23:21,886] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-10-21 00:23:21,886] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-10-21 00:23:26,027] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-10-21 00:23:26,028] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-10-21 00:23:26,885] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-10-21 00:23:26,886] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-10-21 00:23:26,887] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-10-21 00:23:26,888] INFO [src.utils:19] ==================================================
[2025-10-21 00:23:26,889] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-10-21 00:23:26,890] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:23:26,891] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-10-21 00:23:26,891] INFO [src.utils:19] ==================================================
[2025-10-21 00:23:28,654] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:23:28,654] INFO [src.trainer:342] ***** Running training *****
[2025-10-21 00:23:28,655] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:23:28,655] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:23:28,655] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:23:28,655] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:23:28,655] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:23:28,655] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:23:28,655] INFO [src.trainer:343]   Num examples = 192,000
[2025-10-21 00:23:28,656] INFO [src.trainer:344]   Num Epochs = 9,223,372,036,854,775,807
[2025-10-21 00:23:28,656] INFO [src.trainer:345]   Instantaneous batch size per device = 16
[2025-10-21 00:23:28,656] INFO [src.trainer:348]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-10-21 00:23:28,657] INFO [src.trainer:349]   Gradient Accumulation steps = 1
[2025-10-21 00:23:28,657] INFO [src.trainer:350]   Total optimization steps = 6,000
[2025-10-21 00:23:28,666] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
[2025-10-21 00:23:28,669] INFO [src.trainer:351]   Number of trainable parameters = 9,203,712
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1021 00:23:31.725669445 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1021 00:23:31.752644334 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:45:14,  4.05s/it]                                                  {'loss': 16.0737, 'grad_norm': 1055.5081787109375, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:45:14,  4.05s/it]  0%|          | 2/6000 [00:06<5:18:50,  3.19s/it]                                                  {'loss': 14.8217, 'grad_norm': 1114.3555908203125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:18:50,  3.19s/it]  0%|          | 3/6000 [00:09<4:55:12,  2.95s/it]                                                  {'loss': 13.6954, 'grad_norm': 1127.9874267578125, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<4:55:12,  2.95s/it]  0%|          | 4/6000 [00:11<4:43:08,  2.83s/it]                                                  {'loss': 13.1609, 'grad_norm': 983.96826171875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 4/6000 [00:11<4:43:08,  2.83s/it]  0%|          | 5/6000 [00:14<4:37:24,  2.78s/it]                                                  {'loss': 13.3613, 'grad_norm': 1038.01611328125, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:37:24,  2.78s/it]  0%|          | 6/6000 [00:17<4:32:20,  2.73s/it]                                                  {'loss': 13.8825, 'grad_norm': 1064.4500732421875, 'learning_rate': 3e-06, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:32:20,  2.73s/it]  0%|          | 7/6000 [00:19<4:29:25,  2.70s/it]                                                  {'loss': 12.9566, 'grad_norm': 1004.1130981445312, 'learning_rate': 3.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 7/6000 [00:19<4:29:25,  2.70s/it]  0%|          | 8/6000 [00:22<4:26:13,  2.67s/it]                                                  {'loss': 10.8037, 'grad_norm': 831.1907958984375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.0}
  0%|          | 8/6000 [00:22<4:26:13,  2.67s/it]  0%|          | 9/6000 [00:25<4:25:54,  2.66s/it]                                                  {'loss': 6.9333, 'grad_norm': 373.3891296386719, 'learning_rate': 4.5e-06, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:25:54,  2.66s/it]  0%|          | 10/6000 [00:27<4:22:22,  2.63s/it]                                                   {'loss': 8.4666, 'grad_norm': 536.1663818359375, 'learning_rate': 5e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:27<4:22:22,  2.63s/it]  0%|          | 11/6000 [00:30<4:31:37,  2.72s/it]                                                   {'loss': 8.5717, 'grad_norm': 435.63385009765625, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:30<4:31:37,  2.72s/it]  0%|          | 12/6000 [00:33<4:34:00,  2.75s/it]                                                   {'loss': 7.2934, 'grad_norm': 404.93170166015625, 'learning_rate': 6e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:33<4:34:00,  2.75s/it]  0%|          | 13/6000 [00:36<4:31:11,  2.72s/it]                                                   {'loss': 6.5793, 'grad_norm': 382.826171875, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:36<4:31:11,  2.72s/it]  0%|          | 14/6000 [00:38<4:33:26,  2.74s/it]                                                   {'loss': 6.4213, 'grad_norm': 338.8334655761719, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:38<4:33:26,  2.74s/it]  0%|          | 15/6000 [00:41<4:30:45,  2.71s/it]                                                   {'loss': 4.7191, 'grad_norm': 297.348876953125, 'learning_rate': 7.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:41<4:30:45,  2.71s/it]  0%|          | 16/6000 [00:44<4:27:14,  2.68s/it]                                                   {'loss': 4.6472, 'grad_norm': 301.2411193847656, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:44<4:27:14,  2.68s/it]  0%|          | 17/6000 [00:46<4:25:48,  2.67s/it]                                                   {'loss': 3.5993, 'grad_norm': 198.1488494873047, 'learning_rate': 8.500000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:46<4:25:48,  2.67s/it]  0%|          | 18/6000 [00:49<4:25:53,  2.67s/it]                                                   {'loss': 3.2997, 'grad_norm': 88.59478759765625, 'learning_rate': 9e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:49<4:25:53,  2.67s/it]  0%|          | 19/6000 [00:52<4:23:43,  2.65s/it]                                                   {'loss': 3.7238, 'grad_norm': 122.85541534423828, 'learning_rate': 9.5e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:52<4:23:43,  2.65s/it]  0%|          | 20/6000 [00:54<4:23:47,  2.65s/it]                                                   {'loss': 3.1821, 'grad_norm': 62.66623306274414, 'learning_rate': 1e-05, 'epoch': 0.0}
  0%|          | 20/6000 [00:54<4:23:47,  2.65s/it]  0%|          | 21/6000 [00:57<4:27:47,  2.69s/it]                                                   {'loss': 3.0058, 'grad_norm': 76.96527862548828, 'learning_rate': 1.05e-05, 'epoch': 0.0}
  0%|          | 21/6000 [00:57<4:27:47,  2.69s/it]  0%|          | 22/6000 [01:00<4:29:23,  2.70s/it]                                                   {'loss': 3.3722, 'grad_norm': 118.36299896240234, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.0}
  0%|          | 22/6000 [01:00<4:29:23,  2.70s/it]  0%|          | 23/6000 [01:02<4:27:42,  2.69s/it]                                                   {'loss': 3.2195, 'grad_norm': 62.61170196533203, 'learning_rate': 1.1500000000000002e-05, 'epoch': 0.0}
  0%|          | 23/6000 [01:02<4:27:42,  2.69s/it]  0%|          | 24/6000 [01:05<4:29:19,  2.70s/it]                                                   {'loss': 3.0289, 'grad_norm': 26.738901138305664, 'learning_rate': 1.2e-05, 'epoch': 0.0}
  0%|          | 24/6000 [01:05<4:29:19,  2.70s/it]  0%|          | 25/6000 [01:08<4:28:34,  2.70s/it]                                                   {'loss': 3.0728, 'grad_norm': 48.35683822631836, 'learning_rate': 1.25e-05, 'epoch': 0.0}
  0%|          | 25/6000 [01:08<4:28:34,  2.70s/it]  0%|          | 26/6000 [01:10<4:28:17,  2.69s/it]                                                   {'loss': 2.9899, 'grad_norm': 40.7881965637207, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.0}
  0%|          | 26/6000 [01:10<4:28:17,  2.69s/it]  0%|          | 27/6000 [01:13<4:27:20,  2.69s/it]                                                   {'loss': 2.9179, 'grad_norm': 19.270036697387695, 'learning_rate': 1.3500000000000001e-05, 'epoch': 0.0}
  0%|          | 27/6000 [01:13<4:27:20,  2.69s/it]  0%|          | 28/6000 [01:17<4:52:13,  2.94s/it]                                                   {'loss': 2.8696, 'grad_norm': 20.48002052307129, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.0}
  0%|          | 28/6000 [01:17<4:52:13,  2.94s/it]  0%|          | 29/6000 [01:19<4:43:19,  2.85s/it]                                                   {'loss': 2.8291, 'grad_norm': 13.99956226348877, 'learning_rate': 1.45e-05, 'epoch': 0.0}
  0%|          | 29/6000 [01:19<4:43:19,  2.85s/it]  0%|          | 30/6000 [01:22<4:38:10,  2.80s/it]                                                   {'loss': 2.8033, 'grad_norm': 9.712611198425293, 'learning_rate': 1.5e-05, 'epoch': 0.01}
  0%|          | 30/6000 [01:22<4:38:10,  2.80s/it]  1%|          | 31/6000 [01:25<4:34:55,  2.76s/it]                                                   {'loss': 2.8162, 'grad_norm': 10.416481018066406, 'learning_rate': 1.55e-05, 'epoch': 0.01}
  1%|          | 31/6000 [01:25<4:34:55,  2.76s/it]  1%|          | 32/6000 [01:27<4:32:17,  2.74s/it]                                                   {'loss': 2.8221, 'grad_norm': 10.607067108154297, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.01}
  1%|          | 32/6000 [01:27<4:32:17,  2.74s/it]  1%|          | 33/6000 [01:30<4:30:55,  2.72s/it]                                                   {'loss': 2.8413, 'grad_norm': 22.317636489868164, 'learning_rate': 1.65e-05, 'epoch': 0.01}
  1%|          | 33/6000 [01:30<4:30:55,  2.72s/it]  1%|          | 34/6000 [01:33<4:28:47,  2.70s/it]                                                   {'loss': 2.8542, 'grad_norm': 11.824457168579102, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.01}
  1%|          | 34/6000 [01:33<4:28:47,  2.70s/it]  1%|          | 35/6000 [01:35<4:29:46,  2.71s/it]                                                   {'loss': 2.812, 'grad_norm': 7.474212169647217, 'learning_rate': 1.75e-05, 'epoch': 0.01}
  1%|          | 35/6000 [01:35<4:29:46,  2.71s/it]  1%|          | 36/6000 [01:38<4:26:38,  2.68s/it]                                                   {'loss': 2.8277, 'grad_norm': 9.240226745605469, 'learning_rate': 1.8e-05, 'epoch': 0.01}
  1%|          | 36/6000 [01:38<4:26:38,  2.68s/it]  1%|          | 37/6000 [01:41<4:24:31,  2.66s/it]                                                   {'loss': 2.8084, 'grad_norm': 5.752285480499268, 'learning_rate': 1.85e-05, 'epoch': 0.01}
  1%|          | 37/6000 [01:41<4:24:31,  2.66s/it]  1%|          | 38/6000 [01:43<4:23:04,  2.65s/it]                                                   {'loss': 2.7943, 'grad_norm': 6.748263835906982, 'learning_rate': 1.9e-05, 'epoch': 0.01}
  1%|          | 38/6000 [01:43<4:23:04,  2.65s/it]  1%|          | 39/6000 [01:46<4:22:32,  2.64s/it]                                                   {'loss': 2.8578, 'grad_norm': 5.365771293640137, 'learning_rate': 1.9500000000000003e-05, 'epoch': 0.01}
  1%|          | 39/6000 [01:46<4:22:32,  2.64s/it]  1%|          | 40/6000 [01:49<4:22:17,  2.64s/it]                                                   {'loss': 2.805, 'grad_norm': 10.00633716583252, 'learning_rate': 2e-05, 'epoch': 0.01}
  1%|          | 40/6000 [01:49<4:22:17,  2.64s/it]  1%|          | 41/6000 [01:51<4:23:16,  2.65s/it]                                                   {'loss': 2.7886, 'grad_norm': 4.022658824920654, 'learning_rate': 2.05e-05, 'epoch': 0.01}
  1%|          | 41/6000 [01:51<4:23:16,  2.65s/it]  1%|          | 42/6000 [01:54<4:24:26,  2.66s/it]                                                   {'loss': 2.7805, 'grad_norm': 3.6568968296051025, 'learning_rate': 2.1e-05, 'epoch': 0.01}
  1%|          | 42/6000 [01:54<4:24:26,  2.66s/it]  1%|          | 43/6000 [01:58<4:53:51,  2.96s/it]                                                   {'loss': 2.8112, 'grad_norm': 13.409509658813477, 'learning_rate': 2.15e-05, 'epoch': 0.01}
  1%|          | 43/6000 [01:58<4:53:51,  2.96s/it]  1%|          | 44/6000 [02:01<4:57:29,  3.00s/it]                                                   {'loss': 2.7815, 'grad_norm': 3.800635814666748, 'learning_rate': 2.2000000000000003e-05, 'epoch': 0.01}
  1%|          | 44/6000 [02:01<4:57:29,  3.00s/it]  1%|          | 45/6000 [02:03<4:48:48,  2.91s/it]                                                   {'loss': 2.7926, 'grad_norm': 3.3753437995910645, 'learning_rate': 2.25e-05, 'epoch': 0.01}
  1%|          | 45/6000 [02:03<4:48:48,  2.91s/it]  1%|          | 46/6000 [02:06<4:44:27,  2.87s/it]                                                   {'loss': 2.7757, 'grad_norm': 2.776859760284424, 'learning_rate': 2.3000000000000003e-05, 'epoch': 0.01}
  1%|          | 46/6000 [02:06<4:44:27,  2.87s/it]  1%|          | 47/6000 [02:09<4:38:56,  2.81s/it]                                                   {'loss': 2.7759, 'grad_norm': 2.7803542613983154, 'learning_rate': 2.35e-05, 'epoch': 0.01}
  1%|          | 47/6000 [02:09<4:38:56,  2.81s/it]  1%|          | 48/6000 [02:12<4:37:34,  2.80s/it]                                                   {'loss': 2.7862, 'grad_norm': 3.1091887950897217, 'learning_rate': 2.4e-05, 'epoch': 0.01}
  1%|          | 48/6000 [02:12<4:37:34,  2.80s/it]  1%|          | 49/6000 [02:14<4:32:41,  2.75s/it]                                                   {'loss': 2.7838, 'grad_norm': 2.1661319732666016, 'learning_rate': 2.45e-05, 'epoch': 0.01}
  1%|          | 49/6000 [02:14<4:32:41,  2.75s/it]  1%|          | 50/6000 [02:17<4:34:13,  2.77s/it]                                                   {'loss': 2.7787, 'grad_norm': 2.26774001121521, 'learning_rate': 2.5e-05, 'epoch': 0.01}
  1%|          | 50/6000 [02:17<4:34:13,  2.77s/it][2025-10-21 00:25:46,364] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [02:22<5:28:45,  3.32s/it]                                                   {'loss': 2.7902, 'grad_norm': 2.252145528793335, 'learning_rate': 2.5500000000000003e-05, 'epoch': 0.01}
  1%|          | 51/6000 [02:22<5:28:45,  3.32s/it]  1%|          | 52/6000 [02:24<5:07:55,  3.11s/it]                                                   {'loss': 2.7884, 'grad_norm': 2.1265430450439453, 'learning_rate': 2.6000000000000002e-05, 'epoch': 0.01}
  1%|          | 52/6000 [02:24<5:07:55,  3.11s/it]  1%|          | 53/6000 [02:27<4:56:33,  2.99s/it]                                                   {'loss': 2.8475, 'grad_norm': 2.768228530883789, 'learning_rate': 2.6500000000000004e-05, 'epoch': 0.01}
  1%|          | 53/6000 [02:27<4:56:33,  2.99s/it]  1%|          | 54/6000 [02:30<4:45:36,  2.88s/it]                                                   {'loss': 2.7809, 'grad_norm': 2.3799057006835938, 'learning_rate': 2.7000000000000002e-05, 'epoch': 0.01}
  1%|          | 54/6000 [02:30<4:45:36,  2.88s/it]  1%|          | 55/6000 [02:32<4:37:53,  2.80s/it]                                                   {'loss': 2.7969, 'grad_norm': 3.142263650894165, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.01}
  1%|          | 55/6000 [02:32<4:37:53,  2.80s/it]  1%|          | 56/6000 [02:35<4:34:46,  2.77s/it]                                                   {'loss': 2.7832, 'grad_norm': 2.4569900035858154, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.01}
  1%|          | 56/6000 [02:35<4:34:46,  2.77s/it]  1%|          | 57/6000 [02:38<4:31:41,  2.74s/it]                                                   {'loss': 2.7837, 'grad_norm': 2.5815794467926025, 'learning_rate': 2.8499999999999998e-05, 'epoch': 0.01}
  1%|          | 57/6000 [02:38<4:31:41,  2.74s/it]  1%|          | 58/6000 [02:40<4:29:07,  2.72s/it]                                                   {'loss': 2.7716, 'grad_norm': 2.3308663368225098, 'learning_rate': 2.9e-05, 'epoch': 0.01}
  1%|          | 58/6000 [02:40<4:29:07,  2.72s/it]  1%|          | 59/6000 [02:43<4:25:50,  2.68s/it]                                                   {'loss': 2.7849, 'grad_norm': 2.4425761699676514, 'learning_rate': 2.95e-05, 'epoch': 0.01}
  1%|          | 59/6000 [02:43<4:25:50,  2.68s/it]  1%|          | 60/6000 [02:45<4:23:19,  2.66s/it]                                                   {'loss': 2.7868, 'grad_norm': 4.162014007568359, 'learning_rate': 3e-05, 'epoch': 0.01}
  1%|          | 60/6000 [02:45<4:23:19,  2.66s/it]  1%|          | 61/6000 [02:48<4:21:31,  2.64s/it]                                                   {'loss': 2.8061, 'grad_norm': 2.2337419986724854, 'learning_rate': 3.05e-05, 'epoch': 0.01}
  1%|          | 61/6000 [02:48<4:21:31,  2.64s/it]  1%|          | 62/6000 [02:51<4:21:49,  2.65s/it]                                                   {'loss': 2.7916, 'grad_norm': 2.0476906299591064, 'learning_rate': 3.1e-05, 'epoch': 0.01}
  1%|          | 62/6000 [02:51<4:21:49,  2.65s/it]  1%|          | 63/6000 [02:53<4:23:08,  2.66s/it]                                                   {'loss': 2.783, 'grad_norm': 2.936044454574585, 'learning_rate': 3.15e-05, 'epoch': 0.01}
  1%|          | 63/6000 [02:53<4:23:08,  2.66s/it]  1%|          | 64/6000 [02:56<4:22:25,  2.65s/it]                                                   {'loss': 2.7742, 'grad_norm': 1.880509614944458, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.01}
  1%|          | 64/6000 [02:56<4:22:25,  2.65s/it]  1%|          | 65/6000 [02:59<4:20:37,  2.63s/it]                                                   {'loss': 2.7914, 'grad_norm': 2.3534791469573975, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.01}
  1%|          | 65/6000 [02:59<4:20:37,  2.63s/it]  1%|          | 66/6000 [03:02<4:32:30,  2.76s/it]                                                   {'loss': 2.7699, 'grad_norm': 2.400219678878784, 'learning_rate': 3.3e-05, 'epoch': 0.01}
  1%|          | 66/6000 [03:02<4:32:30,  2.76s/it]  1%|          | 67/6000 [03:04<4:30:03,  2.73s/it]                                                   {'loss': 2.7908, 'grad_norm': 2.7698397636413574, 'learning_rate': 3.35e-05, 'epoch': 0.01}
  1%|          | 67/6000 [03:04<4:30:03,  2.73s/it]  1%|          | 68/6000 [03:07<4:26:30,  2.70s/it]                                                   {'loss': 2.7859, 'grad_norm': 2.9338953495025635, 'learning_rate': 3.4000000000000007e-05, 'epoch': 0.01}
  1%|          | 68/6000 [03:07<4:26:30,  2.70s/it]  1%|          | 69/6000 [03:10<4:24:15,  2.67s/it]                                                   {'loss': 2.7945, 'grad_norm': 2.6770401000976562, 'learning_rate': 3.45e-05, 'epoch': 0.01}
  1%|          | 69/6000 [03:10<4:24:15,  2.67s/it]  1%|          | 70/6000 [03:12<4:25:31,  2.69s/it]                                                   {'loss': 2.7911, 'grad_norm': 4.5926642417907715, 'learning_rate': 3.5e-05, 'epoch': 0.01}
  1%|          | 70/6000 [03:12<4:25:31,  2.69s/it]  1%|          | 71/6000 [03:15<4:23:16,  2.66s/it]                                                   {'loss': 2.7916, 'grad_norm': 7.491996765136719, 'learning_rate': 3.55e-05, 'epoch': 0.01}
  1%|          | 71/6000 [03:15<4:23:16,  2.66s/it]  1%|          | 72/6000 [03:18<4:34:02,  2.77s/it]                                                   {'loss': 2.7784, 'grad_norm': 7.405398845672607, 'learning_rate': 3.6e-05, 'epoch': 0.01}
  1%|          | 72/6000 [03:18<4:34:02,  2.77s/it]  1%|          | 73/6000 [03:21<4:39:08,  2.83s/it]                                                   {'loss': 2.8214, 'grad_norm': 10.069657325744629, 'learning_rate': 3.65e-05, 'epoch': 0.01}
  1%|          | 73/6000 [03:21<4:39:08,  2.83s/it]  1%|          | 74/6000 [03:23<4:32:34,  2.76s/it]                                                   {'loss': 2.7706, 'grad_norm': 7.302205562591553, 'learning_rate': 3.7e-05, 'epoch': 0.01}
  1%|          | 74/6000 [03:23<4:32:34,  2.76s/it]  1%|â–         | 75/6000 [03:26<4:28:14,  2.72s/it]                                                   {'loss': 2.8016, 'grad_norm': 7.988410949707031, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:26<4:28:14,  2.72s/it]  1%|â–         | 76/6000 [03:29<4:29:06,  2.73s/it]                                                   {'loss': 2.8006, 'grad_norm': 7.024216651916504, 'learning_rate': 3.8e-05, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:29<4:29:06,  2.73s/it]  1%|â–         | 77/6000 [03:32<4:28:19,  2.72s/it]                                                   {'loss': 2.8449, 'grad_norm': 4.539425373077393, 'learning_rate': 3.85e-05, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:32<4:28:19,  2.72s/it]  1%|â–         | 78/6000 [03:35<4:38:04,  2.82s/it]                                                   {'loss': 2.7778, 'grad_norm': 5.619893550872803, 'learning_rate': 3.9000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:35<4:38:04,  2.82s/it]  1%|â–         | 79/6000 [03:37<4:33:54,  2.78s/it]                                                   {'loss': 2.7773, 'grad_norm': 3.6678037643432617, 'learning_rate': 3.9500000000000005e-05, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:37<4:33:54,  2.78s/it]  1%|â–         | 80/6000 [03:40<4:38:39,  2.82s/it]                                                   {'loss': 2.772, 'grad_norm': 3.935995578765869, 'learning_rate': 4e-05, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:40<4:38:39,  2.82s/it]  1%|â–         | 81/6000 [03:43<4:33:40,  2.77s/it]                                                   {'loss': 2.7815, 'grad_norm': 3.970569133758545, 'learning_rate': 4.05e-05, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:43<4:33:40,  2.77s/it]  1%|â–         | 82/6000 [03:46<4:31:20,  2.75s/it]                                                   {'loss': 2.7724, 'grad_norm': 3.6070709228515625, 'learning_rate': 4.1e-05, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:46<4:31:20,  2.75s/it]  1%|â–         | 83/6000 [03:48<4:28:06,  2.72s/it]                                                   {'loss': 2.7936, 'grad_norm': 2.2596099376678467, 'learning_rate': 4.15e-05, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:48<4:28:06,  2.72s/it]  1%|â–         | 84/6000 [03:51<4:28:32,  2.72s/it]                                                   {'loss': 2.7771, 'grad_norm': 2.9691388607025146, 'learning_rate': 4.2e-05, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:51<4:28:32,  2.72s/it]  1%|â–         | 85/6000 [03:54<4:37:29,  2.81s/it]                                                   {'loss': 2.7848, 'grad_norm': 2.048205614089966, 'learning_rate': 4.25e-05, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:54<4:37:29,  2.81s/it]  1%|â–         | 86/6000 [03:57<4:32:38,  2.77s/it]                                                   {'loss': 2.7854, 'grad_norm': 2.366159200668335, 'learning_rate': 4.3e-05, 'epoch': 0.01}
  1%|â–         | 86/6000 [03:57<4:32:38,  2.77s/it]  1%|â–         | 87/6000 [03:59<4:28:40,  2.73s/it]                                                   {'loss': 2.7893, 'grad_norm': 2.200343132019043, 'learning_rate': 4.35e-05, 'epoch': 0.01}
  1%|â–         | 87/6000 [03:59<4:28:40,  2.73s/it]  1%|â–         | 88/6000 [04:02<4:24:39,  2.69s/it]                                                   {'loss': 2.7736, 'grad_norm': 2.6847939491271973, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:02<4:24:39,  2.69s/it]  1%|â–         | 89/6000 [04:04<4:22:55,  2.67s/it]                                                   {'loss': 2.7818, 'grad_norm': 1.7572240829467773, 'learning_rate': 4.4500000000000004e-05, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:04<4:22:55,  2.67s/it]  2%|â–         | 90/6000 [04:07<4:23:56,  2.68s/it]                                                   {'loss': 2.8272, 'grad_norm': 2.1085500717163086, 'learning_rate': 4.5e-05, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:07<4:23:56,  2.68s/it]  2%|â–         | 91/6000 [04:10<4:24:11,  2.68s/it]                                                   {'loss': 2.7688, 'grad_norm': 4.202662467956543, 'learning_rate': 4.55e-05, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:10<4:24:11,  2.68s/it]  2%|â–         | 92/6000 [04:13<4:34:52,  2.79s/it]                                                   {'loss': 2.7722, 'grad_norm': 3.2430238723754883, 'learning_rate': 4.600000000000001e-05, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:13<4:34:52,  2.79s/it]  2%|â–         | 93/6000 [04:16<4:33:54,  2.78s/it]                                                   {'loss': 2.767, 'grad_norm': 2.139470338821411, 'learning_rate': 4.6500000000000005e-05, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:16<4:33:54,  2.78s/it]  2%|â–         | 94/6000 [04:18<4:30:36,  2.75s/it]                                                   {'loss': 2.7857, 'grad_norm': 2.4539642333984375, 'learning_rate': 4.7e-05, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:18<4:30:36,  2.75s/it]  2%|â–         | 95/6000 [04:21<4:28:36,  2.73s/it]                                                   {'loss': 2.7782, 'grad_norm': 2.706986427307129, 'learning_rate': 4.75e-05, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:21<4:28:36,  2.73s/it]  2%|â–         | 96/6000 [04:24<4:26:02,  2.70s/it]                                                   {'loss': 2.7708, 'grad_norm': 2.445661783218384, 'learning_rate': 4.8e-05, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:24<4:26:02,  2.70s/it]  2%|â–         | 97/6000 [04:26<4:24:23,  2.69s/it]                                                   {'loss': 2.7892, 'grad_norm': 4.666560649871826, 'learning_rate': 4.85e-05, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:26<4:24:23,  2.69s/it]  2%|â–         | 98/6000 [04:29<4:22:57,  2.67s/it]                                                   {'loss': 2.7835, 'grad_norm': 4.60962438583374, 'learning_rate': 4.9e-05, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:29<4:22:57,  2.67s/it]  2%|â–         | 99/6000 [04:32<4:21:26,  2.66s/it]                                                   {'loss': 2.7755, 'grad_norm': 4.1560258865356445, 'learning_rate': 4.9500000000000004e-05, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:32<4:21:26,  2.66s/it]  2%|â–         | 100/6000 [04:34<4:19:32,  2.64s/it]                                                    {'loss': 2.7742, 'grad_norm': 3.4743645191192627, 'learning_rate': 5e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:34<4:19:32,  2.64s/it][2025-10-21 00:28:03,533] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/TailTokenPF_2-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [04:40<5:42:19,  3.48s/it]                                                    {'loss': 2.8134, 'grad_norm': 7.129425048828125, 'learning_rate': 4.9991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:40<5:42:19,  3.48s/it]  2%|â–         | 102/6000 [04:42<5:17:07,  3.23s/it]                                                    {'loss': 2.7858, 'grad_norm': 3.0875465869903564, 'learning_rate': 4.998305084745763e-05, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:42<5:17:07,  3.23s/it]  2%|â–         | 103/6000 [04:45<5:01:36,  3.07s/it]                                                    {'loss': 2.7725, 'grad_norm': 3.471008062362671, 'learning_rate': 4.997457627118644e-05, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:45<5:01:36,  3.07s/it]  2%|â–         | 104/6000 [04:48<4:55:35,  3.01s/it]                                                    {'loss': 2.7728, 'grad_norm': 2.3874318599700928, 'learning_rate': 4.9966101694915254e-05, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:48<4:55:35,  3.01s/it]  2%|â–         | 105/6000 [04:51<4:47:49,  2.93s/it]                                                    {'loss': 2.8016, 'grad_norm': 3.2356674671173096, 'learning_rate': 4.9957627118644066e-05, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:51<4:47:49,  2.93s/it]  2%|â–         | 106/6000 [04:53<4:44:59,  2.90s/it]                                                    {'loss': 2.7711, 'grad_norm': 2.0742030143737793, 'learning_rate': 4.9949152542372884e-05, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:53<4:44:59,  2.90s/it]  2%|â–         | 107/6000 [04:56<4:37:12,  2.82s/it]                                                    {'loss': 2.8033, 'grad_norm': 3.3618831634521484, 'learning_rate': 4.9940677966101695e-05, 'epoch': 0.02}
  2%|â–         | 107/6000 [04:56<4:37:12,  2.82s/it]  2%|â–         | 108/6000 [04:59<4:35:35,  2.81s/it]                                                    {'loss': 2.7798, 'grad_norm': 2.19983172416687, 'learning_rate': 4.993220338983051e-05, 'epoch': 0.02}
  2%|â–         | 108/6000 [04:59<4:35:35,  2.81s/it]  2%|â–         | 109/6000 [05:02<4:43:28,  2.89s/it]                                                    {'loss': 2.8211, 'grad_norm': 2.160031795501709, 'learning_rate': 4.9923728813559324e-05, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:02<4:43:28,  2.89s/it]  2%|â–         | 110/6000 [05:05<4:36:07,  2.81s/it]                                                    {'loss': 2.8441, 'grad_norm': 1.4829579591751099, 'learning_rate': 4.991525423728814e-05, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:05<4:36:07,  2.81s/it]  2%|â–         | 111/6000 [05:07<4:35:04,  2.80s/it]                                                    {'loss': 2.766, 'grad_norm': 2.964695930480957, 'learning_rate': 4.990677966101695e-05, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:07<4:35:04,  2.80s/it]  2%|â–         | 112/6000 [05:11<4:47:34,  2.93s/it]                                                    {'loss': 2.7877, 'grad_norm': 3.471466302871704, 'learning_rate': 4.9898305084745765e-05, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:11<4:47:34,  2.93s/it]  2%|â–         | 113/6000 [05:13<4:39:57,  2.85s/it]                                                    {'loss': 2.7699, 'grad_norm': 2.026376724243164, 'learning_rate': 4.9889830508474576e-05, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:13<4:39:57,  2.85s/it]  2%|â–         | 114/6000 [05:16<4:35:23,  2.81s/it]                                                    {'loss': 2.7653, 'grad_norm': 2.333296060562134, 'learning_rate': 4.9881355932203394e-05, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:16<4:35:23,  2.81s/it]  2%|â–         | 115/6000 [05:19<4:30:48,  2.76s/it]                                                    {'loss': 2.8287, 'grad_norm': 3.2988414764404297, 'learning_rate': 4.9872881355932206e-05, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:19<4:30:48,  2.76s/it]  2%|â–         | 116/6000 [05:21<4:27:00,  2.72s/it]                                                    {'loss': 2.8157, 'grad_norm': 5.508258819580078, 'learning_rate': 4.9864406779661024e-05, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:21<4:27:00,  2.72s/it]  2%|â–         | 117/6000 [05:24<4:26:02,  2.71s/it]                                                    {'loss': 2.9107, 'grad_norm': 2.2570996284484863, 'learning_rate': 4.9855932203389835e-05, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:24<4:26:02,  2.71s/it]  2%|â–         | 118/6000 [05:27<4:41:41,  2.87s/it]                                                    {'loss': 2.7671, 'grad_norm': 3.826429605484009, 'learning_rate': 4.9847457627118646e-05, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:27<4:41:41,  2.87s/it]  2%|â–         | 119/6000 [05:30<4:37:14,  2.83s/it]                                                    {'loss': 2.7671, 'grad_norm': 3.414933681488037, 'learning_rate': 4.983898305084746e-05, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:30<4:37:14,  2.83s/it]  2%|â–         | 120/6000 [05:33<4:33:40,  2.79s/it]                                                    {'loss': 2.7695, 'grad_norm': 3.382544994354248, 'learning_rate': 4.9830508474576276e-05, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:33<4:33:40,  2.79s/it]  2%|â–         | 121/6000 [05:35<4:31:00,  2.77s/it]                                                    {'loss': 2.7775, 'grad_norm': 4.359748363494873, 'learning_rate': 4.982203389830509e-05, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:35<4:31:00,  2.77s/it]  2%|â–         | 122/6000 [05:38<4:31:33,  2.77s/it]                                                    {'loss': 2.769, 'grad_norm': 3.9076900482177734, 'learning_rate': 4.98135593220339e-05, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:38<4:31:33,  2.77s/it]  2%|â–         | 123/6000 [05:41<4:28:54,  2.75s/it]                                                    {'loss': 2.8039, 'grad_norm': 4.779656887054443, 'learning_rate': 4.9805084745762716e-05, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:41<4:28:54,  2.75s/it]  2%|â–         | 124/6000 [05:43<4:25:50,  2.71s/it]                                                    {'loss': 2.8171, 'grad_norm': 3.7418301105499268, 'learning_rate': 4.979661016949153e-05, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:43<4:25:50,  2.71s/it]  2%|â–         | 125/6000 [05:46<4:31:57,  2.78s/it]                                                    {'loss': 2.9617, 'grad_norm': 5.58675479888916, 'learning_rate': 4.978813559322034e-05, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:46<4:31:57,  2.78s/it]  2%|â–         | 126/6000 [05:49<4:30:23,  2.76s/it]                                                    {'loss': 2.7617, 'grad_norm': 3.4411821365356445, 'learning_rate': 4.977966101694915e-05, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:49<4:30:23,  2.76s/it]  2%|â–         | 127/6000 [05:52<4:32:08,  2.78s/it]                                                    {'loss': 2.7809, 'grad_norm': 4.748128890991211, 'learning_rate': 4.977118644067797e-05, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:52<4:32:08,  2.78s/it]  2%|â–         | 128/6000 [05:55<4:29:41,  2.76s/it]                                                    {'loss': 2.7955, 'grad_norm': 3.55894136428833, 'learning_rate': 4.976271186440678e-05, 'epoch': 0.02}
  2%|â–         | 128/6000 [05:55<4:29:41,  2.76s/it]  2%|â–         | 129/6000 [05:57<4:27:33,  2.73s/it]                                                    {'loss': 2.8581, 'grad_norm': 3.0595600605010986, 'learning_rate': 4.97542372881356e-05, 'epoch': 0.02}
  2%|â–         | 129/6000 [05:57<4:27:33,  2.73s/it]  2%|â–         | 130/6000 [06:00<4:26:04,  2.72s/it]                                                    {'loss': 2.783, 'grad_norm': 3.5598127841949463, 'learning_rate': 4.974576271186441e-05, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:00<4:26:04,  2.72s/it]  2%|â–         | 131/6000 [06:03<4:24:54,  2.71s/it]                                                    {'loss': 2.7788, 'grad_norm': 2.2102131843566895, 'learning_rate': 4.973728813559323e-05, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:03<4:24:54,  2.71s/it]