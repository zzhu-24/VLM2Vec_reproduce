==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 18Nov_1-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 08:54:44.869000 126853372774208 torch/distributed/run.py:779] 
W1118 08:54:44.869000 126853372774208 torch/distributed/run.py:779] *****************************************
W1118 08:54:44.869000 126853372774208 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 08:54:44.869000 126853372774208 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 08:54:51,676] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.49it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.55it/s]
wandb: setting up run 4qrjl5o3
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_085452-4qrjl5o3
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 18Nov_1-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/4qrjl5o3
[2025-11-18 08:54:53,343] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.93it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.29it/s]
[2025-11-18 08:54:53,866] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 08:55:00,054] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 08:55:01,527] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 08:55:01,528] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 08:55:04,855] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 08:55:04,855] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 08:55:05,624] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 08:55:05,624] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 08:55:05,624] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 08:55:05,625] INFO [src.utils:19] ==================================================
[2025-11-18 08:55:05,625] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 08:55:05,626] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 08:55:05,626] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 08:55:05,626] INFO [src.utils:19] ==================================================
[2025-11-18 08:55:07,218] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 08:55:07,218] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 08:55:07,218] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 08:55:07,218] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 08:55:07,218] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 08:55:07,218] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 08:55:07,218] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 08:55:07,218] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 08:55:07,218] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 08:55:07,218] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 08:55:07,219] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 08:55:07,219] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 08:55:07,219] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 08:55:07,219] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 08:55:07,221] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 08:55:07,223] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 08:55:07,224] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 08:55:07,226] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank1]:[W1118 08:55:09.482222893 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1118 08:55:09.485316647 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:03<5:50:11,  3.50s/it]                                                  {'loss': 20.6075, 'grad_norm': 1380.298583984375, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:50:11,  3.50s/it]  0%|          | 2/6000 [00:05<4:13:56,  2.54s/it]                                                  {'loss': 17.7575, 'grad_norm': 2039.92822265625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:13:56,  2.54s/it]  0%|          | 3/6000 [00:07<3:46:00,  2.26s/it]                                                  {'loss': 16.2121, 'grad_norm': 2097.839111328125, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:46:00,  2.26s/it]  0%|          | 4/6000 [00:09<3:37:55,  2.18s/it]                                                  {'loss': 16.4865, 'grad_norm': 2219.5732421875, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:37:55,  2.18s/it]  0%|          | 5/6000 [00:11<3:29:34,  2.10s/it]                                                  {'loss': 16.9177, 'grad_norm': 1970.9598388671875, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:29:34,  2.10s/it]  0%|          | 6/6000 [00:13<3:22:48,  2.03s/it]                                                  {'loss': 18.3492, 'grad_norm': 1556.1658935546875, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:22:48,  2.03s/it]  0%|          | 7/6000 [00:15<3:19:00,  1.99s/it]                                                  {'loss': 17.825, 'grad_norm': 1910.0604248046875, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:19:00,  1.99s/it]  0%|          | 8/6000 [00:17<3:16:12,  1.96s/it]                                                  {'loss': 19.3201, 'grad_norm': 1744.9075927734375, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:17<3:16:12,  1.96s/it]  0%|          | 9/6000 [00:18<3:15:30,  1.96s/it]                                                  {'loss': 15.0981, 'grad_norm': 1701.894775390625, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:18<3:15:30,  1.96s/it]  0%|          | 10/6000 [00:20<3:13:08,  1.93s/it]                                                   {'loss': 18.5473, 'grad_norm': 1311.1033935546875, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:13:08,  1.93s/it]  0%|          | 11/6000 [00:22<3:14:33,  1.95s/it]                                                   {'loss': 21.3735, 'grad_norm': 1490.2818603515625, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:14:33,  1.95s/it]  0%|          | 12/6000 [00:24<3:15:04,  1.95s/it]                                                   {'loss': 18.0374, 'grad_norm': 1716.1361083984375, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:15:04,  1.95s/it]  0%|          | 13/6000 [00:26<3:13:26,  1.94s/it]                                                   {'loss': 18.2479, 'grad_norm': 2130.06689453125, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:13:26,  1.94s/it]  0%|          | 14/6000 [00:28<3:13:00,  1.93s/it]                                                   {'loss': 18.2889, 'grad_norm': 2319.984375, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:13:00,  1.93s/it]  0%|          | 15/6000 [00:30<3:12:08,  1.93s/it]                                                   {'loss': 14.4559, 'grad_norm': 2294.859619140625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:12:08,  1.93s/it]  0%|          | 16/6000 [00:32<3:10:54,  1.91s/it]                                                   {'loss': 16.1344, 'grad_norm': 1583.2664794921875, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:10:54,  1.91s/it]  0%|          | 17/6000 [00:34<3:14:12,  1.95s/it]                                                   {'loss': 15.3854, 'grad_norm': 1422.961181640625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:14:12,  1.95s/it]  0%|          | 18/6000 [00:36<3:13:39,  1.94s/it]                                                   {'loss': 12.2976, 'grad_norm': 1453.6129150390625, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:13:39,  1.94s/it]  0%|          | 19/6000 [00:38<3:12:11,  1.93s/it]                                                   {'loss': 13.3374, 'grad_norm': 2032.8348388671875, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:12:11,  1.93s/it]  0%|          | 20/6000 [00:40<3:14:00,  1.95s/it]                                                   {'loss': 14.793, 'grad_norm': 2252.716796875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:14:00,  1.95s/it]  0%|          | 21/6000 [00:42<3:15:38,  1.96s/it]                                                   {'loss': 12.2304, 'grad_norm': 1991.34912109375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:15:38,  1.96s/it]  0%|          | 22/6000 [00:44<3:14:26,  1.95s/it]                                                   {'loss': 11.8426, 'grad_norm': 2039.3536376953125, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:14:26,  1.95s/it]  0%|          | 23/6000 [00:46<3:14:06,  1.95s/it]                                                   {'loss': 10.8735, 'grad_norm': 1652.7474365234375, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:14:06,  1.95s/it]  0%|          | 24/6000 [00:48<3:15:54,  1.97s/it]                                                   {'loss': 10.763, 'grad_norm': 1921.900390625, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:15:54,  1.97s/it]  0%|          | 25/6000 [00:50<3:14:13,  1.95s/it]                                                   {'loss': 11.101, 'grad_norm': 2463.854736328125, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:14:13,  1.95s/it]  0%|          | 26/6000 [00:52<3:15:30,  1.96s/it]                                                   {'loss': 10.4894, 'grad_norm': 1771.1279296875, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:15:30,  1.96s/it]  0%|          | 27/6000 [00:53<3:15:05,  1.96s/it]                                                   {'loss': 6.786, 'grad_norm': 1738.0404052734375, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:53<3:15:05,  1.96s/it]  0%|          | 28/6000 [00:56<3:23:56,  2.05s/it]                                                   {'loss': 7.4346, 'grad_norm': 2561.339599609375, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:23:56,  2.05s/it]  0%|          | 29/6000 [00:58<3:20:04,  2.01s/it]                                                   {'loss': 6.8935, 'grad_norm': 1908.1978759765625, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:20:04,  2.01s/it]  0%|          | 30/6000 [01:00<3:16:38,  1.98s/it]                                                   {'loss': 6.002, 'grad_norm': 1843.8289794921875, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:16:38,  1.98s/it]  1%|          | 31/6000 [01:02<3:18:21,  1.99s/it]                                                   {'loss': 5.2103, 'grad_norm': 1443.18896484375, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:18:21,  1.99s/it]  1%|          | 32/6000 [01:04<3:15:37,  1.97s/it]                                                   {'loss': 6.6699, 'grad_norm': 1345.0511474609375, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:15:37,  1.97s/it]  1%|          | 33/6000 [01:06<3:17:00,  1.98s/it]                                                   {'loss': 4.596, 'grad_norm': 1062.3726806640625, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:17:00,  1.98s/it]  1%|          | 34/6000 [01:07<3:14:23,  1.96s/it]                                                   {'loss': 5.7021, 'grad_norm': 1413.0574951171875, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:07<3:14:23,  1.96s/it]  1%|          | 35/6000 [01:09<3:14:30,  1.96s/it]                                                   {'loss': 4.8353, 'grad_norm': 1240.323486328125, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:09<3:14:30,  1.96s/it]  1%|          | 36/6000 [01:11<3:11:58,  1.93s/it]                                                   {'loss': 4.6144, 'grad_norm': 2331.996337890625, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:11<3:11:58,  1.93s/it]  1%|          | 37/6000 [01:13<3:11:20,  1.93s/it]                                                   {'loss': 4.5045, 'grad_norm': 870.2578735351562, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:13<3:11:20,  1.93s/it]  1%|          | 38/6000 [01:15<3:10:37,  1.92s/it]                                                   {'loss': 4.3819, 'grad_norm': 665.5364990234375, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:15<3:10:37,  1.92s/it]  1%|          | 39/6000 [01:17<3:10:27,  1.92s/it]                                                   {'loss': 4.0581, 'grad_norm': 686.13671875, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:17<3:10:27,  1.92s/it]  1%|          | 40/6000 [01:19<3:09:35,  1.91s/it]                                                   {'loss': 4.3994, 'grad_norm': 942.4226684570312, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:19<3:09:35,  1.91s/it]  1%|          | 41/6000 [01:21<3:08:29,  1.90s/it]                                                   {'loss': 3.6136, 'grad_norm': 824.2537841796875, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:21<3:08:29,  1.90s/it]  1%|          | 42/6000 [01:23<3:08:47,  1.90s/it]                                                   {'loss': 3.5699, 'grad_norm': 626.3458251953125, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:23<3:08:47,  1.90s/it]  1%|          | 43/6000 [01:25<3:21:03,  2.03s/it]                                                   {'loss': 3.8593, 'grad_norm': 612.3572998046875, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:25<3:21:03,  2.03s/it]  1%|          | 44/6000 [01:27<3:21:20,  2.03s/it]                                                   {'loss': 3.3262, 'grad_norm': 722.1362915039062, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:27<3:21:20,  2.03s/it]  1%|          | 45/6000 [01:29<3:18:20,  2.00s/it]                                                   {'loss': 3.2406, 'grad_norm': 290.4057922363281, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:29<3:18:20,  2.00s/it]  1%|          | 46/6000 [01:31<3:16:32,  1.98s/it]                                                   {'loss': 3.3853, 'grad_norm': 369.93145751953125, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:31<3:16:32,  1.98s/it]  1%|          | 47/6000 [01:33<3:14:17,  1.96s/it]                                                   {'loss': 3.4587, 'grad_norm': 308.0115661621094, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:33<3:14:17,  1.96s/it]  1%|          | 48/6000 [01:35<3:13:33,  1.95s/it]                                                   {'loss': 3.2015, 'grad_norm': 299.6376037597656, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:35<3:13:33,  1.95s/it]  1%|          | 49/6000 [01:37<3:13:09,  1.95s/it]                                                   {'loss': 3.0288, 'grad_norm': 378.576416015625, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:37<3:13:09,  1.95s/it]  1%|          | 50/6000 [01:39<3:16:28,  1.98s/it]                                                   {'loss': 3.4444, 'grad_norm': 262.482177734375, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:39<3:16:28,  1.98s/it]  1%|          | 51/6000 [01:41<3:15:35,  1.97s/it]                                                   {'loss': 3.3391, 'grad_norm': 239.82533264160156, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:41<3:15:35,  1.97s/it]  1%|          | 52/6000 [01:43<3:12:37,  1.94s/it]                                                   {'loss': 3.3339, 'grad_norm': 431.7749328613281, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:43<3:12:37,  1.94s/it]  1%|          | 53/6000 [01:44<3:12:58,  1.95s/it]                                                   {'loss': 4.4443, 'grad_norm': 769.2266235351562, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:44<3:12:58,  1.95s/it]  1%|          | 54/6000 [01:46<3:13:16,  1.95s/it]                                                   {'loss': 3.1477, 'grad_norm': 338.8988342285156, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:46<3:13:16,  1.95s/it]  1%|          | 55/6000 [01:48<3:12:03,  1.94s/it]                                                   {'loss': 3.1857, 'grad_norm': 253.52029418945312, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:48<3:12:03,  1.94s/it]  1%|          | 56/6000 [01:50<3:13:24,  1.95s/it]                                                   {'loss': 3.1732, 'grad_norm': 335.7392578125, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:50<3:13:24,  1.95s/it]  1%|          | 57/6000 [01:52<3:13:16,  1.95s/it]                                                   {'loss': 3.1991, 'grad_norm': 321.3367004394531, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:52<3:13:16,  1.95s/it]  1%|          | 58/6000 [01:54<3:13:23,  1.95s/it]                                                   {'loss': 3.2171, 'grad_norm': 351.9100341796875, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:54<3:13:23,  1.95s/it]  1%|          | 59/6000 [01:56<3:13:25,  1.95s/it]                                                   {'loss': 3.227, 'grad_norm': 409.8426208496094, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:56<3:13:25,  1.95s/it]  1%|          | 60/6000 [01:58<3:13:59,  1.96s/it]                                                   {'loss': 3.1646, 'grad_norm': 272.5627746582031, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [01:58<3:13:59,  1.96s/it]  1%|          | 61/6000 [02:00<3:11:09,  1.93s/it]                                                   {'loss': 3.1909, 'grad_norm': 244.72080993652344, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:00<3:11:09,  1.93s/it]  1%|          | 62/6000 [02:02<3:15:10,  1.97s/it]                                                   {'loss': 3.1083, 'grad_norm': 222.85702514648438, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:02<3:15:10,  1.97s/it]  1%|          | 63/6000 [02:04<3:16:44,  1.99s/it]                                                   {'loss': 3.0261, 'grad_norm': 322.6907043457031, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:04<3:16:44,  1.99s/it]  1%|          | 64/6000 [02:06<3:14:24,  1.97s/it]                                                   {'loss': 2.8726, 'grad_norm': 256.3105773925781, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:06<3:14:24,  1.97s/it]  1%|          | 65/6000 [02:08<3:12:03,  1.94s/it]                                                   {'loss': 3.0959, 'grad_norm': 306.97027587890625, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:08<3:12:03,  1.94s/it]  1%|          | 66/6000 [02:10<3:14:29,  1.97s/it]                                                   {'loss': 2.7984, 'grad_norm': 418.1914978027344, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:10<3:14:29,  1.97s/it]  1%|          | 67/6000 [02:12<3:14:53,  1.97s/it]                                                   {'loss': 2.9409, 'grad_norm': 273.9392395019531, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:12<3:14:53,  1.97s/it]  1%|          | 68/6000 [02:14<3:11:42,  1.94s/it]                                                   {'loss': 3.4048, 'grad_norm': 204.43853759765625, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:14<3:11:42,  1.94s/it]  1%|          | 69/6000 [02:16<3:10:24,  1.93s/it]                                                   {'loss': 3.0489, 'grad_norm': 187.42449951171875, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:16<3:10:24,  1.93s/it]  1%|          | 70/6000 [02:18<3:10:48,  1.93s/it]                                                   {'loss': 2.7459, 'grad_norm': 253.4027099609375, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:18<3:10:48,  1.93s/it]  1%|          | 71/6000 [02:19<3:08:37,  1.91s/it]                                                   {'loss': 2.9165, 'grad_norm': 407.1465759277344, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:20<3:08:37,  1.91s/it]  1%|          | 72/6000 [02:21<3:11:19,  1.94s/it]                                                   {'loss': 3.1309, 'grad_norm': 374.0435485839844, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:21<3:11:19,  1.94s/it]  1%|          | 73/6000 [02:23<3:10:25,  1.93s/it]                                                   {'loss': 2.7234, 'grad_norm': 212.2965087890625, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:23<3:10:25,  1.93s/it]  1%|          | 74/6000 [02:25<3:07:28,  1.90s/it]                                                   {'loss': 2.7904, 'grad_norm': 160.5597381591797, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:25<3:07:28,  1.90s/it]  1%|â–         | 75/6000 [02:27<3:07:30,  1.90s/it]                                                   {'loss': 2.5905, 'grad_norm': 124.2146987915039, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:27<3:07:30,  1.90s/it]  1%|â–         | 76/6000 [02:29<3:15:26,  1.98s/it]                                                   {'loss': 2.5472, 'grad_norm': 172.53182983398438, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:29<3:15:26,  1.98s/it]  1%|â–         | 77/6000 [02:31<3:14:11,  1.97s/it]                                                   {'loss': 3.05, 'grad_norm': 300.29205322265625, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:31<3:14:11,  1.97s/it]  1%|â–         | 78/6000 [02:33<3:16:11,  1.99s/it]                                                   {'loss': 2.5645, 'grad_norm': 174.58328247070312, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:34<3:16:11,  1.99s/it]  1%|â–         | 79/6000 [02:35<3:19:55,  2.03s/it]                                                   {'loss': 2.5287, 'grad_norm': 191.55955505371094, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:35<3:19:55,  2.03s/it]  1%|â–         | 80/6000 [02:37<3:19:11,  2.02s/it]                                                   {'loss': 2.5065, 'grad_norm': 175.23190307617188, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:37<3:19:11,  2.02s/it]  1%|â–         | 81/6000 [02:39<3:17:44,  2.00s/it]                                                   {'loss': 2.5972, 'grad_norm': 190.5330352783203, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:39<3:17:44,  2.00s/it]  1%|â–         | 82/6000 [02:41<3:17:31,  2.00s/it]                                                   {'loss': 2.2296, 'grad_norm': 216.7635498046875, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:41<3:17:31,  2.00s/it]  1%|â–         | 83/6000 [02:43<3:15:20,  1.98s/it]                                                   {'loss': 2.3289, 'grad_norm': 183.57420349121094, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:43<3:15:20,  1.98s/it]  1%|â–         | 84/6000 [02:45<3:16:20,  1.99s/it]                                                   {'loss': 2.1789, 'grad_norm': 145.1134490966797, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:45<3:16:20,  1.99s/it]  1%|â–         | 85/6000 [02:47<3:18:38,  2.01s/it]                                                   {'loss': 2.2133, 'grad_norm': 162.16796875, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:47<3:18:38,  2.01s/it]  1%|â–         | 86/6000 [02:49<3:17:19,  2.00s/it]                                                   {'loss': 2.1835, 'grad_norm': 182.8411407470703, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:49<3:17:19,  2.00s/it]  1%|â–         | 87/6000 [02:51<3:15:25,  1.98s/it]                                                   {'loss': 1.8471, 'grad_norm': 381.32586669921875, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:51<3:15:25,  1.98s/it]  1%|â–         | 88/6000 [02:53<3:12:18,  1.95s/it]                                                   {'loss': 2.1383, 'grad_norm': 394.9350280761719, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:53<3:12:18,  1.95s/it]  1%|â–         | 89/6000 [02:55<3:11:11,  1.94s/it]                                                   {'loss': 1.5191, 'grad_norm': 119.98023986816406, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:55<3:11:11,  1.94s/it]  2%|â–         | 90/6000 [02:57<3:14:02,  1.97s/it]                                                   {'loss': 1.3418, 'grad_norm': 149.05296325683594, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [02:57<3:14:02,  1.97s/it]  2%|â–         | 91/6000 [02:59<3:13:58,  1.97s/it]                                                   {'loss': 1.6997, 'grad_norm': 176.99610900878906, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [02:59<3:13:58,  1.97s/it]  2%|â–         | 92/6000 [03:01<3:14:09,  1.97s/it]                                                   {'loss': 1.3912, 'grad_norm': 452.4240417480469, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:01<3:14:09,  1.97s/it]  2%|â–         | 93/6000 [03:03<3:13:27,  1.97s/it]                                                   {'loss': 1.6133, 'grad_norm': 287.81427001953125, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:03<3:13:27,  1.97s/it]  2%|â–         | 94/6000 [03:05<3:10:56,  1.94s/it]                                                   {'loss': 1.6856, 'grad_norm': 256.71063232421875, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:05<3:10:56,  1.94s/it]  2%|â–         | 95/6000 [03:07<3:11:30,  1.95s/it]                                                   {'loss': 1.0641, 'grad_norm': 318.8293762207031, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:07<3:11:30,  1.95s/it]  2%|â–         | 96/6000 [03:09<3:09:56,  1.93s/it]                                                   {'loss': 1.1924, 'grad_norm': 443.3946533203125, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:09<3:09:56,  1.93s/it]  2%|â–         | 97/6000 [03:11<3:10:14,  1.93s/it]                                                   {'loss': 1.4859, 'grad_norm': 393.5127868652344, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:11<3:10:14,  1.93s/it]  2%|â–         | 98/6000 [03:13<3:10:25,  1.94s/it]                                                   {'loss': 0.9877, 'grad_norm': 115.79851531982422, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:13<3:10:25,  1.94s/it]  2%|â–         | 99/6000 [03:15<3:11:08,  1.94s/it]                                                   {'loss': 0.7352, 'grad_norm': 113.50425720214844, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:15<3:11:08,  1.94s/it]  2%|â–         | 100/6000 [03:16<3:09:09,  1.92s/it]                                                    {'loss': 1.1485, 'grad_norm': 164.2667999267578, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:16<3:09:09,  1.92s/it]  2%|â–         | 101/6000 [03:19<3:20:42,  2.04s/it]                                                    {'loss': 0.6516, 'grad_norm': 106.57857513427734, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:19<3:20:42,  2.04s/it]  2%|â–         | 102/6000 [03:21<3:17:50,  2.01s/it]                                                    {'loss': 0.7647, 'grad_norm': 166.56637573242188, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:21<3:17:50,  2.01s/it]  2%|â–         | 103/6000 [03:23<3:15:55,  1.99s/it]                                                    {'loss': 0.7492, 'grad_norm': 94.3575439453125, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:23<3:15:55,  1.99s/it]  2%|â–         | 104/6000 [03:25<3:16:05,  2.00s/it]                                                    {'loss': 0.5907, 'grad_norm': 76.69686889648438, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:25<3:16:05,  2.00s/it]  2%|â–         | 105/6000 [03:27<3:13:57,  1.97s/it]                                                    {'loss': 0.6817, 'grad_norm': 62.17612075805664, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:27<3:13:57,  1.97s/it]  2%|â–         | 106/6000 [03:29<3:15:49,  1.99s/it]                                                    {'loss': 0.426, 'grad_norm': 67.90943908691406, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:29<3:15:49,  1.99s/it]  2%|â–         | 107/6000 [03:31<3:14:21,  1.98s/it]                                                    {'loss': 0.57, 'grad_norm': 73.23336029052734, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:31<3:14:21,  1.98s/it]  2%|â–         | 108/6000 [03:33<3:13:59,  1.98s/it]                                                    {'loss': 0.3136, 'grad_norm': 53.319915771484375, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:33<3:13:59,  1.98s/it]  2%|â–         | 109/6000 [03:35<3:17:12,  2.01s/it]                                                    {'loss': 0.5059, 'grad_norm': 57.810550689697266, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:35<3:17:12,  2.01s/it]  2%|â–         | 110/6000 [03:37<3:16:26,  2.00s/it]                                                    {'loss': 0.3712, 'grad_norm': 52.06761169433594, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:37<3:16:26,  2.00s/it]  2%|â–         | 111/6000 [03:39<3:14:42,  1.98s/it]                                                    {'loss': 0.4565, 'grad_norm': 59.224456787109375, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:39<3:14:42,  1.98s/it]  2%|â–         | 112/6000 [03:41<3:19:34,  2.03s/it]                                                    {'loss': 0.4877, 'grad_norm': 63.5995979309082, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:41<3:19:34,  2.03s/it]  2%|â–         | 113/6000 [03:43<3:17:00,  2.01s/it]                                                    {'loss': 0.3452, 'grad_norm': 67.43367767333984, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:43<3:17:00,  2.01s/it]  2%|â–         | 114/6000 [03:45<3:15:17,  1.99s/it]                                                    {'loss': 0.3113, 'grad_norm': 47.13335418701172, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:45<3:15:17,  1.99s/it]  2%|â–         | 115/6000 [03:47<3:13:23,  1.97s/it]                                                    {'loss': 0.4496, 'grad_norm': 68.44902801513672, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:47<3:13:23,  1.97s/it]  2%|â–         | 116/6000 [03:48<3:11:21,  1.95s/it]                                                    {'loss': 0.4377, 'grad_norm': 55.19437026977539, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:48<3:11:21,  1.95s/it]  2%|â–         | 117/6000 [03:50<3:09:42,  1.93s/it]                                                    {'loss': 0.515, 'grad_norm': 50.740379333496094, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:50<3:09:42,  1.93s/it]  2%|â–         | 118/6000 [03:52<3:14:25,  1.98s/it]                                                    {'loss': 0.4211, 'grad_norm': 48.47934341430664, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:52<3:14:25,  1.98s/it]  2%|â–         | 119/6000 [03:54<3:11:45,  1.96s/it]                                                    {'loss': 0.2856, 'grad_norm': 54.24345397949219, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:54<3:11:45,  1.96s/it]  2%|â–         | 120/6000 [03:56<3:11:50,  1.96s/it]                                                    {'loss': 0.3409, 'grad_norm': 42.61217498779297, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [03:56<3:11:50,  1.96s/it]  2%|â–         | 121/6000 [03:58<3:14:06,  1.98s/it]                                                    {'loss': 0.2741, 'grad_norm': 90.44550323486328, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [03:58<3:14:06,  1.98s/it]  2%|â–         | 122/6000 [04:00<3:13:26,  1.97s/it]                                                    {'loss': 0.2923, 'grad_norm': 47.70569610595703, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:00<3:13:26,  1.97s/it]  2%|â–         | 123/6000 [04:02<3:11:55,  1.96s/it]                                                    {'loss': 0.1602, 'grad_norm': 52.8864860534668, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:02<3:11:55,  1.96s/it]  2%|â–         | 124/6000 [04:04<3:09:33,  1.94s/it]                                                    {'loss': 0.2753, 'grad_norm': 45.63762664794922, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:04<3:09:33,  1.94s/it]  2%|â–         | 125/6000 [04:06<3:14:26,  1.99s/it]                                                    {'loss': 0.1724, 'grad_norm': 50.758827209472656, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:06<3:14:26,  1.99s/it]  2%|â–         | 126/6000 [04:08<3:14:12,  1.98s/it]                                                    {'loss': 0.3773, 'grad_norm': 54.45388412475586, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:08<3:14:12,  1.98s/it]  2%|â–         | 127/6000 [04:10<3:13:55,  1.98s/it]                                                    {'loss': 0.241, 'grad_norm': 31.554765701293945, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:10<3:13:55,  1.98s/it]  2%|â–         | 128/6000 [04:12<3:10:57,  1.95s/it]                                                    {'loss': 0.2017, 'grad_norm': 42.00022506713867, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:12<3:10:57,  1.95s/it]  2%|â–         | 129/6000 [04:14<3:10:29,  1.95s/it]                                                    {'loss': 0.4672, 'grad_norm': 69.58706665039062, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:14<3:10:29,  1.95s/it]  2%|â–         | 130/6000 [04:16<3:10:05,  1.94s/it]                                                    {'loss': 0.1259, 'grad_norm': 23.446380615234375, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:16<3:10:05,  1.94s/it]  2%|â–         | 131/6000 [04:18<3:10:29,  1.95s/it]                                                    {'loss': 0.374, 'grad_norm': 314.8750305175781, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:18<3:10:29,  1.95s/it]  2%|â–         | 132/6000 [04:20<3:09:56,  1.94s/it]                                                    {'loss': 0.1212, 'grad_norm': 26.956979751586914, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:20<3:09:56,  1.94s/it]  2%|â–         | 133/6000 [04:22<3:10:41,  1.95s/it]                                                    {'loss': 0.3019, 'grad_norm': 48.92669677734375, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:22<3:10:41,  1.95s/it]  2%|â–         | 134/6000 [04:24<3:13:07,  1.98s/it]                                                    {'loss': 0.3602, 'grad_norm': 57.58314895629883, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:24<3:13:07,  1.98s/it]  2%|â–         | 135/6000 [04:26<3:10:53,  1.95s/it]                                                    {'loss': 0.3001, 'grad_norm': 54.638710021972656, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:26<3:10:53,  1.95s/it]  2%|â–         | 136/6000 [04:28<3:08:59,  1.93s/it]                                                    {'loss': 0.2466, 'grad_norm': 36.28264617919922, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:28<3:08:59,  1.93s/it]  2%|â–         | 137/6000 [04:30<3:14:12,  1.99s/it]                                                    {'loss': 0.197, 'grad_norm': 37.73796844482422, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:30<3:14:12,  1.99s/it]  2%|â–         | 138/6000 [04:32<3:11:23,  1.96s/it]                                                    {'loss': 0.2319, 'grad_norm': 65.54933166503906, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:32<3:11:23,  1.96s/it]  2%|â–         | 139/6000 [04:34<3:14:31,  1.99s/it]                                                    {'loss': 0.365, 'grad_norm': 45.854103088378906, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:34<3:14:31,  1.99s/it]  2%|â–         | 140/6000 [04:36<3:19:53,  2.05s/it]                                                    {'loss': 0.1938, 'grad_norm': 45.35061264038086, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:36<3:19:53,  2.05s/it]  2%|â–         | 141/6000 [04:38<3:18:00,  2.03s/it]                                                    {'loss': 0.3316, 'grad_norm': 31.417308807373047, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:38<3:18:00,  2.03s/it]  2%|â–         | 142/6000 [04:40<3:13:50,  1.99s/it]                                                    {'loss': 0.1661, 'grad_norm': 26.924427032470703, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:40<3:13:50,  1.99s/it]  2%|â–         | 143/6000 [04:42<3:10:37,  1.95s/it]                                                    {'loss': 0.1048, 'grad_norm': 22.497243881225586, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:42<3:10:37,  1.95s/it]  2%|â–         | 144/6000 [04:44<3:10:26,  1.95s/it]                                                    {'loss': 0.0824, 'grad_norm': 23.482280731201172, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:44<3:10:26,  1.95s/it]  2%|â–         | 145/6000 [04:45<3:08:32,  1.93s/it]                                                    {'loss': 0.0565, 'grad_norm': 24.44199562072754, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:45<3:08:32,  1.93s/it]  2%|â–         | 146/6000 [04:47<3:09:34,  1.94s/it]                                                    {'loss': 0.2564, 'grad_norm': 38.68225860595703, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:47<3:09:34,  1.94s/it]  2%|â–         | 147/6000 [04:49<3:07:36,  1.92s/it]                                                    {'loss': 0.4795, 'grad_norm': 331.6011657714844, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:49<3:07:36,  1.92s/it]  2%|â–         | 148/6000 [04:51<3:11:06,  1.96s/it]                                                    {'loss': 0.1464, 'grad_norm': 33.528907775878906, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:51<3:11:06,  1.96s/it]  2%|â–         | 149/6000 [04:53<3:08:29,  1.93s/it]                                                    {'loss': 0.1831, 'grad_norm': 37.3380012512207, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:53<3:08:29,  1.93s/it]  2%|â–Ž         | 150/6000 [04:55<3:07:20,  1.92s/it]                                                    {'loss': 0.0535, 'grad_norm': 61.50143051147461, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [04:55<3:07:20,  1.92s/it]  3%|â–Ž         | 151/6000 [04:57<3:06:52,  1.92s/it]                                                    {'loss': 0.0866, 'grad_norm': 11.463621139526367, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [04:57<3:06:52,  1.92s/it]  3%|â–Ž         | 152/6000 [04:59<3:07:47,  1.93s/it]                                                    {'loss': 0.1935, 'grad_norm': 46.45035934448242, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [04:59<3:07:47,  1.93s/it]  3%|â–Ž         | 153/6000 [05:01<3:05:51,  1.91s/it]                                                    {'loss': 0.1076, 'grad_norm': 14.83492374420166, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:01<3:05:51,  1.91s/it]  3%|â–Ž         | 154/6000 [05:03<3:06:53,  1.92s/it]                                                    {'loss': 0.1773, 'grad_norm': 34.42214584350586, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:03<3:06:53,  1.92s/it]  3%|â–Ž         | 155/6000 [05:05<3:07:09,  1.92s/it]                                                    {'loss': 0.1138, 'grad_norm': 16.54072380065918, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:05<3:07:09,  1.92s/it]  3%|â–Ž         | 156/6000 [05:07<3:07:28,  1.92s/it]                                                    {'loss': 0.0452, 'grad_norm': 11.816666603088379, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:07<3:07:28,  1.92s/it]  3%|â–Ž         | 157/6000 [05:09<3:16:05,  2.01s/it]                                                    {'loss': 0.103, 'grad_norm': 20.789541244506836, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:09<3:16:05,  2.01s/it]  3%|â–Ž         | 158/6000 [05:11<3:13:19,  1.99s/it]                                                    {'loss': 0.0581, 'grad_norm': 17.40196990966797, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:11<3:13:19,  1.99s/it]  3%|â–Ž         | 159/6000 [05:13<3:12:17,  1.98s/it]                                                    {'loss': 0.1558, 'grad_norm': 28.446537017822266, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:13<3:12:17,  1.98s/it]  3%|â–Ž         | 160/6000 [05:15<3:17:57,  2.03s/it]                                                    {'loss': 0.1017, 'grad_norm': 16.555320739746094, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:15<3:17:57,  2.03s/it]  3%|â–Ž         | 161/6000 [05:17<3:15:55,  2.01s/it]                                                    {'loss': 0.1916, 'grad_norm': 26.999736785888672, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:17<3:15:55,  2.01s/it]  3%|â–Ž         | 162/6000 [05:19<3:16:15,  2.02s/it]                                                    {'loss': 0.041, 'grad_norm': 10.744584083557129, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:19<3:16:15,  2.02s/it]  3%|â–Ž         | 163/6000 [05:21<3:23:08,  2.09s/it]                                                    {'loss': 0.182, 'grad_norm': 38.1089973449707, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:21<3:23:08,  2.09s/it]  3%|â–Ž         | 164/6000 [05:23<3:17:44,  2.03s/it]                                                    {'loss': 0.3573, 'grad_norm': 43.06754684448242, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:23<3:17:44,  2.03s/it]  3%|â–Ž         | 165/6000 [05:25<3:15:13,  2.01s/it]                                                    {'loss': 0.0301, 'grad_norm': 8.346744537353516, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:25<3:15:13,  2.01s/it]  3%|â–Ž         | 166/6000 [05:27<3:12:08,  1.98s/it]                                                    {'loss': 0.1322, 'grad_norm': 21.053651809692383, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:27<3:12:08,  1.98s/it]  3%|â–Ž         | 167/6000 [05:29<3:09:44,  1.95s/it]                                                    {'loss': 0.1121, 'grad_norm': 25.620136260986328, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:29<3:09:44,  1.95s/it]  3%|â–Ž         | 168/6000 [05:31<3:08:54,  1.94s/it]                                                    {'loss': 0.2771, 'grad_norm': 36.42057418823242, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:31<3:08:54,  1.94s/it]  3%|â–Ž         | 169/6000 [05:33<3:13:25,  1.99s/it]                                                    {'loss': 0.2609, 'grad_norm': 36.08107376098633, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:33<3:13:25,  1.99s/it]  3%|â–Ž         | 170/6000 [05:35<3:10:25,  1.96s/it]                                                    {'loss': 0.2149, 'grad_norm': 33.07881164550781, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:35<3:10:25,  1.96s/it]  3%|â–Ž         | 171/6000 [05:37<3:10:26,  1.96s/it]                                                    {'loss': 0.1805, 'grad_norm': 24.71103858947754, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:37<3:10:26,  1.96s/it]  3%|â–Ž         | 172/6000 [05:39<3:10:30,  1.96s/it]                                                    {'loss': 0.2568, 'grad_norm': 44.17470932006836, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:39<3:10:30,  1.96s/it]  3%|â–Ž         | 173/6000 [05:41<3:10:03,  1.96s/it]                                                    {'loss': 0.1186, 'grad_norm': 27.209474563598633, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:41<3:10:03,  1.96s/it]  3%|â–Ž         | 174/6000 [05:43<3:14:56,  2.01s/it]                                                    {'loss': 0.0495, 'grad_norm': 8.369969367980957, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:43<3:14:56,  2.01s/it]  3%|â–Ž         | 175/6000 [05:45<3:12:59,  1.99s/it]                                                    {'loss': 0.2951, 'grad_norm': 32.25879669189453, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:45<3:12:59,  1.99s/it]  3%|â–Ž         | 176/6000 [05:47<3:14:35,  2.00s/it]                                                    {'loss': 0.1387, 'grad_norm': 23.586477279663086, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:47<3:14:35,  2.00s/it]  3%|â–Ž         | 177/6000 [05:49<3:12:51,  1.99s/it]                                                    {'loss': 0.1072, 'grad_norm': 20.044464111328125, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:49<3:12:51,  1.99s/it]