==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 09:33:14.567000 126297746700096 torch/distributed/run.py:779] 
W1118 09:33:14.567000 126297746700096 torch/distributed/run.py:779] *****************************************
W1118 09:33:14.567000 126297746700096 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 09:33:14.567000 126297746700096 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 09:33:22,116] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.65it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.42it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: setting up run 4c0lcccp
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_093322-4c0lcccp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/4c0lcccp
[2025-11-18 09:33:24,047] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.78it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.06it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-18 09:33:24,572] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGenerationWithTail(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 09:33:30,480] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 09:33:32,241] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 09:33:32,242] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 09:33:35,650] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 09:33:35,651] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 09:33:36,612] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 09:33:36,612] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 09:33:36,612] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 09:33:36,613] INFO [src.utils:19] ==================================================
[2025-11-18 09:33:36,613] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 09:33:36,614] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:33:36,614] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:33:36,615] INFO [src.utils:19] ==================================================
[2025-11-18 09:33:38,185] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:33:38,185] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:33:38,185] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:33:38,185] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:33:38,185] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:33:38,185] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:33:38,185] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:33:38,185] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:33:38,185] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:33:38,186] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:33:38,186] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:33:38,186] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:33:38,186] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:33:38,187] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:33:38,189] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 09:33:38,190] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 09:33:38,191] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 09:33:38,193] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:03<5:46:24,  3.46s/it]                                                  {'loss': 21.0466, 'grad_norm': 1055.515869140625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:46:24,  3.46s/it]  0%|          | 2/6000 [00:05<4:15:53,  2.56s/it]                                                  {'loss': 17.8246, 'grad_norm': 1039.970458984375, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:15:53,  2.56s/it]  0%|          | 3/6000 [00:07<3:49:39,  2.30s/it]                                                  {'loss': 16.658, 'grad_norm': 1412.148681640625, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:49:39,  2.30s/it]  0%|          | 4/6000 [00:09<3:38:17,  2.18s/it]                                                  {'loss': 18.4007, 'grad_norm': 1314.22998046875, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:38:17,  2.18s/it]  0%|          | 5/6000 [00:11<3:28:36,  2.09s/it]                                                  {'loss': 19.0279, 'grad_norm': 1451.571533203125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:28:36,  2.09s/it]  0%|          | 6/6000 [00:13<3:23:51,  2.04s/it]                                                  {'loss': 18.7566, 'grad_norm': 1029.45947265625, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:23:51,  2.04s/it]  0%|          | 7/6000 [00:15<3:21:31,  2.02s/it]                                                  {'loss': 18.9572, 'grad_norm': 1505.2340087890625, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:21:31,  2.02s/it]  0%|          | 8/6000 [00:17<3:20:27,  2.01s/it]                                                  {'loss': 18.5684, 'grad_norm': 1409.552001953125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:17<3:20:27,  2.01s/it]  0%|          | 9/6000 [00:19<3:20:25,  2.01s/it]                                                  {'loss': 15.3413, 'grad_norm': 1179.5528564453125, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:19<3:20:25,  2.01s/it]  0%|          | 10/6000 [00:21<3:19:05,  1.99s/it]                                                   {'loss': 18.6121, 'grad_norm': 1091.572021484375, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:21<3:19:05,  1.99s/it]  0%|          | 11/6000 [00:23<3:20:19,  2.01s/it]                                                   {'loss': 22.2545, 'grad_norm': 1417.2913818359375, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:23<3:20:19,  2.01s/it]  0%|          | 12/6000 [00:25<3:19:54,  2.00s/it]                                                   {'loss': 18.1355, 'grad_norm': 1087.60986328125, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:25<3:19:54,  2.00s/it]  0%|          | 13/6000 [00:27<3:18:42,  1.99s/it]                                                   {'loss': 20.7692, 'grad_norm': 1389.55419921875, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:27<3:18:42,  1.99s/it]  0%|          | 14/6000 [00:29<3:17:04,  1.98s/it]                                                   {'loss': 19.73, 'grad_norm': 1601.0943603515625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:29<3:17:04,  1.98s/it]  0%|          | 15/6000 [00:31<3:17:19,  1.98s/it]                                                   {'loss': 16.6179, 'grad_norm': 1182.1458740234375, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:31<3:17:19,  1.98s/it]  0%|          | 16/6000 [00:33<3:15:33,  1.96s/it]                                                   {'loss': 18.5451, 'grad_norm': 1691.5343017578125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:33<3:15:33,  1.96s/it]  0%|          | 17/6000 [00:34<3:14:05,  1.95s/it]                                                   {'loss': 15.3754, 'grad_norm': 1319.144775390625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:14:05,  1.95s/it]  0%|          | 18/6000 [00:36<3:14:58,  1.96s/it]                                                   {'loss': 14.7167, 'grad_norm': 2265.10205078125, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:14:58,  1.96s/it]  0%|          | 19/6000 [00:38<3:12:51,  1.93s/it]                                                   {'loss': 15.5783, 'grad_norm': 1244.4112548828125, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:12:51,  1.93s/it]  0%|          | 20/6000 [00:40<3:16:00,  1.97s/it]                                                   {'loss': 16.4719, 'grad_norm': 1386.7677001953125, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:16:00,  1.97s/it]  0%|          | 21/6000 [00:42<3:15:50,  1.97s/it]                                                   {'loss': 14.2651, 'grad_norm': 1347.8477783203125, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:15:50,  1.97s/it]  0%|          | 22/6000 [00:44<3:14:31,  1.95s/it]                                                   {'loss': 14.3083, 'grad_norm': 1334.347412109375, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:14:31,  1.95s/it]  0%|          | 23/6000 [00:46<3:14:25,  1.95s/it]                                                   {'loss': 13.1152, 'grad_norm': 1234.9417724609375, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:14:25,  1.95s/it]  0%|          | 24/6000 [00:48<3:17:16,  1.98s/it]                                                   {'loss': 9.6315, 'grad_norm': 1258.5380859375, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:17:16,  1.98s/it]  0%|          | 25/6000 [00:50<3:16:26,  1.97s/it]                                                   {'loss': 11.925, 'grad_norm': 1440.9365234375, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:16:26,  1.97s/it]  0%|          | 26/6000 [00:52<3:15:58,  1.97s/it]                                                   {'loss': 12.2126, 'grad_norm': 1695.935546875, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:15:58,  1.97s/it]  0%|          | 27/6000 [00:54<3:15:15,  1.96s/it]                                                   {'loss': 12.3415, 'grad_norm': 1332.339111328125, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:15:15,  1.96s/it]  0%|          | 28/6000 [00:56<3:25:39,  2.07s/it]                                                   {'loss': 8.8737, 'grad_norm': 1341.3692626953125, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:25:39,  2.07s/it]  0%|          | 29/6000 [00:58<3:22:21,  2.03s/it]                                                   {'loss': 9.0332, 'grad_norm': 1658.3853759765625, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:22:21,  2.03s/it]  0%|          | 30/6000 [01:00<3:20:28,  2.01s/it]                                                   {'loss': 8.5896, 'grad_norm': 1456.707275390625, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:20:28,  2.01s/it]  1%|          | 31/6000 [01:02<3:19:48,  2.01s/it]                                                   {'loss': 6.8999, 'grad_norm': 1101.726806640625, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:19:48,  2.01s/it]  1%|          | 32/6000 [01:04<3:19:37,  2.01s/it]                                                   {'loss': 6.8369, 'grad_norm': 877.2868041992188, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:19:37,  2.01s/it]  1%|          | 33/6000 [01:06<3:18:40,  2.00s/it]                                                   {'loss': 4.82, 'grad_norm': 728.893310546875, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:18:40,  2.00s/it]  1%|          | 34/6000 [01:08<3:17:36,  1.99s/it]                                                   {'loss': 5.0005, 'grad_norm': 601.7606201171875, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:17:36,  1.99s/it]  1%|          | 35/6000 [01:10<3:18:26,  2.00s/it]                                                   {'loss': 4.7859, 'grad_norm': 770.27685546875, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:18:26,  2.00s/it]  1%|          | 36/6000 [01:12<3:16:14,  1.97s/it]                                                   {'loss': 5.1357, 'grad_norm': 1142.125732421875, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:16:14,  1.97s/it]  1%|          | 37/6000 [01:14<3:15:53,  1.97s/it]                                                   {'loss': 4.2187, 'grad_norm': 573.4384155273438, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:15:53,  1.97s/it]  1%|          | 38/6000 [01:16<3:15:05,  1.96s/it]                                                   {'loss': 4.2404, 'grad_norm': 901.5332641601562, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:15:05,  1.96s/it]  1%|          | 39/6000 [01:18<3:15:27,  1.97s/it]                                                   {'loss': 4.0363, 'grad_norm': 594.786376953125, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:15:27,  1.97s/it]  1%|          | 40/6000 [01:20<3:15:11,  1.96s/it]                                                   {'loss': 3.8511, 'grad_norm': 400.6147766113281, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:15:11,  1.96s/it]  1%|          | 41/6000 [01:22<3:14:04,  1.95s/it]                                                   {'loss': 4.0639, 'grad_norm': 480.8150329589844, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:22<3:14:04,  1.95s/it]  1%|          | 42/6000 [01:24<3:12:59,  1.94s/it]                                                   {'loss': 3.9321, 'grad_norm': 574.2913818359375, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:24<3:12:59,  1.94s/it]  1%|          | 43/6000 [01:26<3:25:39,  2.07s/it]                                                   {'loss': 3.7681, 'grad_norm': 539.6215209960938, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:25:39,  2.07s/it]  1%|          | 44/6000 [01:28<3:26:01,  2.08s/it]                                                   {'loss': 3.8545, 'grad_norm': 335.9150390625, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:26:01,  2.08s/it]  1%|          | 45/6000 [01:30<3:22:39,  2.04s/it]                                                   {'loss': 3.3131, 'grad_norm': 235.98910522460938, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:22:39,  2.04s/it]  1%|          | 46/6000 [01:32<3:21:54,  2.03s/it]                                                   {'loss': 3.63, 'grad_norm': 395.2117919921875, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:21:54,  2.03s/it]  1%|          | 47/6000 [01:34<3:20:42,  2.02s/it]                                                   {'loss': 3.6213, 'grad_norm': 224.3897247314453, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:20:42,  2.02s/it]  1%|          | 48/6000 [01:36<3:18:53,  2.00s/it]                                                   {'loss': 3.3869, 'grad_norm': 291.5752868652344, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:18:53,  2.00s/it]  1%|          | 49/6000 [01:38<3:16:54,  1.99s/it]                                                   {'loss': 3.3289, 'grad_norm': 211.8898468017578, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:16:54,  1.99s/it]  1%|          | 50/6000 [01:40<3:17:42,  1.99s/it]                                                   {'loss': 3.4252, 'grad_norm': 275.4920349121094, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:17:42,  1.99s/it]  1%|          | 51/6000 [01:42<3:17:34,  1.99s/it]                                                   {'loss': 3.5037, 'grad_norm': 206.04092407226562, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:42<3:17:34,  1.99s/it]  1%|          | 52/6000 [01:44<3:15:45,  1.97s/it]                                                   {'loss': 3.4689, 'grad_norm': 156.5975341796875, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:44<3:15:45,  1.97s/it]  1%|          | 53/6000 [01:46<3:17:44,  2.00s/it]                                                   {'loss': 3.8609, 'grad_norm': 193.1177520751953, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:46<3:17:44,  2.00s/it]  1%|          | 54/6000 [01:48<3:16:01,  1.98s/it]                                                   {'loss': 3.5272, 'grad_norm': 164.2830352783203, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:48<3:16:01,  1.98s/it]  1%|          | 55/6000 [01:50<3:15:37,  1.97s/it]                                                   {'loss': 3.0824, 'grad_norm': 153.0820770263672, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:50<3:15:37,  1.97s/it]  1%|          | 56/6000 [01:52<3:15:43,  1.98s/it]                                                   {'loss': 3.1912, 'grad_norm': 165.63308715820312, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:52<3:15:43,  1.98s/it]  1%|          | 57/6000 [01:54<3:15:07,  1.97s/it]                                                   {'loss': 3.1943, 'grad_norm': 143.99652099609375, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:54<3:15:07,  1.97s/it]  1%|          | 58/6000 [01:56<3:16:23,  1.98s/it]                                                   {'loss': 3.2721, 'grad_norm': 129.0093536376953, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:56<3:16:23,  1.98s/it]  1%|          | 59/6000 [01:58<3:14:08,  1.96s/it]                                                   {'loss': 3.0479, 'grad_norm': 117.81878662109375, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:58<3:14:08,  1.96s/it]  1%|          | 60/6000 [02:00<3:15:19,  1.97s/it]                                                   {'loss': 3.1016, 'grad_norm': 153.1249542236328, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:00<3:15:19,  1.97s/it]  1%|          | 61/6000 [02:02<3:14:21,  1.96s/it]                                                   {'loss': 3.0138, 'grad_norm': 129.09141540527344, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:02<3:14:21,  1.96s/it]  1%|          | 62/6000 [02:04<3:14:49,  1.97s/it]                                                   {'loss': 3.3859, 'grad_norm': 121.94622802734375, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:04<3:14:49,  1.97s/it]  1%|          | 63/6000 [02:06<3:14:57,  1.97s/it]                                                   {'loss': 2.9872, 'grad_norm': 94.79901885986328, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:06<3:14:57,  1.97s/it]  1%|          | 64/6000 [02:08<3:13:32,  1.96s/it]                                                   {'loss': 3.0231, 'grad_norm': 83.44849395751953, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:08<3:13:32,  1.96s/it]  1%|          | 65/6000 [02:10<3:13:48,  1.96s/it]                                                   {'loss': 3.4295, 'grad_norm': 233.2990264892578, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:10<3:13:48,  1.96s/it]  1%|          | 66/6000 [02:12<3:17:24,  2.00s/it]                                                   {'loss': 2.9547, 'grad_norm': 84.94905090332031, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:12<3:17:24,  2.00s/it]  1%|          | 67/6000 [02:14<3:16:44,  1.99s/it]                                                   {'loss': 3.119, 'grad_norm': 90.42733001708984, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:14<3:16:44,  1.99s/it]  1%|          | 68/6000 [02:16<3:15:07,  1.97s/it]                                                   {'loss': 3.0568, 'grad_norm': 70.85105895996094, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:16<3:15:07,  1.97s/it]  1%|          | 69/6000 [02:18<3:13:54,  1.96s/it]                                                   {'loss': 3.009, 'grad_norm': 70.15444946289062, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:18<3:13:54,  1.96s/it]  1%|          | 70/6000 [02:20<3:13:57,  1.96s/it]                                                   {'loss': 2.9516, 'grad_norm': 55.66908645629883, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:20<3:13:57,  1.96s/it]  1%|          | 71/6000 [02:22<3:12:34,  1.95s/it]                                                   {'loss': 3.0586, 'grad_norm': 84.92613220214844, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:22<3:12:34,  1.95s/it]  1%|          | 72/6000 [02:24<3:15:47,  1.98s/it]                                                   {'loss': 2.9578, 'grad_norm': 67.9245834350586, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:24<3:15:47,  1.98s/it]  1%|          | 73/6000 [02:26<3:15:20,  1.98s/it]                                                   {'loss': 2.9114, 'grad_norm': 67.11974334716797, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:26<3:15:20,  1.98s/it]  1%|          | 74/6000 [02:27<3:12:10,  1.95s/it]                                                   {'loss': 2.9254, 'grad_norm': 51.726173400878906, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:27<3:12:10,  1.95s/it]  1%|â–         | 75/6000 [02:30<3:18:52,  2.01s/it]                                                   {'loss': 2.9016, 'grad_norm': 56.70935821533203, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:30<3:18:52,  2.01s/it]  1%|â–         | 76/6000 [02:32<3:24:14,  2.07s/it]                                                   {'loss': 2.9005, 'grad_norm': 58.5963249206543, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:32<3:24:14,  2.07s/it]  1%|â–         | 77/6000 [02:34<3:21:54,  2.05s/it]                                                   {'loss': 3.1151, 'grad_norm': 101.409423828125, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:34<3:21:54,  2.05s/it]  1%|â–         | 78/6000 [02:36<3:21:26,  2.04s/it]                                                   {'loss': 2.8855, 'grad_norm': 64.4407958984375, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:36<3:21:26,  2.04s/it]  1%|â–         | 79/6000 [02:38<3:20:05,  2.03s/it]                                                   {'loss': 2.8419, 'grad_norm': 62.743717193603516, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:38<3:20:05,  2.03s/it]  1%|â–         | 80/6000 [02:40<3:19:21,  2.02s/it]                                                   {'loss': 3.0206, 'grad_norm': 79.96061706542969, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:40<3:19:21,  2.02s/it]  1%|â–         | 81/6000 [02:42<3:17:19,  2.00s/it]                                                   {'loss': 2.7638, 'grad_norm': 51.692901611328125, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:42<3:17:19,  2.00s/it]  1%|â–         | 82/6000 [02:44<3:16:10,  1.99s/it]                                                   {'loss': 2.834, 'grad_norm': 60.10659408569336, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:44<3:16:10,  1.99s/it]  1%|â–         | 83/6000 [02:46<3:16:33,  1.99s/it]                                                   {'loss': 2.8901, 'grad_norm': 82.8072280883789, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:46<3:16:33,  1.99s/it]  1%|â–         | 84/6000 [02:48<3:15:49,  1.99s/it]                                                   {'loss': 2.9927, 'grad_norm': 67.03126525878906, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:48<3:15:49,  1.99s/it]  1%|â–         | 85/6000 [02:50<3:19:29,  2.02s/it]                                                   {'loss': 2.7716, 'grad_norm': 81.02417755126953, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:50<3:19:29,  2.02s/it]  1%|â–         | 86/6000 [02:52<3:17:56,  2.01s/it]                                                   {'loss': 2.8529, 'grad_norm': 56.66597366333008, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:52<3:17:56,  2.01s/it]  1%|â–         | 87/6000 [02:54<3:14:48,  1.98s/it]                                                   {'loss': 2.7918, 'grad_norm': 71.18738555908203, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:54<3:14:48,  1.98s/it]  1%|â–         | 88/6000 [02:56<3:13:57,  1.97s/it]                                                   {'loss': 3.266, 'grad_norm': 80.7896499633789, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:56<3:13:57,  1.97s/it]  1%|â–         | 89/6000 [02:58<3:13:26,  1.96s/it]                                                   {'loss': 2.7072, 'grad_norm': 81.97610473632812, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:58<3:13:26,  1.96s/it]  2%|â–         | 90/6000 [03:00<3:16:37,  2.00s/it]                                                   {'loss': 2.789, 'grad_norm': 53.71354675292969, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [03:00<3:16:37,  2.00s/it]  2%|â–         | 91/6000 [03:02<3:17:06,  2.00s/it]                                                   {'loss': 2.6901, 'grad_norm': 65.63382720947266, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:02<3:17:06,  2.00s/it]  2%|â–         | 92/6000 [03:04<3:16:57,  2.00s/it]                                                   {'loss': 2.5226, 'grad_norm': 76.48966979980469, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:04<3:16:57,  2.00s/it]  2%|â–         | 93/6000 [03:06<3:19:01,  2.02s/it]                                                   {'loss': 2.7245, 'grad_norm': 71.71550750732422, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:06<3:19:01,  2.02s/it]  2%|â–         | 94/6000 [03:08<3:17:38,  2.01s/it]                                                   {'loss': 2.7616, 'grad_norm': 86.79249572753906, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:08<3:17:38,  2.01s/it]  2%|â–         | 95/6000 [03:10<3:16:43,  2.00s/it]                                                   {'loss': 2.684, 'grad_norm': 62.905155181884766, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:10<3:16:43,  2.00s/it]  2%|â–         | 96/6000 [03:12<3:17:49,  2.01s/it]                                                   {'loss': 2.6556, 'grad_norm': 63.904842376708984, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:12<3:17:49,  2.01s/it]  2%|â–         | 97/6000 [03:14<3:16:07,  1.99s/it]                                                   {'loss': 2.6862, 'grad_norm': 73.98912048339844, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:14<3:16:07,  1.99s/it]  2%|â–         | 98/6000 [03:16<3:15:16,  1.99s/it]                                                   {'loss': 2.4946, 'grad_norm': 60.03609848022461, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:16<3:15:16,  1.99s/it]  2%|â–         | 99/6000 [03:18<3:13:40,  1.97s/it]                                                   {'loss': 2.4639, 'grad_norm': 68.30512237548828, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:18<3:13:40,  1.97s/it]  2%|â–         | 100/6000 [03:20<3:11:57,  1.95s/it]                                                    {'loss': 2.3283, 'grad_norm': 48.46339416503906, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:20<3:11:57,  1.95s/it]  2%|â–         | 101/6000 [03:22<3:26:18,  2.10s/it]                                                    {'loss': 2.3148, 'grad_norm': 80.87860870361328, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:22<3:26:18,  2.10s/it]  2%|â–         | 102/6000 [03:24<3:20:52,  2.04s/it]                                                    {'loss': 2.4491, 'grad_norm': 64.2331314086914, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:24<3:20:52,  2.04s/it]  2%|â–         | 103/6000 [03:26<3:18:51,  2.02s/it]                                                    {'loss': 2.2486, 'grad_norm': 66.42546081542969, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:26<3:18:51,  2.02s/it]  2%|â–         | 104/6000 [03:28<3:19:41,  2.03s/it]                                                    {'loss': 2.1296, 'grad_norm': 57.984161376953125, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:28<3:19:41,  2.03s/it]  2%|â–         | 105/6000 [03:30<3:17:32,  2.01s/it]                                                    {'loss': 2.428, 'grad_norm': 62.60950469970703, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:30<3:17:32,  2.01s/it]  2%|â–         | 106/6000 [03:32<3:19:09,  2.03s/it]                                                    {'loss': 1.99, 'grad_norm': 56.232261657714844, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:32<3:19:09,  2.03s/it]  2%|â–         | 107/6000 [03:34<3:16:30,  2.00s/it]                                                    {'loss': 1.7263, 'grad_norm': 72.19951629638672, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:34<3:16:30,  2.00s/it]  2%|â–         | 108/6000 [03:36<3:17:31,  2.01s/it]                                                    {'loss': 1.5626, 'grad_norm': 42.07286071777344, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:36<3:17:31,  2.01s/it]  2%|â–         | 109/6000 [03:38<3:19:43,  2.03s/it]                                                    {'loss': 1.529, 'grad_norm': 50.352294921875, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:38<3:19:43,  2.03s/it]  2%|â–         | 110/6000 [03:40<3:18:42,  2.02s/it]                                                    {'loss': 1.2514, 'grad_norm': 52.068443298339844, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:40<3:18:42,  2.02s/it]  2%|â–         | 111/6000 [03:42<3:20:57,  2.05s/it]                                                    {'loss': 1.93, 'grad_norm': 45.426300048828125, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:42<3:20:57,  2.05s/it]  2%|â–         | 112/6000 [03:44<3:22:18,  2.06s/it]                                                    {'loss': 1.5527, 'grad_norm': 89.3258285522461, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:44<3:22:18,  2.06s/it]  2%|â–         | 113/6000 [03:46<3:20:27,  2.04s/it]                                                    {'loss': 1.4899, 'grad_norm': 42.1580696105957, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:46<3:20:27,  2.04s/it]  2%|â–         | 114/6000 [03:48<3:19:20,  2.03s/it]                                                    {'loss': 1.4462, 'grad_norm': 64.35420227050781, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:48<3:19:20,  2.03s/it]  2%|â–         | 115/6000 [03:50<3:16:59,  2.01s/it]                                                    {'loss': 1.7905, 'grad_norm': 81.30699920654297, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:50<3:16:59,  2.01s/it]  2%|â–         | 116/6000 [03:52<3:14:50,  1.99s/it]                                                    {'loss': 1.5351, 'grad_norm': 86.22052001953125, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:52<3:14:50,  1.99s/it]  2%|â–         | 117/6000 [03:54<3:15:10,  1.99s/it]                                                    {'loss': 0.7839, 'grad_norm': 66.3042984008789, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:54<3:15:10,  1.99s/it]  2%|â–         | 118/6000 [03:56<3:20:34,  2.05s/it]                                                    {'loss': 1.0272, 'grad_norm': 76.65400695800781, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:56<3:20:34,  2.05s/it]  2%|â–         | 119/6000 [03:58<3:16:11,  2.00s/it]                                                    {'loss': 0.9192, 'grad_norm': 81.40592956542969, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:58<3:16:11,  2.00s/it]  2%|â–         | 120/6000 [04:00<3:15:17,  1.99s/it]                                                    {'loss': 0.8311, 'grad_norm': 132.17568969726562, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [04:00<3:15:17,  1.99s/it]  2%|â–         | 121/6000 [04:02<3:17:24,  2.01s/it]                                                    {'loss': 0.6175, 'grad_norm': 114.73413848876953, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [04:02<3:17:24,  2.01s/it]  2%|â–         | 122/6000 [04:04<3:18:41,  2.03s/it]                                                    {'loss': 0.5113, 'grad_norm': 74.68498229980469, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:04<3:18:41,  2.03s/it]  2%|â–         | 123/6000 [04:06<3:16:54,  2.01s/it]                                                    {'loss': 0.3205, 'grad_norm': 47.64706039428711, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:06<3:16:54,  2.01s/it]  2%|â–         | 124/6000 [04:08<3:15:29,  2.00s/it]                                                    {'loss': 0.717, 'grad_norm': 77.7912368774414, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:08<3:15:29,  2.00s/it]  2%|â–         | 125/6000 [04:10<3:19:28,  2.04s/it]                                                    {'loss': 0.4372, 'grad_norm': 66.55471801757812, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:10<3:19:28,  2.04s/it]  2%|â–         | 126/6000 [04:12<3:16:39,  2.01s/it]                                                    {'loss': 0.5489, 'grad_norm': 49.00642776489258, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:12<3:16:39,  2.01s/it]  2%|â–         | 127/6000 [04:14<3:19:09,  2.03s/it]                                                    {'loss': 0.2627, 'grad_norm': 21.845294952392578, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:14<3:19:09,  2.03s/it]  2%|â–         | 128/6000 [04:16<3:17:37,  2.02s/it]                                                    {'loss': 0.2518, 'grad_norm': 37.38977813720703, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:16<3:17:37,  2.02s/it]  2%|â–         | 129/6000 [04:18<3:15:51,  2.00s/it]                                                    {'loss': 0.722, 'grad_norm': 37.36642837524414, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:18<3:15:51,  2.00s/it]  2%|â–         | 130/6000 [04:20<3:14:25,  1.99s/it]                                                    {'loss': 0.2258, 'grad_norm': 29.3568115234375, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:20<3:14:25,  1.99s/it]  2%|â–         | 131/6000 [04:22<3:14:26,  1.99s/it]                                                    {'loss': 0.3053, 'grad_norm': 32.96197509765625, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:22<3:14:26,  1.99s/it]  2%|â–         | 132/6000 [04:24<3:14:10,  1.99s/it]                                                    {'loss': 0.2153, 'grad_norm': 25.377281188964844, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:24<3:14:10,  1.99s/it]  2%|â–         | 133/6000 [04:26<3:14:30,  1.99s/it]                                                    {'loss': 0.3307, 'grad_norm': 31.286611557006836, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:26<3:14:30,  1.99s/it]  2%|â–         | 134/6000 [04:28<3:16:04,  2.01s/it]                                                    {'loss': 0.3239, 'grad_norm': 53.09873580932617, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:28<3:16:04,  2.01s/it]  2%|â–         | 135/6000 [04:30<3:14:36,  1.99s/it]                                                    {'loss': 0.4293, 'grad_norm': 44.021453857421875, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:30<3:14:36,  1.99s/it]  2%|â–         | 136/6000 [04:32<3:12:04,  1.97s/it]                                                    {'loss': 0.1504, 'grad_norm': 27.816822052001953, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:32<3:12:04,  1.97s/it]  2%|â–         | 137/6000 [04:34<3:15:26,  2.00s/it]                                                    {'loss': 0.1954, 'grad_norm': 27.119592666625977, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:34<3:15:26,  2.00s/it]  2%|â–         | 138/6000 [04:36<3:15:59,  2.01s/it]                                                    {'loss': 0.3023, 'grad_norm': 32.090675354003906, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:36<3:15:59,  2.01s/it]  2%|â–         | 139/6000 [04:38<3:15:36,  2.00s/it]                                                    {'loss': 0.4266, 'grad_norm': 37.843040466308594, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:38<3:15:36,  2.00s/it]  2%|â–         | 140/6000 [04:40<3:17:43,  2.02s/it]                                                    {'loss': 0.2435, 'grad_norm': 40.164859771728516, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:40<3:17:43,  2.02s/it]  2%|â–         | 141/6000 [04:42<3:16:53,  2.02s/it]                                                    {'loss': 0.2673, 'grad_norm': 25.654613494873047, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:42<3:16:53,  2.02s/it]  2%|â–         | 142/6000 [04:44<3:14:02,  1.99s/it]                                                    {'loss': 0.166, 'grad_norm': 14.886698722839355, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:44<3:14:02,  1.99s/it]  2%|â–         | 143/6000 [04:46<3:12:25,  1.97s/it]                                                    {'loss': 0.1205, 'grad_norm': 16.466176986694336, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:46<3:12:25,  1.97s/it]  2%|â–         | 144/6000 [04:48<3:11:33,  1.96s/it]                                                    {'loss': 0.0977, 'grad_norm': 20.526302337646484, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:48<3:11:33,  1.96s/it]  2%|â–         | 145/6000 [04:50<3:09:34,  1.94s/it]                                                    {'loss': 0.0975, 'grad_norm': 29.479597091674805, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:50<3:09:34,  1.94s/it]  2%|â–         | 146/6000 [04:52<3:10:27,  1.95s/it]                                                    {'loss': 0.2646, 'grad_norm': 37.301448822021484, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:52<3:10:27,  1.95s/it]  2%|â–         | 147/6000 [04:54<3:09:41,  1.94s/it]                                                    {'loss': 0.2716, 'grad_norm': 28.694244384765625, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:54<3:09:41,  1.94s/it]  2%|â–         | 148/6000 [04:56<3:12:16,  1.97s/it]                                                    {'loss': 0.184, 'grad_norm': 30.192346572875977, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:56<3:12:16,  1.97s/it]  2%|â–         | 149/6000 [04:58<3:10:13,  1.95s/it]                                                    {'loss': 0.1676, 'grad_norm': 28.797807693481445, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:58<3:10:13,  1.95s/it]  2%|â–Ž         | 150/6000 [05:00<3:10:38,  1.96s/it]                                                    {'loss': 0.0481, 'grad_norm': 11.811261177062988, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [05:00<3:10:38,  1.96s/it]  3%|â–Ž         | 151/6000 [05:02<3:10:13,  1.95s/it]                                                    {'loss': 0.111, 'grad_norm': 10.833089828491211, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [05:02<3:10:13,  1.95s/it]  3%|â–Ž         | 152/6000 [05:04<3:12:48,  1.98s/it]                                                    {'loss': 0.0995, 'grad_norm': 15.918586730957031, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:04<3:12:48,  1.98s/it]  3%|â–Ž         | 153/6000 [05:06<3:10:28,  1.95s/it]                                                    {'loss': 0.1207, 'grad_norm': 17.454038619995117, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:06<3:10:28,  1.95s/it]  3%|â–Ž         | 154/6000 [05:08<3:09:37,  1.95s/it]                                                    {'loss': 0.214, 'grad_norm': 35.70630645751953, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:08<3:09:37,  1.95s/it]  3%|â–Ž         | 155/6000 [05:10<3:10:17,  1.95s/it]                                                    {'loss': 0.1415, 'grad_norm': 15.62486457824707, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:10<3:10:17,  1.95s/it]  3%|â–Ž         | 156/6000 [05:12<3:10:44,  1.96s/it]                                                    {'loss': 0.0672, 'grad_norm': 23.128694534301758, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:12<3:10:44,  1.96s/it]  3%|â–Ž         | 157/6000 [05:14<3:20:25,  2.06s/it]                                                    {'loss': 0.1328, 'grad_norm': 28.56801986694336, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:14<3:20:25,  2.06s/it]  3%|â–Ž         | 158/6000 [05:16<3:16:46,  2.02s/it]                                                    {'loss': 0.0801, 'grad_norm': 18.748205184936523, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:16<3:16:46,  2.02s/it]  3%|â–Ž         | 159/6000 [05:18<3:14:57,  2.00s/it]                                                    {'loss': 0.1532, 'grad_norm': 29.600967407226562, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:18<3:14:57,  2.00s/it]  3%|â–Ž         | 160/6000 [05:20<3:21:12,  2.07s/it]                                                    {'loss': 0.1036, 'grad_norm': 9.505682945251465, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:20<3:21:12,  2.07s/it]  3%|â–Ž         | 161/6000 [05:22<3:18:46,  2.04s/it]                                                    {'loss': 0.1809, 'grad_norm': 28.212947845458984, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:22<3:18:46,  2.04s/it]  3%|â–Ž         | 162/6000 [05:24<3:16:24,  2.02s/it]                                                    {'loss': 0.106, 'grad_norm': 22.312206268310547, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:24<3:16:24,  2.02s/it]  3%|â–Ž         | 163/6000 [05:26<3:17:58,  2.04s/it]                                                    {'loss': 0.1188, 'grad_norm': 25.021743774414062, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:26<3:17:58,  2.04s/it]  3%|â–Ž         | 164/6000 [05:28<3:15:56,  2.01s/it]                                                    {'loss': 0.3761, 'grad_norm': 43.07289123535156, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:28<3:15:56,  2.01s/it]  3%|â–Ž         | 165/6000 [05:30<3:15:54,  2.01s/it]                                                    {'loss': 0.0842, 'grad_norm': 20.41836929321289, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:30<3:15:54,  2.01s/it]  3%|â–Ž         | 166/6000 [05:32<3:14:02,  2.00s/it]                                                    {'loss': 0.1263, 'grad_norm': 19.219575881958008, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:32<3:14:02,  2.00s/it]  3%|â–Ž         | 167/6000 [05:34<3:12:11,  1.98s/it]                                                    {'loss': 0.1832, 'grad_norm': 48.06219482421875, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:34<3:12:11,  1.98s/it]  3%|â–Ž         | 168/6000 [05:36<3:11:48,  1.97s/it]                                                    {'loss': 0.257, 'grad_norm': 571.413330078125, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:36<3:11:48,  1.97s/it]  3%|â–Ž         | 169/6000 [05:38<3:16:16,  2.02s/it]                                                    {'loss': 0.3311, 'grad_norm': 30.585851669311523, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:38<3:16:16,  2.02s/it]  3%|â–Ž         | 170/6000 [05:40<3:13:50,  1.99s/it]                                                    {'loss': 0.1868, 'grad_norm': 24.257341384887695, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:40<3:13:50,  1.99s/it]  3%|â–Ž         | 171/6000 [05:42<3:11:07,  1.97s/it]                                                    {'loss': 0.1038, 'grad_norm': 13.978549003601074, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:42<3:11:07,  1.97s/it]  3%|â–Ž         | 172/6000 [05:44<3:12:14,  1.98s/it]                                                    {'loss': 0.3517, 'grad_norm': 53.80499267578125, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:44<3:12:14,  1.98s/it]  3%|â–Ž         | 173/6000 [05:46<3:13:32,  1.99s/it]                                                    {'loss': 0.091, 'grad_norm': 16.675193786621094, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:46<3:13:32,  1.99s/it]  3%|â–Ž         | 174/6000 [05:48<3:19:09,  2.05s/it]                                                    {'loss': 0.0574, 'grad_norm': 10.096842765808105, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:48<3:19:09,  2.05s/it]  3%|â–Ž         | 175/6000 [05:50<3:17:39,  2.04s/it]                                                    {'loss': 0.2837, 'grad_norm': 37.351173400878906, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:50<3:17:39,  2.04s/it]  3%|â–Ž         | 176/6000 [05:52<3:17:00,  2.03s/it]                                                    {'loss': 0.112, 'grad_norm': 25.111915588378906, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:52<3:17:00,  2.03s/it]  3%|â–Ž         | 177/6000 [05:54<3:16:09,  2.02s/it]                                                    {'loss': 0.1413, 'grad_norm': 19.065404891967773, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:54<3:16:09,  2.02s/it]  3%|â–Ž         | 178/6000 [05:56<3:14:28,  2.00s/it]                                                    {'loss': 0.1103, 'grad_norm': 21.065494537353516, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:56<3:14:28,  2.00s/it]  3%|â–Ž         | 179/6000 [05:58<3:12:53,  1.99s/it]                                                    {'loss': 0.1112, 'grad_norm': 17.22034454345703, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [05:58<3:12:53,  1.99s/it]  3%|â–Ž         | 180/6000 [06:00<3:12:21,  1.98s/it]                                                    {'loss': 0.3299, 'grad_norm': 32.819374084472656, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [06:00<3:12:21,  1.98s/it]  3%|â–Ž         | 181/6000 [06:02<3:12:19,  1.98s/it]                                                    {'loss': 0.3552, 'grad_norm': 28.25666046142578, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [06:02<3:12:19,  1.98s/it]  3%|â–Ž         | 182/6000 [06:04<3:10:33,  1.97s/it]                                                    {'loss': 0.1873, 'grad_norm': 24.390193939208984, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [06:04<3:10:33,  1.97s/it]  3%|â–Ž         | 183/6000 [06:06<3:10:20,  1.96s/it]                                                    {'loss': 0.1414, 'grad_norm': 13.009039878845215, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:06<3:10:20,  1.96s/it]  3%|â–Ž         | 184/6000 [06:08<3:12:17,  1.98s/it]                                                    {'loss': 0.1598, 'grad_norm': 19.80301284790039, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:08<3:12:17,  1.98s/it]  3%|â–Ž         | 185/6000 [06:10<3:10:44,  1.97s/it]                                                    {'loss': 0.1021, 'grad_norm': 22.848562240600586, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:10<3:10:44,  1.97s/it]  3%|â–Ž         | 186/6000 [06:12<3:10:31,  1.97s/it]                                                    {'loss': 0.1717, 'grad_norm': 24.153221130371094, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:12<3:10:31,  1.97s/it]  3%|â–Ž         | 187/6000 [06:14<3:11:35,  1.98s/it]                                                    {'loss': 0.1033, 'grad_norm': 18.227182388305664, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:14<3:11:35,  1.98s/it]  3%|â–Ž         | 188/6000 [06:16<3:11:14,  1.97s/it]                                                    {'loss': 0.1462, 'grad_norm': 22.130889892578125, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:16<3:11:14,  1.97s/it]  3%|â–Ž         | 189/6000 [06:18<3:10:36,  1.97s/it]                                                    {'loss': 0.2288, 'grad_norm': 26.05251693725586, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:18<3:10:36,  1.97s/it]  3%|â–Ž         | 190/6000 [06:19<3:08:13,  1.94s/it]                                                    {'loss': 0.0757, 'grad_norm': 15.28443431854248, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:20<3:08:13,  1.94s/it]  3%|â–Ž         | 191/6000 [06:21<3:08:39,  1.95s/it]                                                    {'loss': 0.0943, 'grad_norm': 18.42407989501953, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:21<3:08:39,  1.95s/it]  3%|â–Ž         | 192/6000 [06:24<3:16:16,  2.03s/it]                                                    {'loss': 0.1143, 'grad_norm': 16.904815673828125, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:24<3:16:16,  2.03s/it]  3%|â–Ž         | 193/6000 [06:26<3:16:50,  2.03s/it]                                                    {'loss': 0.267, 'grad_norm': 21.10498046875, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:26<3:16:50,  2.03s/it]  3%|â–Ž         | 194/6000 [06:28<3:13:18,  2.00s/it]                                                    {'loss': 0.1453, 'grad_norm': 19.988576889038086, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:28<3:13:18,  2.00s/it]  3%|â–Ž         | 195/6000 [06:30<3:11:53,  1.98s/it]                                                    {'loss': 0.0791, 'grad_norm': 76.9124984741211, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:30<3:11:53,  1.98s/it]  3%|â–Ž         | 196/6000 [06:32<3:12:40,  1.99s/it]                                                    {'loss': 0.0495, 'grad_norm': 10.21187973022461, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:32<3:12:40,  1.99s/it]  3%|â–Ž         | 197/6000 [06:34<3:16:29,  2.03s/it]                                                    {'loss': 0.0424, 'grad_norm': 6.174013614654541, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:34<3:16:29,  2.03s/it]  3%|â–Ž         | 198/6000 [06:36<3:16:02,  2.03s/it]                                                    {'loss': 0.0726, 'grad_norm': 14.801985740661621, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:36<3:16:02,  2.03s/it]  3%|â–Ž         | 199/6000 [06:38<3:13:33,  2.00s/it]                                                    {'loss': 0.0823, 'grad_norm': 12.27495002746582, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:38<3:13:33,  2.00s/it]  3%|â–Ž         | 200/6000 [06:40<3:15:59,  2.03s/it]                                                    {'loss': 0.216, 'grad_norm': 36.8178825378418, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:40<3:15:59,  2.03s/it]  3%|â–Ž         | 201/6000 [06:42<3:16:42,  2.04s/it]                                                    {'loss': 0.3567, 'grad_norm': 26.48839569091797, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:42<3:16:42,  2.04s/it]  3%|â–Ž         | 202/6000 [06:44<3:13:45,  2.01s/it]                                                    {'loss': 0.1608, 'grad_norm': 23.588653564453125, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:44<3:13:45,  2.01s/it]  3%|â–Ž         | 203/6000 [06:46<3:12:38,  1.99s/it]                                                    {'loss': 0.1166, 'grad_norm': 15.739401817321777, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:46<3:12:38,  1.99s/it]  3%|â–Ž         | 204/6000 [06:48<3:12:07,  1.99s/it]                                                    {'loss': 0.0409, 'grad_norm': 6.116237163543701, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:48<3:12:07,  1.99s/it]  3%|â–Ž         | 205/6000 [06:50<3:09:55,  1.97s/it]                                                    {'loss': 0.1211, 'grad_norm': 18.27950096130371, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:50<3:09:55,  1.97s/it]  3%|â–Ž         | 206/6000 [06:52<3:09:16,  1.96s/it]                                                    {'loss': 0.0255, 'grad_norm': 7.469913959503174, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:52<3:09:16,  1.96s/it]  3%|â–Ž         | 207/6000 [06:54<3:10:27,  1.97s/it]                                                    {'loss': 0.0581, 'grad_norm': 12.292320251464844, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:54<3:10:27,  1.97s/it]  3%|â–Ž         | 208/6000 [06:55<3:08:55,  1.96s/it]                                                    {'loss': 0.1386, 'grad_norm': 21.74284553527832, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [06:55<3:08:55,  1.96s/it]  3%|â–Ž         | 209/6000 [06:57<3:08:08,  1.95s/it]                                                    {'loss': 0.0548, 'grad_norm': 16.8212947845459, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [06:57<3:08:08,  1.95s/it]  4%|â–Ž         | 210/6000 [06:59<3:10:23,  1.97s/it]                                                    {'loss': 0.0729, 'grad_norm': 17.499969482421875, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [06:59<3:10:23,  1.97s/it]  4%|â–Ž         | 211/6000 [07:02<3:15:09,  2.02s/it]                                                    {'loss': 0.1542, 'grad_norm': 28.28451919555664, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [07:02<3:15:09,  2.02s/it]  4%|â–Ž         | 212/6000 [07:04<3:16:36,  2.04s/it]                                                    {'loss': 0.2301, 'grad_norm': 24.23353385925293, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [07:04<3:16:36,  2.04s/it]  4%|â–Ž         | 213/6000 [07:06<3:16:21,  2.04s/it]                                                    {'loss': 0.1793, 'grad_norm': 23.715984344482422, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:06<3:16:21,  2.04s/it]  4%|â–Ž         | 214/6000 [07:08<3:13:34,  2.01s/it]                                                    {'loss': 0.0573, 'grad_norm': 12.016146659851074, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:08<3:13:34,  2.01s/it]  4%|â–Ž         | 215/6000 [07:10<3:11:12,  1.98s/it]                                                    {'loss': 0.1112, 'grad_norm': 23.617177963256836, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:10<3:11:12,  1.98s/it]  4%|â–Ž         | 216/6000 [07:12<3:11:17,  1.98s/it]                                                    {'loss': 0.0161, 'grad_norm': 5.713260173797607, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:12<3:11:17,  1.98s/it]  4%|â–Ž         | 217/6000 [07:14<3:12:07,  1.99s/it]                                                    {'loss': 0.0416, 'grad_norm': 10.677411079406738, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:14<3:12:07,  1.99s/it]  4%|â–Ž         | 218/6000 [07:16<3:11:23,  1.99s/it]                                                    {'loss': 0.0824, 'grad_norm': 13.903515815734863, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:16<3:11:23,  1.99s/it]  4%|â–Ž         | 219/6000 [07:17<3:09:39,  1.97s/it]                                                    {'loss': 0.1623, 'grad_norm': 22.233821868896484, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:17<3:09:39,  1.97s/it]  4%|â–Ž         | 220/6000 [07:19<3:10:03,  1.97s/it]                                                    {'loss': 0.399, 'grad_norm': 31.12919044494629, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:19<3:10:03,  1.97s/it]  4%|â–Ž         | 221/6000 [07:21<3:09:54,  1.97s/it]                                                    {'loss': 0.0813, 'grad_norm': 14.791678428649902, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:21<3:09:54,  1.97s/it]  4%|â–Ž         | 222/6000 [07:23<3:07:32,  1.95s/it]                                                    {'loss': 0.2514, 'grad_norm': 56.059322357177734, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:23<3:07:32,  1.95s/it]  4%|â–Ž         | 223/6000 [07:25<3:07:59,  1.95s/it]                                                    {'loss': 0.1251, 'grad_norm': 21.330005645751953, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:25<3:07:59,  1.95s/it]  4%|â–Ž         | 224/6000 [07:27<3:07:13,  1.94s/it]                                                    {'loss': 0.1101, 'grad_norm': 23.063976287841797, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:27<3:07:13,  1.94s/it]  4%|â–         | 225/6000 [07:29<3:09:02,  1.96s/it]                                                    {'loss': 0.1693, 'grad_norm': 26.342391967773438, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:29<3:09:02,  1.96s/it]  4%|â–         | 226/6000 [07:31<3:13:01,  2.01s/it]                                                    {'loss': 0.146, 'grad_norm': 25.275793075561523, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:31<3:13:01,  2.01s/it]  4%|â–         | 227/6000 [07:33<3:11:44,  1.99s/it]                                                    {'loss': 0.2066, 'grad_norm': 30.658899307250977, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:33<3:11:44,  1.99s/it]  4%|â–         | 228/6000 [07:35<3:10:20,  1.98s/it]                                                    {'loss': 0.0665, 'grad_norm': 12.153347969055176, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:35<3:10:20,  1.98s/it]  4%|â–         | 229/6000 [07:37<3:08:36,  1.96s/it]                                                    {'loss': 0.0595, 'grad_norm': 14.442450523376465, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:37<3:08:36,  1.96s/it]  4%|â–         | 230/6000 [07:39<3:08:53,  1.96s/it]                                                    {'loss': 0.051, 'grad_norm': 13.300312995910645, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:39<3:08:53,  1.96s/it]  4%|â–         | 231/6000 [07:41<3:08:11,  1.96s/it]                                                    {'loss': 0.06, 'grad_norm': 13.47685718536377, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:41<3:08:11,  1.96s/it]  4%|â–         | 232/6000 [07:43<3:09:35,  1.97s/it]                                                    {'loss': 0.161, 'grad_norm': 31.281892776489258, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:43<3:09:35,  1.97s/it]  4%|â–         | 233/6000 [07:45<3:12:24,  2.00s/it]                                                    {'loss': 0.0482, 'grad_norm': 11.968429565429688, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:45<3:12:24,  2.00s/it]  4%|â–         | 234/6000 [07:47<3:12:38,  2.00s/it]                                                    {'loss': 0.0499, 'grad_norm': 13.437089920043945, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:47<3:12:38,  2.00s/it]  4%|â–         | 235/6000 [07:49<3:10:02,  1.98s/it]                                                    {'loss': 0.1062, 'grad_norm': 15.898693084716797, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:49<3:10:02,  1.98s/it]  4%|â–         | 236/6000 [07:51<3:10:41,  1.99s/it]                                                    {'loss': 0.0826, 'grad_norm': 8.32932186126709, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:51<3:10:41,  1.99s/it]  4%|â–         | 237/6000 [07:53<3:11:00,  1.99s/it]                                                    {'loss': 0.3266, 'grad_norm': 25.149662017822266, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:53<3:11:00,  1.99s/it]  4%|â–         | 238/6000 [07:55<3:13:28,  2.01s/it]                                                    {'loss': 0.3605, 'grad_norm': 24.77247428894043, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [07:55<3:13:28,  2.01s/it]  4%|â–         | 239/6000 [07:57<3:17:37,  2.06s/it]                                                    {'loss': 0.0719, 'grad_norm': 17.29431915283203, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [07:57<3:17:37,  2.06s/it]  4%|â–         | 240/6000 [07:59<3:13:23,  2.01s/it]                                                    {'loss': 0.1697, 'grad_norm': 19.680908203125, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [07:59<3:13:23,  2.01s/it]  4%|â–         | 241/6000 [08:01<3:19:09,  2.07s/it]                                                    {'loss': 0.1052, 'grad_norm': 18.18049430847168, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [08:01<3:19:09,  2.07s/it]  4%|â–         | 242/6000 [08:03<3:15:01,  2.03s/it]                                                    {'loss': 0.1308, 'grad_norm': 24.09878921508789, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [08:03<3:15:01,  2.03s/it]  4%|â–         | 243/6000 [08:05<3:14:22,  2.03s/it]                                                    {'loss': 0.2071, 'grad_norm': 21.88162612915039, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [08:05<3:14:22,  2.03s/it]  4%|â–         | 244/6000 [08:07<3:12:07,  2.00s/it]                                                    {'loss': 0.0802, 'grad_norm': 18.554861068725586, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:07<3:12:07,  2.00s/it]  4%|â–         | 245/6000 [08:09<3:11:11,  1.99s/it]                                                    {'loss': 0.1024, 'grad_norm': 21.131752014160156, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:09<3:11:11,  1.99s/it]  4%|â–         | 246/6000 [08:11<3:11:29,  2.00s/it]                                                    {'loss': 0.0132, 'grad_norm': 5.382556915283203, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:11<3:11:29,  2.00s/it]  4%|â–         | 247/6000 [08:13<3:09:56,  1.98s/it]                                                    {'loss': 0.1338, 'grad_norm': 25.020919799804688, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:13<3:09:56,  1.98s/it]  4%|â–         | 248/6000 [08:15<3:11:23,  2.00s/it]                                                    {'loss': 0.0338, 'grad_norm': 9.110211372375488, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:15<3:11:23,  2.00s/it]  4%|â–         | 249/6000 [08:17<3:08:22,  1.97s/it]                                                    {'loss': 0.0317, 'grad_norm': 9.792384147644043, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:17<3:08:22,  1.97s/it]  4%|â–         | 250/6000 [08:19<3:07:32,  1.96s/it]                                                    {'loss': 0.1302, 'grad_norm': 27.319705963134766, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:19<3:07:32,  1.96s/it]  4%|â–         | 251/6000 [08:21<3:05:21,  1.93s/it]                                                    {'loss': 0.0689, 'grad_norm': 13.468103408813477, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:21<3:05:21,  1.93s/it]  4%|â–         | 252/6000 [08:23<3:09:51,  1.98s/it]                                                    {'loss': 0.0692, 'grad_norm': 15.696989059448242, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:23<3:09:51,  1.98s/it]  4%|â–         | 253/6000 [08:25<3:09:04,  1.97s/it]                                                    {'loss': 0.1249, 'grad_norm': 25.59190559387207, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:25<3:09:04,  1.97s/it]  4%|â–         | 254/6000 [08:27<3:09:53,  1.98s/it]                                                    {'loss': 0.201, 'grad_norm': 24.6842098236084, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:27<3:09:53,  1.98s/it]  4%|â–         | 255/6000 [08:29<3:10:49,  1.99s/it]                                                    {'loss': 0.0654, 'grad_norm': 20.633533477783203, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:29<3:10:49,  1.99s/it]  4%|â–         | 256/6000 [08:31<3:13:44,  2.02s/it]                                                    {'loss': 0.0877, 'grad_norm': 16.034374237060547, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:31<3:13:44,  2.02s/it]  4%|â–         | 257/6000 [08:33<3:11:40,  2.00s/it]                                                    {'loss': 0.0279, 'grad_norm': 7.136634826660156, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:33<3:11:40,  2.00s/it]  4%|â–         | 258/6000 [08:35<3:10:56,  2.00s/it]                                                    {'loss': 0.047, 'grad_norm': 9.20361614227295, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:35<3:10:56,  2.00s/it]  4%|â–         | 259/6000 [08:37<3:09:30,  1.98s/it]                                                    {'loss': 0.0193, 'grad_norm': 4.6122002601623535, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:37<3:09:30,  1.98s/it]  4%|â–         | 260/6000 [08:39<3:08:35,  1.97s/it]                                                    {'loss': 0.011, 'grad_norm': 3.7806026935577393, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:39<3:08:35,  1.97s/it]  4%|â–         | 261/6000 [08:41<3:10:40,  1.99s/it]                                                    {'loss': 0.0579, 'grad_norm': 9.9384183883667, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:41<3:10:40,  1.99s/it]  4%|â–         | 262/6000 [08:43<3:09:58,  1.99s/it]                                                    {'loss': 0.0064, 'grad_norm': 1.4696294069290161, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:43<3:09:58,  1.99s/it]  4%|â–         | 263/6000 [08:45<3:12:44,  2.02s/it]                                                    {'loss': 0.1196, 'grad_norm': 20.65981101989746, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:45<3:12:44,  2.02s/it]  4%|â–         | 264/6000 [08:47<3:09:28,  1.98s/it]                                                    {'loss': 0.1487, 'grad_norm': 29.49296760559082, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:47<3:09:28,  1.98s/it]  4%|â–         | 265/6000 [08:49<3:11:05,  2.00s/it]                                                    {'loss': 0.0237, 'grad_norm': 4.953782081604004, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:49<3:11:05,  2.00s/it]  4%|â–         | 266/6000 [08:51<3:09:59,  1.99s/it]                                                    {'loss': 0.033, 'grad_norm': 5.630330562591553, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:51<3:09:59,  1.99s/it]  4%|â–         | 267/6000 [08:53<3:07:55,  1.97s/it]                                                    {'loss': 0.0583, 'grad_norm': 11.231648445129395, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [08:53<3:07:55,  1.97s/it]  4%|â–         | 268/6000 [08:55<3:07:12,  1.96s/it]                                                    {'loss': 0.0485, 'grad_norm': 8.247151374816895, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [08:55<3:07:12,  1.96s/it]  4%|â–         | 269/6000 [08:57<3:08:03,  1.97s/it]                                                    {'loss': 0.0621, 'grad_norm': 14.086291313171387, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [08:57<3:08:03,  1.97s/it]  4%|â–         | 270/6000 [08:59<3:07:04,  1.96s/it]                                                    {'loss': 0.062, 'grad_norm': 18.723196029663086, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [08:59<3:07:04,  1.96s/it]  5%|â–         | 271/6000 [09:01<3:05:55,  1.95s/it]                                                    {'loss': 0.0344, 'grad_norm': 11.251616477966309, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [09:01<3:05:55,  1.95s/it]  5%|â–         | 272/6000 [09:03<3:04:42,  1.93s/it]                                                    {'loss': 0.2789, 'grad_norm': 27.330472946166992, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [09:03<3:04:42,  1.93s/it]  5%|â–         | 273/6000 [09:05<3:06:05,  1.95s/it]                                                    {'loss': 0.0535, 'grad_norm': 12.97628116607666, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [09:05<3:06:05,  1.95s/it]  5%|â–         | 274/6000 [09:07<3:08:50,  1.98s/it]                                                    {'loss': 0.1176, 'grad_norm': 18.56387710571289, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:07<3:08:50,  1.98s/it]  5%|â–         | 275/6000 [09:09<3:09:13,  1.98s/it]                                                    {'loss': 0.2279, 'grad_norm': 29.139209747314453, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:09<3:09:13,  1.98s/it]  5%|â–         | 276/6000 [09:11<3:09:11,  1.98s/it]                                                    {'loss': 0.1947, 'grad_norm': 27.77152442932129, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:11<3:09:11,  1.98s/it]  5%|â–         | 277/6000 [09:13<3:08:06,  1.97s/it]                                                    {'loss': 0.0923, 'grad_norm': 15.629319190979004, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:13<3:08:06,  1.97s/it]  5%|â–         | 278/6000 [09:14<3:08:14,  1.97s/it]                                                    {'loss': 0.0231, 'grad_norm': 5.316053867340088, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:15<3:08:14,  1.97s/it]  5%|â–         | 279/6000 [09:16<3:08:34,  1.98s/it]                                                    {'loss': 0.225, 'grad_norm': 16.78252601623535, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:16<3:08:34,  1.98s/it]  5%|â–         | 280/6000 [09:18<3:07:59,  1.97s/it]                                                    {'loss': 0.0446, 'grad_norm': 6.354341506958008, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:18<3:07:59,  1.97s/it]  5%|â–         | 281/6000 [09:20<3:08:50,  1.98s/it]                                                    {'loss': 0.3841, 'grad_norm': 27.751232147216797, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:20<3:08:50,  1.98s/it]  5%|â–         | 282/6000 [09:23<3:12:11,  2.02s/it]                                                    {'loss': 0.141, 'grad_norm': 19.982933044433594, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:23<3:12:11,  2.02s/it]  5%|â–         | 283/6000 [09:25<3:12:00,  2.02s/it]                                                    {'loss': 0.0238, 'grad_norm': 4.604051113128662, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:25<3:12:00,  2.02s/it]  5%|â–         | 284/6000 [09:27<3:17:19,  2.07s/it]                                                    {'loss': 0.0294, 'grad_norm': 7.597890377044678, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:27<3:17:19,  2.07s/it]  5%|â–         | 285/6000 [09:29<3:15:47,  2.06s/it]                                                    {'loss': 0.0915, 'grad_norm': 16.389190673828125, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:29<3:15:47,  2.06s/it]  5%|â–         | 286/6000 [09:31<3:14:15,  2.04s/it]                                                    {'loss': 0.127, 'grad_norm': 25.0975341796875, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:31<3:14:15,  2.04s/it]  5%|â–         | 287/6000 [09:33<3:11:59,  2.02s/it]                                                    {'loss': 0.1353, 'grad_norm': 19.05995750427246, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:33<3:11:59,  2.02s/it]  5%|â–         | 288/6000 [09:35<3:08:27,  1.98s/it]                                                    {'loss': 0.0086, 'grad_norm': 2.509229898452759, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:35<3:08:27,  1.98s/it]  5%|â–         | 289/6000 [09:37<3:10:27,  2.00s/it]                                                    {'loss': 0.1736, 'grad_norm': 21.044349670410156, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:37<3:10:27,  2.00s/it]  5%|â–         | 290/6000 [09:39<3:08:12,  1.98s/it]                                                    {'loss': 0.0513, 'grad_norm': 8.46724796295166, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:39<3:08:12,  1.98s/it]  5%|â–         | 291/6000 [09:41<3:07:05,  1.97s/it]                                                    {'loss': 0.1552, 'grad_norm': 24.46059226989746, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:41<3:07:05,  1.97s/it]  5%|â–         | 292/6000 [09:42<3:06:12,  1.96s/it]                                                    {'loss': 0.2576, 'grad_norm': 33.111629486083984, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:42<3:06:12,  1.96s/it]  5%|â–         | 293/6000 [09:44<3:05:19,  1.95s/it]                                                    {'loss': 0.1102, 'grad_norm': 27.272361755371094, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:44<3:05:19,  1.95s/it]  5%|â–         | 294/6000 [09:46<3:05:32,  1.95s/it]                                                    {'loss': 0.3429, 'grad_norm': 33.18849182128906, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:46<3:05:32,  1.95s/it]  5%|â–         | 295/6000 [09:48<3:04:15,  1.94s/it]                                                    {'loss': 0.087, 'grad_norm': 10.109823226928711, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:48<3:04:15,  1.94s/it]  5%|â–         | 296/6000 [09:50<3:08:12,  1.98s/it]                                                    {'loss': 0.0907, 'grad_norm': 11.721847534179688, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [09:50<3:08:12,  1.98s/it]  5%|â–         | 297/6000 [09:52<3:06:52,  1.97s/it]                                                    {'loss': 0.0802, 'grad_norm': 28.175678253173828, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [09:52<3:06:52,  1.97s/it]  5%|â–         | 298/6000 [09:54<3:08:09,  1.98s/it]                                                    {'loss': 0.0696, 'grad_norm': 9.644896507263184, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [09:54<3:08:09,  1.98s/it]  5%|â–         | 299/6000 [09:57<3:18:01,  2.08s/it]                                                    {'loss': 0.0845, 'grad_norm': 11.601191520690918, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [09:57<3:18:01,  2.08s/it]  5%|â–Œ         | 300/6000 [09:59<3:15:38,  2.06s/it]                                                    {'loss': 0.0017, 'grad_norm': 0.32215121388435364, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [09:59<3:15:38,  2.06s/it]  5%|â–Œ         | 301/6000 [10:01<3:14:04,  2.04s/it]                                                    {'loss': 0.0877, 'grad_norm': 26.141780853271484, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [10:01<3:14:04,  2.04s/it]  5%|â–Œ         | 302/6000 [10:03<3:11:29,  2.02s/it]                                                    {'loss': 0.039, 'grad_norm': 9.64193344116211, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [10:03<3:11:29,  2.02s/it]  5%|â–Œ         | 303/6000 [10:05<3:11:01,  2.01s/it]                                                    {'loss': 0.0372, 'grad_norm': 11.262923240661621, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [10:05<3:11:01,  2.01s/it]  5%|â–Œ         | 304/6000 [10:07<3:11:38,  2.02s/it]                                                    {'loss': 0.033, 'grad_norm': 8.163543701171875, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:07<3:11:38,  2.02s/it]  5%|â–Œ         | 305/6000 [10:09<3:09:24,  2.00s/it]                                                    {'loss': 0.137, 'grad_norm': 24.10330581665039, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:09<3:09:24,  2.00s/it]  5%|â–Œ         | 306/6000 [10:11<3:08:02,  1.98s/it]                                                    {'loss': 0.0317, 'grad_norm': 6.1495490074157715, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:11<3:08:02,  1.98s/it]  5%|â–Œ         | 307/6000 [10:12<3:07:52,  1.98s/it]                                                    {'loss': 0.0295, 'grad_norm': 7.504853248596191, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:12<3:07:52,  1.98s/it]  5%|â–Œ         | 308/6000 [10:14<3:07:08,  1.97s/it]                                                    {'loss': 0.0815, 'grad_norm': 13.288758277893066, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:14<3:07:08,  1.97s/it]  5%|â–Œ         | 309/6000 [10:16<3:07:36,  1.98s/it]                                                    {'loss': 0.2221, 'grad_norm': 31.25084114074707, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:16<3:07:36,  1.98s/it]  5%|â–Œ         | 310/6000 [10:18<3:06:56,  1.97s/it]                                                    {'loss': 0.0286, 'grad_norm': 5.952070713043213, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:18<3:06:56,  1.97s/it]  5%|â–Œ         | 311/6000 [10:20<3:06:31,  1.97s/it]                                                    {'loss': 0.163, 'grad_norm': 16.384435653686523, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:20<3:06:31,  1.97s/it]  5%|â–Œ         | 312/6000 [10:22<3:06:54,  1.97s/it]                                                    {'loss': 0.2787, 'grad_norm': 37.46700668334961, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:22<3:06:54,  1.97s/it]  5%|â–Œ         | 313/6000 [10:24<3:07:38,  1.98s/it]                                                    {'loss': 0.0966, 'grad_norm': 14.459775924682617, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:24<3:07:38,  1.98s/it]  5%|â–Œ         | 314/6000 [10:26<3:07:33,  1.98s/it]                                                    {'loss': 0.1026, 'grad_norm': 16.606184005737305, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:26<3:07:33,  1.98s/it]  5%|â–Œ         | 315/6000 [10:28<3:07:11,  1.98s/it]                                                    {'loss': 0.0507, 'grad_norm': 6.860091686248779, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:28<3:07:11,  1.98s/it]  5%|â–Œ         | 316/6000 [10:30<3:06:33,  1.97s/it]                                                    {'loss': 0.0131, 'grad_norm': 3.1555464267730713, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:30<3:06:33,  1.97s/it]  5%|â–Œ         | 317/6000 [10:32<3:06:41,  1.97s/it]                                                    {'loss': 0.0626, 'grad_norm': 15.19241714477539, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:32<3:06:41,  1.97s/it]  5%|â–Œ         | 318/6000 [10:34<3:08:37,  1.99s/it]                                                    {'loss': 0.0331, 'grad_norm': 10.249373435974121, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:34<3:08:37,  1.99s/it]  5%|â–Œ         | 319/6000 [10:36<3:08:00,  1.99s/it]                                                    {'loss': 0.4673, 'grad_norm': 26.397682189941406, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:36<3:08:00,  1.99s/it]  5%|â–Œ         | 320/6000 [10:38<3:08:07,  1.99s/it]                                                    {'loss': 0.0814, 'grad_norm': 15.044257164001465, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:38<3:08:07,  1.99s/it]  5%|â–Œ         | 321/6000 [10:40<3:11:57,  2.03s/it]                                                    {'loss': 0.0179, 'grad_norm': 5.535764694213867, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:40<3:11:57,  2.03s/it]  5%|â–Œ         | 322/6000 [10:42<3:09:46,  2.01s/it]                                                    {'loss': 0.0806, 'grad_norm': 15.0773344039917, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:42<3:09:46,  2.01s/it]  5%|â–Œ         | 323/6000 [10:45<3:20:49,  2.12s/it]                                                    {'loss': 0.1369, 'grad_norm': 47.4526252746582, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:45<3:20:49,  2.12s/it]  5%|â–Œ         | 324/6000 [10:47<3:23:55,  2.16s/it]                                                    {'loss': 0.1527, 'grad_norm': 28.06036949157715, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:47<3:23:55,  2.16s/it]  5%|â–Œ         | 325/6000 [10:49<3:17:10,  2.08s/it]                                                    {'loss': 0.1303, 'grad_norm': 21.077024459838867, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:49<3:17:10,  2.08s/it]  5%|â–Œ         | 326/6000 [10:51<3:12:55,  2.04s/it]                                                    {'loss': 0.1042, 'grad_norm': 19.364871978759766, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [10:51<3:12:55,  2.04s/it]  5%|â–Œ         | 327/6000 [10:53<3:08:30,  1.99s/it]                                                    {'loss': 0.0315, 'grad_norm': 7.24875020980835, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [10:53<3:08:30,  1.99s/it]  5%|â–Œ         | 328/6000 [10:55<3:07:39,  1.99s/it]                                                    {'loss': 0.0272, 'grad_norm': 8.021785736083984, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [10:55<3:07:39,  1.99s/it]  5%|â–Œ         | 329/6000 [10:57<3:06:04,  1.97s/it]                                                    {'loss': 0.0727, 'grad_norm': 20.867712020874023, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [10:57<3:06:04,  1.97s/it]  6%|â–Œ         | 330/6000 [10:59<3:06:17,  1.97s/it]                                                    {'loss': 0.1082, 'grad_norm': 17.833358764648438, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [10:59<3:06:17,  1.97s/it]  6%|â–Œ         | 331/6000 [11:00<3:06:04,  1.97s/it]                                                    {'loss': 0.092, 'grad_norm': 17.463685989379883, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [11:00<3:06:04,  1.97s/it]  6%|â–Œ         | 332/6000 [11:02<3:04:09,  1.95s/it]                                                    {'loss': 0.0625, 'grad_norm': 15.888565063476562, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [11:02<3:04:09,  1.95s/it]  6%|â–Œ         | 333/6000 [11:04<3:03:25,  1.94s/it]                                                    {'loss': 0.022, 'grad_norm': 7.067695140838623, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [11:04<3:03:25,  1.94s/it]  6%|â–Œ         | 334/6000 [11:06<3:05:26,  1.96s/it]                                                    {'loss': 0.0358, 'grad_norm': 6.248499393463135, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [11:06<3:05:26,  1.96s/it]  6%|â–Œ         | 335/6000 [11:08<3:05:14,  1.96s/it]                                                    {'loss': 0.0367, 'grad_norm': 8.04927921295166, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:08<3:05:14,  1.96s/it]  6%|â–Œ         | 336/6000 [11:10<3:03:42,  1.95s/it]                                                    {'loss': 0.103, 'grad_norm': 17.3264102935791, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:10<3:03:42,  1.95s/it]  6%|â–Œ         | 337/6000 [11:12<3:04:56,  1.96s/it]                                                    {'loss': 0.0651, 'grad_norm': 9.981369972229004, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [11:12<3:04:56,  1.96s/it]  6%|â–Œ         | 338/6000 [11:14<3:03:23,  1.94s/it]                                                    {'loss': 0.1456, 'grad_norm': 17.985368728637695, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [11:14<3:03:23,  1.94s/it]  6%|â–Œ         | 339/6000 [11:16<3:04:39,  1.96s/it]                                                    {'loss': 0.0084, 'grad_norm': 2.829322099685669, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [11:16<3:04:39,  1.96s/it]  6%|â–Œ         | 340/6000 [11:18<3:07:40,  1.99s/it]                                                    {'loss': 0.171, 'grad_norm': 26.850061416625977, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [11:18<3:07:40,  1.99s/it]  6%|â–Œ         | 341/6000 [11:20<3:05:23,  1.97s/it]                                                    {'loss': 0.0428, 'grad_norm': 15.816191673278809, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [11:20<3:05:23,  1.97s/it]  6%|â–Œ         | 342/6000 [11:22<3:04:24,  1.96s/it]                                                    {'loss': 0.198, 'grad_norm': 22.483108520507812, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [11:22<3:04:24,  1.96s/it]  6%|â–Œ         | 343/6000 [11:24<3:12:00,  2.04s/it]                                                    {'loss': 0.1308, 'grad_norm': 29.255111694335938, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [11:24<3:12:00,  2.04s/it]  6%|â–Œ         | 344/6000 [11:26<3:09:22,  2.01s/it]                                                    {'loss': 0.1593, 'grad_norm': 38.366214752197266, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [11:26<3:09:22,  2.01s/it]  6%|â–Œ         | 345/6000 [11:28<3:07:00,  1.98s/it]                                                    {'loss': 0.033, 'grad_norm': 9.670310974121094, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [11:28<3:07:00,  1.98s/it]  6%|â–Œ         | 346/6000 [11:30<3:08:14,  2.00s/it]                                                    {'loss': 0.1883, 'grad_norm': 26.654434204101562, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [11:30<3:08:14,  2.00s/it]  6%|â–Œ         | 347/6000 [11:32<3:06:13,  1.98s/it]                                                    {'loss': 0.0117, 'grad_norm': 2.4537670612335205, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [11:32<3:06:13,  1.98s/it]  6%|â–Œ         | 348/6000 [11:34<3:07:42,  1.99s/it]                                                    {'loss': 0.1788, 'grad_norm': 21.860326766967773, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [11:34<3:07:42,  1.99s/it]  6%|â–Œ         | 349/6000 [11:36<3:07:54,  2.00s/it]                                                    {'loss': 0.0102, 'grad_norm': 5.6232380867004395, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [11:36<3:07:54,  2.00s/it]  6%|â–Œ         | 350/6000 [11:38<3:07:22,  1.99s/it]                                                    {'loss': 0.2786, 'grad_norm': 27.487064361572266, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [11:38<3:07:22,  1.99s/it]  6%|â–Œ         | 351/6000 [11:40<3:10:48,  2.03s/it]                                                    {'loss': 0.0924, 'grad_norm': 18.676321029663086, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [11:40<3:10:48,  2.03s/it]  6%|â–Œ         | 352/6000 [11:42<3:07:37,  1.99s/it]                                                    {'loss': 0.0354, 'grad_norm': 9.474929809570312, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [11:42<3:07:37,  1.99s/it]  6%|â–Œ         | 353/6000 [11:44<3:05:19,  1.97s/it]                                                    {'loss': 0.1187, 'grad_norm': 18.242441177368164, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [11:44<3:05:19,  1.97s/it]