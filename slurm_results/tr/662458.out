==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 18Nov-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token False --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 08:09:33.709000 136116035548992 torch/distributed/run.py:779] 
W1118 08:09:33.709000 136116035548992 torch/distributed/run.py:779] *****************************************
W1118 08:09:33.709000 136116035548992 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 08:09:33.709000 136116035548992 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 08:09:40,173] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.50it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.57it/s]
wandb: setting up run r11gtxl2
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_080940-r11gtxl2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 18Nov-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/r11gtxl2
[2025-11-18 08:09:41,866] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.71it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.94it/s]
[2025-11-18 08:09:42,389] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 08:09:48,561] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 08:09:50,083] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 08:09:50,084] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 08:09:53,596] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 08:09:53,597] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 08:09:54,347] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 08:09:54,348] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 08:09:54,348] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 08:09:54,349] INFO [src.utils:19] ==================================================
[2025-11-18 08:09:54,349] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 08:09:54,349] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 08:09:54,350] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 08:09:54,350] INFO [src.utils:19] ==================================================
[2025-11-18 08:09:55,952] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 08:09:55,952] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 08:09:55,952] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 08:09:55,952] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 08:09:55,952] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 08:09:55,952] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 08:09:55,952] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 08:09:55,952] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 08:09:55,952] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 08:09:55,952] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 08:09:55,953] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 08:09:55,953] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 08:09:55,953] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 08:09:55,953] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 08:09:55,955] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 08:09:55,957] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 08:09:55,958] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 08:09:55,960] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank1]:[W1118 08:09:58.189646972 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1118 08:09:58.192491095 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:03<5:41:09,  3.41s/it]                                                  {'loss': 12.7815, 'grad_norm': 661.7625732421875, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:41:09,  3.41s/it]  0%|          | 2/6000 [00:05<4:12:16,  2.52s/it]                                                  {'loss': 18.2151, 'grad_norm': 176.24261474609375, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:12:16,  2.52s/it]  0%|          | 3/6000 [00:07<3:45:05,  2.25s/it]                                                  {'loss': 10.9842, 'grad_norm': 394.3957824707031, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:45:05,  2.25s/it]  0%|          | 4/6000 [00:09<3:36:34,  2.17s/it]                                                  {'loss': 17.114, 'grad_norm': 310.61907958984375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:36:34,  2.17s/it]  0%|          | 5/6000 [00:11<3:26:58,  2.07s/it]                                                  {'loss': 16.076, 'grad_norm': 220.31243896484375, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:26:58,  2.07s/it]  0%|          | 6/6000 [00:13<3:23:10,  2.03s/it]                                                  {'loss': 14.1949, 'grad_norm': 185.58799743652344, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:23:10,  2.03s/it]  0%|          | 7/6000 [00:15<3:19:25,  2.00s/it]                                                  {'loss': 13.1892, 'grad_norm': 377.355224609375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:19:25,  2.00s/it]  0%|          | 8/6000 [00:16<3:14:54,  1.95s/it]                                                  {'loss': 25.9537, 'grad_norm': 462.18255615234375, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:16<3:14:54,  1.95s/it]  0%|          | 9/6000 [00:18<3:15:41,  1.96s/it]                                                  {'loss': 14.854, 'grad_norm': 162.29129028320312, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:18<3:15:41,  1.96s/it]  0%|          | 10/6000 [00:20<3:16:16,  1.97s/it]                                                   {'loss': 17.4751, 'grad_norm': 330.4199523925781, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:16:16,  1.97s/it]  0%|          | 11/6000 [00:22<3:17:37,  1.98s/it]                                                   {'loss': 12.386, 'grad_norm': 171.8050994873047, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:17:37,  1.98s/it]  0%|          | 12/6000 [00:24<3:18:25,  1.99s/it]                                                   {'loss': 11.6489, 'grad_norm': 150.08750915527344, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:18:25,  1.99s/it]  0%|          | 13/6000 [00:26<3:16:36,  1.97s/it]                                                   {'loss': 9.5036, 'grad_norm': 181.0001220703125, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:16:36,  1.97s/it]  0%|          | 14/6000 [00:28<3:15:22,  1.96s/it]                                                   {'loss': 14.0328, 'grad_norm': 493.6187744140625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:15:22,  1.96s/it]  0%|          | 15/6000 [00:30<3:15:41,  1.96s/it]                                                   {'loss': 10.727, 'grad_norm': 131.73275756835938, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:15:41,  1.96s/it]  0%|          | 16/6000 [00:32<3:13:45,  1.94s/it]                                                   {'loss': 9.4846, 'grad_norm': 346.4884033203125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:13:45,  1.94s/it]  0%|          | 17/6000 [00:34<3:14:44,  1.95s/it]                                                   {'loss': 14.5382, 'grad_norm': 230.19863891601562, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:14:44,  1.95s/it]  0%|          | 18/6000 [00:36<3:14:54,  1.95s/it]                                                   {'loss': 9.5441, 'grad_norm': 193.21560668945312, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:14:54,  1.95s/it]  0%|          | 19/6000 [00:38<3:15:04,  1.96s/it]                                                   {'loss': 14.7639, 'grad_norm': 149.64398193359375, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:15:04,  1.96s/it]  0%|          | 20/6000 [00:40<3:16:21,  1.97s/it]                                                   {'loss': 9.2428, 'grad_norm': 285.08038330078125, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:16:21,  1.97s/it]  0%|          | 21/6000 [00:42<3:19:07,  2.00s/it]                                                   {'loss': 10.2607, 'grad_norm': 220.6127166748047, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:19:07,  2.00s/it]  0%|          | 22/6000 [00:44<3:18:53,  2.00s/it]                                                   {'loss': 16.5967, 'grad_norm': 138.9967803955078, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:18:53,  2.00s/it]  0%|          | 23/6000 [00:46<3:18:16,  1.99s/it]                                                   {'loss': 19.2734, 'grad_norm': 124.1054916381836, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:18:16,  1.99s/it]  0%|          | 24/6000 [00:48<3:20:21,  2.01s/it]                                                   {'loss': 11.7096, 'grad_norm': 98.82069396972656, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:20:21,  2.01s/it]  0%|          | 25/6000 [00:50<3:20:28,  2.01s/it]                                                   {'loss': 16.0234, 'grad_norm': 218.86228942871094, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:20:28,  2.01s/it]  0%|          | 26/6000 [00:52<3:18:17,  1.99s/it]                                                   {'loss': 9.1878, 'grad_norm': 102.28063201904297, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:18:17,  1.99s/it]  0%|          | 27/6000 [00:54<3:17:12,  1.98s/it]                                                   {'loss': 10.2052, 'grad_norm': 105.67778778076172, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:17:12,  1.98s/it]  0%|          | 28/6000 [00:56<3:27:53,  2.09s/it]                                                   {'loss': 10.6436, 'grad_norm': 123.81524658203125, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:27:53,  2.09s/it]  0%|          | 29/6000 [00:58<3:23:13,  2.04s/it]                                                   {'loss': 18.6124, 'grad_norm': 119.00970458984375, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:23:13,  2.04s/it]  0%|          | 30/6000 [01:00<3:21:21,  2.02s/it]                                                   {'loss': 13.3707, 'grad_norm': 120.11949920654297, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:21:21,  2.02s/it]  1%|          | 31/6000 [01:02<3:19:21,  2.00s/it]                                                   {'loss': 12.0322, 'grad_norm': 132.06716918945312, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:19:21,  2.00s/it]  1%|          | 32/6000 [01:04<3:17:37,  1.99s/it]                                                   {'loss': 16.4257, 'grad_norm': 205.19654846191406, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:17:37,  1.99s/it]  1%|          | 33/6000 [01:06<3:17:36,  1.99s/it]                                                   {'loss': 8.8649, 'grad_norm': 393.2487487792969, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:17:36,  1.99s/it]  1%|          | 34/6000 [01:08<3:16:23,  1.98s/it]                                                   {'loss': 13.6486, 'grad_norm': 262.07061767578125, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:16:23,  1.98s/it]  1%|          | 35/6000 [01:10<3:17:13,  1.98s/it]                                                   {'loss': 6.9435, 'grad_norm': 144.5128936767578, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:17:13,  1.98s/it]  1%|          | 36/6000 [01:12<3:16:04,  1.97s/it]                                                   {'loss': 15.0674, 'grad_norm': 266.2099914550781, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:16:04,  1.97s/it]  1%|          | 37/6000 [01:14<3:14:16,  1.95s/it]                                                   {'loss': 11.7829, 'grad_norm': 270.5201721191406, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:14:16,  1.95s/it]  1%|          | 38/6000 [01:16<3:13:29,  1.95s/it]                                                   {'loss': 18.1139, 'grad_norm': 361.92633056640625, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:13:29,  1.95s/it]  1%|          | 39/6000 [01:18<3:14:31,  1.96s/it]                                                   {'loss': 9.7449, 'grad_norm': 172.1324462890625, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:14:31,  1.96s/it]  1%|          | 40/6000 [01:20<3:15:18,  1.97s/it]                                                   {'loss': 8.5432, 'grad_norm': 217.2655029296875, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:15:18,  1.97s/it]  1%|          | 41/6000 [01:22<3:13:52,  1.95s/it]                                                   {'loss': 15.0809, 'grad_norm': 196.3878631591797, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:22<3:13:52,  1.95s/it]  1%|          | 42/6000 [01:24<3:13:33,  1.95s/it]                                                   {'loss': 14.3253, 'grad_norm': 142.226806640625, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:24<3:13:33,  1.95s/it]  1%|          | 43/6000 [01:26<3:25:28,  2.07s/it]                                                   {'loss': 7.9866, 'grad_norm': 391.3896789550781, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:25:28,  2.07s/it]  1%|          | 44/6000 [01:28<3:24:42,  2.06s/it]                                                   {'loss': 7.6013, 'grad_norm': 509.8873596191406, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:24:42,  2.06s/it]  1%|          | 45/6000 [01:30<3:22:00,  2.04s/it]                                                   {'loss': 7.0363, 'grad_norm': 201.13502502441406, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:22:00,  2.04s/it]  1%|          | 46/6000 [01:32<3:23:40,  2.05s/it]                                                   {'loss': 13.7819, 'grad_norm': 369.347900390625, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:23:40,  2.05s/it]  1%|          | 47/6000 [01:34<3:19:48,  2.01s/it]                                                   {'loss': 6.9203, 'grad_norm': 170.60140991210938, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:19:48,  2.01s/it]  1%|          | 48/6000 [01:36<3:17:06,  1.99s/it]                                                   {'loss': 9.3473, 'grad_norm': 122.87303924560547, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:17:06,  1.99s/it]  1%|          | 49/6000 [01:38<3:16:10,  1.98s/it]                                                   {'loss': 13.2553, 'grad_norm': 1003.2537231445312, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:16:10,  1.98s/it]  1%|          | 50/6000 [01:40<3:16:47,  1.98s/it]                                                   {'loss': 12.3513, 'grad_norm': 255.4560546875, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:16:47,  1.98s/it]  1%|          | 51/6000 [01:42<3:17:27,  1.99s/it]                                                   {'loss': 11.3686, 'grad_norm': 530.3970336914062, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:42<3:17:27,  1.99s/it]  1%|          | 52/6000 [01:44<3:13:37,  1.95s/it]                                                   {'loss': 12.6168, 'grad_norm': 376.0818786621094, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:44<3:13:37,  1.95s/it]  1%|          | 53/6000 [01:46<3:13:55,  1.96s/it]                                                   {'loss': 11.2388, 'grad_norm': 357.5500183105469, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:46<3:13:55,  1.96s/it]  1%|          | 54/6000 [01:48<3:12:22,  1.94s/it]                                                   {'loss': 16.4982, 'grad_norm': 277.4849548339844, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:48<3:12:22,  1.94s/it]  1%|          | 55/6000 [01:50<3:10:58,  1.93s/it]                                                   {'loss': 13.6112, 'grad_norm': 479.56170654296875, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:50<3:10:58,  1.93s/it]  1%|          | 56/6000 [01:52<3:13:11,  1.95s/it]                                                   {'loss': 13.8099, 'grad_norm': 224.66390991210938, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:52<3:13:11,  1.95s/it]  1%|          | 57/6000 [01:54<3:12:57,  1.95s/it]                                                   {'loss': 14.2608, 'grad_norm': 290.88555908203125, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:54<3:12:57,  1.95s/it]  1%|          | 58/6000 [01:56<3:11:46,  1.94s/it]                                                   {'loss': 8.3645, 'grad_norm': 209.4645233154297, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:56<3:11:46,  1.94s/it]  1%|          | 59/6000 [01:57<3:10:08,  1.92s/it]                                                   {'loss': 11.8533, 'grad_norm': 632.1359252929688, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:57<3:10:08,  1.92s/it]  1%|          | 60/6000 [01:59<3:11:15,  1.93s/it]                                                   {'loss': 13.2403, 'grad_norm': 229.86305236816406, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [01:59<3:11:15,  1.93s/it]  1%|          | 61/6000 [02:01<3:09:37,  1.92s/it]                                                   {'loss': 16.6784, 'grad_norm': 727.4962158203125, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:01<3:09:37,  1.92s/it]  1%|          | 62/6000 [02:03<3:10:56,  1.93s/it]                                                   {'loss': 11.2113, 'grad_norm': 333.6750183105469, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:03<3:10:56,  1.93s/it]  1%|          | 63/6000 [02:05<3:11:18,  1.93s/it]                                                   {'loss': 10.6116, 'grad_norm': 421.4167175292969, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:05<3:11:18,  1.93s/it]  1%|          | 64/6000 [02:07<3:09:10,  1.91s/it]                                                   {'loss': 11.3556, 'grad_norm': 487.30462646484375, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:07<3:09:10,  1.91s/it]  1%|          | 65/6000 [02:09<3:08:59,  1.91s/it]                                                   {'loss': 9.2751, 'grad_norm': 1584.5364990234375, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:09<3:08:59,  1.91s/it]  1%|          | 66/6000 [02:11<3:16:26,  1.99s/it]                                                   {'loss': 6.3879, 'grad_norm': 13251.65234375, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:11<3:16:26,  1.99s/it]  1%|          | 67/6000 [02:13<3:16:26,  1.99s/it]                                                   {'loss': 5.9153, 'grad_norm': 2475.463623046875, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:13<3:16:26,  1.99s/it]  1%|          | 68/6000 [02:15<3:13:23,  1.96s/it]                                                   {'loss': 12.416, 'grad_norm': 2963.981689453125, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:15<3:13:23,  1.96s/it]  1%|          | 69/6000 [02:17<3:11:59,  1.94s/it]                                                   {'loss': 7.9836, 'grad_norm': 1028.9359130859375, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:17<3:11:59,  1.94s/it]  1%|          | 70/6000 [02:19<3:12:28,  1.95s/it]                                                   {'loss': 7.0595, 'grad_norm': 906.6981201171875, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:19<3:12:28,  1.95s/it]  1%|          | 71/6000 [02:21<3:09:55,  1.92s/it]                                                   {'loss': 8.0613, 'grad_norm': 1348.0396728515625, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:21<3:09:55,  1.92s/it]  1%|          | 72/6000 [02:23<3:13:09,  1.96s/it]                                                   {'loss': 9.4651, 'grad_norm': 954.4326782226562, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:23<3:13:09,  1.96s/it]  1%|          | 73/6000 [02:25<3:11:45,  1.94s/it]                                                   {'loss': 10.4563, 'grad_norm': 1259.780029296875, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:25<3:11:45,  1.94s/it]  1%|          | 74/6000 [02:26<3:09:14,  1.92s/it]                                                   {'loss': 8.5182, 'grad_norm': 372.3978576660156, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:26<3:09:14,  1.92s/it]  1%|â–         | 75/6000 [02:29<3:15:16,  1.98s/it]                                                   {'loss': 12.1415, 'grad_norm': 1685.2371826171875, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:29<3:15:16,  1.98s/it]  1%|â–         | 76/6000 [02:31<3:14:32,  1.97s/it]                                                   {'loss': 5.4701, 'grad_norm': 1627.6973876953125, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:31<3:14:32,  1.97s/it]  1%|â–         | 77/6000 [02:32<3:13:26,  1.96s/it]                                                   {'loss': 6.5569, 'grad_norm': 1581.1640625, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:33<3:13:26,  1.96s/it]  1%|â–         | 78/6000 [02:35<3:21:54,  2.05s/it]                                                   {'loss': 8.5936, 'grad_norm': 2297.53466796875, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:35<3:21:54,  2.05s/it]  1%|â–         | 79/6000 [02:37<3:18:43,  2.01s/it]                                                   {'loss': 5.2387, 'grad_norm': 269.08416748046875, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:37<3:18:43,  2.01s/it]  1%|â–         | 80/6000 [02:39<3:15:39,  1.98s/it]                                                   {'loss': 7.8263, 'grad_norm': 861.248046875, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:39<3:15:39,  1.98s/it]  1%|â–         | 81/6000 [02:40<3:12:46,  1.95s/it]                                                   {'loss': 7.3379, 'grad_norm': 1438.8221435546875, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:40<3:12:46,  1.95s/it]  1%|â–         | 82/6000 [02:42<3:11:08,  1.94s/it]                                                   {'loss': 10.528, 'grad_norm': 1590.4052734375, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:42<3:11:08,  1.94s/it]  1%|â–         | 83/6000 [02:44<3:13:57,  1.97s/it]                                                   {'loss': 7.8485, 'grad_norm': 1295.133056640625, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:44<3:13:57,  1.97s/it]  1%|â–         | 84/6000 [02:46<3:14:16,  1.97s/it]                                                   {'loss': 5.2224, 'grad_norm': 313.9352111816406, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:46<3:14:16,  1.97s/it]  1%|â–         | 85/6000 [02:48<3:15:57,  1.99s/it]                                                   {'loss': 8.021, 'grad_norm': 2432.80908203125, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:48<3:15:57,  1.99s/it]  1%|â–         | 86/6000 [02:50<3:15:13,  1.98s/it]                                                   {'loss': 5.4093, 'grad_norm': 2066.77685546875, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:50<3:15:13,  1.98s/it]  1%|â–         | 87/6000 [02:52<3:15:45,  1.99s/it]                                                   {'loss': 6.3246, 'grad_norm': 5784.43701171875, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:52<3:15:45,  1.99s/it]  1%|â–         | 88/6000 [02:54<3:13:39,  1.97s/it]                                                   {'loss': 7.4021, 'grad_norm': 1461.7950439453125, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:54<3:13:39,  1.97s/it]  1%|â–         | 89/6000 [02:56<3:12:12,  1.95s/it]                                                   {'loss': 4.5184, 'grad_norm': 267.1976623535156, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:56<3:12:12,  1.95s/it]  2%|â–         | 90/6000 [02:58<3:14:03,  1.97s/it]                                                   {'loss': 4.9064, 'grad_norm': 467.4476013183594, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [02:58<3:14:03,  1.97s/it]  2%|â–         | 91/6000 [03:00<3:12:52,  1.96s/it]                                                   {'loss': 5.3365, 'grad_norm': 817.44140625, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:00<3:12:52,  1.96s/it]  2%|â–         | 92/6000 [03:02<3:14:36,  1.98s/it]                                                   {'loss': 7.4368, 'grad_norm': 1702.04248046875, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:02<3:14:36,  1.98s/it]  2%|â–         | 93/6000 [03:04<3:15:28,  1.99s/it]                                                   {'loss': 7.6071, 'grad_norm': 1140.00537109375, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:04<3:15:28,  1.99s/it]  2%|â–         | 94/6000 [03:06<3:13:25,  1.97s/it]                                                   {'loss': 5.4851, 'grad_norm': 884.9464721679688, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:06<3:13:25,  1.97s/it]  2%|â–         | 95/6000 [03:08<3:12:58,  1.96s/it]                                                   {'loss': 4.3814, 'grad_norm': 119.87645721435547, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:08<3:12:58,  1.96s/it]  2%|â–         | 96/6000 [03:10<3:11:58,  1.95s/it]                                                   {'loss': 7.399, 'grad_norm': 5273.88037109375, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:10<3:11:58,  1.95s/it]  2%|â–         | 97/6000 [03:12<3:10:18,  1.93s/it]                                                   {'loss': 7.8453, 'grad_norm': 6932.50732421875, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:12<3:10:18,  1.93s/it]  2%|â–         | 98/6000 [03:14<3:09:33,  1.93s/it]                                                   {'loss': 8.9309, 'grad_norm': 5227.9189453125, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:14<3:09:33,  1.93s/it]  2%|â–         | 99/6000 [03:16<3:08:08,  1.91s/it]                                                   {'loss': 6.3408, 'grad_norm': 1277.620849609375, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:16<3:08:08,  1.91s/it]  2%|â–         | 100/6000 [03:18<3:07:51,  1.91s/it]                                                    {'loss': 5.8944, 'grad_norm': 427.3535461425781, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:18<3:07:51,  1.91s/it]  2%|â–         | 101/6000 [03:20<3:20:12,  2.04s/it]                                                    {'loss': 5.6305, 'grad_norm': 634.4122314453125, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:20<3:20:12,  2.04s/it]  2%|â–         | 102/6000 [03:22<3:16:27,  2.00s/it]                                                    {'loss': 4.1864, 'grad_norm': 301.2786865234375, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:22<3:16:27,  2.00s/it]  2%|â–         | 103/6000 [03:24<3:14:58,  1.98s/it]                                                    {'loss': 5.9043, 'grad_norm': 591.4072875976562, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:24<3:14:58,  1.98s/it]  2%|â–         | 104/6000 [03:26<3:16:33,  2.00s/it]                                                    {'loss': 4.3834, 'grad_norm': 237.04510498046875, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:26<3:16:33,  2.00s/it]  2%|â–         | 105/6000 [03:28<3:14:40,  1.98s/it]                                                    {'loss': 6.732, 'grad_norm': 1879.4454345703125, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:28<3:14:40,  1.98s/it]  2%|â–         | 106/6000 [03:30<3:15:35,  1.99s/it]                                                    {'loss': 7.2428, 'grad_norm': 2410.2451171875, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:30<3:15:35,  1.99s/it]  2%|â–         | 107/6000 [03:32<3:13:36,  1.97s/it]                                                    {'loss': 6.6951, 'grad_norm': 1378.6741943359375, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:32<3:13:36,  1.97s/it]  2%|â–         | 108/6000 [03:34<3:14:12,  1.98s/it]                                                    {'loss': 5.871, 'grad_norm': 275.71435546875, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:34<3:14:12,  1.98s/it]  2%|â–         | 109/6000 [03:36<3:17:02,  2.01s/it]                                                    {'loss': 4.6598, 'grad_norm': 188.21376037597656, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:36<3:17:02,  2.01s/it]  2%|â–         | 110/6000 [03:38<3:17:07,  2.01s/it]                                                    {'loss': 4.6649, 'grad_norm': 543.7881469726562, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:38<3:17:07,  2.01s/it]  2%|â–         | 111/6000 [03:40<3:15:41,  1.99s/it]                                                    {'loss': 6.1983, 'grad_norm': 144.3934783935547, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:40<3:15:41,  1.99s/it]  2%|â–         | 112/6000 [03:42<3:17:58,  2.02s/it]                                                    {'loss': 4.597, 'grad_norm': 362.0908203125, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:42<3:17:58,  2.02s/it]  2%|â–         | 113/6000 [03:44<3:15:32,  1.99s/it]                                                    {'loss': 4.398, 'grad_norm': 395.40423583984375, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:44<3:15:32,  1.99s/it]  2%|â–         | 114/6000 [03:46<3:14:56,  1.99s/it]                                                    {'loss': 4.4238, 'grad_norm': 416.3680419921875, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:46<3:14:56,  1.99s/it]  2%|â–         | 115/6000 [03:48<3:13:43,  1.98s/it]                                                    {'loss': 5.312, 'grad_norm': 549.0028686523438, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:48<3:13:43,  1.98s/it]  2%|â–         | 116/6000 [03:50<3:10:52,  1.95s/it]                                                    {'loss': 4.0838, 'grad_norm': 440.09454345703125, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:50<3:10:52,  1.95s/it]  2%|â–         | 117/6000 [03:51<3:09:04,  1.93s/it]                                                    {'loss': 4.5855, 'grad_norm': 382.2781066894531, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:51<3:09:04,  1.93s/it]  2%|â–         | 118/6000 [03:54<3:15:00,  1.99s/it]                                                    {'loss': 3.9593, 'grad_norm': 243.65652465820312, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:54<3:15:00,  1.99s/it]  2%|â–         | 119/6000 [03:55<3:12:52,  1.97s/it]                                                    {'loss': 4.569, 'grad_norm': 412.8424072265625, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:55<3:12:52,  1.97s/it]  2%|â–         | 120/6000 [03:57<3:10:52,  1.95s/it]                                                    {'loss': 4.1703, 'grad_norm': 92.64900970458984, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [03:57<3:10:52,  1.95s/it]  2%|â–         | 121/6000 [03:59<3:10:21,  1.94s/it]                                                    {'loss': 4.7001, 'grad_norm': 524.4600219726562, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [03:59<3:10:21,  1.94s/it]  2%|â–         | 122/6000 [04:01<3:09:49,  1.94s/it]                                                    {'loss': 3.9179, 'grad_norm': 382.6928405761719, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:01<3:09:49,  1.94s/it]  2%|â–         | 123/6000 [04:03<3:09:31,  1.93s/it]                                                    {'loss': 4.2741, 'grad_norm': 340.4205322265625, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:03<3:09:31,  1.93s/it]  2%|â–         | 124/6000 [04:05<3:08:41,  1.93s/it]                                                    {'loss': 4.1587, 'grad_norm': 83.51278686523438, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:05<3:08:41,  1.93s/it]  2%|â–         | 125/6000 [04:07<3:12:02,  1.96s/it]                                                    {'loss': 4.2192, 'grad_norm': 97.34955596923828, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:07<3:12:02,  1.96s/it]  2%|â–         | 126/6000 [04:09<3:11:19,  1.95s/it]                                                    {'loss': 4.637, 'grad_norm': 794.10986328125, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:09<3:11:19,  1.95s/it]  2%|â–         | 127/6000 [04:11<3:11:32,  1.96s/it]                                                    {'loss': 4.7508, 'grad_norm': 727.1929321289062, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:11<3:11:32,  1.96s/it]  2%|â–         | 128/6000 [04:13<3:10:04,  1.94s/it]                                                    {'loss': 4.5869, 'grad_norm': 112.16236114501953, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:13<3:10:04,  1.94s/it]  2%|â–         | 129/6000 [04:15<3:11:56,  1.96s/it]                                                    {'loss': 4.7648, 'grad_norm': 171.35183715820312, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:15<3:11:56,  1.96s/it]  2%|â–         | 130/6000 [04:17<3:10:25,  1.95s/it]                                                    {'loss': 3.769, 'grad_norm': 189.8397216796875, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:17<3:10:25,  1.95s/it]  2%|â–         | 131/6000 [04:19<3:11:49,  1.96s/it]                                                    {'loss': 3.9528, 'grad_norm': 302.76495361328125, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:19<3:11:49,  1.96s/it]  2%|â–         | 132/6000 [04:21<3:12:03,  1.96s/it]                                                    {'loss': 4.5256, 'grad_norm': 203.2902374267578, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:21<3:12:03,  1.96s/it]  2%|â–         | 133/6000 [04:23<3:10:39,  1.95s/it]                                                    {'loss': 4.1207, 'grad_norm': 174.83218383789062, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:23<3:10:39,  1.95s/it]  2%|â–         | 134/6000 [04:25<3:10:59,  1.95s/it]                                                    {'loss': 4.0825, 'grad_norm': 615.6021728515625, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:25<3:10:59,  1.95s/it]  2%|â–         | 135/6000 [04:27<3:09:53,  1.94s/it]                                                    {'loss': 4.1296, 'grad_norm': 464.20501708984375, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:27<3:09:53,  1.94s/it]  2%|â–         | 136/6000 [04:28<3:08:27,  1.93s/it]                                                    {'loss': 4.1844, 'grad_norm': 271.0710144042969, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:28<3:08:27,  1.93s/it]  2%|â–         | 137/6000 [04:31<3:14:09,  1.99s/it]                                                    {'loss': 3.7765, 'grad_norm': 189.0791778564453, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:31<3:14:09,  1.99s/it]  2%|â–         | 138/6000 [04:32<3:11:04,  1.96s/it]                                                    {'loss': 4.8269, 'grad_norm': 613.5025634765625, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:32<3:11:04,  1.96s/it]  2%|â–         | 139/6000 [04:34<3:12:33,  1.97s/it]                                                    {'loss': 4.1983, 'grad_norm': 168.83761596679688, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:34<3:12:33,  1.97s/it]  2%|â–         | 140/6000 [04:37<3:14:47,  1.99s/it]                                                    {'loss': 4.348, 'grad_norm': 77.1631088256836, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:37<3:14:47,  1.99s/it]  2%|â–         | 141/6000 [04:38<3:13:13,  1.98s/it]                                                    {'loss': 4.1634, 'grad_norm': 93.23385620117188, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:38<3:13:13,  1.98s/it]  2%|â–         | 142/6000 [04:40<3:10:19,  1.95s/it]                                                    {'loss': 4.5712, 'grad_norm': 314.2369689941406, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:40<3:10:19,  1.95s/it]  2%|â–         | 143/6000 [04:42<3:08:39,  1.93s/it]                                                    {'loss': 4.131, 'grad_norm': 73.16992950439453, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:42<3:08:39,  1.93s/it]  2%|â–         | 144/6000 [04:44<3:09:51,  1.95s/it]                                                    {'loss': 3.6249, 'grad_norm': 191.38650512695312, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:44<3:09:51,  1.95s/it]  2%|â–         | 145/6000 [04:46<3:08:20,  1.93s/it]                                                    {'loss': 4.2522, 'grad_norm': 82.91968536376953, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:46<3:08:20,  1.93s/it]  2%|â–         | 146/6000 [04:48<3:09:08,  1.94s/it]                                                    {'loss': 4.0404, 'grad_norm': 125.405517578125, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:48<3:09:08,  1.94s/it]  2%|â–         | 147/6000 [04:50<3:08:20,  1.93s/it]                                                    {'loss': 3.8856, 'grad_norm': 52.379478454589844, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:50<3:08:20,  1.93s/it]  2%|â–         | 148/6000 [04:52<3:09:23,  1.94s/it]                                                    {'loss': 3.7801, 'grad_norm': 54.24728775024414, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:52<3:09:23,  1.94s/it]  2%|â–         | 149/6000 [04:54<3:08:11,  1.93s/it]                                                    {'loss': 3.9142, 'grad_norm': 177.9102783203125, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:54<3:08:11,  1.93s/it]  2%|â–Ž         | 150/6000 [04:56<3:09:15,  1.94s/it]                                                    {'loss': 3.7675, 'grad_norm': 56.782100677490234, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [04:56<3:09:15,  1.94s/it]  3%|â–Ž         | 151/6000 [04:58<3:07:57,  1.93s/it]                                                    {'loss': 3.6388, 'grad_norm': 172.9748077392578, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [04:58<3:07:57,  1.93s/it]  3%|â–Ž         | 152/6000 [05:00<3:09:54,  1.95s/it]                                                    {'loss': 3.7402, 'grad_norm': 82.03978729248047, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:00<3:09:54,  1.95s/it]  3%|â–Ž         | 153/6000 [05:02<3:07:54,  1.93s/it]                                                    {'loss': 3.9186, 'grad_norm': 48.27581787109375, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:02<3:07:54,  1.93s/it]  3%|â–Ž         | 154/6000 [05:04<3:14:42,  2.00s/it]                                                    {'loss': 3.5455, 'grad_norm': 71.26808166503906, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:04<3:14:42,  2.00s/it]  3%|â–Ž         | 155/6000 [05:06<3:12:15,  1.97s/it]                                                    {'loss': 3.5484, 'grad_norm': 31.906675338745117, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:06<3:12:15,  1.97s/it]  3%|â–Ž         | 156/6000 [05:08<3:11:56,  1.97s/it]                                                    {'loss': 3.9903, 'grad_norm': 434.6281433105469, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:08<3:11:56,  1.97s/it]  3%|â–Ž         | 157/6000 [05:10<3:13:20,  1.99s/it]                                                    {'loss': 3.629, 'grad_norm': 208.34573364257812, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:10<3:13:20,  1.99s/it]  3%|â–Ž         | 158/6000 [05:12<3:11:46,  1.97s/it]                                                    {'loss': 3.661, 'grad_norm': 151.4691619873047, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:12<3:11:46,  1.97s/it]  3%|â–Ž         | 159/6000 [05:13<3:09:15,  1.94s/it]                                                    {'loss': 3.6914, 'grad_norm': 87.22897338867188, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:13<3:09:15,  1.94s/it]  3%|â–Ž         | 160/6000 [05:16<3:15:27,  2.01s/it]                                                    {'loss': 3.567, 'grad_norm': 59.790157318115234, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:16<3:15:27,  2.01s/it]  3%|â–Ž         | 161/6000 [05:18<3:18:38,  2.04s/it]                                                    {'loss': 3.6208, 'grad_norm': 254.588623046875, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:18<3:18:38,  2.04s/it]  3%|â–Ž         | 162/6000 [05:20<3:13:57,  1.99s/it]                                                    {'loss': 3.674, 'grad_norm': 267.8275451660156, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:20<3:13:57,  1.99s/it]  3%|â–Ž         | 163/6000 [05:22<3:15:52,  2.01s/it]                                                    {'loss': 3.5267, 'grad_norm': 165.0, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:22<3:15:52,  2.01s/it]  3%|â–Ž         | 164/6000 [05:24<3:13:59,  1.99s/it]                                                    {'loss': 3.6664, 'grad_norm': 102.92342376708984, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:24<3:13:59,  1.99s/it]  3%|â–Ž         | 165/6000 [05:26<3:15:29,  2.01s/it]                                                    {'loss': 3.5326, 'grad_norm': 64.28900146484375, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:26<3:15:29,  2.01s/it]  3%|â–Ž         | 166/6000 [05:28<3:12:08,  1.98s/it]                                                    {'loss': 3.6102, 'grad_norm': 47.75712966918945, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:28<3:12:08,  1.98s/it]  3%|â–Ž         | 167/6000 [05:30<3:11:34,  1.97s/it]                                                    {'loss': 3.6323, 'grad_norm': 70.36112976074219, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:30<3:11:34,  1.97s/it]  3%|â–Ž         | 168/6000 [05:31<3:10:02,  1.96s/it]                                                    {'loss': 3.5297, 'grad_norm': 46.80158233642578, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:31<3:10:02,  1.96s/it]  3%|â–Ž         | 169/6000 [05:34<3:16:04,  2.02s/it]                                                    {'loss': 3.5481, 'grad_norm': 142.67196655273438, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:34<3:16:04,  2.02s/it]  3%|â–Ž         | 170/6000 [05:36<3:13:24,  1.99s/it]                                                    {'loss': 3.6378, 'grad_norm': 55.67295455932617, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:36<3:13:24,  1.99s/it]  3%|â–Ž         | 171/6000 [05:37<3:11:02,  1.97s/it]                                                    {'loss': 3.6322, 'grad_norm': 64.91648864746094, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:37<3:11:02,  1.97s/it]  3%|â–Ž         | 172/6000 [05:39<3:11:20,  1.97s/it]                                                    {'loss': 3.8492, 'grad_norm': 52.52031707763672, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:39<3:11:20,  1.97s/it]  3%|â–Ž         | 173/6000 [05:41<3:10:47,  1.96s/it]                                                    {'loss': 3.5295, 'grad_norm': 61.84878921508789, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:41<3:10:47,  1.96s/it]  3%|â–Ž         | 174/6000 [05:44<3:16:44,  2.03s/it]                                                    {'loss': 3.5277, 'grad_norm': 31.04482078552246, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:44<3:16:44,  2.03s/it]  3%|â–Ž         | 175/6000 [05:45<3:13:55,  2.00s/it]                                                    {'loss': 3.503, 'grad_norm': 27.649734497070312, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:45<3:13:55,  2.00s/it]  3%|â–Ž         | 176/6000 [05:48<3:14:27,  2.00s/it]                                                    {'loss': 3.5563, 'grad_norm': 15.603963851928711, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:48<3:14:27,  2.00s/it]  3%|â–Ž         | 177/6000 [05:49<3:13:20,  1.99s/it]                                                    {'loss': 3.5094, 'grad_norm': 26.789220809936523, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:49<3:13:20,  1.99s/it]  3%|â–Ž         | 178/6000 [05:51<3:14:21,  2.00s/it]                                                    {'loss': 3.5136, 'grad_norm': 161.01768493652344, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:52<3:14:21,  2.00s/it]  3%|â–Ž         | 179/6000 [05:53<3:10:40,  1.97s/it]                                                    {'loss': 3.5187, 'grad_norm': 90.96553802490234, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [05:53<3:10:40,  1.97s/it]  3%|â–Ž         | 180/6000 [05:55<3:10:07,  1.96s/it]                                                    {'loss': 3.6158, 'grad_norm': 90.17985534667969, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [05:55<3:10:07,  1.96s/it]  3%|â–Ž         | 181/6000 [05:57<3:10:24,  1.96s/it]                                                    {'loss': 3.5554, 'grad_norm': 47.591495513916016, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [05:57<3:10:24,  1.96s/it]  3%|â–Ž         | 182/6000 [05:59<3:09:39,  1.96s/it]                                                    {'loss': 3.5458, 'grad_norm': 44.24080276489258, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [05:59<3:09:39,  1.96s/it]  3%|â–Ž         | 183/6000 [06:01<3:10:13,  1.96s/it]                                                    {'loss': 3.5033, 'grad_norm': 36.80074691772461, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:01<3:10:13,  1.96s/it]  3%|â–Ž         | 184/6000 [06:03<3:10:42,  1.97s/it]                                                    {'loss': 3.5013, 'grad_norm': 31.06665802001953, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:03<3:10:42,  1.97s/it]  3%|â–Ž         | 185/6000 [06:05<3:08:49,  1.95s/it]                                                    {'loss': 3.4997, 'grad_norm': 15.139771461486816, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:05<3:08:49,  1.95s/it]  3%|â–Ž         | 186/6000 [06:07<3:07:57,  1.94s/it]                                                    {'loss': 3.518, 'grad_norm': 60.512454986572266, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:07<3:07:57,  1.94s/it]  3%|â–Ž         | 187/6000 [06:09<3:10:17,  1.96s/it]                                                    {'loss': 3.6022, 'grad_norm': 1217.3389892578125, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:09<3:10:17,  1.96s/it]  3%|â–Ž         | 188/6000 [06:11<3:08:57,  1.95s/it]                                                    {'loss': 3.7477, 'grad_norm': 2986.59814453125, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:11<3:08:57,  1.95s/it]  3%|â–Ž         | 189/6000 [06:13<3:07:59,  1.94s/it]                                                    {'loss': 3.6778, 'grad_norm': 1610.2620849609375, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:13<3:07:59,  1.94s/it]  3%|â–Ž         | 190/6000 [06:15<3:06:55,  1.93s/it]                                                    {'loss': 3.5545, 'grad_norm': 142.58226013183594, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:15<3:06:55,  1.93s/it]  3%|â–Ž         | 191/6000 [06:17<3:07:37,  1.94s/it]                                                    {'loss': 3.5344, 'grad_norm': 25.7940731048584, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:17<3:07:37,  1.94s/it]  3%|â–Ž         | 192/6000 [06:19<3:14:21,  2.01s/it]                                                    {'loss': 3.5539, 'grad_norm': 30.069713592529297, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:19<3:14:21,  2.01s/it]  3%|â–Ž         | 193/6000 [06:21<3:15:15,  2.02s/it]                                                    {'loss': 3.4952, 'grad_norm': 20.109086990356445, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:21<3:15:15,  2.02s/it]  3%|â–Ž         | 194/6000 [06:23<3:11:34,  1.98s/it]                                                    {'loss': 3.5278, 'grad_norm': 96.71092224121094, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:23<3:11:34,  1.98s/it]  3%|â–Ž         | 195/6000 [06:25<3:09:43,  1.96s/it]                                                    {'loss': 3.5094, 'grad_norm': 82.53874206542969, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:25<3:09:43,  1.96s/it]  3%|â–Ž         | 196/6000 [06:27<3:09:14,  1.96s/it]                                                    {'loss': 3.5273, 'grad_norm': 45.85000228881836, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:27<3:09:14,  1.96s/it]  3%|â–Ž         | 197/6000 [06:29<3:12:31,  1.99s/it]                                                    {'loss': 3.4745, 'grad_norm': 8.049181938171387, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:29<3:12:31,  1.99s/it]  3%|â–Ž         | 198/6000 [06:31<3:11:59,  1.99s/it]                                                    {'loss': 3.4939, 'grad_norm': 60.70344161987305, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:31<3:11:59,  1.99s/it]  3%|â–Ž         | 199/6000 [06:33<3:09:03,  1.96s/it]                                                    {'loss': 3.5239, 'grad_norm': 130.89627075195312, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:33<3:09:03,  1.96s/it]  3%|â–Ž         | 200/6000 [06:35<3:12:20,  1.99s/it]                                                    {'loss': 3.5276, 'grad_norm': 148.92652893066406, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:35<3:12:20,  1.99s/it]  3%|â–Ž         | 201/6000 [06:37<3:13:10,  2.00s/it]                                                    {'loss': 3.5495, 'grad_norm': 169.6676025390625, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:37<3:13:10,  2.00s/it]  3%|â–Ž         | 202/6000 [06:39<3:10:35,  1.97s/it]                                                    {'loss': 3.5037, 'grad_norm': 43.05314254760742, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:39<3:10:35,  1.97s/it]  3%|â–Ž         | 203/6000 [06:41<3:10:38,  1.97s/it]                                                    {'loss': 3.5317, 'grad_norm': 18.090789794921875, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:41<3:10:38,  1.97s/it]  3%|â–Ž         | 204/6000 [06:43<3:10:34,  1.97s/it]                                                    {'loss': 3.478, 'grad_norm': 28.938735961914062, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:43<3:10:34,  1.97s/it]  3%|â–Ž         | 205/6000 [06:44<3:09:01,  1.96s/it]                                                    {'loss': 3.5479, 'grad_norm': 178.66476440429688, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:45<3:09:01,  1.96s/it]  3%|â–Ž         | 206/6000 [06:46<3:08:25,  1.95s/it]                                                    {'loss': 3.5484, 'grad_norm': 67.39334106445312, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:46<3:08:25,  1.95s/it]  3%|â–Ž         | 207/6000 [06:48<3:07:45,  1.94s/it]                                                    {'loss': 3.5303, 'grad_norm': 53.18810272216797, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:48<3:07:45,  1.94s/it]  3%|â–Ž         | 208/6000 [06:50<3:05:52,  1.93s/it]                                                    {'loss': 3.5391, 'grad_norm': 19.272871017456055, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [06:50<3:05:52,  1.93s/it]  3%|â–Ž         | 209/6000 [06:52<3:04:51,  1.92s/it]                                                    {'loss': 3.4984, 'grad_norm': 10.992183685302734, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [06:52<3:04:51,  1.92s/it]  4%|â–Ž         | 210/6000 [06:54<3:07:00,  1.94s/it]                                                    {'loss': 3.4967, 'grad_norm': 15.061786651611328, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [06:54<3:07:00,  1.94s/it]  4%|â–Ž         | 211/6000 [06:56<3:08:52,  1.96s/it]                                                    {'loss': 3.4752, 'grad_norm': 35.545650482177734, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [06:56<3:08:52,  1.96s/it]  4%|â–Ž         | 212/6000 [06:58<3:10:39,  1.98s/it]                                                    {'loss': 4.2215, 'grad_norm': 6722.92578125, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [06:58<3:10:39,  1.98s/it]  4%|â–Ž         | 213/6000 [07:00<3:10:02,  1.97s/it]                                                    {'loss': 3.5393, 'grad_norm': 409.3009338378906, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:00<3:10:02,  1.97s/it]  4%|â–Ž         | 214/6000 [07:02<3:07:53,  1.95s/it]                                                    {'loss': 3.7457, 'grad_norm': 2955.45068359375, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:02<3:07:53,  1.95s/it]  4%|â–Ž         | 215/6000 [07:04<3:08:39,  1.96s/it]                                                    {'loss': 3.6676, 'grad_norm': 78.82025909423828, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:04<3:08:39,  1.96s/it]  4%|â–Ž         | 216/6000 [07:06<3:08:19,  1.95s/it]                                                    {'loss': 3.5189, 'grad_norm': 26.448463439941406, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:06<3:08:19,  1.95s/it]  4%|â–Ž         | 217/6000 [07:08<3:08:35,  1.96s/it]                                                    {'loss': 3.5369, 'grad_norm': 181.76220703125, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:08<3:08:35,  1.96s/it]  4%|â–Ž         | 218/6000 [07:10<3:07:56,  1.95s/it]                                                    {'loss': 3.5868, 'grad_norm': 376.3565368652344, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:10<3:07:56,  1.95s/it]  4%|â–Ž         | 219/6000 [07:12<3:07:30,  1.95s/it]                                                    {'loss': 3.6026, 'grad_norm': 391.4672546386719, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:12<3:07:30,  1.95s/it]  4%|â–Ž         | 220/6000 [07:14<3:08:12,  1.95s/it]                                                    {'loss': 3.5282, 'grad_norm': 190.677734375, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:14<3:08:12,  1.95s/it]  4%|â–Ž         | 221/6000 [07:16<3:07:17,  1.94s/it]                                                    {'loss': 3.5505, 'grad_norm': 192.11643981933594, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:16<3:07:17,  1.94s/it]  4%|â–Ž         | 222/6000 [07:18<3:07:00,  1.94s/it]                                                    {'loss': 3.5037, 'grad_norm': 14.952398300170898, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:18<3:07:00,  1.94s/it]  4%|â–Ž         | 223/6000 [07:20<3:07:48,  1.95s/it]                                                    {'loss': 3.5099, 'grad_norm': 38.44029998779297, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:20<3:07:48,  1.95s/it]  4%|â–Ž         | 224/6000 [07:21<3:05:57,  1.93s/it]                                                    {'loss': 3.4796, 'grad_norm': 16.711231231689453, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:21<3:05:57,  1.93s/it]  4%|â–         | 225/6000 [07:23<3:07:29,  1.95s/it]                                                    {'loss': 3.4819, 'grad_norm': 50.948177337646484, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:23<3:07:29,  1.95s/it]  4%|â–         | 226/6000 [07:26<3:11:04,  1.99s/it]                                                    {'loss': 3.5897, 'grad_norm': 29.8061466217041, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:26<3:11:04,  1.99s/it]  4%|â–         | 227/6000 [07:27<3:08:53,  1.96s/it]                                                    {'loss': 3.5117, 'grad_norm': 36.2681884765625, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:27<3:08:53,  1.96s/it]  4%|â–         | 228/6000 [07:29<3:09:00,  1.96s/it]                                                    {'loss': 3.5158, 'grad_norm': 193.21983337402344, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:29<3:09:00,  1.96s/it]  4%|â–         | 229/6000 [07:31<3:08:24,  1.96s/it]                                                    {'loss': 3.5573, 'grad_norm': 41.381221771240234, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:31<3:08:24,  1.96s/it]  4%|â–         | 230/6000 [07:33<3:08:46,  1.96s/it]                                                    {'loss': 3.4879, 'grad_norm': 24.10905647277832, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:33<3:08:46,  1.96s/it]  4%|â–         | 231/6000 [07:35<3:06:45,  1.94s/it]                                                    {'loss': 3.4913, 'grad_norm': 14.646157264709473, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:35<3:06:45,  1.94s/it]  4%|â–         | 232/6000 [07:37<3:07:16,  1.95s/it]                                                    {'loss': 3.5193, 'grad_norm': 156.634033203125, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:37<3:07:16,  1.95s/it]  4%|â–         | 233/6000 [07:39<3:09:07,  1.97s/it]                                                    {'loss': 3.4963, 'grad_norm': 86.29942321777344, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:39<3:09:07,  1.97s/it]  4%|â–         | 234/6000 [07:41<3:09:29,  1.97s/it]                                                    {'loss': 3.5019, 'grad_norm': 57.87317657470703, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:41<3:09:29,  1.97s/it]  4%|â–         | 235/6000 [07:43<3:06:56,  1.95s/it]                                                    {'loss': 3.4938, 'grad_norm': 33.15868377685547, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:43<3:06:56,  1.95s/it]  4%|â–         | 236/6000 [07:45<3:13:27,  2.01s/it]                                                    {'loss': 3.485, 'grad_norm': 10.975972175598145, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:45<3:13:27,  2.01s/it]  4%|â–         | 237/6000 [07:47<3:12:05,  2.00s/it]                                                    {'loss': 3.472, 'grad_norm': 84.93614196777344, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:47<3:12:05,  2.00s/it]  4%|â–         | 238/6000 [07:49<3:15:00,  2.03s/it]                                                    {'loss': 3.4821, 'grad_norm': 16.335338592529297, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [07:49<3:15:00,  2.03s/it]  4%|â–         | 239/6000 [07:51<3:14:08,  2.02s/it]                                                    {'loss': 3.4774, 'grad_norm': 4.831114292144775, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [07:51<3:14:08,  2.02s/it]  4%|â–         | 240/6000 [07:53<3:10:44,  1.99s/it]                                                    {'loss': 3.4907, 'grad_norm': 13.850772857666016, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [07:53<3:10:44,  1.99s/it]  4%|â–         | 241/6000 [07:55<3:10:46,  1.99s/it]                                                    {'loss': 3.488, 'grad_norm': 21.281862258911133, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [07:55<3:10:46,  1.99s/it]  4%|â–         | 242/6000 [07:57<3:10:57,  1.99s/it]                                                    {'loss': 3.4846, 'grad_norm': 24.682079315185547, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [07:57<3:10:57,  1.99s/it]  4%|â–         | 243/6000 [07:59<3:10:51,  1.99s/it]                                                    {'loss': 3.4705, 'grad_norm': 17.368165969848633, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [07:59<3:10:51,  1.99s/it]  4%|â–         | 244/6000 [08:01<3:10:06,  1.98s/it]                                                    {'loss': 3.4835, 'grad_norm': 94.19524383544922, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:01<3:10:06,  1.98s/it]  4%|â–         | 245/6000 [08:03<3:13:37,  2.02s/it]                                                    {'loss': 3.5197, 'grad_norm': 11.25446891784668, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:03<3:13:37,  2.02s/it]  4%|â–         | 246/6000 [08:05<3:12:21,  2.01s/it]                                                    {'loss': 3.4876, 'grad_norm': 21.18503189086914, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:05<3:12:21,  2.01s/it]  4%|â–         | 247/6000 [08:07<3:12:51,  2.01s/it]                                                    {'loss': 3.507, 'grad_norm': 19.031646728515625, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:07<3:12:51,  2.01s/it]  4%|â–         | 248/6000 [08:09<3:12:28,  2.01s/it]                                                    {'loss': 3.502, 'grad_norm': 16.655284881591797, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:09<3:12:28,  2.01s/it]  4%|â–         | 249/6000 [08:11<3:09:23,  1.98s/it]                                                    {'loss': 3.5333, 'grad_norm': 19.785160064697266, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:11<3:09:23,  1.98s/it]  4%|â–         | 250/6000 [08:13<3:07:40,  1.96s/it]                                                    {'loss': 3.4918, 'grad_norm': 16.22850799560547, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:13<3:07:40,  1.96s/it]  4%|â–         | 251/6000 [08:15<3:05:18,  1.93s/it]                                                    {'loss': 3.5048, 'grad_norm': 82.61131286621094, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:15<3:05:18,  1.93s/it]  4%|â–         | 252/6000 [08:17<3:08:01,  1.96s/it]                                                    {'loss': 3.4766, 'grad_norm': 43.003639221191406, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:17<3:08:01,  1.96s/it]  4%|â–         | 253/6000 [08:19<3:06:28,  1.95s/it]                                                    {'loss': 3.4801, 'grad_norm': 19.334766387939453, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:19<3:06:28,  1.95s/it]  4%|â–         | 254/6000 [08:21<3:07:14,  1.96s/it]                                                    {'loss': 3.4853, 'grad_norm': 7.348148822784424, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:21<3:07:14,  1.96s/it]  4%|â–         | 255/6000 [08:23<3:06:34,  1.95s/it]                                                    {'loss': 3.5347, 'grad_norm': 26.151561737060547, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:23<3:06:34,  1.95s/it]  4%|â–         | 256/6000 [08:25<3:09:24,  1.98s/it]                                                    {'loss': 3.4818, 'grad_norm': 23.1387939453125, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:25<3:09:24,  1.98s/it]  4%|â–         | 257/6000 [08:27<3:09:35,  1.98s/it]                                                    {'loss': 3.4901, 'grad_norm': 22.471324920654297, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:27<3:09:35,  1.98s/it]  4%|â–         | 258/6000 [08:29<3:07:07,  1.96s/it]                                                    {'loss': 3.4693, 'grad_norm': 7.997613430023193, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:29<3:07:07,  1.96s/it]  4%|â–         | 259/6000 [08:31<3:05:04,  1.93s/it]                                                    {'loss': 3.4907, 'grad_norm': 9.018681526184082, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:31<3:05:04,  1.93s/it]  4%|â–         | 260/6000 [08:33<3:06:28,  1.95s/it]                                                    {'loss': 3.4899, 'grad_norm': 8.382928848266602, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:33<3:06:28,  1.95s/it]  4%|â–         | 261/6000 [08:35<3:07:38,  1.96s/it]                                                    {'loss': 3.4847, 'grad_norm': 38.87708282470703, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:35<3:07:38,  1.96s/it]  4%|â–         | 262/6000 [08:37<3:07:24,  1.96s/it]                                                    {'loss': 3.462, 'grad_norm': 20.957792282104492, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:37<3:07:24,  1.96s/it]  4%|â–         | 263/6000 [08:39<3:09:41,  1.98s/it]                                                    {'loss': 3.4858, 'grad_norm': 27.369686126708984, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:39<3:09:41,  1.98s/it]  4%|â–         | 264/6000 [08:40<3:07:57,  1.97s/it]                                                    {'loss': 3.4701, 'grad_norm': 77.47396087646484, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:40<3:07:57,  1.97s/it]  4%|â–         | 265/6000 [08:42<3:08:44,  1.97s/it]                                                    {'loss': 3.5052, 'grad_norm': 687.881591796875, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:42<3:08:44,  1.97s/it]  4%|â–         | 266/6000 [08:44<3:09:15,  1.98s/it]                                                    {'loss': 3.4694, 'grad_norm': 94.95194244384766, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:44<3:09:15,  1.98s/it]  4%|â–         | 267/6000 [08:46<3:07:22,  1.96s/it]                                                    {'loss': 3.5139, 'grad_norm': 562.8545532226562, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [08:46<3:07:22,  1.96s/it]  4%|â–         | 268/6000 [08:48<3:07:11,  1.96s/it]                                                    {'loss': 3.4693, 'grad_norm': 20.858957290649414, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [08:48<3:07:11,  1.96s/it]  4%|â–         | 269/6000 [08:50<3:08:28,  1.97s/it]                                                    {'loss': 3.4756, 'grad_norm': 5.160262584686279, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [08:50<3:08:28,  1.97s/it]  4%|â–         | 270/6000 [08:52<3:08:15,  1.97s/it]                                                    {'loss': 3.4882, 'grad_norm': 10.46925163269043, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [08:52<3:08:15,  1.97s/it]  5%|â–         | 271/6000 [08:54<3:06:00,  1.95s/it]                                                    {'loss': 3.4738, 'grad_norm': 6.102543830871582, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [08:54<3:06:00,  1.95s/it]  5%|â–         | 272/6000 [08:56<3:05:46,  1.95s/it]                                                    {'loss': 3.4958, 'grad_norm': 33.28712463378906, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [08:56<3:05:46,  1.95s/it]  5%|â–         | 273/6000 [08:58<3:05:58,  1.95s/it]                                                    {'loss': 3.5007, 'grad_norm': 20.350391387939453, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [08:58<3:05:58,  1.95s/it]  5%|â–         | 274/6000 [09:00<3:07:51,  1.97s/it]                                                    {'loss': 3.462, 'grad_norm': 5.5015716552734375, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:00<3:07:51,  1.97s/it]  5%|â–         | 275/6000 [09:02<3:09:19,  1.98s/it]                                                    {'loss': 3.4796, 'grad_norm': 42.095733642578125, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:02<3:09:19,  1.98s/it]  5%|â–         | 276/6000 [09:04<3:09:25,  1.99s/it]                                                    {'loss': 3.4816, 'grad_norm': 31.62367057800293, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:04<3:09:25,  1.99s/it]  5%|â–         | 277/6000 [09:06<3:06:32,  1.96s/it]                                                    {'loss': 3.4867, 'grad_norm': 22.427892684936523, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:06<3:06:32,  1.96s/it]  5%|â–         | 278/6000 [09:08<3:04:57,  1.94s/it]                                                    {'loss': 3.4733, 'grad_norm': 8.732654571533203, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:08<3:04:57,  1.94s/it]  5%|â–         | 279/6000 [09:10<3:06:15,  1.95s/it]                                                    {'loss': 3.4709, 'grad_norm': 14.454117774963379, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:10<3:06:15,  1.95s/it]  5%|â–         | 280/6000 [09:12<3:04:56,  1.94s/it]                                                    {'loss': 3.4697, 'grad_norm': 7.294074058532715, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:12<3:04:56,  1.94s/it]  5%|â–         | 281/6000 [09:14<3:07:38,  1.97s/it]                                                    {'loss': 3.49, 'grad_norm': 50.78936004638672, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:14<3:07:38,  1.97s/it]  5%|â–         | 282/6000 [09:16<3:12:10,  2.02s/it]                                                    {'loss': 3.4757, 'grad_norm': 14.062481880187988, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:16<3:12:10,  2.02s/it]  5%|â–         | 283/6000 [09:18<3:11:10,  2.01s/it]                                                    {'loss': 3.4799, 'grad_norm': 26.417118072509766, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:18<3:11:10,  2.01s/it]  5%|â–         | 284/6000 [09:20<3:16:17,  2.06s/it]                                                    {'loss': 3.4754, 'grad_norm': 11.422682762145996, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:20<3:16:17,  2.06s/it]  5%|â–         | 285/6000 [09:22<3:15:16,  2.05s/it]                                                    {'loss': 3.4719, 'grad_norm': 15.202619552612305, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:22<3:15:16,  2.05s/it]  5%|â–         | 286/6000 [09:24<3:10:58,  2.01s/it]                                                    {'loss': 3.48, 'grad_norm': 10.413142204284668, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:24<3:10:58,  2.01s/it]  5%|â–         | 287/6000 [09:26<3:11:02,  2.01s/it]                                                    {'loss': 3.4714, 'grad_norm': 7.600481033325195, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:26<3:11:02,  2.01s/it]  5%|â–         | 288/6000 [09:28<3:08:07,  1.98s/it]                                                    {'loss': 3.4712, 'grad_norm': 7.681769371032715, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:28<3:08:07,  1.98s/it]  5%|â–         | 289/6000 [09:30<3:08:04,  1.98s/it]                                                    {'loss': 3.4827, 'grad_norm': 21.133146286010742, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:30<3:08:04,  1.98s/it]  5%|â–         | 290/6000 [09:32<3:08:00,  1.98s/it]                                                    {'loss': 3.4701, 'grad_norm': 5.543783664703369, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:32<3:08:00,  1.98s/it]  5%|â–         | 291/6000 [09:34<3:06:07,  1.96s/it]                                                    {'loss': 3.4744, 'grad_norm': 37.73650360107422, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:34<3:06:07,  1.96s/it]  5%|â–         | 292/6000 [09:36<3:04:06,  1.94s/it]                                                    {'loss': 3.4792, 'grad_norm': 18.879833221435547, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:36<3:04:06,  1.94s/it]  5%|â–         | 293/6000 [09:38<3:03:44,  1.93s/it]                                                    {'loss': 3.4744, 'grad_norm': 6.45536994934082, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:38<3:03:44,  1.93s/it]  5%|â–         | 294/6000 [09:40<3:03:40,  1.93s/it]                                                    {'loss': 3.4758, 'grad_norm': 11.037309646606445, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:40<3:03:40,  1.93s/it]  5%|â–         | 295/6000 [09:41<3:02:05,  1.92s/it]                                                    {'loss': 3.4664, 'grad_norm': 10.403305053710938, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:41<3:02:05,  1.92s/it]  5%|â–         | 296/6000 [09:43<3:04:06,  1.94s/it]                                                    {'loss': 3.4656, 'grad_norm': 16.02619171142578, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [09:43<3:04:06,  1.94s/it]  5%|â–         | 297/6000 [09:45<3:02:48,  1.92s/it]                                                    {'loss': 3.4704, 'grad_norm': 40.947593688964844, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [09:45<3:02:48,  1.92s/it]  5%|â–         | 298/6000 [09:47<3:04:44,  1.94s/it]                                                    {'loss': 3.5019, 'grad_norm': 91.60181427001953, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [09:47<3:04:44,  1.94s/it]  5%|â–         | 299/6000 [09:50<3:15:00,  2.05s/it]                                                    {'loss': 3.4969, 'grad_norm': 85.7973861694336, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [09:50<3:15:00,  2.05s/it]  5%|â–Œ         | 300/6000 [09:52<3:13:42,  2.04s/it]                                                    {'loss': 3.4803, 'grad_norm': 43.02547836303711, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [09:52<3:13:42,  2.04s/it]  5%|â–Œ         | 301/6000 [09:54<3:11:23,  2.02s/it]                                                    {'loss': 3.4819, 'grad_norm': 38.138370513916016, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [09:54<3:11:23,  2.02s/it]  5%|â–Œ         | 302/6000 [09:56<3:13:05,  2.03s/it]                                                    {'loss': 3.471, 'grad_norm': 18.082778930664062, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [09:56<3:13:05,  2.03s/it]  5%|â–Œ         | 303/6000 [09:58<3:12:17,  2.03s/it]                                                    {'loss': 3.4703, 'grad_norm': 7.001375675201416, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [09:58<3:12:17,  2.03s/it]  5%|â–Œ         | 304/6000 [10:00<3:09:36,  2.00s/it]                                                    {'loss': 3.4595, 'grad_norm': 11.313715934753418, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:00<3:09:36,  2.00s/it]  5%|â–Œ         | 305/6000 [10:02<3:08:12,  1.98s/it]                                                    {'loss': 3.716, 'grad_norm': 4007.462890625, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:02<3:08:12,  1.98s/it]  5%|â–Œ         | 306/6000 [10:03<3:05:27,  1.95s/it]                                                    {'loss': 6.2974, 'grad_norm': 12513.5263671875, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:03<3:05:27,  1.95s/it]  5%|â–Œ         | 307/6000 [10:05<3:05:03,  1.95s/it]                                                    {'loss': 5.0883, 'grad_norm': 18272.896484375, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:05<3:05:03,  1.95s/it]  5%|â–Œ         | 308/6000 [10:07<3:03:35,  1.94s/it]                                                    {'loss': 4.4792, 'grad_norm': 14948.1826171875, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:07<3:03:35,  1.94s/it]  5%|â–Œ         | 309/6000 [10:09<3:07:05,  1.97s/it]                                                    {'loss': 4.7607, 'grad_norm': 6537.9521484375, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:09<3:07:05,  1.97s/it]  5%|â–Œ         | 310/6000 [10:11<3:05:29,  1.96s/it]                                                    {'loss': 3.4775, 'grad_norm': 76.82827758789062, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:11<3:05:29,  1.96s/it]  5%|â–Œ         | 311/6000 [10:13<3:04:40,  1.95s/it]                                                    {'loss': 3.4772, 'grad_norm': 42.59223556518555, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:13<3:04:40,  1.95s/it]  5%|â–Œ         | 312/6000 [10:15<3:04:44,  1.95s/it]                                                    {'loss': 3.4729, 'grad_norm': 13.612364768981934, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:15<3:04:44,  1.95s/it]  5%|â–Œ         | 313/6000 [10:17<3:05:19,  1.96s/it]                                                    {'loss': 3.4716, 'grad_norm': 8.870320320129395, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:17<3:05:19,  1.96s/it]  5%|â–Œ         | 314/6000 [10:19<3:04:16,  1.94s/it]                                                    {'loss': 3.4693, 'grad_norm': 22.610429763793945, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:19<3:04:16,  1.94s/it]  5%|â–Œ         | 315/6000 [10:21<3:05:35,  1.96s/it]                                                    {'loss': 3.4947, 'grad_norm': 135.0072021484375, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:21<3:05:35,  1.96s/it]  5%|â–Œ         | 316/6000 [10:23<3:05:27,  1.96s/it]                                                    {'loss': 3.6649, 'grad_norm': 556.764404296875, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:23<3:05:27,  1.96s/it]  5%|â–Œ         | 317/6000 [10:25<3:05:27,  1.96s/it]                                                    {'loss': 3.5505, 'grad_norm': 328.47308349609375, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:25<3:05:27,  1.96s/it]  5%|â–Œ         | 318/6000 [10:27<3:11:54,  2.03s/it]                                                    {'loss': 3.6717, 'grad_norm': 640.3939819335938, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:27<3:11:54,  2.03s/it]  5%|â–Œ         | 319/6000 [10:29<3:09:05,  2.00s/it]                                                    {'loss': 3.5574, 'grad_norm': 329.81671142578125, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:29<3:09:05,  2.00s/it]  5%|â–Œ         | 320/6000 [10:31<3:09:50,  2.01s/it]                                                    {'loss': 3.4496, 'grad_norm': 24.549226760864258, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:31<3:09:50,  2.01s/it]  5%|â–Œ         | 321/6000 [10:33<3:11:42,  2.03s/it]                                                    {'loss': 3.4623, 'grad_norm': 23.24203109741211, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:33<3:11:42,  2.03s/it]  5%|â–Œ         | 322/6000 [10:35<3:07:36,  1.98s/it]                                                    {'loss': 3.4818, 'grad_norm': 20.740646362304688, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:35<3:07:36,  1.98s/it]  5%|â–Œ         | 323/6000 [10:37<3:12:06,  2.03s/it]                                                    {'loss': 3.4755, 'grad_norm': 7.341897487640381, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:37<3:12:06,  2.03s/it]  5%|â–Œ         | 324/6000 [10:39<3:08:16,  1.99s/it]                                                    {'loss': 3.4741, 'grad_norm': 15.247342109680176, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:39<3:08:16,  1.99s/it]  5%|â–Œ         | 325/6000 [10:41<3:06:09,  1.97s/it]                                                    {'loss': 3.4706, 'grad_norm': 14.252025604248047, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:41<3:06:09,  1.97s/it]  5%|â–Œ         | 326/6000 [10:43<3:05:14,  1.96s/it]                                                    {'loss': 3.496, 'grad_norm': 83.92326354980469, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [10:43<3:05:14,  1.96s/it]  5%|â–Œ         | 327/6000 [10:45<3:02:14,  1.93s/it]                                                    {'loss': 3.4375, 'grad_norm': 194.22825622558594, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [10:45<3:02:14,  1.93s/it]  5%|â–Œ         | 328/6000 [10:47<3:02:40,  1.93s/it]                                                    {'loss': 3.473, 'grad_norm': 27.937164306640625, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [10:47<3:02:40,  1.93s/it]  5%|â–Œ         | 329/6000 [10:49<3:08:13,  1.99s/it]                                                    {'loss': 3.5077, 'grad_norm': 251.12203979492188, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [10:49<3:08:13,  1.99s/it]  6%|â–Œ         | 330/6000 [10:51<3:06:35,  1.97s/it]                                                    {'loss': 3.4621, 'grad_norm': 36.288414001464844, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [10:51<3:06:35,  1.97s/it]  6%|â–Œ         | 331/6000 [10:53<3:04:50,  1.96s/it]                                                    {'loss': 3.4798, 'grad_norm': 64.10245513916016, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [10:53<3:04:50,  1.96s/it]  6%|â–Œ         | 332/6000 [10:55<3:03:05,  1.94s/it]                                                    {'loss': 3.4911, 'grad_norm': 185.03009033203125, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [10:55<3:03:05,  1.94s/it]  6%|â–Œ         | 333/6000 [10:57<3:02:22,  1.93s/it]                                                    {'loss': 3.4758, 'grad_norm': 13.00861930847168, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [10:57<3:02:22,  1.93s/it]  6%|â–Œ         | 334/6000 [10:58<3:02:53,  1.94s/it]                                                    {'loss': 3.4877, 'grad_norm': 13.87495231628418, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [10:58<3:02:53,  1.94s/it]  6%|â–Œ         | 335/6000 [11:00<3:04:26,  1.95s/it]                                                    {'loss': 3.5054, 'grad_norm': 8.710225105285645, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:00<3:04:26,  1.95s/it]  6%|â–Œ         | 336/6000 [11:02<3:03:13,  1.94s/it]                                                    {'loss': 3.4806, 'grad_norm': 6.158807277679443, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:02<3:03:13,  1.94s/it]  6%|â–Œ         | 337/6000 [11:04<3:02:40,  1.94s/it]                                                    {'loss': 3.4716, 'grad_norm': 5.675127029418945, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [11:04<3:02:40,  1.94s/it]  6%|â–Œ         | 338/6000 [11:06<3:01:19,  1.92s/it]                                                    {'loss': 3.4742, 'grad_norm': 8.238582611083984, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [11:06<3:01:19,  1.92s/it]  6%|â–Œ         | 339/6000 [11:08<3:01:59,  1.93s/it]                                                    {'loss': 3.4703, 'grad_norm': 5.297203063964844, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [11:08<3:01:59,  1.93s/it]  6%|â–Œ         | 340/6000 [11:10<3:03:51,  1.95s/it]                                                    {'loss': 3.4686, 'grad_norm': 22.591827392578125, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [11:10<3:03:51,  1.95s/it]  6%|â–Œ         | 341/6000 [11:12<3:02:12,  1.93s/it]                                                    {'loss': 3.5055, 'grad_norm': 18.3056583404541, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [11:12<3:02:12,  1.93s/it]  6%|â–Œ         | 342/6000 [11:14<3:02:22,  1.93s/it]                                                    {'loss': 3.4827, 'grad_norm': 12.900078773498535, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [11:14<3:02:22,  1.93s/it]  6%|â–Œ         | 343/6000 [11:16<3:09:19,  2.01s/it]                                                    {'loss': 3.4636, 'grad_norm': 14.752486228942871, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [11:16<3:09:19,  2.01s/it]  6%|â–Œ         | 344/6000 [11:18<3:06:54,  1.98s/it]                                                    {'loss': 3.4838, 'grad_norm': 65.39918518066406, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [11:18<3:06:54,  1.98s/it]  6%|â–Œ         | 345/6000 [11:20<3:05:56,  1.97s/it]                                                    {'loss': 3.4454, 'grad_norm': 31.610382080078125, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [11:20<3:05:56,  1.97s/it]  6%|â–Œ         | 346/6000 [11:22<3:04:55,  1.96s/it]                                                    {'loss': 3.5631, 'grad_norm': 1311.1981201171875, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [11:22<3:04:55,  1.96s/it]  6%|â–Œ         | 347/6000 [11:24<3:03:18,  1.95s/it]                                                    {'loss': 3.6606, 'grad_norm': 1809.35302734375, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [11:24<3:03:18,  1.95s/it]  6%|â–Œ         | 348/6000 [11:26<3:03:28,  1.95s/it]                                                    {'loss': 3.586, 'grad_norm': 1330.7388916015625, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [11:26<3:03:28,  1.95s/it]  6%|â–Œ         | 349/6000 [11:28<3:02:38,  1.94s/it]                                                    {'loss': 3.5047, 'grad_norm': 99.2926025390625, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [11:28<3:02:38,  1.94s/it]  6%|â–Œ         | 350/6000 [11:30<3:01:55,  1.93s/it]                                                    {'loss': 3.4831, 'grad_norm': 55.98316192626953, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [11:30<3:01:55,  1.93s/it]  6%|â–Œ         | 351/6000 [11:32<3:04:16,  1.96s/it]                                                    {'loss': 3.49, 'grad_norm': 23.20351219177246, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [11:32<3:04:16,  1.96s/it]  6%|â–Œ         | 352/6000 [11:34<3:01:17,  1.93s/it]                                                    {'loss': 3.4813, 'grad_norm': 6.3597002029418945, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [11:34<3:01:17,  1.93s/it]  6%|â–Œ         | 353/6000 [11:35<3:00:22,  1.92s/it]                                                    {'loss': 3.483, 'grad_norm': 31.875539779663086, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [11:35<3:00:22,  1.92s/it]  6%|â–Œ         | 354/6000 [11:37<3:01:01,  1.92s/it]                                                    {'loss': 3.4881, 'grad_norm': 49.137367248535156, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [11:37<3:01:01,  1.92s/it]  6%|â–Œ         | 355/6000 [11:39<3:00:56,  1.92s/it]                                                    {'loss': 3.4638, 'grad_norm': 13.65654182434082, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [11:39<3:00:56,  1.92s/it]  6%|â–Œ         | 356/6000 [11:41<3:03:28,  1.95s/it]                                                    {'loss': 3.497, 'grad_norm': 76.81350708007812, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [11:41<3:03:28,  1.95s/it]  6%|â–Œ         | 357/6000 [11:43<3:02:51,  1.94s/it]                                                    {'loss': 3.4779, 'grad_norm': 59.775665283203125, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [11:43<3:02:51,  1.94s/it]  6%|â–Œ         | 358/6000 [11:45<3:00:42,  1.92s/it]                                                    {'loss': 3.4833, 'grad_norm': 26.03365707397461, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [11:45<3:00:42,  1.92s/it]  6%|â–Œ         | 359/6000 [11:47<3:00:53,  1.92s/it]                                                    {'loss': 3.4695, 'grad_norm': 7.658762454986572, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [11:47<3:00:53,  1.92s/it]  6%|â–Œ         | 360/6000 [11:49<3:01:19,  1.93s/it]                                                    {'loss': 3.4786, 'grad_norm': 10.282552719116211, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [11:49<3:01:19,  1.93s/it]  6%|â–Œ         | 361/6000 [11:51<3:00:45,  1.92s/it]                                                    {'loss': 3.4711, 'grad_norm': 5.649621963500977, 'learning_rate': 9.557627118644068e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [11:51<3:00:45,  1.92s/it]  6%|â–Œ         | 362/6000 [11:53<2:59:14,  1.91s/it]                                                    {'loss': 3.476, 'grad_norm': 15.794502258300781, 'learning_rate': 9.555932203389831e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [11:53<2:59:14,  1.91s/it]  6%|â–Œ         | 363/6000 [11:55<2:58:28,  1.90s/it]                                                    {'loss': 3.4771, 'grad_norm': 5.8078532218933105, 'learning_rate': 9.554237288135594e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [11:55<2:58:28,  1.90s/it]  6%|â–Œ         | 364/6000 [11:57<2:59:45,  1.91s/it]                                                    {'loss': 3.4876, 'grad_norm': 7.0011725425720215, 'learning_rate': 9.552542372881358e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [11:57<2:59:45,  1.91s/it]  6%|â–Œ         | 365/6000 [11:59<3:00:20,  1.92s/it]                                                    {'loss': 3.4701, 'grad_norm': 9.359773635864258, 'learning_rate': 9.55084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [11:59<3:00:20,  1.92s/it]  6%|â–Œ         | 366/6000 [12:00<2:59:40,  1.91s/it]                                                    {'loss': 3.4687, 'grad_norm': 11.378073692321777, 'learning_rate': 9.549152542372883e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [12:00<2:59:40,  1.91s/it]  6%|â–Œ         | 367/6000 [12:02<3:02:28,  1.94s/it]                                                    {'loss': 3.4746, 'grad_norm': 4.438503742218018, 'learning_rate': 9.547457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [12:02<3:02:28,  1.94s/it]  6%|â–Œ         | 368/6000 [12:04<3:06:05,  1.98s/it]                                                    {'loss': 3.4683, 'grad_norm': 4.501437187194824, 'learning_rate': 9.545762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [12:04<3:06:05,  1.98s/it]  6%|â–Œ         | 369/6000 [12:06<3:04:31,  1.97s/it]                                                    {'loss': 3.4775, 'grad_norm': 26.63028907775879, 'learning_rate': 9.54406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [12:06<3:04:31,  1.97s/it]  6%|â–Œ         | 370/6000 [12:08<3:06:15,  1.98s/it]                                                    {'loss': 3.4546, 'grad_norm': 15.943325996398926, 'learning_rate': 9.542372881355934e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [12:08<3:06:15,  1.98s/it]  6%|â–Œ         | 371/6000 [12:10<3:04:09,  1.96s/it]                                                    {'loss': 3.4739, 'grad_norm': 21.95086669921875, 'learning_rate': 9.540677966101696e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [12:10<3:04:09,  1.96s/it]  6%|â–Œ         | 372/6000 [12:12<3:05:33,  1.98s/it]                                                    {'loss': 3.4786, 'grad_norm': 46.37590789794922, 'learning_rate': 9.538983050847457e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [12:12<3:05:33,  1.98s/it]  6%|â–Œ         | 373/6000 [12:14<3:04:07,  1.96s/it]                                                    {'loss': 3.4787, 'grad_norm': 17.18350601196289, 'learning_rate': 9.53728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [12:14<3:04:07,  1.96s/it]  6%|â–Œ         | 374/6000 [12:16<3:05:20,  1.98s/it]                                                    {'loss': 3.4843, 'grad_norm': 27.603235244750977, 'learning_rate': 9.535593220338984e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [12:16<3:05:20,  1.98s/it]  6%|â–‹         | 375/6000 [12:18<3:02:42,  1.95s/it]                                                    {'loss': 3.4788, 'grad_norm': 23.40658950805664, 'learning_rate': 9.533898305084747e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [12:18<3:02:42,  1.95s/it]  6%|â–‹         | 376/6000 [12:20<3:00:37,  1.93s/it]                                                    {'loss': 3.4641, 'grad_norm': 4.66377067565918, 'learning_rate': 9.532203389830508e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [12:20<3:00:37,  1.93s/it]  6%|â–‹         | 377/6000 [12:22<3:02:11,  1.94s/it]                                                    {'loss': 3.47, 'grad_norm': 9.444315910339355, 'learning_rate': 9.530508474576272e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [12:22<3:02:11,  1.94s/it]  6%|â–‹         | 378/6000 [12:24<3:03:43,  1.96s/it]                                                    {'loss': 3.4968, 'grad_norm': 39.18149948120117, 'learning_rate': 9.528813559322035e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [12:24<3:03:43,  1.96s/it]  6%|â–‹         | 379/6000 [12:26<3:03:10,  1.96s/it]                                                    {'loss': 3.4885, 'grad_norm': 285.0472412109375, 'learning_rate': 9.527118644067798e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [12:26<3:03:10,  1.96s/it]  6%|â–‹         | 380/6000 [12:28<3:04:01,  1.96s/it]                                                    {'loss': 3.7188, 'grad_norm': 3490.4599609375, 'learning_rate': 9.52542372881356e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [12:28<3:04:01,  1.96s/it]  6%|â–‹         | 381/6000 [12:30<3:02:36,  1.95s/it]                                                    {'loss': 4.4639, 'grad_norm': 4796.9267578125, 'learning_rate': 9.523728813559323e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [12:30<3:02:36,  1.95s/it]  6%|â–‹         | 382/6000 [12:32<3:01:42,  1.94s/it]                                                    {'loss': 3.5353, 'grad_norm': 655.6858520507812, 'learning_rate': 9.522033898305085e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [12:32<3:01:42,  1.94s/it]  6%|â–‹         | 383/6000 [12:34<3:02:21,  1.95s/it]                                                    {'loss': 3.6338, 'grad_norm': 223.05906677246094, 'learning_rate': 9.520338983050848e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [12:34<3:02:21,  1.95s/it]  6%|â–‹         | 384/6000 [12:36<3:04:37,  1.97s/it]                                                    {'loss': 3.4778, 'grad_norm': 8.211111068725586, 'learning_rate': 9.518644067796611e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [12:36<3:04:37,  1.97s/it]  6%|â–‹         | 385/6000 [12:38<3:04:40,  1.97s/it]                                                    {'loss': 3.4697, 'grad_norm': 10.288857460021973, 'learning_rate': 9.516949152542375e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [12:38<3:04:40,  1.97s/it]