==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 10:37:22.438000 124167153067840 torch/distributed/run.py:779] 
W1118 10:37:22.438000 124167153067840 torch/distributed/run.py:779] *****************************************
W1118 10:37:22.438000 124167153067840 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 10:37:22.438000 124167153067840 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 10:37:29,762] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.80it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: setting up run mek74pmg
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_103730-mek74pmg
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/mek74pmg
[2025-11-18 10:37:31,949] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.41it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-18 10:37:32,467] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGenerationWithTail(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 10:37:38,260] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 10:37:39,942] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 10:37:39,943] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 10:37:43,500] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 10:37:43,501] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 10:37:44,384] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 10:37:44,384] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 10:37:44,384] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 10:37:44,385] INFO [src.utils:19] ==================================================
[2025-11-18 10:37:44,385] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 10:37:44,386] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 10:37:44,386] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 10:37:44,386] INFO [src.utils:19] ==================================================
[2025-11-18 10:37:46,105] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 10:37:46,105] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 10:37:46,106] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 10:37:46,106] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 10:37:46,106] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 10:37:46,106] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 10:37:46,106] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 10:37:46,106] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 10:37:46,106] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 10:37:46,106] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 10:37:46,107] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 10:37:46,107] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 10:37:46,107] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 10:37:46,107] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 10:37:46,109] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 10:37:46,110] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 10:37:46,112] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 10:37:46,113] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:03<5:42:36,  3.43s/it]                                                  {'loss': 21.0466, 'grad_norm': 1042.1378173828125, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:42:36,  3.43s/it]  0%|          | 2/6000 [00:05<4:15:52,  2.56s/it]                                                  {'loss': 17.8246, 'grad_norm': 1045.6488037109375, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:15:52,  2.56s/it]  0%|          | 3/6000 [00:07<3:47:35,  2.28s/it]                                                  {'loss': 16.8166, 'grad_norm': 1388.0048828125, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:47:35,  2.28s/it]  0%|          | 4/6000 [00:09<3:38:32,  2.19s/it]                                                  {'loss': 18.3273, 'grad_norm': 1321.69140625, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:38:32,  2.19s/it]  0%|          | 5/6000 [00:11<3:30:53,  2.11s/it]                                                  {'loss': 18.9043, 'grad_norm': 1433.0794677734375, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:30:53,  2.11s/it]  0%|          | 6/6000 [00:13<3:24:43,  2.05s/it]                                                  {'loss': 18.7149, 'grad_norm': 967.0491943359375, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:24:43,  2.05s/it]  0%|          | 7/6000 [00:15<3:22:21,  2.03s/it]                                                  {'loss': 18.9383, 'grad_norm': 1499.6026611328125, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:22:21,  2.03s/it]  0%|          | 8/6000 [00:17<3:20:04,  2.00s/it]                                                  {'loss': 18.7708, 'grad_norm': 1699.463623046875, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:17<3:20:04,  2.00s/it]  0%|          | 9/6000 [00:19<3:19:15,  2.00s/it]                                                  {'loss': 15.3432, 'grad_norm': 1145.921630859375, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:19<3:19:15,  2.00s/it]  0%|          | 10/6000 [00:21<3:19:29,  2.00s/it]                                                   {'loss': 18.748, 'grad_norm': 980.1192626953125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:21<3:19:29,  2.00s/it]  0%|          | 11/6000 [00:23<3:19:40,  2.00s/it]                                                   {'loss': 22.409, 'grad_norm': 1424.219970703125, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:23<3:19:40,  2.00s/it]  0%|          | 12/6000 [00:25<3:19:56,  2.00s/it]                                                   {'loss': 18.1771, 'grad_norm': 1059.6876220703125, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:25<3:19:56,  2.00s/it]  0%|          | 13/6000 [00:27<3:19:10,  2.00s/it]                                                   {'loss': 20.4388, 'grad_norm': 1671.24853515625, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:27<3:19:10,  2.00s/it]  0%|          | 14/6000 [00:29<3:17:26,  1.98s/it]                                                   {'loss': 19.5939, 'grad_norm': 1447.3570556640625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:29<3:17:26,  1.98s/it]  0%|          | 15/6000 [00:31<3:17:03,  1.98s/it]                                                   {'loss': 16.5891, 'grad_norm': 1194.1002197265625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:31<3:17:03,  1.98s/it]  0%|          | 16/6000 [00:33<3:16:21,  1.97s/it]                                                   {'loss': 18.4459, 'grad_norm': 1397.1905517578125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:33<3:16:21,  1.97s/it]  0%|          | 17/6000 [00:34<3:14:37,  1.95s/it]                                                   {'loss': 15.5257, 'grad_norm': 1239.8843994140625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:14:37,  1.95s/it]  0%|          | 18/6000 [00:36<3:14:57,  1.96s/it]                                                   {'loss': 14.6673, 'grad_norm': 2169.867431640625, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:14:57,  1.96s/it]  0%|          | 19/6000 [00:38<3:14:43,  1.95s/it]                                                   {'loss': 15.4965, 'grad_norm': 1158.8336181640625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:14:43,  1.95s/it]  0%|          | 20/6000 [00:40<3:14:21,  1.95s/it]                                                   {'loss': 16.0717, 'grad_norm': 1213.148193359375, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:14:21,  1.95s/it]  0%|          | 21/6000 [00:42<3:14:59,  1.96s/it]                                                   {'loss': 13.7285, 'grad_norm': 1244.1156005859375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:14:59,  1.96s/it]  0%|          | 22/6000 [00:44<3:14:43,  1.95s/it]                                                   {'loss': 14.0975, 'grad_norm': 1427.9293212890625, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:14:43,  1.95s/it]  0%|          | 23/6000 [00:46<3:16:05,  1.97s/it]                                                   {'loss': 13.0143, 'grad_norm': 1334.013916015625, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:16:05,  1.97s/it]  0%|          | 24/6000 [00:48<3:17:27,  1.98s/it]                                                   {'loss': 9.5475, 'grad_norm': 1431.9007568359375, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:17:27,  1.98s/it]  0%|          | 25/6000 [00:50<3:17:39,  1.98s/it]                                                   {'loss': 11.8619, 'grad_norm': 1394.46435546875, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:17:39,  1.98s/it]  0%|          | 26/6000 [00:52<3:17:01,  1.98s/it]                                                   {'loss': 12.1801, 'grad_norm': 1815.4637451171875, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:17:01,  1.98s/it]  0%|          | 27/6000 [00:54<3:16:46,  1.98s/it]                                                   {'loss': 12.0784, 'grad_norm': 1321.5062255859375, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:16:46,  1.98s/it]  0%|          | 28/6000 [00:56<3:26:21,  2.07s/it]                                                   {'loss': 8.3312, 'grad_norm': 1211.9063720703125, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:26:21,  2.07s/it]  0%|          | 29/6000 [00:58<3:21:55,  2.03s/it]                                                   {'loss': 8.7146, 'grad_norm': 1624.5947265625, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:21:55,  2.03s/it]  0%|          | 30/6000 [01:00<3:18:35,  2.00s/it]                                                   {'loss': 8.1631, 'grad_norm': 1186.9559326171875, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:18:35,  2.00s/it]  1%|          | 31/6000 [01:02<3:18:20,  1.99s/it]                                                   {'loss': 6.5054, 'grad_norm': 1022.1326293945312, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:18:20,  1.99s/it]  1%|          | 32/6000 [01:04<3:17:04,  1.98s/it]                                                   {'loss': 6.8747, 'grad_norm': 899.2710571289062, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:17:04,  1.98s/it]  1%|          | 33/6000 [01:06<3:16:45,  1.98s/it]                                                   {'loss': 4.4141, 'grad_norm': 588.6402587890625, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:16:45,  1.98s/it]  1%|          | 34/6000 [01:08<3:16:32,  1.98s/it]                                                   {'loss': 4.7287, 'grad_norm': 557.8272705078125, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:16:32,  1.98s/it]  1%|          | 35/6000 [01:10<3:17:20,  1.99s/it]                                                   {'loss': 4.5906, 'grad_norm': 785.9798583984375, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:17:20,  1.99s/it]  1%|          | 36/6000 [01:12<3:15:42,  1.97s/it]                                                   {'loss': 4.7544, 'grad_norm': 864.8128662109375, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:15:42,  1.97s/it]  1%|          | 37/6000 [01:14<3:14:14,  1.95s/it]                                                   {'loss': 4.0823, 'grad_norm': 636.2759399414062, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:14:14,  1.95s/it]  1%|          | 38/6000 [01:16<3:14:00,  1.95s/it]                                                   {'loss': 4.0394, 'grad_norm': 552.2255859375, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:14:00,  1.95s/it]  1%|          | 39/6000 [01:18<3:14:03,  1.95s/it]                                                   {'loss': 3.9002, 'grad_norm': 436.2689514160156, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:14:03,  1.95s/it]  1%|          | 40/6000 [01:20<3:15:49,  1.97s/it]                                                   {'loss': 3.7797, 'grad_norm': 440.6607666015625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:15:49,  1.97s/it]  1%|          | 41/6000 [01:22<3:16:36,  1.98s/it]                                                   {'loss': 4.0333, 'grad_norm': 509.6224060058594, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:22<3:16:36,  1.98s/it]  1%|          | 42/6000 [01:24<3:15:00,  1.96s/it]                                                   {'loss': 3.9235, 'grad_norm': 470.06805419921875, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:24<3:15:00,  1.96s/it]  1%|          | 43/6000 [01:26<3:27:59,  2.09s/it]                                                   {'loss': 3.693, 'grad_norm': 477.66033935546875, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:27:59,  2.09s/it]  1%|          | 44/6000 [01:28<3:27:25,  2.09s/it]                                                   {'loss': 3.6661, 'grad_norm': 265.29998779296875, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:27:25,  2.09s/it]  1%|          | 45/6000 [01:30<3:24:15,  2.06s/it]                                                   {'loss': 3.3558, 'grad_norm': 288.4962463378906, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:24:15,  2.06s/it]  1%|          | 46/6000 [01:32<3:22:11,  2.04s/it]                                                   {'loss': 3.5732, 'grad_norm': 261.8900146484375, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:22:11,  2.04s/it]  1%|          | 47/6000 [01:34<3:19:02,  2.01s/it]                                                   {'loss': 3.4999, 'grad_norm': 214.72344970703125, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:19:02,  2.01s/it]  1%|          | 48/6000 [01:36<3:16:51,  1.98s/it]                                                   {'loss': 3.4586, 'grad_norm': 348.3104553222656, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:16:51,  1.98s/it]  1%|          | 49/6000 [01:38<3:17:53,  2.00s/it]                                                   {'loss': 3.32, 'grad_norm': 218.91091918945312, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:17:53,  2.00s/it]  1%|          | 50/6000 [01:40<3:17:43,  1.99s/it]                                                   {'loss': 3.2563, 'grad_norm': 223.01979064941406, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:17:43,  1.99s/it][2025-11-18 10:39:26,866] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [01:43<3:38:11,  2.20s/it]                                                   {'loss': 3.3185, 'grad_norm': 150.90672302246094, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:43<3:38:11,  2.20s/it]  1%|          | 52/6000 [01:45<3:30:36,  2.12s/it]                                                   {'loss': 3.4005, 'grad_norm': 144.8096466064453, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:45<3:30:36,  2.12s/it]  1%|          | 53/6000 [01:47<3:27:50,  2.10s/it]                                                   {'loss': 3.9722, 'grad_norm': 236.20953369140625, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:47<3:27:50,  2.10s/it]  1%|          | 54/6000 [01:49<3:22:44,  2.05s/it]                                                   {'loss': 3.5226, 'grad_norm': 213.54791259765625, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:49<3:22:44,  2.05s/it]  1%|          | 55/6000 [01:51<3:20:05,  2.02s/it]                                                   {'loss': 3.1811, 'grad_norm': 196.70315551757812, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:51<3:20:05,  2.02s/it]  1%|          | 56/6000 [01:53<3:18:03,  2.00s/it]                                                   {'loss': 3.2243, 'grad_norm': 176.49900817871094, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:53<3:18:03,  2.00s/it]  1%|          | 57/6000 [01:55<3:16:05,  1.98s/it]                                                   {'loss': 3.3018, 'grad_norm': 191.60565185546875, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:55<3:16:05,  1.98s/it]  1%|          | 58/6000 [01:57<3:15:02,  1.97s/it]                                                   {'loss': 3.3237, 'grad_norm': 163.03004455566406, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:57<3:15:02,  1.97s/it]  1%|          | 59/6000 [01:59<3:13:05,  1.95s/it]                                                   {'loss': 3.242, 'grad_norm': 413.932861328125, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:59<3:13:05,  1.95s/it]  1%|          | 60/6000 [02:01<3:14:04,  1.96s/it]                                                   {'loss': 3.0978, 'grad_norm': 144.6361846923828, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:01<3:14:04,  1.96s/it]  1%|          | 61/6000 [02:02<3:12:11,  1.94s/it]                                                   {'loss': 3.047, 'grad_norm': 113.37480163574219, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:02<3:12:11,  1.94s/it]  1%|          | 62/6000 [02:04<3:13:04,  1.95s/it]                                                   {'loss': 3.3363, 'grad_norm': 147.78240966796875, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:04<3:13:04,  1.95s/it]  1%|          | 63/6000 [02:06<3:13:29,  1.96s/it]                                                   {'loss': 2.9127, 'grad_norm': 125.5897216796875, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:06<3:13:29,  1.96s/it]  1%|          | 64/6000 [02:08<3:12:16,  1.94s/it]                                                   {'loss': 3.0361, 'grad_norm': 90.79330444335938, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:08<3:12:16,  1.94s/it]  1%|          | 65/6000 [02:10<3:15:02,  1.97s/it]                                                   {'loss': 3.4002, 'grad_norm': 496.28411865234375, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:10<3:15:02,  1.97s/it]  1%|          | 66/6000 [02:12<3:17:43,  2.00s/it]                                                   {'loss': 3.0693, 'grad_norm': 98.92369842529297, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:12<3:17:43,  2.00s/it]  1%|          | 67/6000 [02:14<3:15:57,  1.98s/it]                                                   {'loss': 3.0135, 'grad_norm': 101.59786987304688, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:14<3:15:57,  1.98s/it]  1%|          | 68/6000 [02:16<3:15:24,  1.98s/it]                                                   {'loss': 3.1892, 'grad_norm': 97.93025207519531, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:16<3:15:24,  1.98s/it]  1%|          | 69/6000 [02:18<3:14:36,  1.97s/it]                                                   {'loss': 2.97, 'grad_norm': 90.66752624511719, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:18<3:14:36,  1.97s/it]  1%|          | 70/6000 [02:20<3:22:44,  2.05s/it]                                                   {'loss': 2.9576, 'grad_norm': 111.69105529785156, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:20<3:22:44,  2.05s/it]  1%|          | 71/6000 [02:22<3:18:44,  2.01s/it]                                                   {'loss': 2.991, 'grad_norm': 93.30947875976562, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:22<3:18:44,  2.01s/it]  1%|          | 72/6000 [02:24<3:19:50,  2.02s/it]                                                   {'loss': 2.9433, 'grad_norm': 101.2009506225586, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:24<3:19:50,  2.02s/it]  1%|          | 73/6000 [02:26<3:18:31,  2.01s/it]                                                   {'loss': 2.9112, 'grad_norm': 90.2870864868164, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:26<3:18:31,  2.01s/it]  1%|          | 74/6000 [02:28<3:15:38,  1.98s/it]                                                   {'loss': 2.8922, 'grad_norm': 64.4112319946289, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:28<3:15:38,  1.98s/it]  1%|â–         | 75/6000 [02:30<3:14:36,  1.97s/it]                                                   {'loss': 2.8952, 'grad_norm': 60.897605895996094, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:30<3:14:36,  1.97s/it]  1%|â–         | 76/6000 [02:32<3:16:00,  1.99s/it]                                                   {'loss': 2.9291, 'grad_norm': 77.06754302978516, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:32<3:16:00,  1.99s/it]  1%|â–         | 77/6000 [02:34<3:16:29,  1.99s/it]                                                   {'loss': 3.1143, 'grad_norm': 126.2643814086914, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:34<3:16:29,  1.99s/it]  1%|â–         | 78/6000 [02:37<3:26:33,  2.09s/it]                                                   {'loss': 3.0335, 'grad_norm': 70.29727172851562, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:37<3:26:33,  2.09s/it]  1%|â–         | 79/6000 [02:39<3:22:47,  2.06s/it]                                                   {'loss': 2.751, 'grad_norm': 70.3464126586914, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:39<3:22:47,  2.06s/it]  1%|â–         | 80/6000 [02:41<3:19:22,  2.02s/it]                                                   {'loss': 2.9734, 'grad_norm': 83.62367248535156, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:41<3:19:22,  2.02s/it]  1%|â–         | 81/6000 [02:42<3:16:56,  2.00s/it]                                                   {'loss': 2.8102, 'grad_norm': 52.4171028137207, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:42<3:16:56,  2.00s/it]  1%|â–         | 82/6000 [02:44<3:16:10,  1.99s/it]                                                   {'loss': 2.7832, 'grad_norm': 73.35941314697266, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:44<3:16:10,  1.99s/it]  1%|â–         | 83/6000 [02:46<3:18:21,  2.01s/it]                                                   {'loss': 2.8711, 'grad_norm': 82.69416046142578, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:47<3:18:21,  2.01s/it]  1%|â–         | 84/6000 [02:49<3:18:10,  2.01s/it]                                                   {'loss': 3.0088, 'grad_norm': 84.00382232666016, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:49<3:18:10,  2.01s/it]  1%|â–         | 85/6000 [02:51<3:20:44,  2.04s/it]                                                   {'loss': 2.7905, 'grad_norm': 64.08734893798828, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:51<3:20:44,  2.04s/it]  1%|â–         | 86/6000 [02:53<3:19:03,  2.02s/it]                                                   {'loss': 2.9158, 'grad_norm': 108.9326400756836, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:53<3:19:03,  2.02s/it]  1%|â–         | 87/6000 [02:55<3:17:56,  2.01s/it]                                                   {'loss': 2.8244, 'grad_norm': 71.74050903320312, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:55<3:17:56,  2.01s/it]  1%|â–         | 88/6000 [02:57<3:16:32,  1.99s/it]                                                   {'loss': 3.317, 'grad_norm': 112.22151184082031, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:57<3:16:32,  1.99s/it]  1%|â–         | 89/6000 [02:58<3:14:23,  1.97s/it]                                                   {'loss': 2.8066, 'grad_norm': 91.6467514038086, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:58<3:14:23,  1.97s/it]  2%|â–         | 90/6000 [03:00<3:14:22,  1.97s/it]                                                   {'loss': 2.7805, 'grad_norm': 66.96565246582031, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [03:00<3:14:22,  1.97s/it]  2%|â–         | 91/6000 [03:02<3:14:01,  1.97s/it]                                                   {'loss': 2.7013, 'grad_norm': 68.82862091064453, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:02<3:14:01,  1.97s/it]  2%|â–         | 92/6000 [03:04<3:15:34,  1.99s/it]                                                   {'loss': 2.7052, 'grad_norm': 84.8736572265625, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:04<3:15:34,  1.99s/it]  2%|â–         | 93/6000 [03:06<3:16:11,  1.99s/it]                                                   {'loss': 2.7742, 'grad_norm': 79.49423217773438, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:06<3:16:11,  1.99s/it]  2%|â–         | 94/6000 [03:08<3:14:23,  1.97s/it]                                                   {'loss': 2.8127, 'grad_norm': 104.2978744506836, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:08<3:14:23,  1.97s/it]  2%|â–         | 95/6000 [03:10<3:14:38,  1.98s/it]                                                   {'loss': 2.764, 'grad_norm': 66.28153228759766, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:10<3:14:38,  1.98s/it]  2%|â–         | 96/6000 [03:12<3:17:06,  2.00s/it]                                                   {'loss': 2.711, 'grad_norm': 63.42966842651367, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:12<3:17:06,  2.00s/it]  2%|â–         | 97/6000 [03:14<3:16:29,  2.00s/it]                                                   {'loss': 2.7984, 'grad_norm': 114.52787017822266, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:14<3:16:29,  2.00s/it]  2%|â–         | 98/6000 [03:16<3:15:08,  1.98s/it]                                                   {'loss': 2.664, 'grad_norm': 68.48668670654297, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:16<3:15:08,  1.98s/it]  2%|â–         | 99/6000 [03:18<3:15:27,  1.99s/it]                                                   {'loss': 2.6632, 'grad_norm': 71.49485778808594, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:18<3:15:27,  1.99s/it]  2%|â–         | 100/6000 [03:20<3:13:45,  1.97s/it]                                                    {'loss': 2.5215, 'grad_norm': 51.64399719238281, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:20<3:13:45,  1.97s/it][2025-11-18 10:41:06,895] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [03:23<3:46:18,  2.30s/it]                                                    {'loss': 2.54, 'grad_norm': 71.29457092285156, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:23<3:46:18,  2.30s/it]  2%|â–         | 102/6000 [03:25<3:37:52,  2.22s/it]                                                    {'loss': 2.6154, 'grad_norm': 64.32484436035156, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:25<3:37:52,  2.22s/it]  2%|â–         | 103/6000 [03:27<3:31:22,  2.15s/it]                                                    {'loss': 2.3865, 'grad_norm': 74.33702087402344, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:27<3:31:22,  2.15s/it]  2%|â–         | 104/6000 [03:29<3:27:49,  2.11s/it]                                                    {'loss': 2.239, 'grad_norm': 52.31059265136719, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:29<3:27:49,  2.11s/it]  2%|â–         | 105/6000 [03:31<3:25:32,  2.09s/it]                                                    {'loss': 2.4502, 'grad_norm': 62.586795806884766, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:31<3:25:32,  2.09s/it]  2%|â–         | 106/6000 [03:33<3:24:53,  2.09s/it]                                                    {'loss': 2.0737, 'grad_norm': 73.19738006591797, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:33<3:24:53,  2.09s/it]  2%|â–         | 107/6000 [03:35<3:20:03,  2.04s/it]                                                    {'loss': 1.865, 'grad_norm': 62.936187744140625, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:35<3:20:03,  2.04s/it]  2%|â–         | 108/6000 [03:37<3:18:11,  2.02s/it]                                                    {'loss': 1.7976, 'grad_norm': 62.605995178222656, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:37<3:18:11,  2.02s/it]  2%|â–         | 109/6000 [03:39<3:20:31,  2.04s/it]                                                    {'loss': 1.6396, 'grad_norm': 63.29072952270508, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:39<3:20:31,  2.04s/it]  2%|â–         | 110/6000 [03:42<3:19:44,  2.03s/it]                                                    {'loss': 1.3057, 'grad_norm': 55.67499923706055, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:42<3:19:44,  2.03s/it]  2%|â–         | 111/6000 [03:43<3:17:16,  2.01s/it]                                                    {'loss': 1.9332, 'grad_norm': 44.622589111328125, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:43<3:17:16,  2.01s/it]  2%|â–         | 112/6000 [03:46<3:21:24,  2.05s/it]                                                    {'loss': 1.7051, 'grad_norm': 106.37686920166016, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:46<3:21:24,  2.05s/it]  2%|â–         | 113/6000 [03:48<3:19:55,  2.04s/it]                                                    {'loss': 1.5617, 'grad_norm': 34.59471130371094, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:48<3:19:55,  2.04s/it]  2%|â–         | 114/6000 [03:50<3:19:49,  2.04s/it]                                                    {'loss': 1.5642, 'grad_norm': 40.95466232299805, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:50<3:19:49,  2.04s/it]  2%|â–         | 115/6000 [03:52<3:18:39,  2.03s/it]                                                    {'loss': 2.1078, 'grad_norm': 52.21893310546875, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:52<3:18:39,  2.03s/it]  2%|â–         | 116/6000 [03:54<3:16:37,  2.01s/it]                                                    {'loss': 1.9365, 'grad_norm': 69.5276870727539, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:54<3:16:37,  2.01s/it]  2%|â–         | 117/6000 [03:56<3:17:14,  2.01s/it]                                                    {'loss': 0.883, 'grad_norm': 63.98780059814453, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:56<3:17:14,  2.01s/it]  2%|â–         | 118/6000 [03:58<3:21:51,  2.06s/it]                                                    {'loss': 1.6415, 'grad_norm': 81.77090454101562, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:58<3:21:51,  2.06s/it]  2%|â–         | 119/6000 [04:00<3:17:07,  2.01s/it]                                                    {'loss': 1.47, 'grad_norm': 74.99873352050781, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [04:00<3:17:07,  2.01s/it]  2%|â–         | 120/6000 [04:02<3:16:24,  2.00s/it]                                                    {'loss': 1.1041, 'grad_norm': 87.29293060302734, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [04:02<3:16:24,  2.00s/it]  2%|â–         | 121/6000 [04:04<3:16:54,  2.01s/it]                                                    {'loss': 0.82, 'grad_norm': 88.66685485839844, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [04:04<3:16:54,  2.01s/it]  2%|â–         | 122/6000 [04:06<3:15:42,  2.00s/it]                                                    {'loss': 0.731, 'grad_norm': 70.96549987792969, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:06<3:15:42,  2.00s/it]  2%|â–         | 123/6000 [04:08<3:15:47,  2.00s/it]                                                    {'loss': 0.5383, 'grad_norm': 80.51116943359375, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:08<3:15:47,  2.00s/it]  2%|â–         | 124/6000 [04:10<3:15:08,  1.99s/it]                                                    {'loss': 0.946, 'grad_norm': 129.9242706298828, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:10<3:15:08,  1.99s/it]  2%|â–         | 125/6000 [04:12<3:20:09,  2.04s/it]                                                    {'loss': 0.6072, 'grad_norm': 93.29009246826172, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:12<3:20:09,  2.04s/it]  2%|â–         | 126/6000 [04:14<3:17:38,  2.02s/it]                                                    {'loss': 0.7528, 'grad_norm': 93.7206802368164, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:14<3:17:38,  2.02s/it]  2%|â–         | 127/6000 [04:16<3:17:03,  2.01s/it]                                                    {'loss': 0.3769, 'grad_norm': 57.09722900390625, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:16<3:17:03,  2.01s/it]  2%|â–         | 128/6000 [04:18<3:15:30,  2.00s/it]                                                    {'loss': 0.3565, 'grad_norm': 83.90882110595703, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:18<3:15:30,  2.00s/it]  2%|â–         | 129/6000 [04:20<3:15:40,  2.00s/it]                                                    {'loss': 0.7812, 'grad_norm': 74.13648223876953, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:20<3:15:40,  2.00s/it]  2%|â–         | 130/6000 [04:22<3:14:43,  1.99s/it]                                                    {'loss': 0.2553, 'grad_norm': 39.08823776245117, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:22<3:14:43,  1.99s/it]  2%|â–         | 131/6000 [04:24<3:14:55,  1.99s/it]                                                    {'loss': 0.3178, 'grad_norm': 43.12724304199219, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:24<3:14:55,  1.99s/it]  2%|â–         | 132/6000 [04:26<3:13:12,  1.98s/it]                                                    {'loss': 0.2684, 'grad_norm': 55.794769287109375, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:26<3:13:12,  1.98s/it]  2%|â–         | 133/6000 [04:28<3:13:52,  1.98s/it]                                                    {'loss': 0.3878, 'grad_norm': 51.13041687011719, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:28<3:13:52,  1.98s/it]  2%|â–         | 134/6000 [04:30<3:13:26,  1.98s/it]                                                    {'loss': 0.3829, 'grad_norm': 66.56103515625, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:30<3:13:26,  1.98s/it]  2%|â–         | 135/6000 [04:32<3:12:19,  1.97s/it]                                                    {'loss': 0.3366, 'grad_norm': 54.605934143066406, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:32<3:12:19,  1.97s/it]  2%|â–         | 136/6000 [04:34<3:12:14,  1.97s/it]                                                    {'loss': 0.1911, 'grad_norm': 30.326425552368164, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:34<3:12:14,  1.97s/it]  2%|â–         | 137/6000 [04:36<3:16:08,  2.01s/it]                                                    {'loss': 0.3158, 'grad_norm': 65.67816925048828, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:36<3:16:08,  2.01s/it]  2%|â–         | 138/6000 [04:38<3:13:46,  1.98s/it]                                                    {'loss': 0.3276, 'grad_norm': 46.03280258178711, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:38<3:13:46,  1.98s/it]  2%|â–         | 139/6000 [04:40<3:18:02,  2.03s/it]                                                    {'loss': 0.4082, 'grad_norm': 48.87726593017578, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:40<3:18:02,  2.03s/it]  2%|â–         | 140/6000 [04:42<3:21:41,  2.07s/it]                                                    {'loss': 0.2629, 'grad_norm': 43.9326171875, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:42<3:21:41,  2.07s/it]  2%|â–         | 141/6000 [04:44<3:19:09,  2.04s/it]                                                    {'loss': 0.3258, 'grad_norm': 37.5328369140625, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:44<3:19:09,  2.04s/it]  2%|â–         | 142/6000 [04:46<3:16:37,  2.01s/it]                                                    {'loss': 0.2015, 'grad_norm': 26.430009841918945, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:46<3:16:37,  2.01s/it]  2%|â–         | 143/6000 [04:48<3:16:31,  2.01s/it]                                                    {'loss': 0.1707, 'grad_norm': 26.385679244995117, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:48<3:16:31,  2.01s/it]  2%|â–         | 144/6000 [04:50<3:13:50,  1.99s/it]                                                    {'loss': 0.0883, 'grad_norm': 13.557870864868164, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:50<3:13:50,  1.99s/it]  2%|â–         | 145/6000 [04:52<3:11:37,  1.96s/it]                                                    {'loss': 0.1165, 'grad_norm': 26.657716751098633, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:52<3:11:37,  1.96s/it]  2%|â–         | 146/6000 [04:54<3:13:34,  1.98s/it]                                                    {'loss': 0.36, 'grad_norm': 37.03329086303711, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:54<3:13:34,  1.98s/it]  2%|â–         | 147/6000 [04:56<3:20:13,  2.05s/it]                                                    {'loss': 0.3529, 'grad_norm': 106.46730041503906, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:56<3:20:13,  2.05s/it]  2%|â–         | 148/6000 [04:58<3:17:51,  2.03s/it]                                                    {'loss': 0.1517, 'grad_norm': 22.185117721557617, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:58<3:17:51,  2.03s/it]  2%|â–         | 149/6000 [05:00<3:14:55,  2.00s/it]                                                    {'loss': 0.2137, 'grad_norm': 39.370059967041016, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [05:00<3:14:55,  2.00s/it]  2%|â–Ž         | 150/6000 [05:02<3:14:20,  1.99s/it]                                                    {'loss': 0.0561, 'grad_norm': 13.587936401367188, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [05:02<3:14:20,  1.99s/it][2025-11-18 10:42:48,377] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [05:05<3:36:52,  2.22s/it]                                                    {'loss': 0.1184, 'grad_norm': 14.84667682647705, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [05:05<3:36:52,  2.22s/it]  3%|â–Ž         | 152/6000 [05:07<3:31:28,  2.17s/it]                                                    {'loss': 0.1521, 'grad_norm': 24.618240356445312, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:07<3:31:28,  2.17s/it]  3%|â–Ž         | 153/6000 [05:08<3:23:53,  2.09s/it]                                                    {'loss': 0.1208, 'grad_norm': 18.34292221069336, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:08<3:23:53,  2.09s/it]  3%|â–Ž         | 154/6000 [05:10<3:19:49,  2.05s/it]                                                    {'loss': 0.2386, 'grad_norm': 36.81035614013672, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:10<3:19:49,  2.05s/it]  3%|â–Ž         | 155/6000 [05:12<3:16:44,  2.02s/it]                                                    {'loss': 0.1685, 'grad_norm': 23.572071075439453, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:12<3:16:44,  2.02s/it]  3%|â–Ž         | 156/6000 [05:14<3:15:30,  2.01s/it]                                                    {'loss': 0.0478, 'grad_norm': 12.51750659942627, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:14<3:15:30,  2.01s/it]  3%|â–Ž         | 157/6000 [05:17<3:20:14,  2.06s/it]                                                    {'loss': 0.1182, 'grad_norm': 21.161409378051758, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:17<3:20:14,  2.06s/it]  3%|â–Ž         | 158/6000 [05:18<3:17:17,  2.03s/it]                                                    {'loss': 0.0859, 'grad_norm': 26.4747314453125, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:18<3:17:17,  2.03s/it]  3%|â–Ž         | 159/6000 [05:20<3:16:07,  2.01s/it]                                                    {'loss': 0.1257, 'grad_norm': 27.084096908569336, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:20<3:16:07,  2.01s/it]  3%|â–Ž         | 160/6000 [05:23<3:21:45,  2.07s/it]                                                    {'loss': 0.0911, 'grad_norm': 11.341414451599121, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:23<3:21:45,  2.07s/it]  3%|â–Ž         | 161/6000 [05:25<3:19:32,  2.05s/it]                                                    {'loss': 0.2556, 'grad_norm': 49.512489318847656, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:25<3:19:32,  2.05s/it]  3%|â–Ž         | 162/6000 [05:27<3:24:37,  2.10s/it]                                                    {'loss': 0.1166, 'grad_norm': 30.857234954833984, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:27<3:24:37,  2.10s/it]  3%|â–Ž         | 163/6000 [05:29<3:25:22,  2.11s/it]                                                    {'loss': 0.103, 'grad_norm': 16.315406799316406, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:29<3:25:22,  2.11s/it]  3%|â–Ž         | 164/6000 [05:31<3:20:55,  2.07s/it]                                                    {'loss': 0.3327, 'grad_norm': 44.24772644042969, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:31<3:20:55,  2.07s/it]  3%|â–Ž         | 165/6000 [05:33<3:19:08,  2.05s/it]                                                    {'loss': 0.0873, 'grad_norm': 17.758256912231445, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:33<3:19:08,  2.05s/it]  3%|â–Ž         | 166/6000 [05:35<3:15:59,  2.02s/it]                                                    {'loss': 0.1287, 'grad_norm': 17.369081497192383, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:35<3:15:59,  2.02s/it]  3%|â–Ž         | 167/6000 [05:37<3:13:45,  1.99s/it]                                                    {'loss': 0.1824, 'grad_norm': 56.93098068237305, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:37<3:13:45,  1.99s/it]  3%|â–Ž         | 168/6000 [05:39<3:12:05,  1.98s/it]                                                    {'loss': 0.341, 'grad_norm': 108.8441162109375, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:39<3:12:05,  1.98s/it]  3%|â–Ž         | 169/6000 [05:41<3:16:03,  2.02s/it]                                                    {'loss': 0.3482, 'grad_norm': 36.57057189941406, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:41<3:16:03,  2.02s/it]  3%|â–Ž         | 170/6000 [05:43<3:14:54,  2.01s/it]                                                    {'loss': 0.1811, 'grad_norm': 26.504310607910156, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:43<3:14:54,  2.01s/it]  3%|â–Ž         | 171/6000 [05:45<3:11:45,  1.97s/it]                                                    {'loss': 0.1482, 'grad_norm': 19.79182243347168, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:45<3:11:45,  1.97s/it]  3%|â–Ž         | 172/6000 [05:47<3:14:22,  2.00s/it]                                                    {'loss': 0.214, 'grad_norm': 38.4088020324707, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:47<3:14:22,  2.00s/it]  3%|â–Ž         | 173/6000 [05:49<3:15:14,  2.01s/it]                                                    {'loss': 0.0794, 'grad_norm': 15.653508186340332, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:49<3:15:14,  2.01s/it]  3%|â–Ž         | 174/6000 [05:51<3:22:54,  2.09s/it]                                                    {'loss': 0.0481, 'grad_norm': 10.399679183959961, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:51<3:22:54,  2.09s/it]  3%|â–Ž         | 175/6000 [05:53<3:21:13,  2.07s/it]                                                    {'loss': 0.2867, 'grad_norm': 37.25271987915039, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:53<3:21:13,  2.07s/it]  3%|â–Ž         | 176/6000 [05:55<3:22:06,  2.08s/it]                                                    {'loss': 0.0668, 'grad_norm': 19.673856735229492, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:55<3:22:06,  2.08s/it]  3%|â–Ž         | 177/6000 [05:57<3:18:43,  2.05s/it]                                                    {'loss': 0.1492, 'grad_norm': 20.30381202697754, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:57<3:18:43,  2.05s/it]  3%|â–Ž         | 178/6000 [05:59<3:18:46,  2.05s/it]                                                    {'loss': 0.1133, 'grad_norm': 27.881855010986328, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:59<3:18:46,  2.05s/it]  3%|â–Ž         | 179/6000 [06:01<3:17:30,  2.04s/it]                                                    {'loss': 0.08, 'grad_norm': 10.336895942687988, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [06:01<3:17:30,  2.04s/it]  3%|â–Ž         | 180/6000 [06:03<3:15:57,  2.02s/it]                                                    {'loss': 0.3545, 'grad_norm': 30.239290237426758, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [06:03<3:15:57,  2.02s/it]  3%|â–Ž         | 181/6000 [06:05<3:16:55,  2.03s/it]                                                    {'loss': 0.348, 'grad_norm': 32.16301345825195, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [06:05<3:16:55,  2.03s/it]  3%|â–Ž         | 182/6000 [06:07<3:14:49,  2.01s/it]                                                    {'loss': 0.2379, 'grad_norm': 28.34344482421875, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [06:07<3:14:49,  2.01s/it]  3%|â–Ž         | 183/6000 [06:09<3:13:24,  1.99s/it]                                                    {'loss': 0.1214, 'grad_norm': 11.408003807067871, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:09<3:13:24,  1.99s/it]  3%|â–Ž         | 184/6000 [06:11<3:13:25,  2.00s/it]                                                    {'loss': 0.1626, 'grad_norm': 19.572080612182617, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:11<3:13:25,  2.00s/it]  3%|â–Ž         | 185/6000 [06:13<3:12:37,  1.99s/it]                                                    {'loss': 0.1255, 'grad_norm': 26.81003189086914, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:13<3:12:37,  1.99s/it]  3%|â–Ž         | 186/6000 [06:15<3:13:23,  2.00s/it]                                                    {'loss': 0.1767, 'grad_norm': 26.06975555419922, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:15<3:13:23,  2.00s/it]  3%|â–Ž         | 187/6000 [06:17<3:15:06,  2.01s/it]                                                    {'loss': 0.1133, 'grad_norm': 15.430912017822266, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:17<3:15:06,  2.01s/it]  3%|â–Ž         | 188/6000 [06:19<3:15:22,  2.02s/it]                                                    {'loss': 0.1742, 'grad_norm': 29.101572036743164, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:19<3:15:22,  2.02s/it]  3%|â–Ž         | 189/6000 [06:21<3:13:43,  2.00s/it]                                                    {'loss': 0.2085, 'grad_norm': 23.097929000854492, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:21<3:13:43,  2.00s/it]  3%|â–Ž         | 190/6000 [06:23<3:11:09,  1.97s/it]                                                    {'loss': 0.1194, 'grad_norm': 24.895620346069336, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:23<3:11:09,  1.97s/it]  3%|â–Ž         | 191/6000 [06:25<3:11:37,  1.98s/it]                                                    {'loss': 0.071, 'grad_norm': 13.364930152893066, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:25<3:11:37,  1.98s/it]  3%|â–Ž         | 192/6000 [06:27<3:18:37,  2.05s/it]                                                    {'loss': 0.1171, 'grad_norm': 18.998615264892578, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:27<3:18:37,  2.05s/it]  3%|â–Ž         | 193/6000 [06:29<3:17:40,  2.04s/it]                                                    {'loss': 0.3052, 'grad_norm': 27.36942481994629, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:29<3:17:40,  2.04s/it]  3%|â–Ž         | 194/6000 [06:31<3:15:19,  2.02s/it]                                                    {'loss': 0.1723, 'grad_norm': 20.84848403930664, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:31<3:15:19,  2.02s/it]  3%|â–Ž         | 195/6000 [06:33<3:14:20,  2.01s/it]                                                    {'loss': 0.0249, 'grad_norm': 6.117461204528809, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:33<3:14:20,  2.01s/it]  3%|â–Ž         | 196/6000 [06:35<3:14:28,  2.01s/it]                                                    {'loss': 0.056, 'grad_norm': 12.740374565124512, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:35<3:14:28,  2.01s/it]  3%|â–Ž         | 197/6000 [06:37<3:13:54,  2.00s/it]                                                    {'loss': 0.0486, 'grad_norm': 8.856208801269531, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:37<3:13:54,  2.00s/it]  3%|â–Ž         | 198/6000 [06:39<3:13:29,  2.00s/it]                                                    {'loss': 0.0709, 'grad_norm': 12.7284517288208, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:39<3:13:29,  2.00s/it]  3%|â–Ž         | 199/6000 [06:41<3:13:03,  2.00s/it]                                                    {'loss': 0.0551, 'grad_norm': 8.621281623840332, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:41<3:13:03,  2.00s/it]  3%|â–Ž         | 200/6000 [06:43<3:15:57,  2.03s/it]                                                    {'loss': 0.2887, 'grad_norm': 36.80008316040039, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:43<3:15:57,  2.03s/it][2025-11-18 10:44:30,121] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [06:46<3:38:23,  2.26s/it]                                                    {'loss': 0.3392, 'grad_norm': 32.86563491821289, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:46<3:38:23,  2.26s/it]  3%|â–Ž         | 202/6000 [06:48<3:30:49,  2.18s/it]                                                    {'loss': 0.1843, 'grad_norm': 23.596614837646484, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:48<3:30:49,  2.18s/it]  3%|â–Ž         | 203/6000 [06:50<3:25:42,  2.13s/it]                                                    {'loss': 0.1343, 'grad_norm': 25.085906982421875, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:50<3:25:42,  2.13s/it]  3%|â–Ž         | 204/6000 [06:52<3:21:31,  2.09s/it]                                                    {'loss': 0.05, 'grad_norm': 15.920568466186523, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:52<3:21:31,  2.09s/it]  3%|â–Ž         | 205/6000 [06:54<3:17:25,  2.04s/it]                                                    {'loss': 0.1491, 'grad_norm': 21.880155563354492, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:54<3:17:25,  2.04s/it]  3%|â–Ž         | 206/6000 [06:56<3:15:23,  2.02s/it]                                                    {'loss': 0.0423, 'grad_norm': 16.070186614990234, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:56<3:15:23,  2.02s/it]  3%|â–Ž         | 207/6000 [06:58<3:16:30,  2.04s/it]                                                    {'loss': 0.0399, 'grad_norm': 8.233804702758789, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:58<3:16:30,  2.04s/it]  3%|â–Ž         | 208/6000 [07:00<3:14:25,  2.01s/it]                                                    {'loss': 0.1498, 'grad_norm': 21.220670700073242, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [07:00<3:14:25,  2.01s/it]  3%|â–Ž         | 209/6000 [07:02<3:15:24,  2.02s/it]                                                    {'loss': 0.0408, 'grad_norm': 8.60237979888916, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [07:02<3:15:24,  2.02s/it]  4%|â–Ž         | 210/6000 [07:04<3:16:54,  2.04s/it]                                                    {'loss': 0.0667, 'grad_norm': 12.643510818481445, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [07:04<3:16:54,  2.04s/it]  4%|â–Ž         | 211/6000 [07:06<3:17:10,  2.04s/it]                                                    {'loss': 0.1214, 'grad_norm': 24.78799819946289, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [07:06<3:17:10,  2.04s/it]  4%|â–Ž         | 212/6000 [07:08<3:18:16,  2.06s/it]                                                    {'loss': 0.1658, 'grad_norm': 17.83155632019043, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [07:08<3:18:16,  2.06s/it]  4%|â–Ž         | 213/6000 [07:11<3:18:27,  2.06s/it]                                                    {'loss': 0.1613, 'grad_norm': 23.825092315673828, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:11<3:18:27,  2.06s/it]  4%|â–Ž         | 214/6000 [07:13<3:16:43,  2.04s/it]                                                    {'loss': 0.0712, 'grad_norm': 15.275269508361816, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:13<3:16:43,  2.04s/it]  4%|â–Ž         | 215/6000 [07:15<3:14:23,  2.02s/it]                                                    {'loss': 0.1061, 'grad_norm': 22.341650009155273, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:15<3:14:23,  2.02s/it]  4%|â–Ž         | 216/6000 [07:17<3:16:36,  2.04s/it]                                                    {'loss': 0.018, 'grad_norm': 4.721280574798584, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:17<3:16:36,  2.04s/it]  4%|â–Ž         | 217/6000 [07:19<3:16:35,  2.04s/it]                                                    {'loss': 0.0441, 'grad_norm': 13.341894149780273, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:19<3:16:35,  2.04s/it]  4%|â–Ž         | 218/6000 [07:21<3:15:05,  2.02s/it]                                                    {'loss': 0.1265, 'grad_norm': 23.39322853088379, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:21<3:15:05,  2.02s/it]  4%|â–Ž         | 219/6000 [07:23<3:13:52,  2.01s/it]                                                    {'loss': 0.2043, 'grad_norm': 30.121496200561523, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:23<3:13:52,  2.01s/it]  4%|â–Ž         | 220/6000 [07:25<3:12:42,  2.00s/it]                                                    {'loss': 0.4685, 'grad_norm': 35.76264953613281, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:25<3:12:42,  2.00s/it]  4%|â–Ž         | 221/6000 [07:27<3:11:38,  1.99s/it]                                                    {'loss': 0.0702, 'grad_norm': 10.390129089355469, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:27<3:11:38,  1.99s/it]  4%|â–Ž         | 222/6000 [07:29<3:11:41,  1.99s/it]                                                    {'loss': 0.1622, 'grad_norm': 30.86445426940918, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:29<3:11:41,  1.99s/it]  4%|â–Ž         | 223/6000 [07:31<3:19:02,  2.07s/it]                                                    {'loss': 0.109, 'grad_norm': 17.688093185424805, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:31<3:19:02,  2.07s/it]  4%|â–Ž         | 224/6000 [07:33<3:14:11,  2.02s/it]                                                    {'loss': 0.146, 'grad_norm': 26.03845977783203, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:33<3:14:11,  2.02s/it]  4%|â–         | 225/6000 [07:35<3:16:11,  2.04s/it]                                                    {'loss': 0.1561, 'grad_norm': 20.27405548095703, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:35<3:16:11,  2.04s/it]  4%|â–         | 226/6000 [07:37<3:19:01,  2.07s/it]                                                    {'loss': 0.1696, 'grad_norm': 30.03635025024414, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:37<3:19:01,  2.07s/it]  4%|â–         | 227/6000 [07:39<3:15:35,  2.03s/it]                                                    {'loss': 0.1723, 'grad_norm': 36.18309020996094, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:39<3:15:35,  2.03s/it]  4%|â–         | 228/6000 [07:41<3:13:28,  2.01s/it]                                                    {'loss': 0.0499, 'grad_norm': 12.017585754394531, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:41<3:13:28,  2.01s/it]  4%|â–         | 229/6000 [07:43<3:11:12,  1.99s/it]                                                    {'loss': 0.0628, 'grad_norm': 15.765317916870117, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:43<3:11:12,  1.99s/it]  4%|â–         | 230/6000 [07:45<3:12:29,  2.00s/it]                                                    {'loss': 0.0446, 'grad_norm': 13.026472091674805, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:45<3:12:29,  2.00s/it]  4%|â–         | 231/6000 [07:47<3:10:40,  1.98s/it]                                                    {'loss': 0.0671, 'grad_norm': 12.808629989624023, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:47<3:10:40,  1.98s/it]  4%|â–         | 232/6000 [07:49<3:10:53,  1.99s/it]                                                    {'loss': 0.1935, 'grad_norm': 33.632240295410156, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:49<3:10:53,  1.99s/it]  4%|â–         | 233/6000 [07:51<3:12:25,  2.00s/it]                                                    {'loss': 0.056, 'grad_norm': 15.19346809387207, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:51<3:12:25,  2.00s/it]  4%|â–         | 234/6000 [07:53<3:13:48,  2.02s/it]                                                    {'loss': 0.0399, 'grad_norm': 12.711341857910156, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:53<3:13:48,  2.02s/it]  4%|â–         | 235/6000 [07:55<3:12:50,  2.01s/it]                                                    {'loss': 0.1646, 'grad_norm': 59.96259307861328, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:55<3:12:50,  2.01s/it]  4%|â–         | 236/6000 [07:57<3:14:26,  2.02s/it]                                                    {'loss': 0.0785, 'grad_norm': 8.788405418395996, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:57<3:14:26,  2.02s/it]  4%|â–         | 237/6000 [07:59<3:14:51,  2.03s/it]                                                    {'loss': 0.3834, 'grad_norm': 27.977783203125, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:59<3:14:51,  2.03s/it]  4%|â–         | 238/6000 [08:01<3:20:47,  2.09s/it]                                                    {'loss': 0.3505, 'grad_norm': 26.484130859375, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [08:01<3:20:47,  2.09s/it]  4%|â–         | 239/6000 [08:03<3:17:03,  2.05s/it]                                                    {'loss': 0.1333, 'grad_norm': 38.33226013183594, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [08:03<3:17:03,  2.05s/it]  4%|â–         | 240/6000 [08:05<3:15:56,  2.04s/it]                                                    {'loss': 0.1741, 'grad_norm': 26.628549575805664, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [08:05<3:15:56,  2.04s/it]  4%|â–         | 241/6000 [08:07<3:17:03,  2.05s/it]                                                    {'loss': 0.1096, 'grad_norm': 18.911418914794922, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [08:07<3:17:03,  2.05s/it]  4%|â–         | 242/6000 [08:09<3:14:13,  2.02s/it]                                                    {'loss': 0.1137, 'grad_norm': 24.07103729248047, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [08:09<3:14:13,  2.02s/it]  4%|â–         | 243/6000 [08:11<3:16:42,  2.05s/it]                                                    {'loss': 0.1957, 'grad_norm': 21.737504959106445, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [08:11<3:16:42,  2.05s/it]  4%|â–         | 244/6000 [08:13<3:21:26,  2.10s/it]                                                    {'loss': 0.1053, 'grad_norm': 20.90196990966797, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:13<3:21:26,  2.10s/it]  4%|â–         | 245/6000 [08:15<3:18:28,  2.07s/it]                                                    {'loss': 0.087, 'grad_norm': 21.59157943725586, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:15<3:18:28,  2.07s/it]  4%|â–         | 246/6000 [08:18<3:18:34,  2.07s/it]                                                    {'loss': 0.0472, 'grad_norm': 21.538219451904297, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:18<3:18:34,  2.07s/it]  4%|â–         | 247/6000 [08:20<3:16:43,  2.05s/it]                                                    {'loss': 0.1265, 'grad_norm': 30.453346252441406, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:20<3:16:43,  2.05s/it]  4%|â–         | 248/6000 [08:22<3:15:38,  2.04s/it]                                                    {'loss': 0.0378, 'grad_norm': 9.966081619262695, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:22<3:15:38,  2.04s/it]  4%|â–         | 249/6000 [08:24<3:12:23,  2.01s/it]                                                    {'loss': 0.0427, 'grad_norm': 11.963946342468262, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:24<3:12:23,  2.01s/it]  4%|â–         | 250/6000 [08:26<3:11:56,  2.00s/it]                                                    {'loss': 0.1062, 'grad_norm': 35.009891510009766, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:26<3:11:56,  2.00s/it][2025-11-18 10:46:12,136] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [08:28<3:36:54,  2.26s/it]                                                    {'loss': 0.0786, 'grad_norm': 19.92081642150879, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:28<3:36:54,  2.26s/it]  4%|â–         | 252/6000 [08:30<3:30:45,  2.20s/it]                                                    {'loss': 0.0512, 'grad_norm': 14.891756057739258, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:30<3:30:45,  2.20s/it]  4%|â–         | 253/6000 [08:32<3:26:58,  2.16s/it]                                                    {'loss': 0.0817, 'grad_norm': 20.560312271118164, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:33<3:26:58,  2.16s/it]  4%|â–         | 254/6000 [08:35<3:24:10,  2.13s/it]                                                    {'loss': 0.2236, 'grad_norm': 25.547971725463867, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:35<3:24:10,  2.13s/it]  4%|â–         | 255/6000 [08:37<3:19:18,  2.08s/it]                                                    {'loss': 0.0724, 'grad_norm': 24.980220794677734, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:37<3:19:18,  2.08s/it]  4%|â–         | 256/6000 [08:39<3:18:40,  2.08s/it]                                                    {'loss': 0.0735, 'grad_norm': 15.321136474609375, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:39<3:18:40,  2.08s/it]  4%|â–         | 257/6000 [08:41<3:15:12,  2.04s/it]                                                    {'loss': 0.0473, 'grad_norm': 13.37406063079834, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:41<3:15:12,  2.04s/it]  4%|â–         | 258/6000 [08:43<3:14:19,  2.03s/it]                                                    {'loss': 0.0689, 'grad_norm': 13.40088939666748, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:43<3:14:19,  2.03s/it]  4%|â–         | 259/6000 [08:45<3:14:09,  2.03s/it]                                                    {'loss': 0.0325, 'grad_norm': 9.120488166809082, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:45<3:14:09,  2.03s/it]  4%|â–         | 260/6000 [08:47<3:14:24,  2.03s/it]                                                    {'loss': 0.0117, 'grad_norm': 4.815153121948242, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:47<3:14:24,  2.03s/it]  4%|â–         | 261/6000 [08:49<3:16:44,  2.06s/it]                                                    {'loss': 0.0739, 'grad_norm': 13.051347732543945, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:49<3:16:44,  2.06s/it]  4%|â–         | 262/6000 [08:51<3:17:01,  2.06s/it]                                                    {'loss': 0.0099, 'grad_norm': 2.7795403003692627, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:51<3:17:01,  2.06s/it]  4%|â–         | 263/6000 [08:53<3:17:58,  2.07s/it]                                                    {'loss': 0.0743, 'grad_norm': 14.414361000061035, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:53<3:17:58,  2.07s/it]  4%|â–         | 264/6000 [08:55<3:14:26,  2.03s/it]                                                    {'loss': 0.1618, 'grad_norm': 30.919363021850586, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:55<3:14:26,  2.03s/it]  4%|â–         | 265/6000 [08:57<3:14:51,  2.04s/it]                                                    {'loss': 0.0157, 'grad_norm': 5.149939060211182, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:57<3:14:51,  2.04s/it]  4%|â–         | 266/6000 [08:59<3:13:33,  2.03s/it]                                                    {'loss': 0.0238, 'grad_norm': 4.377072334289551, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:59<3:13:33,  2.03s/it]  4%|â–         | 267/6000 [09:01<3:12:45,  2.02s/it]                                                    {'loss': 0.0464, 'grad_norm': 8.841761589050293, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [09:01<3:12:45,  2.02s/it]  4%|â–         | 268/6000 [09:03<3:12:21,  2.01s/it]                                                    {'loss': 0.0463, 'grad_norm': 10.0369234085083, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [09:03<3:12:21,  2.01s/it]  4%|â–         | 269/6000 [09:05<3:12:04,  2.01s/it]                                                    {'loss': 0.0739, 'grad_norm': 14.83734130859375, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [09:05<3:12:04,  2.01s/it]  4%|â–         | 270/6000 [09:07<3:12:15,  2.01s/it]                                                    {'loss': 0.0601, 'grad_norm': 10.975912094116211, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [09:07<3:12:15,  2.01s/it]  5%|â–         | 271/6000 [09:09<3:10:33,  2.00s/it]                                                    {'loss': 0.0372, 'grad_norm': 10.872321128845215, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [09:09<3:10:33,  2.00s/it]  5%|â–         | 272/6000 [09:11<3:09:39,  1.99s/it]                                                    {'loss': 0.2227, 'grad_norm': 29.86271095275879, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [09:11<3:09:39,  1.99s/it]  5%|â–         | 273/6000 [09:13<3:09:47,  1.99s/it]                                                    {'loss': 0.036, 'grad_norm': 12.252461433410645, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [09:13<3:09:47,  1.99s/it]  5%|â–         | 274/6000 [09:15<3:12:08,  2.01s/it]                                                    {'loss': 0.0812, 'grad_norm': 16.24811363220215, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:15<3:12:08,  2.01s/it]  5%|â–         | 275/6000 [09:17<3:12:38,  2.02s/it]                                                    {'loss': 0.3151, 'grad_norm': 29.11339569091797, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:17<3:12:38,  2.02s/it]  5%|â–         | 276/6000 [09:19<3:11:52,  2.01s/it]                                                    {'loss': 0.1216, 'grad_norm': 21.947608947753906, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:19<3:11:52,  2.01s/it]  5%|â–         | 277/6000 [09:21<3:09:37,  1.99s/it]                                                    {'loss': 0.1057, 'grad_norm': 15.05379867553711, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:21<3:09:37,  1.99s/it]  5%|â–         | 278/6000 [09:23<3:09:18,  1.99s/it]                                                    {'loss': 0.0107, 'grad_norm': 2.0277466773986816, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:23<3:09:18,  1.99s/it]  5%|â–         | 279/6000 [09:25<3:10:33,  2.00s/it]                                                    {'loss': 0.2342, 'grad_norm': 24.214502334594727, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:25<3:10:33,  2.00s/it]  5%|â–         | 280/6000 [09:27<3:10:55,  2.00s/it]                                                    {'loss': 0.0599, 'grad_norm': 9.61586856842041, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:27<3:10:55,  2.00s/it]  5%|â–         | 281/6000 [09:29<3:13:43,  2.03s/it]                                                    {'loss': 0.4244, 'grad_norm': 25.57067108154297, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:29<3:13:43,  2.03s/it]  5%|â–         | 282/6000 [09:31<3:17:53,  2.08s/it]                                                    {'loss': 0.1143, 'grad_norm': 19.672775268554688, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:31<3:17:53,  2.08s/it]  5%|â–         | 283/6000 [09:33<3:19:38,  2.10s/it]                                                    {'loss': 0.0231, 'grad_norm': 4.828193664550781, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:33<3:19:38,  2.10s/it]  5%|â–         | 284/6000 [09:36<3:24:35,  2.15s/it]                                                    {'loss': 0.0534, 'grad_norm': 14.674884796142578, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:36<3:24:35,  2.15s/it]  5%|â–         | 285/6000 [09:38<3:21:31,  2.12s/it]                                                    {'loss': 0.1201, 'grad_norm': 21.874313354492188, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:38<3:21:31,  2.12s/it]  5%|â–         | 286/6000 [09:40<3:23:07,  2.13s/it]                                                    {'loss': 0.1446, 'grad_norm': 26.283531188964844, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:40<3:23:07,  2.13s/it]  5%|â–         | 287/6000 [09:42<3:22:51,  2.13s/it]                                                    {'loss': 0.1085, 'grad_norm': 18.2193660736084, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:42<3:22:51,  2.13s/it]  5%|â–         | 288/6000 [09:44<3:20:26,  2.11s/it]                                                    {'loss': 0.0146, 'grad_norm': 4.554331302642822, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:44<3:20:26,  2.11s/it]  5%|â–         | 289/6000 [09:46<3:19:00,  2.09s/it]                                                    {'loss': 0.1292, 'grad_norm': 19.550582885742188, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:46<3:19:00,  2.09s/it]  5%|â–         | 290/6000 [09:48<3:15:56,  2.06s/it]                                                    {'loss': 0.0515, 'grad_norm': 8.186546325683594, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:48<3:15:56,  2.06s/it]  5%|â–         | 291/6000 [09:50<3:12:07,  2.02s/it]                                                    {'loss': 0.2627, 'grad_norm': 27.05608367919922, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:50<3:12:07,  2.02s/it]  5%|â–         | 292/6000 [09:52<3:10:04,  2.00s/it]                                                    {'loss': 0.2655, 'grad_norm': 30.52979278564453, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:52<3:10:04,  2.00s/it]  5%|â–         | 293/6000 [09:54<3:09:00,  1.99s/it]                                                    {'loss': 0.0502, 'grad_norm': 10.506529808044434, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:54<3:09:00,  1.99s/it]  5%|â–         | 294/6000 [09:56<3:10:16,  2.00s/it]                                                    {'loss': 0.3848, 'grad_norm': 35.36341094970703, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:56<3:10:16,  2.00s/it]  5%|â–         | 295/6000 [09:58<3:09:10,  1.99s/it]                                                    {'loss': 0.1003, 'grad_norm': 10.636614799499512, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:58<3:09:10,  1.99s/it]  5%|â–         | 296/6000 [10:00<3:12:50,  2.03s/it]                                                    {'loss': 0.0806, 'grad_norm': 9.944231986999512, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [10:00<3:12:50,  2.03s/it]  5%|â–         | 297/6000 [10:02<3:11:55,  2.02s/it]                                                    {'loss': 0.0507, 'grad_norm': 15.894322395324707, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [10:02<3:11:55,  2.02s/it]  5%|â–         | 298/6000 [10:04<3:11:38,  2.02s/it]                                                    {'loss': 0.0607, 'grad_norm': 5.99454402923584, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [10:04<3:11:38,  2.02s/it]  5%|â–         | 299/6000 [10:06<3:22:00,  2.13s/it]                                                    {'loss': 0.0894, 'grad_norm': 14.651915550231934, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [10:06<3:22:00,  2.13s/it]  5%|â–Œ         | 300/6000 [10:08<3:18:45,  2.09s/it]                                                    {'loss': 0.0074, 'grad_norm': 3.4244110584259033, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [10:08<3:18:45,  2.09s/it][2025-11-18 10:47:54,982] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [10:11<3:37:21,  2.29s/it]                                                    {'loss': 0.1078, 'grad_norm': 25.24219512939453, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [10:11<3:37:21,  2.29s/it]  5%|â–Œ         | 302/6000 [10:13<3:34:54,  2.26s/it]                                                    {'loss': 0.0292, 'grad_norm': 7.008764266967773, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [10:13<3:34:54,  2.26s/it]  5%|â–Œ         | 303/6000 [10:15<3:27:15,  2.18s/it]                                                    {'loss': 0.0497, 'grad_norm': 13.75619888305664, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [10:15<3:27:15,  2.18s/it]  5%|â–Œ         | 304/6000 [10:17<3:24:16,  2.15s/it]                                                    {'loss': 0.0536, 'grad_norm': 13.303486824035645, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:17<3:24:16,  2.15s/it]  5%|â–Œ         | 305/6000 [10:19<3:18:54,  2.10s/it]                                                    {'loss': 0.2518, 'grad_norm': 32.28766632080078, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:19<3:18:54,  2.10s/it]  5%|â–Œ         | 306/6000 [10:21<3:13:31,  2.04s/it]                                                    {'loss': 0.0553, 'grad_norm': 13.122623443603516, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:21<3:13:31,  2.04s/it]  5%|â–Œ         | 307/6000 [10:23<3:11:35,  2.02s/it]                                                    {'loss': 0.038, 'grad_norm': 6.6480712890625, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:23<3:11:35,  2.02s/it]  5%|â–Œ         | 308/6000 [10:25<3:10:30,  2.01s/it]                                                    {'loss': 0.0722, 'grad_norm': 11.604772567749023, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:25<3:10:30,  2.01s/it]  5%|â–Œ         | 309/6000 [10:27<3:09:16,  2.00s/it]                                                    {'loss': 0.1452, 'grad_norm': 21.434473037719727, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:27<3:09:16,  2.00s/it]  5%|â–Œ         | 310/6000 [10:29<3:08:12,  1.98s/it]                                                    {'loss': 0.0226, 'grad_norm': 5.970999717712402, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:29<3:08:12,  1.98s/it]  5%|â–Œ         | 311/6000 [10:31<3:09:56,  2.00s/it]                                                    {'loss': 0.215, 'grad_norm': 32.77587127685547, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:31<3:09:56,  2.00s/it]  5%|â–Œ         | 312/6000 [10:33<3:10:35,  2.01s/it]                                                    {'loss': 0.3001, 'grad_norm': 36.842281341552734, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:33<3:10:35,  2.01s/it]  5%|â–Œ         | 313/6000 [10:35<3:09:42,  2.00s/it]                                                    {'loss': 0.0956, 'grad_norm': 15.937995910644531, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:35<3:09:42,  2.00s/it]  5%|â–Œ         | 314/6000 [10:37<3:10:38,  2.01s/it]                                                    {'loss': 0.1099, 'grad_norm': 14.608124732971191, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:37<3:10:38,  2.01s/it]  5%|â–Œ         | 315/6000 [10:39<3:08:28,  1.99s/it]                                                    {'loss': 0.0534, 'grad_norm': 4.9767866134643555, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:39<3:08:28,  1.99s/it]  5%|â–Œ         | 316/6000 [10:41<3:07:19,  1.98s/it]                                                    {'loss': 0.0184, 'grad_norm': 5.155722141265869, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:41<3:07:19,  1.98s/it]  5%|â–Œ         | 317/6000 [10:43<3:08:25,  1.99s/it]                                                    {'loss': 0.0757, 'grad_norm': 15.825654029846191, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:43<3:08:25,  1.99s/it]  5%|â–Œ         | 318/6000 [10:45<3:09:39,  2.00s/it]                                                    {'loss': 0.0723, 'grad_norm': 19.99907875061035, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:45<3:09:39,  2.00s/it]  5%|â–Œ         | 319/6000 [10:47<3:08:19,  1.99s/it]                                                    {'loss': 0.4887, 'grad_norm': 30.088464736938477, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:47<3:08:19,  1.99s/it]  5%|â–Œ         | 320/6000 [10:49<3:06:34,  1.97s/it]                                                    {'loss': 0.1089, 'grad_norm': 18.46799087524414, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:49<3:06:34,  1.97s/it]  5%|â–Œ         | 321/6000 [10:51<3:10:12,  2.01s/it]                                                    {'loss': 0.0173, 'grad_norm': 5.128052711486816, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:51<3:10:12,  2.01s/it]  5%|â–Œ         | 322/6000 [10:53<3:08:23,  1.99s/it]                                                    {'loss': 0.0618, 'grad_norm': 13.305516242980957, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:53<3:08:23,  1.99s/it]  5%|â–Œ         | 323/6000 [10:55<3:12:52,  2.04s/it]                                                    {'loss': 0.0548, 'grad_norm': 17.195899963378906, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:55<3:12:52,  2.04s/it]  5%|â–Œ         | 324/6000 [10:57<3:11:54,  2.03s/it]                                                    {'loss': 0.1209, 'grad_norm': 27.81697654724121, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:57<3:11:54,  2.03s/it]  5%|â–Œ         | 325/6000 [10:59<3:12:31,  2.04s/it]                                                    {'loss': 0.0923, 'grad_norm': 15.420112609863281, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:59<3:12:31,  2.04s/it]  5%|â–Œ         | 326/6000 [11:01<3:10:53,  2.02s/it]                                                    {'loss': 0.072, 'grad_norm': 11.006471633911133, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [11:01<3:10:53,  2.02s/it]  5%|â–Œ         | 327/6000 [11:03<3:08:17,  1.99s/it]                                                    {'loss': 0.0375, 'grad_norm': 7.813234806060791, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [11:03<3:08:17,  1.99s/it]  5%|â–Œ         | 328/6000 [11:06<3:17:42,  2.09s/it]                                                    {'loss': 0.0263, 'grad_norm': 8.267679214477539, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [11:06<3:17:42,  2.09s/it]  5%|â–Œ         | 329/6000 [11:08<3:15:22,  2.07s/it]                                                    {'loss': 0.1622, 'grad_norm': 27.636585235595703, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [11:08<3:15:22,  2.07s/it]  6%|â–Œ         | 330/6000 [11:10<3:13:54,  2.05s/it]                                                    {'loss': 0.1262, 'grad_norm': 19.28524398803711, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [11:10<3:13:54,  2.05s/it]  6%|â–Œ         | 331/6000 [11:12<3:11:26,  2.03s/it]                                                    {'loss': 0.1011, 'grad_norm': 21.089216232299805, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [11:12<3:11:26,  2.03s/it]  6%|â–Œ         | 332/6000 [11:13<3:08:34,  2.00s/it]                                                    {'loss': 0.0502, 'grad_norm': 9.282598495483398, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [11:13<3:08:34,  2.00s/it]  6%|â–Œ         | 333/6000 [11:15<3:07:59,  1.99s/it]                                                    {'loss': 0.0335, 'grad_norm': 12.358168601989746, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [11:15<3:07:59,  1.99s/it]  6%|â–Œ         | 334/6000 [11:17<3:08:05,  1.99s/it]                                                    {'loss': 0.0399, 'grad_norm': 7.041129112243652, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [11:17<3:08:05,  1.99s/it]  6%|â–Œ         | 335/6000 [11:19<3:08:02,  1.99s/it]                                                    {'loss': 0.0327, 'grad_norm': 7.225538730621338, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:19<3:08:02,  1.99s/it]  6%|â–Œ         | 336/6000 [11:21<3:06:13,  1.97s/it]                                                    {'loss': 0.0742, 'grad_norm': 11.126313209533691, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:21<3:06:13,  1.97s/it]  6%|â–Œ         | 337/6000 [11:23<3:06:14,  1.97s/it]                                                    {'loss': 0.0564, 'grad_norm': 10.335762977600098, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [11:23<3:06:14,  1.97s/it]  6%|â–Œ         | 338/6000 [11:25<3:04:30,  1.96s/it]                                                    {'loss': 0.1651, 'grad_norm': 17.81058120727539, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [11:25<3:04:30,  1.96s/it]  6%|â–Œ         | 339/6000 [11:27<3:04:34,  1.96s/it]                                                    {'loss': 0.011, 'grad_norm': 4.176511287689209, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [11:27<3:04:34,  1.96s/it]  6%|â–Œ         | 340/6000 [11:29<3:07:38,  1.99s/it]                                                    {'loss': 0.0982, 'grad_norm': 22.466562271118164, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [11:29<3:07:38,  1.99s/it]  6%|â–Œ         | 341/6000 [11:31<3:05:49,  1.97s/it]                                                    {'loss': 0.0317, 'grad_norm': 5.481649875640869, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [11:31<3:05:49,  1.97s/it]  6%|â–Œ         | 342/6000 [11:33<3:05:54,  1.97s/it]                                                    {'loss': 0.2576, 'grad_norm': 42.9207649230957, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [11:33<3:05:54,  1.97s/it]  6%|â–Œ         | 343/6000 [11:35<3:13:27,  2.05s/it]                                                    {'loss': 0.196, 'grad_norm': 33.99162673950195, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [11:35<3:13:27,  2.05s/it]  6%|â–Œ         | 344/6000 [11:37<3:11:21,  2.03s/it]                                                    {'loss': 0.1954, 'grad_norm': 42.37538146972656, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [11:37<3:11:21,  2.03s/it]  6%|â–Œ         | 345/6000 [11:39<3:09:21,  2.01s/it]                                                    {'loss': 0.0323, 'grad_norm': 8.213796615600586, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [11:39<3:09:21,  2.01s/it]  6%|â–Œ         | 346/6000 [11:41<3:08:49,  2.00s/it]                                                    {'loss': 0.1169, 'grad_norm': 16.71442985534668, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [11:41<3:08:49,  2.00s/it]  6%|â–Œ         | 347/6000 [11:43<3:07:11,  1.99s/it]                                                    {'loss': 0.0139, 'grad_norm': 3.8213789463043213, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [11:43<3:07:11,  1.99s/it]  6%|â–Œ         | 348/6000 [11:45<3:07:06,  1.99s/it]                                                    {'loss': 0.1501, 'grad_norm': 18.980133056640625, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [11:45<3:07:06,  1.99s/it]  6%|â–Œ         | 349/6000 [11:47<3:07:23,  1.99s/it]                                                    {'loss': 0.0067, 'grad_norm': 2.282132148742676, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [11:47<3:07:23,  1.99s/it]  6%|â–Œ         | 350/6000 [11:49<3:07:59,  2.00s/it]                                                    {'loss': 0.2946, 'grad_norm': 29.33723258972168, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [11:49<3:07:59,  2.00s/it][2025-11-18 10:49:35,897] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [11:52<3:30:58,  2.24s/it]                                                    {'loss': 0.0708, 'grad_norm': 14.51834774017334, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [11:52<3:30:58,  2.24s/it]  6%|â–Œ         | 352/6000 [11:54<3:22:37,  2.15s/it]                                                    {'loss': 0.0557, 'grad_norm': 15.76059341430664, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [11:54<3:22:37,  2.15s/it]  6%|â–Œ         | 353/6000 [11:56<3:17:41,  2.10s/it]                                                    {'loss': 0.1435, 'grad_norm': 20.72418785095215, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [11:56<3:17:41,  2.10s/it]  6%|â–Œ         | 354/6000 [11:58<3:14:31,  2.07s/it]                                                    {'loss': 0.0605, 'grad_norm': 11.417633056640625, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [11:58<3:14:31,  2.07s/it]  6%|â–Œ         | 355/6000 [12:00<3:10:51,  2.03s/it]                                                    {'loss': 0.0407, 'grad_norm': 12.42082691192627, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [12:00<3:10:51,  2.03s/it]  6%|â–Œ         | 356/6000 [12:02<3:09:14,  2.01s/it]                                                    {'loss': 0.0364, 'grad_norm': 8.86899185180664, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [12:02<3:09:14,  2.01s/it]  6%|â–Œ         | 357/6000 [12:04<3:08:03,  2.00s/it]                                                    {'loss': 0.066, 'grad_norm': 12.104598045349121, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [12:04<3:08:03,  2.00s/it]  6%|â–Œ         | 358/6000 [12:06<3:06:18,  1.98s/it]                                                    {'loss': 0.0769, 'grad_norm': 26.0163631439209, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [12:06<3:06:18,  1.98s/it]  6%|â–Œ         | 359/6000 [12:08<3:07:16,  1.99s/it]                                                    {'loss': 0.0733, 'grad_norm': 18.394020080566406, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [12:08<3:07:16,  1.99s/it]  6%|â–Œ         | 360/6000 [12:10<3:05:50,  1.98s/it]                                                    {'loss': 0.1989, 'grad_norm': 20.20941162109375, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [12:10<3:05:50,  1.98s/it]  6%|â–Œ         | 361/6000 [12:12<3:03:53,  1.96s/it]                                                    {'loss': 0.3265, 'grad_norm': 35.438724517822266, 'learning_rate': 9.557627118644068e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [12:12<3:03:53,  1.96s/it]  6%|â–Œ         | 362/6000 [12:14<3:03:25,  1.95s/it]                                                    {'loss': 0.1749, 'grad_norm': 20.93428611755371, 'learning_rate': 9.555932203389831e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [12:14<3:03:25,  1.95s/it]  6%|â–Œ         | 363/6000 [12:16<3:03:07,  1.95s/it]                                                    {'loss': 0.0195, 'grad_norm': 5.02980899810791, 'learning_rate': 9.554237288135594e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [12:16<3:03:07,  1.95s/it]  6%|â–Œ         | 364/6000 [12:18<3:05:25,  1.97s/it]                                                    {'loss': 0.0601, 'grad_norm': 11.941238403320312, 'learning_rate': 9.552542372881358e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [12:18<3:05:25,  1.97s/it]  6%|â–Œ         | 365/6000 [12:20<3:04:48,  1.97s/it]                                                    {'loss': 0.0389, 'grad_norm': 15.729655265808105, 'learning_rate': 9.55084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [12:20<3:04:48,  1.97s/it]  6%|â–Œ         | 366/6000 [12:21<3:03:48,  1.96s/it]                                                    {'loss': 0.0145, 'grad_norm': 3.5440409183502197, 'learning_rate': 9.549152542372883e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [12:21<3:03:48,  1.96s/it]  6%|â–Œ         | 367/6000 [12:23<3:05:29,  1.98s/it]                                                    {'loss': 0.0442, 'grad_norm': 10.30856990814209, 'learning_rate': 9.547457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [12:24<3:05:29,  1.98s/it]  6%|â–Œ         | 368/6000 [12:26<3:09:51,  2.02s/it]                                                    {'loss': 0.109, 'grad_norm': 20.562957763671875, 'learning_rate': 9.545762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [12:26<3:09:51,  2.02s/it]  6%|â–Œ         | 369/6000 [12:28<3:13:01,  2.06s/it]                                                    {'loss': 0.0683, 'grad_norm': 11.377470970153809, 'learning_rate': 9.54406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [12:28<3:13:01,  2.06s/it]  6%|â–Œ         | 370/6000 [12:30<3:12:08,  2.05s/it]                                                    {'loss': 0.0046, 'grad_norm': 1.542659878730774, 'learning_rate': 9.542372881355934e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [12:30<3:12:08,  2.05s/it]  6%|â–Œ         | 371/6000 [12:32<3:08:54,  2.01s/it]                                                    {'loss': 0.0371, 'grad_norm': 7.928197860717773, 'learning_rate': 9.540677966101696e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [12:32<3:08:54,  2.01s/it]  6%|â–Œ         | 372/6000 [12:34<3:06:56,  1.99s/it]                                                    {'loss': 0.0152, 'grad_norm': 4.475196838378906, 'learning_rate': 9.538983050847457e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [12:34<3:06:56,  1.99s/it]  6%|â–Œ         | 373/6000 [12:36<3:06:25,  1.99s/it]                                                    {'loss': 0.0091, 'grad_norm': 3.4577255249023438, 'learning_rate': 9.53728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [12:36<3:06:25,  1.99s/it]  6%|â–Œ         | 374/6000 [12:38<3:07:35,  2.00s/it]                                                    {'loss': 0.3651, 'grad_norm': 31.989595413208008, 'learning_rate': 9.535593220338984e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [12:38<3:07:35,  2.00s/it]  6%|â–‹         | 375/6000 [12:40<3:05:46,  1.98s/it]                                                    {'loss': 0.0489, 'grad_norm': 12.200448036193848, 'learning_rate': 9.533898305084747e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [12:40<3:05:46,  1.98s/it]  6%|â–‹         | 376/6000 [12:42<3:05:41,  1.98s/it]                                                    {'loss': 0.0565, 'grad_norm': 20.96184539794922, 'learning_rate': 9.532203389830508e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [12:42<3:05:41,  1.98s/it]  6%|â–‹         | 377/6000 [12:44<3:03:56,  1.96s/it]                                                    {'loss': 0.0999, 'grad_norm': 21.30707359313965, 'learning_rate': 9.530508474576272e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [12:44<3:03:56,  1.96s/it]  6%|â–‹         | 378/6000 [12:46<3:06:15,  1.99s/it]                                                    {'loss': 0.0717, 'grad_norm': 6.991079807281494, 'learning_rate': 9.528813559322035e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [12:46<3:06:15,  1.99s/it]  6%|â–‹         | 379/6000 [12:48<3:06:09,  1.99s/it]                                                    {'loss': 0.0226, 'grad_norm': 9.028334617614746, 'learning_rate': 9.527118644067798e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [12:48<3:06:09,  1.99s/it]  6%|â–‹         | 380/6000 [12:50<3:13:40,  2.07s/it]                                                    {'loss': 0.1086, 'grad_norm': 19.893531799316406, 'learning_rate': 9.52542372881356e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [12:50<3:13:40,  2.07s/it]  6%|â–‹         | 381/6000 [12:52<3:10:29,  2.03s/it]                                                    {'loss': 0.0208, 'grad_norm': 8.578195571899414, 'learning_rate': 9.523728813559323e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [12:52<3:10:29,  2.03s/it]  6%|â–‹         | 382/6000 [12:54<3:08:58,  2.02s/it]                                                    {'loss': 0.0389, 'grad_norm': 9.68350887298584, 'learning_rate': 9.522033898305085e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [12:54<3:08:58,  2.02s/it]  6%|â–‹         | 383/6000 [12:56<3:07:56,  2.01s/it]                                                    {'loss': 0.1048, 'grad_norm': 17.912479400634766, 'learning_rate': 9.520338983050848e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [12:56<3:07:56,  2.01s/it]  6%|â–‹         | 384/6000 [12:58<3:06:32,  1.99s/it]                                                    {'loss': 0.1109, 'grad_norm': 21.6566162109375, 'learning_rate': 9.518644067796611e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [12:58<3:06:32,  1.99s/it]  6%|â–‹         | 385/6000 [13:00<3:07:08,  2.00s/it]                                                    {'loss': 0.0066, 'grad_norm': 2.05617618560791, 'learning_rate': 9.516949152542375e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [13:00<3:07:08,  2.00s/it]  6%|â–‹         | 386/6000 [13:02<3:05:52,  1.99s/it]                                                    {'loss': 0.0613, 'grad_norm': 19.128713607788086, 'learning_rate': 9.515254237288136e-06, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [13:02<3:05:52,  1.99s/it]  6%|â–‹         | 387/6000 [13:04<3:09:08,  2.02s/it]                                                    {'loss': 0.0442, 'grad_norm': 11.974771499633789, 'learning_rate': 9.5135593220339e-06, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [13:04<3:09:08,  2.02s/it]  6%|â–‹         | 388/6000 [13:06<3:09:56,  2.03s/it]                                                    {'loss': 0.0547, 'grad_norm': 15.551405906677246, 'learning_rate': 9.511864406779661e-06, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [13:06<3:09:56,  2.03s/it]  6%|â–‹         | 389/6000 [13:08<3:09:19,  2.02s/it]                                                    {'loss': 0.0371, 'grad_norm': 14.905143737792969, 'learning_rate': 9.510169491525424e-06, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [13:08<3:09:19,  2.02s/it]  6%|â–‹         | 390/6000 [13:10<3:08:51,  2.02s/it]                                                    {'loss': 0.0129, 'grad_norm': 4.062182426452637, 'learning_rate': 9.508474576271188e-06, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [13:10<3:08:51,  2.02s/it]  7%|â–‹         | 391/6000 [13:12<3:05:57,  1.99s/it]                                                    {'loss': 0.3874, 'grad_norm': 42.48469924926758, 'learning_rate': 9.506779661016949e-06, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [13:12<3:05:57,  1.99s/it]  7%|â–‹         | 392/6000 [13:14<3:06:31,  2.00s/it]                                                    {'loss': 0.0025, 'grad_norm': 1.0554580688476562, 'learning_rate': 9.505084745762712e-06, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [13:14<3:06:31,  2.00s/it]  7%|â–‹         | 393/6000 [13:16<3:06:13,  1.99s/it]                                                    {'loss': 0.0333, 'grad_norm': 10.50210189819336, 'learning_rate': 9.503389830508476e-06, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [13:16<3:06:13,  1.99s/it]  7%|â–‹         | 394/6000 [13:18<3:05:40,  1.99s/it]                                                    {'loss': 0.06, 'grad_norm': 14.891043663024902, 'learning_rate': 9.501694915254239e-06, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [13:18<3:05:40,  1.99s/it]  7%|â–‹         | 395/6000 [13:20<3:05:48,  1.99s/it]                                                    {'loss': 0.0917, 'grad_norm': 30.228174209594727, 'learning_rate': 9.5e-06, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [13:20<3:05:48,  1.99s/it]  7%|â–‹         | 396/6000 [13:22<3:05:23,  1.98s/it]                                                    {'loss': 0.0233, 'grad_norm': 4.5270514488220215, 'learning_rate': 9.498305084745764e-06, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [13:22<3:05:23,  1.98s/it]  7%|â–‹         | 397/6000 [13:24<3:09:20,  2.03s/it]                                                    {'loss': 0.0801, 'grad_norm': 8.53029727935791, 'learning_rate': 9.496610169491525e-06, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [13:24<3:09:20,  2.03s/it]  7%|â–‹         | 398/6000 [13:26<3:06:24,  2.00s/it]                                                    {'loss': 0.0724, 'grad_norm': 14.264562606811523, 'learning_rate': 9.494915254237289e-06, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [13:26<3:06:24,  2.00s/it]  7%|â–‹         | 399/6000 [13:28<3:05:20,  1.99s/it]                                                    {'loss': 0.1447, 'grad_norm': 24.80599021911621, 'learning_rate': 9.493220338983052e-06, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [13:28<3:05:20,  1.99s/it]  7%|â–‹         | 400/6000 [13:30<3:05:14,  1.98s/it]                                                    {'loss': 0.0078, 'grad_norm': 2.7573702335357666, 'learning_rate': 9.491525423728815e-06, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [13:30<3:05:14,  1.98s/it][2025-11-18 10:51:16,314] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [13:32<3:24:19,  2.19s/it]                                                    {'loss': 0.036, 'grad_norm': 4.693691730499268, 'learning_rate': 9.489830508474577e-06, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [13:32<3:24:19,  2.19s/it]  7%|â–‹         | 402/6000 [13:34<3:17:58,  2.12s/it]                                                    {'loss': 0.0902, 'grad_norm': 17.725318908691406, 'learning_rate': 9.48813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [13:34<3:17:58,  2.12s/it]  7%|â–‹         | 403/6000 [13:36<3:13:44,  2.08s/it]                                                    {'loss': 0.1408, 'grad_norm': 23.737674713134766, 'learning_rate': 9.486440677966102e-06, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [13:36<3:13:44,  2.08s/it]  7%|â–‹         | 404/6000 [13:38<3:10:34,  2.04s/it]                                                    {'loss': 0.2449, 'grad_norm': 30.02109146118164, 'learning_rate': 9.484745762711865e-06, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [13:38<3:10:34,  2.04s/it]  7%|â–‹         | 405/6000 [13:40<3:08:04,  2.02s/it]                                                    {'loss': 0.0785, 'grad_norm': 17.61540985107422, 'learning_rate': 9.483050847457628e-06, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [13:40<3:08:04,  2.02s/it]  7%|â–‹         | 406/6000 [13:42<3:06:57,  2.01s/it]                                                    {'loss': 0.0011, 'grad_norm': 0.37629300355911255, 'learning_rate': 9.481355932203391e-06, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [13:42<3:06:57,  2.01s/it]  7%|â–‹         | 407/6000 [13:44<3:05:49,  1.99s/it]                                                    {'loss': 0.0319, 'grad_norm': 8.340252876281738, 'learning_rate': 9.479661016949153e-06, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [13:44<3:05:49,  1.99s/it]  7%|â–‹         | 408/6000 [13:46<3:09:41,  2.04s/it]                                                    {'loss': 0.1101, 'grad_norm': 22.28524398803711, 'learning_rate': 9.477966101694916e-06, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [13:46<3:09:41,  2.04s/it]  7%|â–‹         | 409/6000 [13:48<3:09:32,  2.03s/it]                                                    {'loss': 0.0653, 'grad_norm': 12.398139953613281, 'learning_rate': 9.476271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [13:48<3:09:32,  2.03s/it]  7%|â–‹         | 410/6000 [13:50<3:13:40,  2.08s/it]                                                    {'loss': 0.2101, 'grad_norm': 25.852672576904297, 'learning_rate': 9.474576271186441e-06, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [13:50<3:13:40,  2.08s/it]  7%|â–‹         | 411/6000 [13:53<3:19:07,  2.14s/it]                                                    {'loss': 0.0144, 'grad_norm': 2.269824743270874, 'learning_rate': 9.472881355932204e-06, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [13:53<3:19:07,  2.14s/it]  7%|â–‹         | 412/6000 [13:55<3:12:33,  2.07s/it]                                                    {'loss': 0.1038, 'grad_norm': 17.209564208984375, 'learning_rate': 9.471186440677966e-06, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [13:55<3:12:33,  2.07s/it]  7%|â–‹         | 413/6000 [13:57<3:11:33,  2.06s/it]                                                    {'loss': 0.3263, 'grad_norm': 24.97626495361328, 'learning_rate': 9.46949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [13:57<3:11:33,  2.06s/it]  7%|â–‹         | 414/6000 [13:59<3:07:53,  2.02s/it]                                                    {'loss': 0.0624, 'grad_norm': 15.28177261352539, 'learning_rate': 9.467796610169493e-06, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [13:59<3:07:53,  2.02s/it]  7%|â–‹         | 415/6000 [14:01<3:09:15,  2.03s/it]                                                    {'loss': 0.1293, 'grad_norm': 23.24688148498535, 'learning_rate': 9.466101694915256e-06, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [14:01<3:09:15,  2.03s/it]  7%|â–‹         | 416/6000 [14:03<3:09:48,  2.04s/it]                                                    {'loss': 0.0854, 'grad_norm': 18.65563201904297, 'learning_rate': 9.464406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [14:03<3:09:48,  2.04s/it]  7%|â–‹         | 417/6000 [14:05<3:08:29,  2.03s/it]                                                    {'loss': 0.0432, 'grad_norm': 19.231958389282227, 'learning_rate': 9.46271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [14:05<3:08:29,  2.03s/it]  7%|â–‹         | 418/6000 [14:07<3:06:16,  2.00s/it]                                                    {'loss': 0.1109, 'grad_norm': 16.66615104675293, 'learning_rate': 9.461016949152542e-06, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [14:07<3:06:16,  2.00s/it]  7%|â–‹         | 419/6000 [14:09<3:09:23,  2.04s/it]                                                    {'loss': 0.0407, 'grad_norm': 7.827696800231934, 'learning_rate': 9.459322033898306e-06, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [14:09<3:09:23,  2.04s/it]  7%|â–‹         | 420/6000 [14:11<3:08:46,  2.03s/it]                                                    {'loss': 0.004, 'grad_norm': 0.9195915460586548, 'learning_rate': 9.457627118644069e-06, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [14:11<3:08:46,  2.03s/it]  7%|â–‹         | 421/6000 [14:13<3:07:04,  2.01s/it]                                                    {'loss': 0.1552, 'grad_norm': 31.861196517944336, 'learning_rate': 9.455932203389832e-06, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [14:13<3:07:04,  2.01s/it]  7%|â–‹         | 422/6000 [14:15<3:07:07,  2.01s/it]                                                    {'loss': 0.1028, 'grad_norm': 18.89142608642578, 'learning_rate': 9.454237288135594e-06, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [14:15<3:07:07,  2.01s/it]  7%|â–‹         | 423/6000 [14:17<3:05:41,  2.00s/it]                                                    {'loss': 0.0721, 'grad_norm': 13.213544845581055, 'learning_rate': 9.452542372881357e-06, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [14:17<3:05:41,  2.00s/it]  7%|â–‹         | 424/6000 [14:19<3:03:44,  1.98s/it]                                                    {'loss': 0.0085, 'grad_norm': 2.6478800773620605, 'learning_rate': 9.450847457627119e-06, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [14:19<3:03:44,  1.98s/it]  7%|â–‹         | 425/6000 [14:21<3:02:34,  1.96s/it]                                                    {'loss': 0.0838, 'grad_norm': 19.482017517089844, 'learning_rate': 9.449152542372882e-06, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [14:21<3:02:34,  1.96s/it]  7%|â–‹         | 426/6000 [14:23<3:01:22,  1.95s/it]                                                    {'loss': 0.0208, 'grad_norm': 5.581593990325928, 'learning_rate': 9.447457627118645e-06, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [14:23<3:01:22,  1.95s/it]  7%|â–‹         | 427/6000 [14:25<3:02:28,  1.96s/it]                                                    {'loss': 0.1602, 'grad_norm': 13.655067443847656, 'learning_rate': 9.445762711864408e-06, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [14:25<3:02:28,  1.96s/it]  7%|â–‹         | 428/6000 [14:27<3:03:07,  1.97s/it]                                                    {'loss': 0.0218, 'grad_norm': 7.689836025238037, 'learning_rate': 9.44406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [14:27<3:03:07,  1.97s/it]  7%|â–‹         | 429/6000 [14:28<3:01:53,  1.96s/it]                                                    {'loss': 0.0345, 'grad_norm': 8.762350082397461, 'learning_rate': 9.442372881355933e-06, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [14:28<3:01:53,  1.96s/it]  7%|â–‹         | 430/6000 [14:30<3:02:25,  1.97s/it]                                                    {'loss': 0.2291, 'grad_norm': 23.879531860351562, 'learning_rate': 9.440677966101696e-06, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [14:30<3:02:25,  1.97s/it]  7%|â–‹         | 431/6000 [14:32<3:01:32,  1.96s/it]                                                    {'loss': 0.0957, 'grad_norm': 21.922775268554688, 'learning_rate': 9.43898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [14:32<3:01:32,  1.96s/it]  7%|â–‹         | 432/6000 [14:34<3:03:59,  1.98s/it]                                                    {'loss': 0.1464, 'grad_norm': 17.841388702392578, 'learning_rate': 9.437288135593221e-06, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [14:34<3:03:59,  1.98s/it]  7%|â–‹         | 433/6000 [14:36<3:02:05,  1.96s/it]                                                    {'loss': 0.0083, 'grad_norm': 2.4289588928222656, 'learning_rate': 9.435593220338983e-06, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [14:36<3:02:05,  1.96s/it]  7%|â–‹         | 434/6000 [14:38<3:01:15,  1.95s/it]                                                    {'loss': 0.0084, 'grad_norm': 3.3215713500976562, 'learning_rate': 9.433898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [14:38<3:01:15,  1.95s/it]  7%|â–‹         | 435/6000 [14:40<3:01:59,  1.96s/it]                                                    {'loss': 0.022, 'grad_norm': 6.309343338012695, 'learning_rate': 9.43220338983051e-06, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [14:40<3:01:59,  1.96s/it]  7%|â–‹         | 436/6000 [14:42<3:03:11,  1.98s/it]                                                    {'loss': 0.0097, 'grad_norm': 2.424110174179077, 'learning_rate': 9.430508474576273e-06, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [14:42<3:03:11,  1.98s/it]  7%|â–‹         | 437/6000 [14:44<3:02:26,  1.97s/it]                                                    {'loss': 0.0458, 'grad_norm': 11.38681697845459, 'learning_rate': 9.428813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [14:44<3:02:26,  1.97s/it]  7%|â–‹         | 438/6000 [14:46<3:03:49,  1.98s/it]                                                    {'loss': 0.0288, 'grad_norm': 7.936181545257568, 'learning_rate': 9.427118644067798e-06, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [14:46<3:03:49,  1.98s/it]  7%|â–‹         | 439/6000 [14:48<3:03:59,  1.99s/it]                                                    {'loss': 0.0624, 'grad_norm': 11.89488410949707, 'learning_rate': 9.425423728813559e-06, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [14:48<3:03:59,  1.99s/it]  7%|â–‹         | 440/6000 [14:50<3:03:58,  1.99s/it]                                                    {'loss': 0.025, 'grad_norm': 8.975011825561523, 'learning_rate': 9.423728813559322e-06, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [14:50<3:03:58,  1.99s/it]  7%|â–‹         | 441/6000 [14:52<3:06:12,  2.01s/it]                                                    {'loss': 0.0091, 'grad_norm': 3.4943485260009766, 'learning_rate': 9.422033898305086e-06, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [14:52<3:06:12,  2.01s/it]  7%|â–‹         | 442/6000 [14:54<3:05:35,  2.00s/it]                                                    {'loss': 0.1134, 'grad_norm': 23.386932373046875, 'learning_rate': 9.420338983050849e-06, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [14:54<3:05:35,  2.00s/it]  7%|â–‹         | 443/6000 [14:56<3:03:41,  1.98s/it]                                                    {'loss': 0.0242, 'grad_norm': 5.710024833679199, 'learning_rate': 9.41864406779661e-06, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [14:56<3:03:41,  1.98s/it]  7%|â–‹         | 444/6000 [14:58<3:02:32,  1.97s/it]                                                    {'loss': 0.0135, 'grad_norm': 7.8595147132873535, 'learning_rate': 9.416949152542374e-06, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [14:58<3:02:32,  1.97s/it]  7%|â–‹         | 445/6000 [15:00<3:03:02,  1.98s/it]                                                    {'loss': 0.1363, 'grad_norm': 21.45534896850586, 'learning_rate': 9.415254237288135e-06, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [15:00<3:03:02,  1.98s/it]  7%|â–‹         | 446/6000 [15:02<3:02:36,  1.97s/it]                                                    {'loss': 0.0137, 'grad_norm': 6.250262260437012, 'learning_rate': 9.413559322033899e-06, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [15:02<3:02:36,  1.97s/it]  7%|â–‹         | 447/6000 [15:04<3:02:28,  1.97s/it]                                                    {'loss': 0.1458, 'grad_norm': 27.898042678833008, 'learning_rate': 9.411864406779662e-06, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [15:04<3:02:28,  1.97s/it]  7%|â–‹         | 448/6000 [15:06<3:01:45,  1.96s/it]                                                    {'loss': 0.0701, 'grad_norm': 17.545120239257812, 'learning_rate': 9.410169491525425e-06, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [15:06<3:01:45,  1.96s/it]  7%|â–‹         | 449/6000 [15:08<3:03:11,  1.98s/it]                                                    {'loss': 0.0636, 'grad_norm': 17.55279541015625, 'learning_rate': 9.408474576271187e-06, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [15:08<3:03:11,  1.98s/it]  8%|â–Š         | 450/6000 [15:10<3:03:26,  1.98s/it]                                                    {'loss': 0.1994, 'grad_norm': 30.203113555908203, 'learning_rate': 9.40677966101695e-06, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [15:10<3:03:26,  1.98s/it][2025-11-18 10:52:56,660] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 451/6000 [15:13<3:24:31,  2.21s/it]                                                    {'loss': 0.0564, 'grad_norm': 113.49838256835938, 'learning_rate': 9.405084745762713e-06, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [15:13<3:24:31,  2.21s/it]  8%|â–Š         | 452/6000 [15:15<3:19:46,  2.16s/it]                                                    {'loss': 0.1205, 'grad_norm': 9.885090827941895, 'learning_rate': 9.403389830508477e-06, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [15:15<3:19:46,  2.16s/it]  8%|â–Š         | 453/6000 [15:17<3:14:31,  2.10s/it]                                                    {'loss': 0.1967, 'grad_norm': 23.213871002197266, 'learning_rate': 9.401694915254238e-06, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [15:17<3:14:31,  2.10s/it]  8%|â–Š         | 454/6000 [15:19<3:10:31,  2.06s/it]                                                    {'loss': 0.1524, 'grad_norm': 21.44278907775879, 'learning_rate': 9.4e-06, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [15:19<3:10:31,  2.06s/it]  8%|â–Š         | 455/6000 [15:21<3:05:44,  2.01s/it]                                                    {'loss': 0.07, 'grad_norm': 11.817984580993652, 'learning_rate': 9.398305084745763e-06, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [15:21<3:05:44,  2.01s/it]  8%|â–Š         | 456/6000 [15:23<3:05:46,  2.01s/it]                                                    {'loss': 0.2761, 'grad_norm': 26.950098037719727, 'learning_rate': 9.396610169491526e-06, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [15:23<3:05:46,  2.01s/it]  8%|â–Š         | 457/6000 [15:25<3:02:56,  1.98s/it]                                                    {'loss': 0.0408, 'grad_norm': 8.372591018676758, 'learning_rate': 9.39491525423729e-06, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [15:25<3:02:56,  1.98s/it]  8%|â–Š         | 458/6000 [15:26<3:01:22,  1.96s/it]                                                    {'loss': 0.0569, 'grad_norm': 14.884835243225098, 'learning_rate': 9.393220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [15:26<3:01:22,  1.96s/it]  8%|â–Š         | 459/6000 [15:29<3:08:45,  2.04s/it]                                                    {'loss': 0.188, 'grad_norm': 20.255311965942383, 'learning_rate': 9.391525423728814e-06, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [15:29<3:08:45,  2.04s/it]  8%|â–Š         | 460/6000 [15:31<3:08:04,  2.04s/it]                                                    {'loss': 0.0247, 'grad_norm': 6.784455299377441, 'learning_rate': 9.389830508474576e-06, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [15:31<3:08:04,  2.04s/it]  8%|â–Š         | 461/6000 [15:33<3:05:58,  2.01s/it]                                                    {'loss': 0.0743, 'grad_norm': 8.464004516601562, 'learning_rate': 9.38813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [15:33<3:05:58,  2.01s/it]  8%|â–Š         | 462/6000 [15:35<3:03:08,  1.98s/it]                                                    {'loss': 0.0566, 'grad_norm': 6.298216819763184, 'learning_rate': 9.386440677966103e-06, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [15:35<3:03:08,  1.98s/it]  8%|â–Š         | 463/6000 [15:37<3:03:54,  1.99s/it]                                                    {'loss': 0.1156, 'grad_norm': 16.67593765258789, 'learning_rate': 9.384745762711866e-06, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [15:37<3:03:54,  1.99s/it]  8%|â–Š         | 464/6000 [15:39<3:02:14,  1.98s/it]                                                    {'loss': 0.2598, 'grad_norm': 25.663448333740234, 'learning_rate': 9.383050847457627e-06, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [15:39<3:02:14,  1.98s/it]  8%|â–Š         | 465/6000 [15:41<3:02:39,  1.98s/it]                                                    {'loss': 0.2952, 'grad_norm': 23.257978439331055, 'learning_rate': 9.38135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [15:41<3:02:39,  1.98s/it]  8%|â–Š         | 466/6000 [15:43<3:03:41,  1.99s/it]                                                    {'loss': 0.0611, 'grad_norm': 9.964700698852539, 'learning_rate': 9.379661016949152e-06, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [15:43<3:03:41,  1.99s/it]  8%|â–Š         | 467/6000 [15:45<3:02:30,  1.98s/it]                                                    {'loss': 0.0919, 'grad_norm': 13.506168365478516, 'learning_rate': 9.377966101694916e-06, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [15:45<3:02:30,  1.98s/it]  8%|â–Š         | 468/6000 [15:47<3:04:06,  2.00s/it]                                                    {'loss': 0.0842, 'grad_norm': 10.312809944152832, 'learning_rate': 9.376271186440679e-06, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [15:47<3:04:06,  2.00s/it]  8%|â–Š         | 469/6000 [15:49<3:02:57,  1.98s/it]                                                    {'loss': 0.3117, 'grad_norm': 22.091575622558594, 'learning_rate': 9.374576271186442e-06, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [15:49<3:02:57,  1.98s/it]  8%|â–Š         | 470/6000 [15:50<3:00:45,  1.96s/it]                                                    {'loss': 0.0224, 'grad_norm': 6.637126922607422, 'learning_rate': 9.372881355932204e-06, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [15:50<3:00:45,  1.96s/it]  8%|â–Š         | 471/6000 [15:52<2:59:20,  1.95s/it]                                                    {'loss': 0.2106, 'grad_norm': 29.012901306152344, 'learning_rate': 9.371186440677967e-06, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [15:52<2:59:20,  1.95s/it]