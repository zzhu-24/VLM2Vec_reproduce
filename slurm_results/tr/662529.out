==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 50 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 09:49:34.568000 127667337836352 torch/distributed/run.py:779] 
W1118 09:49:34.568000 127667337836352 torch/distributed/run.py:779] *****************************************
W1118 09:49:34.568000 127667337836352 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 09:49:34.568000 127667337836352 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 09:49:41,447] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.70it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.94it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: setting up run 5jmjq48p
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_094941-5jmjq48p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/5jmjq48p
[2025-11-18 09:49:43,092] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.66it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.85it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-18 09:49:43,621] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGenerationWithTail(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 09:49:49,721] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 09:49:51,266] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 09:49:51,267] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 09:49:54,838] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 09:49:54,839] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 09:49:55,717] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 09:49:55,718] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 09:49:55,718] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 09:49:55,719] INFO [src.utils:19] ==================================================
[2025-11-18 09:49:55,719] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 09:49:55,720] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:49:55,720] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:49:55,720] INFO [src.utils:19] ==================================================
[2025-11-18 09:49:57,344] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:49:57,344] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:49:57,344] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:49:57,344] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:49:57,344] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:49:57,344] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:49:57,344] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:49:57,344] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:49:57,344] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:49:57,344] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:49:57,344] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:49:57,345] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:49:57,345] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:49:57,345] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:49:57,347] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 09:49:57,348] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-18 09:49:57,350] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 09:49:57,351] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:03<5:40:34,  3.41s/it]                                                  {'loss': 21.0466, 'grad_norm': 1069.351318359375, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:40:34,  3.41s/it]  0%|          | 2/6000 [00:05<4:15:22,  2.55s/it]                                                  {'loss': 17.8246, 'grad_norm': 1077.6939697265625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:15:22,  2.55s/it]  0%|          | 3/6000 [00:07<3:49:00,  2.29s/it]                                                  {'loss': 16.5815, 'grad_norm': 1788.7740478515625, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:49:00,  2.29s/it]  0%|          | 4/6000 [00:09<3:38:06,  2.18s/it]                                                  {'loss': 18.3531, 'grad_norm': 1386.70751953125, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:38:06,  2.18s/it]  0%|          | 5/6000 [00:11<3:28:12,  2.08s/it]                                                  {'loss': 19.0756, 'grad_norm': 1563.8358154296875, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:28:12,  2.08s/it]  0%|          | 6/6000 [00:13<3:23:05,  2.03s/it]                                                  {'loss': 18.8372, 'grad_norm': 1057.7586669921875, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:23:05,  2.03s/it]  0%|          | 7/6000 [00:15<3:19:22,  2.00s/it]                                                  {'loss': 19.1534, 'grad_norm': 1587.4427490234375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:19:22,  2.00s/it]  0%|          | 8/6000 [00:16<3:15:20,  1.96s/it]                                                  {'loss': 18.8733, 'grad_norm': 1661.335205078125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:16<3:15:20,  1.96s/it]  0%|          | 9/6000 [00:18<3:15:12,  1.95s/it]                                                  {'loss': 15.2229, 'grad_norm': 1290.0606689453125, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:18<3:15:12,  1.95s/it]  0%|          | 10/6000 [00:20<3:12:15,  1.93s/it]                                                   {'loss': 18.7181, 'grad_norm': 995.5575561523438, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:12:15,  1.93s/it]  0%|          | 11/6000 [00:22<3:15:10,  1.96s/it]                                                   {'loss': 22.2544, 'grad_norm': 1498.31201171875, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:15:10,  1.96s/it]  0%|          | 12/6000 [00:24<3:18:11,  1.99s/it]                                                   {'loss': 18.2091, 'grad_norm': 1110.728271484375, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:18:11,  1.99s/it]  0%|          | 13/6000 [00:26<3:18:11,  1.99s/it]                                                   {'loss': 20.565, 'grad_norm': 1620.6370849609375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:18:11,  1.99s/it]  0%|          | 14/6000 [00:28<3:17:44,  1.98s/it]                                                   {'loss': 19.7018, 'grad_norm': 1431.1268310546875, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:17:44,  1.98s/it]  0%|          | 15/6000 [00:30<3:18:10,  1.99s/it]                                                   {'loss': 16.6995, 'grad_norm': 1233.9609375, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:18:10,  1.99s/it]  0%|          | 16/6000 [00:32<3:17:17,  1.98s/it]                                                   {'loss': 18.433, 'grad_norm': 1517.930419921875, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:17:17,  1.98s/it]  0%|          | 17/6000 [00:34<3:16:00,  1.97s/it]                                                   {'loss': 15.2943, 'grad_norm': 1306.607421875, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:16:00,  1.97s/it]  0%|          | 18/6000 [00:36<3:16:05,  1.97s/it]                                                   {'loss': 14.471, 'grad_norm': 1986.1956787109375, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:16:05,  1.97s/it]  0%|          | 19/6000 [00:38<3:14:00,  1.95s/it]                                                   {'loss': 15.6454, 'grad_norm': 1386.275146484375, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:14:00,  1.95s/it]  0%|          | 20/6000 [00:40<3:16:34,  1.97s/it]                                                   {'loss': 16.3316, 'grad_norm': 1339.5875244140625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:16:34,  1.97s/it]  0%|          | 21/6000 [00:42<3:18:12,  1.99s/it]                                                   {'loss': 14.0715, 'grad_norm': 1487.6484375, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:18:12,  1.99s/it]  0%|          | 22/6000 [00:44<3:16:43,  1.97s/it]                                                   {'loss': 14.2289, 'grad_norm': 1416.266357421875, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:16:43,  1.97s/it]  0%|          | 23/6000 [00:46<3:17:09,  1.98s/it]                                                   {'loss': 13.2721, 'grad_norm': 1202.3167724609375, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:17:09,  1.98s/it]  0%|          | 24/6000 [00:48<3:19:14,  2.00s/it]                                                   {'loss': 10.089, 'grad_norm': 4075.6220703125, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:19:14,  2.00s/it]  0%|          | 25/6000 [00:50<3:16:45,  1.98s/it]                                                   {'loss': 11.8146, 'grad_norm': 1546.7232666015625, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:16:45,  1.98s/it]  0%|          | 26/6000 [00:52<3:16:41,  1.98s/it]                                                   {'loss': 12.3232, 'grad_norm': 1380.9058837890625, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:16:41,  1.98s/it]  0%|          | 27/6000 [00:54<3:17:33,  1.98s/it]                                                   {'loss': 12.0035, 'grad_norm': 1416.4112548828125, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:17:33,  1.98s/it]  0%|          | 28/6000 [00:56<3:27:32,  2.09s/it]                                                   {'loss': 8.9028, 'grad_norm': 1381.1607666015625, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:27:32,  2.09s/it]  0%|          | 29/6000 [00:58<3:24:20,  2.05s/it]                                                   {'loss': 8.6204, 'grad_norm': 1464.2452392578125, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:24:20,  2.05s/it]  0%|          | 30/6000 [01:00<3:21:18,  2.02s/it]                                                   {'loss': 8.0428, 'grad_norm': 1643.87890625, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:21:18,  2.02s/it]  1%|          | 31/6000 [01:02<3:20:34,  2.02s/it]                                                   {'loss': 6.6726, 'grad_norm': 1097.9530029296875, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:20:34,  2.02s/it]  1%|          | 32/6000 [01:04<3:18:20,  1.99s/it]                                                   {'loss': 6.6772, 'grad_norm': 913.0609130859375, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:18:20,  1.99s/it]  1%|          | 33/6000 [01:06<3:18:11,  1.99s/it]                                                   {'loss': 4.7324, 'grad_norm': 707.9573974609375, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:18:11,  1.99s/it]  1%|          | 34/6000 [01:08<3:17:56,  1.99s/it]                                                   {'loss': 4.933, 'grad_norm': 710.5473022460938, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:17:56,  1.99s/it]  1%|          | 35/6000 [01:10<3:16:48,  1.98s/it]                                                   {'loss': 4.6117, 'grad_norm': 746.2273559570312, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:16:48,  1.98s/it]  1%|          | 36/6000 [01:12<3:16:05,  1.97s/it]                                                   {'loss': 5.2776, 'grad_norm': 1097.9698486328125, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:16:05,  1.97s/it]  1%|          | 37/6000 [01:14<3:13:45,  1.95s/it]                                                   {'loss': 4.1584, 'grad_norm': 622.4751586914062, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:13:45,  1.95s/it]  1%|          | 38/6000 [01:16<3:13:09,  1.94s/it]                                                   {'loss': 4.0387, 'grad_norm': 542.6331176757812, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:13:09,  1.94s/it]  1%|          | 39/6000 [01:18<3:14:00,  1.95s/it]                                                   {'loss': 4.0679, 'grad_norm': 574.6370849609375, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:14:00,  1.95s/it]  1%|          | 40/6000 [01:20<3:14:33,  1.96s/it]                                                   {'loss': 3.736, 'grad_norm': 387.05914306640625, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:14:33,  1.96s/it]  1%|          | 41/6000 [01:22<3:14:10,  1.96s/it]                                                   {'loss': 4.1409, 'grad_norm': 548.4644775390625, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:22<3:14:10,  1.96s/it]  1%|          | 42/6000 [01:24<3:12:52,  1.94s/it]                                                   {'loss': 3.9035, 'grad_norm': 602.7800903320312, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:24<3:12:52,  1.94s/it]  1%|          | 43/6000 [01:26<3:25:36,  2.07s/it]                                                   {'loss': 3.5999, 'grad_norm': 538.283203125, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:25:36,  2.07s/it]  1%|          | 44/6000 [01:28<3:25:51,  2.07s/it]                                                   {'loss': 3.7198, 'grad_norm': 308.3961181640625, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:25:51,  2.07s/it]  1%|          | 45/6000 [01:30<3:22:33,  2.04s/it]                                                   {'loss': 3.3525, 'grad_norm': 277.2463073730469, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:22:33,  2.04s/it]  1%|          | 46/6000 [01:32<3:20:32,  2.02s/it]                                                   {'loss': 3.6179, 'grad_norm': 298.2930603027344, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:20:32,  2.02s/it]  1%|          | 47/6000 [01:34<3:20:16,  2.02s/it]                                                   {'loss': 3.5946, 'grad_norm': 259.7156677246094, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:20:16,  2.02s/it]  1%|          | 48/6000 [01:36<3:18:55,  2.01s/it]                                                   {'loss': 3.5039, 'grad_norm': 297.2676696777344, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:18:55,  2.01s/it]  1%|          | 49/6000 [01:38<3:18:28,  2.00s/it]                                                   {'loss': 3.3257, 'grad_norm': 267.7792663574219, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:18:28,  2.00s/it]  1%|          | 50/6000 [01:40<3:17:43,  1.99s/it]                                                   {'loss': 3.37, 'grad_norm': 294.56890869140625, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:17:43,  1.99s/it][2025-11-18 09:51:37,969] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50
[2025-11-18 09:51:38,291] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-50/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  1%|          | 51/6000 [01:43<3:37:58,  2.20s/it]                                                   {'loss': 3.4551, 'grad_norm': 203.25125122070312, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:43<3:37:58,  2.20s/it]  1%|          | 52/6000 [01:45<3:30:09,  2.12s/it]                                                   {'loss': 3.4445, 'grad_norm': 161.99356079101562, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:45<3:30:09,  2.12s/it]  1%|          | 53/6000 [01:47<3:28:44,  2.11s/it]                                                   {'loss': 3.8525, 'grad_norm': 431.4342346191406, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:47<3:28:44,  2.11s/it]  1%|          | 54/6000 [01:49<3:22:37,  2.04s/it]                                                   {'loss': 3.623, 'grad_norm': 259.517333984375, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:49<3:22:37,  2.04s/it]  1%|          | 55/6000 [01:51<3:18:55,  2.01s/it]                                                   {'loss': 3.1464, 'grad_norm': 221.44857788085938, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:51<3:18:55,  2.01s/it]  1%|          | 56/6000 [01:53<3:16:51,  1.99s/it]                                                   {'loss': 3.2967, 'grad_norm': 210.45172119140625, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:53<3:16:51,  1.99s/it]  1%|          | 57/6000 [01:55<3:16:35,  1.98s/it]                                                   {'loss': 3.1651, 'grad_norm': 211.89419555664062, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:55<3:16:35,  1.98s/it]  1%|          | 58/6000 [01:56<3:15:05,  1.97s/it]                                                   {'loss': 3.2719, 'grad_norm': 131.01620483398438, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:56<3:15:05,  1.97s/it]  1%|          | 59/6000 [01:58<3:12:51,  1.95s/it]                                                   {'loss': 3.0989, 'grad_norm': 165.1202392578125, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:58<3:12:51,  1.95s/it]  1%|          | 60/6000 [02:00<3:12:10,  1.94s/it]                                                   {'loss': 3.1579, 'grad_norm': 140.64820861816406, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:00<3:12:10,  1.94s/it]  1%|          | 61/6000 [02:02<3:11:58,  1.94s/it]                                                   {'loss': 2.9927, 'grad_norm': 110.61555480957031, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:02<3:11:58,  1.94s/it]  1%|          | 62/6000 [02:04<3:12:32,  1.95s/it]                                                   {'loss': 3.385, 'grad_norm': 170.1160430908203, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:04<3:12:32,  1.95s/it]  1%|          | 63/6000 [02:06<3:13:01,  1.95s/it]                                                   {'loss': 2.9197, 'grad_norm': 85.19436645507812, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:06<3:13:01,  1.95s/it]  1%|          | 64/6000 [02:08<3:13:05,  1.95s/it]                                                   {'loss': 3.0271, 'grad_norm': 83.61669158935547, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:08<3:13:05,  1.95s/it]  1%|          | 65/6000 [02:10<3:12:09,  1.94s/it]                                                   {'loss': 3.5114, 'grad_norm': 425.9588317871094, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:10<3:12:09,  1.94s/it]  1%|          | 66/6000 [02:12<3:15:31,  1.98s/it]                                                   {'loss': 2.9194, 'grad_norm': 78.35978698730469, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:12<3:15:31,  1.98s/it]  1%|          | 67/6000 [02:14<3:15:17,  1.97s/it]                                                   {'loss': 3.0938, 'grad_norm': 93.5488052368164, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:14<3:15:17,  1.97s/it]  1%|          | 68/6000 [02:16<3:14:37,  1.97s/it]                                                   {'loss': 3.1509, 'grad_norm': 75.54224395751953, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:16<3:14:37,  1.97s/it]  1%|          | 69/6000 [02:18<3:14:33,  1.97s/it]                                                   {'loss': 3.0264, 'grad_norm': 76.74507141113281, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:18<3:14:33,  1.97s/it]  1%|          | 70/6000 [02:20<3:16:10,  1.98s/it]                                                   {'loss': 2.9649, 'grad_norm': 86.13558197021484, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:20<3:16:10,  1.98s/it]  1%|          | 71/6000 [02:22<3:20:37,  2.03s/it]                                                   {'loss': 3.0997, 'grad_norm': 92.73820495605469, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:22<3:20:37,  2.03s/it]  1%|          | 72/6000 [02:24<3:20:28,  2.03s/it]                                                   {'loss': 2.9978, 'grad_norm': 75.51264953613281, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:24<3:20:28,  2.03s/it]  1%|          | 73/6000 [02:26<3:18:12,  2.01s/it]                                                   {'loss': 2.9109, 'grad_norm': 89.36650848388672, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:26<3:18:12,  2.01s/it]  1%|          | 74/6000 [02:28<3:14:12,  1.97s/it]                                                   {'loss': 2.9101, 'grad_norm': 61.05388259887695, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:28<3:14:12,  1.97s/it]  1%|â–         | 75/6000 [02:30<3:12:25,  1.95s/it]                                                   {'loss': 2.9295, 'grad_norm': 60.42760467529297, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:30<3:12:25,  1.95s/it]  1%|â–         | 76/6000 [02:32<3:12:26,  1.95s/it]                                                   {'loss': 2.9088, 'grad_norm': 78.06971740722656, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:32<3:12:26,  1.95s/it]  1%|â–         | 77/6000 [02:34<3:19:31,  2.02s/it]                                                   {'loss': 3.1926, 'grad_norm': 117.91846466064453, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:34<3:19:31,  2.02s/it]  1%|â–         | 78/6000 [02:36<3:19:15,  2.02s/it]                                                   {'loss': 2.9611, 'grad_norm': 67.79438781738281, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:36<3:19:15,  2.02s/it]  1%|â–         | 79/6000 [02:38<3:18:19,  2.01s/it]                                                   {'loss': 2.8745, 'grad_norm': 136.8687286376953, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:38<3:18:19,  2.01s/it]  1%|â–         | 80/6000 [02:40<3:17:47,  2.00s/it]                                                   {'loss': 2.9954, 'grad_norm': 100.8056869506836, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:40<3:17:47,  2.00s/it]  1%|â–         | 81/6000 [02:42<3:17:15,  2.00s/it]                                                   {'loss': 2.8156, 'grad_norm': 60.19970703125, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:42<3:17:15,  2.00s/it]  1%|â–         | 82/6000 [02:44<3:17:12,  2.00s/it]                                                   {'loss': 2.9397, 'grad_norm': 64.264404296875, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:44<3:17:12,  2.00s/it]  1%|â–         | 83/6000 [02:46<3:16:06,  1.99s/it]                                                   {'loss': 2.9048, 'grad_norm': 78.96659851074219, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:46<3:16:06,  1.99s/it]  1%|â–         | 84/6000 [02:48<3:15:23,  1.98s/it]                                                   {'loss': 3.0337, 'grad_norm': 67.47682189941406, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:48<3:15:23,  1.98s/it]  1%|â–         | 85/6000 [02:50<3:17:50,  2.01s/it]                                                   {'loss': 2.8022, 'grad_norm': 53.666831970214844, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:50<3:17:50,  2.01s/it]  1%|â–         | 86/6000 [02:52<3:17:02,  2.00s/it]                                                   {'loss': 2.8408, 'grad_norm': 53.72129440307617, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:52<3:17:02,  2.00s/it]  1%|â–         | 87/6000 [02:54<3:14:13,  1.97s/it]                                                   {'loss': 2.892, 'grad_norm': 53.03213882446289, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:54<3:14:13,  1.97s/it]  1%|â–         | 88/6000 [02:56<3:13:16,  1.96s/it]                                                   {'loss': 3.3961, 'grad_norm': 73.22412872314453, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:56<3:13:16,  1.96s/it]  1%|â–         | 89/6000 [02:58<3:12:33,  1.95s/it]                                                   {'loss': 2.7551, 'grad_norm': 51.66304016113281, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:58<3:12:33,  1.95s/it]  2%|â–         | 90/6000 [03:00<3:12:47,  1.96s/it]                                                   {'loss': 2.8488, 'grad_norm': 43.51302719116211, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [03:00<3:12:47,  1.96s/it]  2%|â–         | 91/6000 [03:02<3:12:30,  1.95s/it]                                                   {'loss': 2.6621, 'grad_norm': 41.5609245300293, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:02<3:12:30,  1.95s/it]  2%|â–         | 92/6000 [03:04<3:14:20,  1.97s/it]                                                   {'loss': 2.7593, 'grad_norm': 65.4051284790039, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:04<3:14:20,  1.97s/it]  2%|â–         | 93/6000 [03:06<3:14:57,  1.98s/it]                                                   {'loss': 2.7971, 'grad_norm': 55.88555145263672, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:06<3:14:57,  1.98s/it]  2%|â–         | 94/6000 [03:08<3:15:31,  1.99s/it]                                                   {'loss': 2.9507, 'grad_norm': 68.26045227050781, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:08<3:15:31,  1.99s/it]  2%|â–         | 95/6000 [03:10<3:15:25,  1.99s/it]                                                   {'loss': 2.847, 'grad_norm': 48.66565704345703, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:10<3:15:25,  1.99s/it]  2%|â–         | 96/6000 [03:12<3:13:09,  1.96s/it]                                                   {'loss': 2.8199, 'grad_norm': 52.99598693847656, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:12<3:13:09,  1.96s/it]  2%|â–         | 97/6000 [03:14<3:13:52,  1.97s/it]                                                   {'loss': 2.8412, 'grad_norm': 75.34896850585938, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:14<3:13:52,  1.97s/it]  2%|â–         | 98/6000 [03:16<3:15:02,  1.98s/it]                                                   {'loss': 2.7079, 'grad_norm': 48.95573425292969, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:16<3:15:02,  1.98s/it]  2%|â–         | 99/6000 [03:18<3:14:09,  1.97s/it]                                                   {'loss': 2.7769, 'grad_norm': 69.27410125732422, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:18<3:14:09,  1.97s/it]  2%|â–         | 100/6000 [03:19<3:11:54,  1.95s/it]                                                    {'loss': 2.6087, 'grad_norm': 59.706214904785156, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:19<3:11:54,  1.95s/it][2025-11-18 09:53:17,311] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100
[2025-11-18 09:53:17,612] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  2%|â–         | 101/6000 [03:23<3:44:58,  2.29s/it]                                                    {'loss': 2.6385, 'grad_norm': 57.660945892333984, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:23<3:44:58,  2.29s/it]  2%|â–         | 102/6000 [03:24<3:33:54,  2.18s/it]                                                    {'loss': 2.7483, 'grad_norm': 74.46939849853516, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:24<3:33:54,  2.18s/it]  2%|â–         | 103/6000 [03:26<3:28:35,  2.12s/it]                                                    {'loss': 2.6712, 'grad_norm': 57.76376724243164, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:26<3:28:35,  2.12s/it]  2%|â–         | 104/6000 [03:29<3:28:50,  2.13s/it]                                                    {'loss': 2.5987, 'grad_norm': 55.00155258178711, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:29<3:28:50,  2.13s/it]  2%|â–         | 105/6000 [03:31<3:23:47,  2.07s/it]                                                    {'loss': 2.754, 'grad_norm': 57.51296615600586, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:31<3:23:47,  2.07s/it]  2%|â–         | 106/6000 [03:33<3:25:16,  2.09s/it]                                                    {'loss': 2.5096, 'grad_norm': 56.063655853271484, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:33<3:25:16,  2.09s/it]  2%|â–         | 107/6000 [03:35<3:20:34,  2.04s/it]                                                    {'loss': 2.4084, 'grad_norm': 57.28376007080078, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:35<3:20:34,  2.04s/it]  2%|â–         | 108/6000 [03:37<3:20:27,  2.04s/it]                                                    {'loss': 2.2607, 'grad_norm': 72.77152252197266, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:37<3:20:27,  2.04s/it]  2%|â–         | 109/6000 [03:39<3:21:29,  2.05s/it]                                                    {'loss': 2.2368, 'grad_norm': 63.75996780395508, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:39<3:21:29,  2.05s/it]  2%|â–         | 110/6000 [03:41<3:19:03,  2.03s/it]                                                    {'loss': 2.0234, 'grad_norm': 78.93629455566406, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:41<3:19:03,  2.03s/it]  2%|â–         | 111/6000 [03:43<3:19:24,  2.03s/it]                                                    {'loss': 2.2772, 'grad_norm': 52.134117126464844, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:43<3:19:24,  2.03s/it]  2%|â–         | 112/6000 [03:45<3:21:43,  2.06s/it]                                                    {'loss': 2.1456, 'grad_norm': 119.51008605957031, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:45<3:21:43,  2.06s/it]  2%|â–         | 113/6000 [03:47<3:20:52,  2.05s/it]                                                    {'loss': 1.9983, 'grad_norm': 59.5008659362793, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:47<3:20:52,  2.05s/it]  2%|â–         | 114/6000 [03:49<3:18:44,  2.03s/it]                                                    {'loss': 2.0715, 'grad_norm': 95.00015258789062, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:49<3:18:44,  2.03s/it]  2%|â–         | 115/6000 [03:51<3:18:23,  2.02s/it]                                                    {'loss': 2.3711, 'grad_norm': 47.19188690185547, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:51<3:18:23,  2.02s/it]  2%|â–         | 116/6000 [03:53<3:15:27,  1.99s/it]                                                    {'loss': 2.1229, 'grad_norm': 48.943355560302734, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:53<3:15:27,  1.99s/it]  2%|â–         | 117/6000 [03:55<3:13:16,  1.97s/it]                                                    {'loss': 1.4418, 'grad_norm': 60.666473388671875, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:55<3:13:16,  1.97s/it]  2%|â–         | 118/6000 [03:57<3:18:37,  2.03s/it]                                                    {'loss': 1.9577, 'grad_norm': 46.382469177246094, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:57<3:18:37,  2.03s/it]  2%|â–         | 119/6000 [03:59<3:14:16,  1.98s/it]                                                    {'loss': 1.9339, 'grad_norm': 55.98674774169922, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:59<3:14:16,  1.98s/it]  2%|â–         | 120/6000 [04:01<3:13:07,  1.97s/it]                                                    {'loss': 1.669, 'grad_norm': 50.7817268371582, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [04:01<3:13:07,  1.97s/it]  2%|â–         | 121/6000 [04:03<3:13:34,  1.98s/it]                                                    {'loss': 1.5707, 'grad_norm': 52.48556900024414, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [04:03<3:13:34,  1.98s/it]  2%|â–         | 122/6000 [04:05<3:14:08,  1.98s/it]                                                    {'loss': 1.3125, 'grad_norm': 55.51401901245117, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:05<3:14:08,  1.98s/it]  2%|â–         | 123/6000 [04:07<3:13:27,  1.98s/it]                                                    {'loss': 1.0776, 'grad_norm': 57.652015686035156, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:07<3:13:27,  1.98s/it]  2%|â–         | 124/6000 [04:09<3:13:25,  1.98s/it]                                                    {'loss': 1.5011, 'grad_norm': 84.15451049804688, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:09<3:13:25,  1.98s/it]  2%|â–         | 125/6000 [04:11<3:17:23,  2.02s/it]                                                    {'loss': 1.5223, 'grad_norm': 117.38201904296875, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:11<3:17:23,  2.02s/it]  2%|â–         | 126/6000 [04:13<3:15:01,  1.99s/it]                                                    {'loss': 1.0952, 'grad_norm': 79.4593734741211, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:13<3:15:01,  1.99s/it]  2%|â–         | 127/6000 [04:15<3:16:40,  2.01s/it]                                                    {'loss': 0.6416, 'grad_norm': 73.595703125, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:15<3:16:40,  2.01s/it]  2%|â–         | 128/6000 [04:17<3:15:58,  2.00s/it]                                                    {'loss': 0.5935, 'grad_norm': 99.07693481445312, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:17<3:15:58,  2.00s/it]  2%|â–         | 129/6000 [04:19<3:14:13,  1.98s/it]                                                    {'loss': 0.786, 'grad_norm': 71.5103759765625, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:19<3:14:13,  1.98s/it]  2%|â–         | 130/6000 [04:21<3:12:59,  1.97s/it]                                                    {'loss': 0.456, 'grad_norm': 81.3139877319336, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:21<3:12:59,  1.97s/it]  2%|â–         | 131/6000 [04:22<3:11:25,  1.96s/it]                                                    {'loss': 0.4609, 'grad_norm': 82.30314636230469, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:22<3:11:25,  1.96s/it]  2%|â–         | 132/6000 [04:24<3:11:38,  1.96s/it]                                                    {'loss': 0.466, 'grad_norm': 119.2837142944336, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:24<3:11:38,  1.96s/it]  2%|â–         | 133/6000 [04:26<3:12:01,  1.96s/it]                                                    {'loss': 0.5911, 'grad_norm': 130.64820861816406, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:26<3:12:01,  1.96s/it]  2%|â–         | 134/6000 [04:28<3:12:13,  1.97s/it]                                                    {'loss': 0.5309, 'grad_norm': 92.48887634277344, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:28<3:12:13,  1.97s/it]  2%|â–         | 135/6000 [04:30<3:11:21,  1.96s/it]                                                    {'loss': 0.4854, 'grad_norm': 59.651912689208984, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:30<3:11:21,  1.96s/it]  2%|â–         | 136/6000 [04:32<3:10:05,  1.95s/it]                                                    {'loss': 0.1742, 'grad_norm': 21.842512130737305, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:32<3:10:05,  1.95s/it]  2%|â–         | 137/6000 [04:34<3:14:55,  1.99s/it]                                                    {'loss': 0.2507, 'grad_norm': 54.70597457885742, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:34<3:14:55,  1.99s/it]  2%|â–         | 138/6000 [04:36<3:13:18,  1.98s/it]                                                    {'loss': 0.3689, 'grad_norm': 44.185142517089844, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:36<3:13:18,  1.98s/it]  2%|â–         | 139/6000 [04:38<3:14:43,  1.99s/it]                                                    {'loss': 0.5823, 'grad_norm': 58.2297248840332, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:38<3:14:43,  1.99s/it]  2%|â–         | 140/6000 [04:40<3:17:53,  2.03s/it]                                                    {'loss': 0.3916, 'grad_norm': 53.81743621826172, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:40<3:17:53,  2.03s/it]  2%|â–         | 141/6000 [04:42<3:16:11,  2.01s/it]                                                    {'loss': 0.3545, 'grad_norm': 38.674827575683594, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:42<3:16:11,  2.01s/it]  2%|â–         | 142/6000 [04:44<3:13:31,  1.98s/it]                                                    {'loss': 0.2088, 'grad_norm': 25.072572708129883, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:44<3:13:31,  1.98s/it]  2%|â–         | 143/6000 [04:46<3:11:41,  1.96s/it]                                                    {'loss': 0.2013, 'grad_norm': 27.673738479614258, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:46<3:11:41,  1.96s/it]  2%|â–         | 144/6000 [04:48<3:11:09,  1.96s/it]                                                    {'loss': 0.1116, 'grad_norm': 14.709391593933105, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:48<3:11:09,  1.96s/it]  2%|â–         | 145/6000 [04:50<3:08:29,  1.93s/it]                                                    {'loss': 0.0849, 'grad_norm': 12.662179946899414, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:50<3:08:29,  1.93s/it]  2%|â–         | 146/6000 [04:52<3:10:00,  1.95s/it]                                                    {'loss': 0.3246, 'grad_norm': 38.3782844543457, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:52<3:10:00,  1.95s/it]  2%|â–         | 147/6000 [04:54<3:09:09,  1.94s/it]                                                    {'loss': 0.3376, 'grad_norm': 93.49280548095703, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:54<3:09:09,  1.94s/it]  2%|â–         | 148/6000 [04:56<3:11:04,  1.96s/it]                                                    {'loss': 0.1919, 'grad_norm': 30.742820739746094, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:56<3:11:04,  1.96s/it]  2%|â–         | 149/6000 [04:58<3:17:00,  2.02s/it]                                                    {'loss': 0.2234, 'grad_norm': 34.699684143066406, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:58<3:17:00,  2.02s/it]  2%|â–Ž         | 150/6000 [05:00<3:14:35,  2.00s/it]                                                    {'loss': 0.0659, 'grad_norm': 12.753283500671387, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [05:00<3:14:35,  2.00s/it][2025-11-18 09:54:57,912] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150
[2025-11-18 09:54:58,205] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 151/6000 [05:03<3:34:56,  2.20s/it]                                                    {'loss': 0.1305, 'grad_norm': 13.402701377868652, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [05:03<3:34:56,  2.20s/it]  3%|â–Ž         | 152/6000 [05:05<3:30:45,  2.16s/it]                                                    {'loss': 0.1508, 'grad_norm': 20.86187744140625, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:05<3:30:45,  2.16s/it]  3%|â–Ž         | 153/6000 [05:07<3:23:27,  2.09s/it]                                                    {'loss': 0.1292, 'grad_norm': 21.22998046875, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:07<3:23:27,  2.09s/it]  3%|â–Ž         | 154/6000 [05:09<3:19:36,  2.05s/it]                                                    {'loss': 0.1836, 'grad_norm': 29.505144119262695, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:09<3:19:36,  2.05s/it]  3%|â–Ž         | 155/6000 [05:11<3:17:23,  2.03s/it]                                                    {'loss': 0.128, 'grad_norm': 18.799358367919922, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:11<3:17:23,  2.03s/it]  3%|â–Ž         | 156/6000 [05:13<3:15:45,  2.01s/it]                                                    {'loss': 0.0807, 'grad_norm': 35.013946533203125, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:13<3:15:45,  2.01s/it]  3%|â–Ž         | 157/6000 [05:15<3:17:18,  2.03s/it]                                                    {'loss': 0.1358, 'grad_norm': 30.627870559692383, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:15<3:17:18,  2.03s/it]  3%|â–Ž         | 158/6000 [05:17<3:14:59,  2.00s/it]                                                    {'loss': 0.0642, 'grad_norm': 13.736684799194336, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:17<3:14:59,  2.00s/it]  3%|â–Ž         | 159/6000 [05:19<3:12:52,  1.98s/it]                                                    {'loss': 0.157, 'grad_norm': 37.89456558227539, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:19<3:12:52,  1.98s/it]  3%|â–Ž         | 160/6000 [05:21<3:24:58,  2.11s/it]                                                    {'loss': 0.1076, 'grad_norm': 14.26515007019043, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:21<3:24:58,  2.11s/it]  3%|â–Ž         | 161/6000 [05:23<3:20:51,  2.06s/it]                                                    {'loss': 0.1713, 'grad_norm': 24.06695556640625, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:23<3:20:51,  2.06s/it]  3%|â–Ž         | 162/6000 [05:25<3:16:45,  2.02s/it]                                                    {'loss': 0.0914, 'grad_norm': 14.643328666687012, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:25<3:16:45,  2.02s/it]  3%|â–Ž         | 163/6000 [05:27<3:18:48,  2.04s/it]                                                    {'loss': 0.1241, 'grad_norm': 20.336172103881836, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:27<3:18:48,  2.04s/it]  3%|â–Ž         | 164/6000 [05:29<3:15:36,  2.01s/it]                                                    {'loss': 0.3363, 'grad_norm': 31.485763549804688, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:29<3:15:36,  2.01s/it]  3%|â–Ž         | 165/6000 [05:31<3:16:48,  2.02s/it]                                                    {'loss': 0.073, 'grad_norm': 12.585763931274414, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:31<3:16:48,  2.02s/it]  3%|â–Ž         | 166/6000 [05:33<3:13:47,  1.99s/it]                                                    {'loss': 0.1173, 'grad_norm': 15.41775894165039, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:33<3:13:47,  1.99s/it]  3%|â–Ž         | 167/6000 [05:35<3:12:21,  1.98s/it]                                                    {'loss': 0.29, 'grad_norm': 74.13148498535156, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:35<3:12:21,  1.98s/it]  3%|â–Ž         | 168/6000 [05:37<3:11:02,  1.97s/it]                                                    {'loss': 0.2085, 'grad_norm': 33.914920806884766, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:37<3:11:02,  1.97s/it]  3%|â–Ž         | 169/6000 [05:39<3:15:39,  2.01s/it]                                                    {'loss': 0.297, 'grad_norm': 38.98237228393555, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:39<3:15:39,  2.01s/it]  3%|â–Ž         | 170/6000 [05:41<3:14:13,  2.00s/it]                                                    {'loss': 0.1719, 'grad_norm': 20.79194450378418, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:41<3:14:13,  2.00s/it]  3%|â–Ž         | 171/6000 [05:43<3:11:53,  1.98s/it]                                                    {'loss': 0.1408, 'grad_norm': 17.38123893737793, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:43<3:11:53,  1.98s/it]  3%|â–Ž         | 172/6000 [05:45<3:13:02,  1.99s/it]                                                    {'loss': 0.2423, 'grad_norm': 42.48380661010742, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:45<3:13:02,  1.99s/it]  3%|â–Ž         | 173/6000 [05:47<3:13:06,  1.99s/it]                                                    {'loss': 0.0776, 'grad_norm': 15.086772918701172, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:47<3:13:06,  1.99s/it]  3%|â–Ž         | 174/6000 [05:49<3:19:11,  2.05s/it]                                                    {'loss': 0.045, 'grad_norm': 9.877551078796387, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:49<3:19:11,  2.05s/it]  3%|â–Ž         | 175/6000 [05:51<3:16:46,  2.03s/it]                                                    {'loss': 0.2636, 'grad_norm': 35.75090789794922, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:51<3:16:46,  2.03s/it]  3%|â–Ž         | 176/6000 [05:53<3:16:15,  2.02s/it]                                                    {'loss': 0.0799, 'grad_norm': 20.33339500427246, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:53<3:16:15,  2.02s/it]  3%|â–Ž         | 177/6000 [05:55<3:15:14,  2.01s/it]                                                    {'loss': 0.1295, 'grad_norm': 18.3162841796875, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:55<3:15:14,  2.01s/it]  3%|â–Ž         | 178/6000 [05:57<3:14:05,  2.00s/it]                                                    {'loss': 0.1513, 'grad_norm': 24.41773223876953, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:57<3:14:05,  2.00s/it]  3%|â–Ž         | 179/6000 [05:59<3:11:59,  1.98s/it]                                                    {'loss': 0.0773, 'grad_norm': 8.66010856628418, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [05:59<3:11:59,  1.98s/it]  3%|â–Ž         | 180/6000 [06:01<3:13:02,  1.99s/it]                                                    {'loss': 0.3336, 'grad_norm': 27.380964279174805, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [06:01<3:13:02,  1.99s/it]  3%|â–Ž         | 181/6000 [06:03<3:15:31,  2.02s/it]                                                    {'loss': 0.329, 'grad_norm': 32.713321685791016, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [06:03<3:15:31,  2.02s/it]  3%|â–Ž         | 182/6000 [06:05<3:14:32,  2.01s/it]                                                    {'loss': 0.1947, 'grad_norm': 26.34096908569336, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [06:05<3:14:32,  2.01s/it]  3%|â–Ž         | 183/6000 [06:07<3:15:10,  2.01s/it]                                                    {'loss': 0.1517, 'grad_norm': 16.838916778564453, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:07<3:15:10,  2.01s/it]  3%|â–Ž         | 184/6000 [06:09<3:15:08,  2.01s/it]                                                    {'loss': 0.1707, 'grad_norm': 17.012269973754883, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:09<3:15:08,  2.01s/it]  3%|â–Ž         | 185/6000 [06:11<3:12:56,  1.99s/it]                                                    {'loss': 0.1714, 'grad_norm': 22.896760940551758, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:11<3:12:56,  1.99s/it]  3%|â–Ž         | 186/6000 [06:13<3:12:40,  1.99s/it]                                                    {'loss': 0.1421, 'grad_norm': 22.1920108795166, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:13<3:12:40,  1.99s/it]  3%|â–Ž         | 187/6000 [06:15<3:12:59,  1.99s/it]                                                    {'loss': 0.1458, 'grad_norm': 18.63217544555664, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:15<3:12:59,  1.99s/it]  3%|â–Ž         | 188/6000 [06:17<3:11:56,  1.98s/it]                                                    {'loss': 0.1881, 'grad_norm': 29.30349349975586, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:17<3:11:56,  1.98s/it]  3%|â–Ž         | 189/6000 [06:19<3:10:54,  1.97s/it]                                                    {'loss': 0.225, 'grad_norm': 26.391035079956055, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:19<3:10:54,  1.97s/it]  3%|â–Ž         | 190/6000 [06:21<3:09:33,  1.96s/it]                                                    {'loss': 0.0925, 'grad_norm': 19.34986114501953, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:21<3:09:33,  1.96s/it]  3%|â–Ž         | 191/6000 [06:23<3:10:04,  1.96s/it]                                                    {'loss': 0.0765, 'grad_norm': 13.590498924255371, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:23<3:10:04,  1.96s/it]  3%|â–Ž         | 192/6000 [06:25<3:17:01,  2.04s/it]                                                    {'loss': 0.1183, 'grad_norm': 17.783069610595703, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:25<3:17:01,  2.04s/it]  3%|â–Ž         | 193/6000 [06:27<3:16:12,  2.03s/it]                                                    {'loss': 0.2448, 'grad_norm': 21.198881149291992, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:27<3:16:12,  2.03s/it]  3%|â–Ž         | 194/6000 [06:29<3:12:49,  1.99s/it]                                                    {'loss': 0.1609, 'grad_norm': 20.160369873046875, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:29<3:12:49,  1.99s/it]  3%|â–Ž         | 195/6000 [06:31<3:12:27,  1.99s/it]                                                    {'loss': 0.0922, 'grad_norm': 102.57079315185547, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:31<3:12:27,  1.99s/it]  3%|â–Ž         | 196/6000 [06:33<3:11:41,  1.98s/it]                                                    {'loss': 0.0606, 'grad_norm': 11.743502616882324, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:33<3:11:41,  1.98s/it]  3%|â–Ž         | 197/6000 [06:35<3:12:28,  1.99s/it]                                                    {'loss': 0.0732, 'grad_norm': 15.87545394897461, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:35<3:12:28,  1.99s/it]  3%|â–Ž         | 198/6000 [06:37<3:12:25,  1.99s/it]                                                    {'loss': 0.0524, 'grad_norm': 7.831129550933838, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:37<3:12:25,  1.99s/it]  3%|â–Ž         | 199/6000 [06:39<3:11:55,  1.99s/it]                                                    {'loss': 0.083, 'grad_norm': 11.57292366027832, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:39<3:11:55,  1.99s/it]  3%|â–Ž         | 200/6000 [06:41<3:15:27,  2.02s/it]                                                    {'loss': 0.28, 'grad_norm': 38.28730010986328, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:41<3:15:27,  2.02s/it][2025-11-18 09:56:38,683] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200
[2025-11-18 09:56:38,964] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  3%|â–Ž         | 201/6000 [06:44<3:36:38,  2.24s/it]                                                    {'loss': 0.3727, 'grad_norm': 30.742713928222656, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:44<3:36:38,  2.24s/it]  3%|â–Ž         | 202/6000 [06:46<3:29:09,  2.16s/it]                                                    {'loss': 0.1749, 'grad_norm': 22.543228149414062, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:46<3:29:09,  2.16s/it]  3%|â–Ž         | 203/6000 [06:47<3:22:47,  2.10s/it]                                                    {'loss': 0.1369, 'grad_norm': 18.640361785888672, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:48<3:22:47,  2.10s/it]  3%|â–Ž         | 204/6000 [06:49<3:18:51,  2.06s/it]                                                    {'loss': 0.0381, 'grad_norm': 5.669208526611328, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:49<3:18:51,  2.06s/it]  3%|â–Ž         | 205/6000 [06:51<3:15:06,  2.02s/it]                                                    {'loss': 0.1379, 'grad_norm': 19.149187088012695, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:51<3:15:06,  2.02s/it]  3%|â–Ž         | 206/6000 [06:53<3:11:52,  1.99s/it]                                                    {'loss': 0.0308, 'grad_norm': 10.207271575927734, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:53<3:11:52,  1.99s/it]  3%|â–Ž         | 207/6000 [06:55<3:13:20,  2.00s/it]                                                    {'loss': 0.0607, 'grad_norm': 11.886077880859375, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:55<3:13:20,  2.00s/it]  3%|â–Ž         | 208/6000 [06:57<3:10:12,  1.97s/it]                                                    {'loss': 0.1737, 'grad_norm': 20.540185928344727, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [06:57<3:10:12,  1.97s/it]  3%|â–Ž         | 209/6000 [06:59<3:10:59,  1.98s/it]                                                    {'loss': 0.0589, 'grad_norm': 12.158981323242188, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [06:59<3:10:59,  1.98s/it]  4%|â–Ž         | 210/6000 [07:01<3:13:07,  2.00s/it]                                                    {'loss': 0.0668, 'grad_norm': 12.856836318969727, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [07:01<3:13:07,  2.00s/it]  4%|â–Ž         | 211/6000 [07:03<3:12:57,  2.00s/it]                                                    {'loss': 0.1158, 'grad_norm': 23.38189697265625, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [07:03<3:12:57,  2.00s/it]  4%|â–Ž         | 212/6000 [07:05<3:14:09,  2.01s/it]                                                    {'loss': 0.2069, 'grad_norm': 20.827686309814453, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [07:05<3:14:09,  2.01s/it]  4%|â–Ž         | 213/6000 [07:07<3:14:39,  2.02s/it]                                                    {'loss': 0.1504, 'grad_norm': 28.420400619506836, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:07<3:14:39,  2.02s/it]  4%|â–Ž         | 214/6000 [07:09<3:12:39,  2.00s/it]                                                    {'loss': 0.0509, 'grad_norm': 10.01359748840332, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:09<3:12:39,  2.00s/it]  4%|â–Ž         | 215/6000 [07:11<3:10:39,  1.98s/it]                                                    {'loss': 0.0996, 'grad_norm': 21.26910400390625, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:11<3:10:39,  1.98s/it]  4%|â–Ž         | 216/6000 [07:13<3:09:58,  1.97s/it]                                                    {'loss': 0.0184, 'grad_norm': 4.866502285003662, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:13<3:09:58,  1.97s/it]  4%|â–Ž         | 217/6000 [07:15<3:11:14,  1.98s/it]                                                    {'loss': 0.0552, 'grad_norm': 11.849786758422852, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:15<3:11:14,  1.98s/it]  4%|â–Ž         | 218/6000 [07:17<3:12:37,  2.00s/it]                                                    {'loss': 0.0823, 'grad_norm': 14.329257011413574, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:17<3:12:37,  2.00s/it]  4%|â–Ž         | 219/6000 [07:19<3:12:16,  2.00s/it]                                                    {'loss': 0.1906, 'grad_norm': 26.066387176513672, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:19<3:12:16,  2.00s/it]  4%|â–Ž         | 220/6000 [07:21<3:11:27,  1.99s/it]                                                    {'loss': 0.4522, 'grad_norm': 35.04044723510742, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:21<3:11:27,  1.99s/it]  4%|â–Ž         | 221/6000 [07:23<3:10:12,  1.97s/it]                                                    {'loss': 0.0665, 'grad_norm': 10.5281343460083, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:23<3:10:12,  1.97s/it]  4%|â–Ž         | 222/6000 [07:25<3:08:09,  1.95s/it]                                                    {'loss': 0.1749, 'grad_norm': 47.99601364135742, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:25<3:08:09,  1.95s/it]  4%|â–Ž         | 223/6000 [07:27<3:08:14,  1.96s/it]                                                    {'loss': 0.1263, 'grad_norm': 21.626632690429688, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:27<3:08:14,  1.96s/it]  4%|â–Ž         | 224/6000 [07:29<3:06:56,  1.94s/it]                                                    {'loss': 0.1147, 'grad_norm': 25.18796730041504, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:29<3:06:56,  1.94s/it]  4%|â–         | 225/6000 [07:31<3:08:10,  1.96s/it]                                                    {'loss': 0.1905, 'grad_norm': 26.205636978149414, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:31<3:08:10,  1.96s/it]  4%|â–         | 226/6000 [07:33<3:12:46,  2.00s/it]                                                    {'loss': 0.1556, 'grad_norm': 29.634225845336914, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:33<3:12:46,  2.00s/it]  4%|â–         | 227/6000 [07:35<3:10:29,  1.98s/it]                                                    {'loss': 0.223, 'grad_norm': 34.1649055480957, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:35<3:10:29,  1.98s/it]  4%|â–         | 228/6000 [07:37<3:14:00,  2.02s/it]                                                    {'loss': 0.0654, 'grad_norm': 12.338591575622559, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:37<3:14:00,  2.02s/it]  4%|â–         | 229/6000 [07:39<3:10:20,  1.98s/it]                                                    {'loss': 0.053, 'grad_norm': 10.432653427124023, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:39<3:10:20,  1.98s/it]  4%|â–         | 230/6000 [07:41<3:10:01,  1.98s/it]                                                    {'loss': 0.0579, 'grad_norm': 15.865151405334473, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:41<3:10:01,  1.98s/it]  4%|â–         | 231/6000 [07:43<3:09:28,  1.97s/it]                                                    {'loss': 0.0548, 'grad_norm': 10.521379470825195, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:43<3:09:28,  1.97s/it]  4%|â–         | 232/6000 [07:45<3:11:05,  1.99s/it]                                                    {'loss': 0.2587, 'grad_norm': 35.886817932128906, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:45<3:11:05,  1.99s/it]  4%|â–         | 233/6000 [07:47<3:11:37,  1.99s/it]                                                    {'loss': 0.0476, 'grad_norm': 13.01211166381836, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:47<3:11:37,  1.99s/it]  4%|â–         | 234/6000 [07:49<3:12:28,  2.00s/it]                                                    {'loss': 0.0389, 'grad_norm': 10.514145851135254, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:49<3:12:28,  2.00s/it]  4%|â–         | 235/6000 [07:51<3:08:20,  1.96s/it]                                                    {'loss': 0.0991, 'grad_norm': 14.841460227966309, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:51<3:08:20,  1.96s/it]  4%|â–         | 236/6000 [07:53<3:07:37,  1.95s/it]                                                    {'loss': 0.1131, 'grad_norm': 13.263676643371582, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:53<3:07:37,  1.95s/it]  4%|â–         | 237/6000 [07:55<3:07:15,  1.95s/it]                                                    {'loss': 0.3331, 'grad_norm': 23.32448387145996, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:55<3:07:15,  1.95s/it]  4%|â–         | 238/6000 [07:57<3:10:16,  1.98s/it]                                                    {'loss': 0.3641, 'grad_norm': 24.805286407470703, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [07:57<3:10:16,  1.98s/it]  4%|â–         | 239/6000 [07:59<3:08:39,  1.96s/it]                                                    {'loss': 0.1528, 'grad_norm': 34.831905364990234, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [07:59<3:08:39,  1.96s/it]  4%|â–         | 240/6000 [08:01<3:06:45,  1.95s/it]                                                    {'loss': 0.167, 'grad_norm': 21.56529426574707, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [08:01<3:06:45,  1.95s/it]  4%|â–         | 241/6000 [08:03<3:09:19,  1.97s/it]                                                    {'loss': 0.1106, 'grad_norm': 16.262571334838867, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [08:03<3:09:19,  1.97s/it]  4%|â–         | 242/6000 [08:05<3:08:23,  1.96s/it]                                                    {'loss': 0.102, 'grad_norm': 20.64659309387207, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [08:05<3:08:23,  1.96s/it]  4%|â–         | 243/6000 [08:07<3:09:19,  1.97s/it]                                                    {'loss': 0.205, 'grad_norm': 18.52950668334961, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [08:07<3:09:19,  1.97s/it]  4%|â–         | 244/6000 [08:08<3:08:06,  1.96s/it]                                                    {'loss': 0.065, 'grad_norm': 13.50693130493164, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:08<3:08:06,  1.96s/it]  4%|â–         | 245/6000 [08:10<3:07:44,  1.96s/it]                                                    {'loss': 0.1061, 'grad_norm': 23.907135009765625, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:11<3:07:44,  1.96s/it]  4%|â–         | 246/6000 [08:13<3:14:16,  2.03s/it]                                                    {'loss': 0.0294, 'grad_norm': 11.271957397460938, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:13<3:14:16,  2.03s/it]  4%|â–         | 247/6000 [08:15<3:12:20,  2.01s/it]                                                    {'loss': 0.1339, 'grad_norm': 26.441530227661133, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:15<3:12:20,  2.01s/it]  4%|â–         | 248/6000 [08:17<3:13:19,  2.02s/it]                                                    {'loss': 0.0522, 'grad_norm': 12.954903602600098, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:17<3:13:19,  2.02s/it]  4%|â–         | 249/6000 [08:18<3:10:05,  1.98s/it]                                                    {'loss': 0.0247, 'grad_norm': 6.942595958709717, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:18<3:10:05,  1.98s/it]  4%|â–         | 250/6000 [08:20<3:08:55,  1.97s/it]                                                    {'loss': 0.1019, 'grad_norm': 23.522201538085938, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:20<3:08:55,  1.97s/it][2025-11-18 09:58:18,305] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250
[2025-11-18 09:58:18,587] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  4%|â–         | 251/6000 [08:23<3:28:03,  2.17s/it]                                                    {'loss': 0.038, 'grad_norm': 8.654181480407715, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:23<3:28:03,  2.17s/it]  4%|â–         | 252/6000 [08:25<3:27:49,  2.17s/it]                                                    {'loss': 0.0474, 'grad_norm': 10.019623756408691, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:25<3:27:49,  2.17s/it]  4%|â–         | 253/6000 [08:27<3:22:19,  2.11s/it]                                                    {'loss': 0.1302, 'grad_norm': 23.86121940612793, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:27<3:22:19,  2.11s/it]  4%|â–         | 254/6000 [08:29<3:18:45,  2.08s/it]                                                    {'loss': 0.1983, 'grad_norm': 22.13462257385254, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:29<3:18:45,  2.08s/it]  4%|â–         | 255/6000 [08:31<3:15:06,  2.04s/it]                                                    {'loss': 0.0996, 'grad_norm': 48.006832122802734, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:31<3:15:06,  2.04s/it]  4%|â–         | 256/6000 [08:33<3:17:14,  2.06s/it]                                                    {'loss': 0.1138, 'grad_norm': 20.363733291625977, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:33<3:17:14,  2.06s/it]  4%|â–         | 257/6000 [08:35<3:13:36,  2.02s/it]                                                    {'loss': 0.0278, 'grad_norm': 7.1601338386535645, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:35<3:13:36,  2.02s/it]  4%|â–         | 258/6000 [08:37<3:10:58,  2.00s/it]                                                    {'loss': 0.0445, 'grad_norm': 6.192387580871582, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:37<3:10:58,  2.00s/it]  4%|â–         | 259/6000 [08:39<3:08:18,  1.97s/it]                                                    {'loss': 0.0314, 'grad_norm': 8.701858520507812, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:39<3:08:18,  1.97s/it]  4%|â–         | 260/6000 [08:41<3:07:21,  1.96s/it]                                                    {'loss': 0.0085, 'grad_norm': 3.1078121662139893, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:41<3:07:21,  1.96s/it]  4%|â–         | 261/6000 [08:43<3:07:50,  1.96s/it]                                                    {'loss': 0.048, 'grad_norm': 7.365753650665283, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:43<3:07:50,  1.96s/it]  4%|â–         | 262/6000 [08:45<3:07:13,  1.96s/it]                                                    {'loss': 0.0091, 'grad_norm': 2.9028379917144775, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:45<3:07:13,  1.96s/it]  4%|â–         | 263/6000 [08:47<3:10:35,  1.99s/it]                                                    {'loss': 0.0596, 'grad_norm': 12.953185081481934, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:47<3:10:35,  1.99s/it]  4%|â–         | 264/6000 [08:49<3:07:14,  1.96s/it]                                                    {'loss': 0.1685, 'grad_norm': 30.970664978027344, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:49<3:07:14,  1.96s/it]  4%|â–         | 265/6000 [08:51<3:08:32,  1.97s/it]                                                    {'loss': 0.0122, 'grad_norm': 2.629096269607544, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:51<3:08:32,  1.97s/it]  4%|â–         | 266/6000 [08:53<3:07:58,  1.97s/it]                                                    {'loss': 0.0263, 'grad_norm': 6.204450607299805, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:53<3:07:58,  1.97s/it]  4%|â–         | 267/6000 [08:55<3:04:43,  1.93s/it]                                                    {'loss': 0.053, 'grad_norm': 11.068642616271973, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [08:55<3:04:43,  1.93s/it]  4%|â–         | 268/6000 [08:57<3:04:49,  1.93s/it]                                                    {'loss': 0.057, 'grad_norm': 8.413607597351074, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [08:57<3:04:49,  1.93s/it]  4%|â–         | 269/6000 [08:59<3:07:31,  1.96s/it]                                                    {'loss': 0.0512, 'grad_norm': 10.983818054199219, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [08:59<3:07:31,  1.96s/it]  4%|â–         | 270/6000 [09:01<3:07:00,  1.96s/it]                                                    {'loss': 0.0372, 'grad_norm': 7.082306385040283, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [09:01<3:07:00,  1.96s/it]  5%|â–         | 271/6000 [09:02<3:05:34,  1.94s/it]                                                    {'loss': 0.0189, 'grad_norm': 5.396790981292725, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [09:02<3:05:34,  1.94s/it]  5%|â–         | 272/6000 [09:04<3:05:46,  1.95s/it]                                                    {'loss': 0.2594, 'grad_norm': 32.188392639160156, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [09:04<3:05:46,  1.95s/it]  5%|â–         | 273/6000 [09:06<3:06:23,  1.95s/it]                                                    {'loss': 0.0699, 'grad_norm': 14.915190696716309, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [09:06<3:06:23,  1.95s/it]  5%|â–         | 274/6000 [09:08<3:08:25,  1.97s/it]                                                    {'loss': 0.127, 'grad_norm': 29.37390899658203, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:08<3:08:25,  1.97s/it]  5%|â–         | 275/6000 [09:10<3:08:36,  1.98s/it]                                                    {'loss': 0.3565, 'grad_norm': 30.97698211669922, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:10<3:08:36,  1.98s/it]  5%|â–         | 276/6000 [09:12<3:07:37,  1.97s/it]                                                    {'loss': 0.1372, 'grad_norm': 19.882959365844727, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:12<3:07:37,  1.97s/it]  5%|â–         | 277/6000 [09:14<3:07:01,  1.96s/it]                                                    {'loss': 0.1399, 'grad_norm': 22.245718002319336, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:14<3:07:01,  1.96s/it]  5%|â–         | 278/6000 [09:16<3:06:06,  1.95s/it]                                                    {'loss': 0.0228, 'grad_norm': 5.612300872802734, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:16<3:06:06,  1.95s/it]  5%|â–         | 279/6000 [09:18<3:06:05,  1.95s/it]                                                    {'loss': 0.216, 'grad_norm': 21.111949920654297, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:18<3:06:05,  1.95s/it]  5%|â–         | 280/6000 [09:20<3:05:51,  1.95s/it]                                                    {'loss': 0.0555, 'grad_norm': 9.88197135925293, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:20<3:05:51,  1.95s/it]  5%|â–         | 281/6000 [09:22<3:06:19,  1.95s/it]                                                    {'loss': 0.4327, 'grad_norm': 31.34615135192871, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:22<3:06:19,  1.95s/it]  5%|â–         | 282/6000 [09:24<3:10:57,  2.00s/it]                                                    {'loss': 0.1681, 'grad_norm': 23.405168533325195, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:24<3:10:57,  2.00s/it]  5%|â–         | 283/6000 [09:26<3:10:36,  2.00s/it]                                                    {'loss': 0.0239, 'grad_norm': 5.892726898193359, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:26<3:10:36,  2.00s/it]  5%|â–         | 284/6000 [09:28<3:16:10,  2.06s/it]                                                    {'loss': 0.0466, 'grad_norm': 11.227807998657227, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:28<3:16:10,  2.06s/it]  5%|â–         | 285/6000 [09:30<3:14:48,  2.05s/it]                                                    {'loss': 0.1091, 'grad_norm': 17.798128128051758, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:30<3:14:48,  2.05s/it]  5%|â–         | 286/6000 [09:32<3:12:24,  2.02s/it]                                                    {'loss': 0.1078, 'grad_norm': 24.281248092651367, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:32<3:12:24,  2.02s/it]  5%|â–         | 287/6000 [09:34<3:12:37,  2.02s/it]                                                    {'loss': 0.116, 'grad_norm': 20.007909774780273, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:34<3:12:37,  2.02s/it]  5%|â–         | 288/6000 [09:36<3:09:46,  1.99s/it]                                                    {'loss': 0.0145, 'grad_norm': 3.9230687618255615, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:36<3:09:46,  1.99s/it]  5%|â–         | 289/6000 [09:38<3:10:18,  2.00s/it]                                                    {'loss': 0.1667, 'grad_norm': 20.27507781982422, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:38<3:10:18,  2.00s/it]  5%|â–         | 290/6000 [09:40<3:08:41,  1.98s/it]                                                    {'loss': 0.0463, 'grad_norm': 9.359110832214355, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:40<3:08:41,  1.98s/it]  5%|â–         | 291/6000 [09:42<3:06:37,  1.96s/it]                                                    {'loss': 0.2598, 'grad_norm': 26.257810592651367, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:42<3:06:37,  1.96s/it]  5%|â–         | 292/6000 [09:44<3:05:58,  1.95s/it]                                                    {'loss': 0.2336, 'grad_norm': 28.193063735961914, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:44<3:05:58,  1.95s/it]  5%|â–         | 293/6000 [09:46<3:06:07,  1.96s/it]                                                    {'loss': 0.0693, 'grad_norm': 14.386347770690918, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:46<3:06:07,  1.96s/it]  5%|â–         | 294/6000 [09:48<3:06:14,  1.96s/it]                                                    {'loss': 0.3435, 'grad_norm': 33.31663131713867, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:48<3:06:14,  1.96s/it]  5%|â–         | 295/6000 [09:50<3:06:02,  1.96s/it]                                                    {'loss': 0.108, 'grad_norm': 14.439499855041504, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:50<3:06:02,  1.96s/it]  5%|â–         | 296/6000 [09:52<3:10:30,  2.00s/it]                                                    {'loss': 0.0861, 'grad_norm': 9.779705047607422, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [09:52<3:10:30,  2.00s/it]  5%|â–         | 297/6000 [09:54<3:09:28,  1.99s/it]                                                    {'loss': 0.0685, 'grad_norm': 19.368986129760742, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [09:54<3:09:28,  1.99s/it]  5%|â–         | 298/6000 [09:56<3:09:13,  1.99s/it]                                                    {'loss': 0.0739, 'grad_norm': 10.253676414489746, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [09:56<3:09:13,  1.99s/it]  5%|â–         | 299/6000 [09:58<3:20:06,  2.11s/it]                                                    {'loss': 0.093, 'grad_norm': 11.226479530334473, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [09:58<3:20:06,  2.11s/it]  5%|â–Œ         | 300/6000 [10:00<3:17:03,  2.07s/it]                                                    {'loss': 0.002, 'grad_norm': 0.5046998858451843, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [10:00<3:17:03,  2.07s/it][2025-11-18 09:59:58,331] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300
[2025-11-18 09:59:58,631] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  5%|â–Œ         | 301/6000 [10:03<3:38:35,  2.30s/it]                                                    {'loss': 0.0511, 'grad_norm': 12.409212112426758, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [10:03<3:38:35,  2.30s/it]  5%|â–Œ         | 302/6000 [10:05<3:29:11,  2.20s/it]                                                    {'loss': 0.0363, 'grad_norm': 9.625249862670898, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [10:05<3:29:11,  2.20s/it]  5%|â–Œ         | 303/6000 [10:07<3:22:05,  2.13s/it]                                                    {'loss': 0.0369, 'grad_norm': 9.846258163452148, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [10:07<3:22:05,  2.13s/it]  5%|â–Œ         | 304/6000 [10:09<3:17:39,  2.08s/it]                                                    {'loss': 0.029, 'grad_norm': 7.990399360656738, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:09<3:17:39,  2.08s/it]  5%|â–Œ         | 305/6000 [10:11<3:13:06,  2.03s/it]                                                    {'loss': 0.2001, 'grad_norm': 27.53535270690918, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:11<3:13:06,  2.03s/it]  5%|â–Œ         | 306/6000 [10:13<3:16:07,  2.07s/it]                                                    {'loss': 0.0283, 'grad_norm': 4.480109214782715, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:13<3:16:07,  2.07s/it]  5%|â–Œ         | 307/6000 [10:15<3:12:08,  2.03s/it]                                                    {'loss': 0.032, 'grad_norm': 7.783193111419678, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:15<3:12:08,  2.03s/it]  5%|â–Œ         | 308/6000 [10:17<3:10:35,  2.01s/it]                                                    {'loss': 0.0836, 'grad_norm': 12.101845741271973, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:17<3:10:35,  2.01s/it]  5%|â–Œ         | 309/6000 [10:19<3:10:29,  2.01s/it]                                                    {'loss': 0.2094, 'grad_norm': 20.539705276489258, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:19<3:10:29,  2.01s/it]  5%|â–Œ         | 310/6000 [10:21<3:08:21,  1.99s/it]                                                    {'loss': 0.0311, 'grad_norm': 8.503936767578125, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:21<3:08:21,  1.99s/it]  5%|â–Œ         | 311/6000 [10:23<3:07:37,  1.98s/it]                                                    {'loss': 0.1938, 'grad_norm': 25.622163772583008, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:23<3:07:37,  1.98s/it]  5%|â–Œ         | 312/6000 [10:25<3:09:15,  2.00s/it]                                                    {'loss': 0.305, 'grad_norm': 43.46404266357422, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:25<3:09:15,  2.00s/it]  5%|â–Œ         | 313/6000 [10:27<3:07:38,  1.98s/it]                                                    {'loss': 0.1141, 'grad_norm': 18.595867156982422, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:27<3:07:38,  1.98s/it]  5%|â–Œ         | 314/6000 [10:29<3:06:39,  1.97s/it]                                                    {'loss': 0.1265, 'grad_norm': 17.35280990600586, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:29<3:06:39,  1.97s/it]  5%|â–Œ         | 315/6000 [10:31<3:06:25,  1.97s/it]                                                    {'loss': 0.04, 'grad_norm': 3.828444004058838, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:31<3:06:25,  1.97s/it]  5%|â–Œ         | 316/6000 [10:33<3:06:09,  1.97s/it]                                                    {'loss': 0.0155, 'grad_norm': 3.015310287475586, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:33<3:06:09,  1.97s/it]  5%|â–Œ         | 317/6000 [10:35<3:06:06,  1.96s/it]                                                    {'loss': 0.0723, 'grad_norm': 13.930564880371094, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:35<3:06:06,  1.96s/it]  5%|â–Œ         | 318/6000 [10:37<3:08:55,  2.00s/it]                                                    {'loss': 0.042, 'grad_norm': 14.627364158630371, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:37<3:08:55,  2.00s/it]  5%|â–Œ         | 319/6000 [10:39<3:06:49,  1.97s/it]                                                    {'loss': 0.5499, 'grad_norm': 27.238985061645508, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:39<3:06:49,  1.97s/it]  5%|â–Œ         | 320/6000 [10:41<3:06:50,  1.97s/it]                                                    {'loss': 0.0661, 'grad_norm': 11.214369773864746, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:41<3:06:50,  1.97s/it]  5%|â–Œ         | 321/6000 [10:43<3:10:03,  2.01s/it]                                                    {'loss': 0.0235, 'grad_norm': 6.634994029998779, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:43<3:10:03,  2.01s/it]  5%|â–Œ         | 322/6000 [10:45<3:08:22,  1.99s/it]                                                    {'loss': 0.0597, 'grad_norm': 11.611703872680664, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:45<3:08:22,  1.99s/it]  5%|â–Œ         | 323/6000 [10:47<3:12:53,  2.04s/it]                                                    {'loss': 0.0959, 'grad_norm': 50.000064849853516, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:47<3:12:53,  2.04s/it]  5%|â–Œ         | 324/6000 [10:49<3:10:16,  2.01s/it]                                                    {'loss': 0.1673, 'grad_norm': 26.620296478271484, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:49<3:10:16,  2.01s/it]  5%|â–Œ         | 325/6000 [10:51<3:07:58,  1.99s/it]                                                    {'loss': 0.1371, 'grad_norm': 20.0075740814209, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:51<3:07:58,  1.99s/it]  5%|â–Œ         | 326/6000 [10:53<3:06:13,  1.97s/it]                                                    {'loss': 0.1374, 'grad_norm': 23.891315460205078, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [10:53<3:06:13,  1.97s/it]  5%|â–Œ         | 327/6000 [10:55<3:04:42,  1.95s/it]                                                    {'loss': 0.0393, 'grad_norm': 8.182963371276855, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [10:55<3:04:42,  1.95s/it]  5%|â–Œ         | 328/6000 [10:57<3:06:44,  1.98s/it]                                                    {'loss': 0.0149, 'grad_norm': 3.650865077972412, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [10:57<3:06:44,  1.98s/it]  5%|â–Œ         | 329/6000 [10:59<3:05:45,  1.97s/it]                                                    {'loss': 0.0514, 'grad_norm': 13.536240577697754, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [10:59<3:05:45,  1.97s/it]  6%|â–Œ         | 330/6000 [11:01<3:11:28,  2.03s/it]                                                    {'loss': 0.1308, 'grad_norm': 19.467823028564453, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [11:01<3:11:28,  2.03s/it]  6%|â–Œ         | 331/6000 [11:03<3:08:52,  2.00s/it]                                                    {'loss': 0.1142, 'grad_norm': 22.149372100830078, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [11:03<3:08:52,  2.00s/it]  6%|â–Œ         | 332/6000 [11:05<3:05:53,  1.97s/it]                                                    {'loss': 0.0661, 'grad_norm': 11.717950820922852, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [11:05<3:05:53,  1.97s/it]  6%|â–Œ         | 333/6000 [11:07<3:05:20,  1.96s/it]                                                    {'loss': 0.0242, 'grad_norm': 7.673202037811279, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [11:07<3:05:20,  1.96s/it]  6%|â–Œ         | 334/6000 [11:09<3:06:37,  1.98s/it]                                                    {'loss': 0.037, 'grad_norm': 7.354861259460449, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [11:09<3:06:37,  1.98s/it]  6%|â–Œ         | 335/6000 [11:11<3:06:55,  1.98s/it]                                                    {'loss': 0.0326, 'grad_norm': 7.029345512390137, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:11<3:06:55,  1.98s/it]  6%|â–Œ         | 336/6000 [11:13<3:04:21,  1.95s/it]                                                    {'loss': 0.0799, 'grad_norm': 11.75965690612793, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:13<3:04:21,  1.95s/it]  6%|â–Œ         | 337/6000 [11:14<3:03:58,  1.95s/it]                                                    {'loss': 0.0357, 'grad_norm': 4.830157279968262, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [11:14<3:03:58,  1.95s/it]  6%|â–Œ         | 338/6000 [11:16<3:02:46,  1.94s/it]                                                    {'loss': 0.1379, 'grad_norm': 14.27108097076416, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [11:16<3:02:46,  1.94s/it]  6%|â–Œ         | 339/6000 [11:18<3:02:22,  1.93s/it]                                                    {'loss': 0.0088, 'grad_norm': 2.0202510356903076, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [11:18<3:02:22,  1.93s/it]  6%|â–Œ         | 340/6000 [11:20<3:04:55,  1.96s/it]                                                    {'loss': 0.1834, 'grad_norm': 27.14336585998535, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [11:20<3:04:55,  1.96s/it]  6%|â–Œ         | 341/6000 [11:22<3:02:28,  1.93s/it]                                                    {'loss': 0.0398, 'grad_norm': 7.680929183959961, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [11:22<3:02:28,  1.93s/it]  6%|â–Œ         | 342/6000 [11:24<3:02:38,  1.94s/it]                                                    {'loss': 0.2155, 'grad_norm': 23.110702514648438, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [11:24<3:02:38,  1.94s/it]  6%|â–Œ         | 343/6000 [11:26<3:10:32,  2.02s/it]                                                    {'loss': 0.2799, 'grad_norm': 38.40581130981445, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [11:26<3:10:32,  2.02s/it]  6%|â–Œ         | 344/6000 [11:28<3:07:54,  1.99s/it]                                                    {'loss': 0.1395, 'grad_norm': 25.21738052368164, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [11:28<3:07:54,  1.99s/it]  6%|â–Œ         | 345/6000 [11:30<3:05:36,  1.97s/it]                                                    {'loss': 0.0447, 'grad_norm': 10.366490364074707, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [11:30<3:05:36,  1.97s/it]  6%|â–Œ         | 346/6000 [11:32<3:06:17,  1.98s/it]                                                    {'loss': 0.0939, 'grad_norm': 17.74294662475586, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [11:32<3:06:17,  1.98s/it]  6%|â–Œ         | 347/6000 [11:34<3:03:35,  1.95s/it]                                                    {'loss': 0.0111, 'grad_norm': 2.2419068813323975, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [11:34<3:03:35,  1.95s/it]  6%|â–Œ         | 348/6000 [11:36<3:03:44,  1.95s/it]                                                    {'loss': 0.1761, 'grad_norm': 18.995105743408203, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [11:36<3:03:44,  1.95s/it]  6%|â–Œ         | 349/6000 [11:38<3:03:31,  1.95s/it]                                                    {'loss': 0.0065, 'grad_norm': 2.309269666671753, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [11:38<3:03:31,  1.95s/it]  6%|â–Œ         | 350/6000 [11:40<3:03:45,  1.95s/it]                                                    {'loss': 0.2575, 'grad_norm': 24.17738914489746, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [11:40<3:03:45,  1.95s/it][2025-11-18 10:01:37,831] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350
[2025-11-18 10:01:38,160] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-350/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  6%|â–Œ         | 351/6000 [11:43<3:29:19,  2.22s/it]                                                    {'loss': 0.0974, 'grad_norm': 18.615528106689453, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [11:43<3:29:19,  2.22s/it]  6%|â–Œ         | 352/6000 [11:45<3:19:53,  2.12s/it]                                                    {'loss': 0.0802, 'grad_norm': 19.580110549926758, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [11:45<3:19:53,  2.12s/it]  6%|â–Œ         | 353/6000 [11:47<3:13:58,  2.06s/it]                                                    {'loss': 0.1195, 'grad_norm': 18.438316345214844, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [11:47<3:13:58,  2.06s/it]  6%|â–Œ         | 354/6000 [11:49<3:12:05,  2.04s/it]                                                    {'loss': 0.0602, 'grad_norm': 11.132539749145508, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [11:49<3:12:05,  2.04s/it]  6%|â–Œ         | 355/6000 [11:51<3:08:08,  2.00s/it]                                                    {'loss': 0.0166, 'grad_norm': 5.670363426208496, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [11:51<3:08:08,  2.00s/it]  6%|â–Œ         | 356/6000 [11:53<3:09:10,  2.01s/it]                                                    {'loss': 0.0402, 'grad_norm': 9.909637451171875, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [11:53<3:09:10,  2.01s/it]  6%|â–Œ         | 357/6000 [11:54<3:06:57,  1.99s/it]                                                    {'loss': 0.0517, 'grad_norm': 10.646512031555176, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [11:54<3:06:57,  1.99s/it]  6%|â–Œ         | 358/6000 [11:56<3:03:56,  1.96s/it]                                                    {'loss': 0.1197, 'grad_norm': 25.505252838134766, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [11:56<3:03:56,  1.96s/it]  6%|â–Œ         | 359/6000 [11:58<3:04:47,  1.97s/it]                                                    {'loss': 0.1331, 'grad_norm': 24.271347045898438, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [11:58<3:04:47,  1.97s/it]  6%|â–Œ         | 360/6000 [12:00<3:04:46,  1.97s/it]                                                    {'loss': 0.2326, 'grad_norm': 21.995471954345703, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [12:00<3:04:46,  1.97s/it]  6%|â–Œ         | 361/6000 [12:02<3:03:58,  1.96s/it]                                                    {'loss': 0.2919, 'grad_norm': 32.3400993347168, 'learning_rate': 9.557627118644068e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [12:02<3:03:58,  1.96s/it]  6%|â–Œ         | 362/6000 [12:04<3:01:45,  1.93s/it]                                                    {'loss': 0.1745, 'grad_norm': 18.296632766723633, 'learning_rate': 9.555932203389831e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [12:04<3:01:45,  1.93s/it]  6%|â–Œ         | 363/6000 [12:06<3:01:49,  1.94s/it]                                                    {'loss': 0.0134, 'grad_norm': 2.6630709171295166, 'learning_rate': 9.554237288135594e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [12:06<3:01:49,  1.94s/it]  6%|â–Œ         | 364/6000 [12:08<3:03:26,  1.95s/it]                                                    {'loss': 0.062, 'grad_norm': 12.441450119018555, 'learning_rate': 9.552542372881358e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [12:08<3:03:26,  1.95s/it]  6%|â–Œ         | 365/6000 [12:10<3:02:57,  1.95s/it]                                                    {'loss': 0.0174, 'grad_norm': 4.854598522186279, 'learning_rate': 9.55084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [12:10<3:02:57,  1.95s/it]  6%|â–Œ         | 366/6000 [12:12<3:01:57,  1.94s/it]                                                    {'loss': 0.0121, 'grad_norm': 2.804365873336792, 'learning_rate': 9.549152542372883e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [12:12<3:01:57,  1.94s/it]  6%|â–Œ         | 367/6000 [12:14<3:02:59,  1.95s/it]                                                    {'loss': 0.0505, 'grad_norm': 8.440291404724121, 'learning_rate': 9.547457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [12:14<3:02:59,  1.95s/it]  6%|â–Œ         | 368/6000 [12:16<3:07:05,  1.99s/it]                                                    {'loss': 0.1118, 'grad_norm': 19.33210563659668, 'learning_rate': 9.545762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [12:16<3:07:05,  1.99s/it]  6%|â–Œ         | 369/6000 [12:18<3:04:58,  1.97s/it]                                                    {'loss': 0.0849, 'grad_norm': 13.69939136505127, 'learning_rate': 9.54406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [12:18<3:04:58,  1.97s/it]  6%|â–Œ         | 370/6000 [12:20<3:05:56,  1.98s/it]                                                    {'loss': 0.004, 'grad_norm': 0.7561917901039124, 'learning_rate': 9.542372881355934e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [12:20<3:05:56,  1.98s/it]  6%|â–Œ         | 371/6000 [12:22<3:04:54,  1.97s/it]                                                    {'loss': 0.0453, 'grad_norm': 8.47865104675293, 'learning_rate': 9.540677966101696e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [12:22<3:04:54,  1.97s/it]  6%|â–Œ         | 372/6000 [12:24<3:03:48,  1.96s/it]                                                    {'loss': 0.0539, 'grad_norm': 19.629514694213867, 'learning_rate': 9.538983050847457e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [12:24<3:03:48,  1.96s/it]  6%|â–Œ         | 373/6000 [12:26<3:03:22,  1.96s/it]                                                    {'loss': 0.0145, 'grad_norm': 5.363392353057861, 'learning_rate': 9.53728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [12:26<3:03:22,  1.96s/it]  6%|â–Œ         | 374/6000 [12:28<3:03:43,  1.96s/it]                                                    {'loss': 0.3209, 'grad_norm': 27.169469833374023, 'learning_rate': 9.535593220338984e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [12:28<3:03:43,  1.96s/it]  6%|â–‹         | 375/6000 [12:30<3:03:11,  1.95s/it]                                                    {'loss': 0.0525, 'grad_norm': 11.791241645812988, 'learning_rate': 9.533898305084747e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [12:30<3:03:11,  1.95s/it]  6%|â–‹         | 376/6000 [12:32<3:03:10,  1.95s/it]                                                    {'loss': 0.0109, 'grad_norm': 3.763091802597046, 'learning_rate': 9.532203389830508e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [12:32<3:03:10,  1.95s/it]  6%|â–‹         | 377/6000 [12:34<3:03:31,  1.96s/it]                                                    {'loss': 0.0504, 'grad_norm': 13.233124732971191, 'learning_rate': 9.530508474576272e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [12:34<3:03:31,  1.96s/it]  6%|â–‹         | 378/6000 [12:36<3:04:17,  1.97s/it]                                                    {'loss': 0.0838, 'grad_norm': 8.861360549926758, 'learning_rate': 9.528813559322035e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [12:36<3:04:17,  1.97s/it]  6%|â–‹         | 379/6000 [12:38<3:04:36,  1.97s/it]                                                    {'loss': 0.0051, 'grad_norm': 2.203026056289673, 'learning_rate': 9.527118644067798e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [12:38<3:04:36,  1.97s/it]  6%|â–‹         | 380/6000 [12:40<3:07:33,  2.00s/it]                                                    {'loss': 0.1058, 'grad_norm': 16.934377670288086, 'learning_rate': 9.52542372881356e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [12:40<3:07:33,  2.00s/it]  6%|â–‹         | 381/6000 [12:42<3:07:46,  2.01s/it]                                                    {'loss': 0.0349, 'grad_norm': 11.452226638793945, 'learning_rate': 9.523728813559323e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [12:42<3:07:46,  2.01s/it]  6%|â–‹         | 382/6000 [12:44<3:05:59,  1.99s/it]                                                    {'loss': 0.0365, 'grad_norm': 9.265954971313477, 'learning_rate': 9.522033898305085e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [12:44<3:05:59,  1.99s/it]  6%|â–‹         | 383/6000 [12:46<3:04:46,  1.97s/it]                                                    {'loss': 0.0574, 'grad_norm': 12.21814250946045, 'learning_rate': 9.520338983050848e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [12:46<3:04:46,  1.97s/it]  6%|â–‹         | 384/6000 [12:48<3:07:22,  2.00s/it]                                                    {'loss': 0.1247, 'grad_norm': 15.84045696258545, 'learning_rate': 9.518644067796611e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [12:48<3:07:22,  2.00s/it]  6%|â–‹         | 385/6000 [12:50<3:08:41,  2.02s/it]                                                    {'loss': 0.007, 'grad_norm': 1.6251980066299438, 'learning_rate': 9.516949152542375e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [12:50<3:08:41,  2.02s/it]  6%|â–‹         | 386/6000 [12:52<3:06:45,  2.00s/it]                                                    {'loss': 0.062, 'grad_norm': 16.741947174072266, 'learning_rate': 9.515254237288136e-06, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [12:52<3:06:45,  2.00s/it]  6%|â–‹         | 387/6000 [12:54<3:15:08,  2.09s/it]                                                    {'loss': 0.0291, 'grad_norm': 8.123710632324219, 'learning_rate': 9.5135593220339e-06, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [12:54<3:15:08,  2.09s/it]  6%|â–‹         | 388/6000 [12:56<3:13:41,  2.07s/it]                                                    {'loss': 0.0686, 'grad_norm': 15.104432106018066, 'learning_rate': 9.511864406779661e-06, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [12:56<3:13:41,  2.07s/it]  6%|â–‹         | 389/6000 [12:58<3:09:25,  2.03s/it]                                                    {'loss': 0.0364, 'grad_norm': 14.380508422851562, 'learning_rate': 9.510169491525424e-06, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [12:58<3:09:25,  2.03s/it]  6%|â–‹         | 390/6000 [13:00<3:08:10,  2.01s/it]                                                    {'loss': 0.0258, 'grad_norm': 7.381977558135986, 'learning_rate': 9.508474576271188e-06, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [13:00<3:08:10,  2.01s/it]  7%|â–‹         | 391/6000 [13:02<3:05:36,  1.99s/it]                                                    {'loss': 0.4425, 'grad_norm': 40.458927154541016, 'learning_rate': 9.506779661016949e-06, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [13:02<3:05:36,  1.99s/it]  7%|â–‹         | 392/6000 [13:04<3:06:39,  2.00s/it]                                                    {'loss': 0.0027, 'grad_norm': 1.1658390760421753, 'learning_rate': 9.505084745762712e-06, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [13:04<3:06:39,  2.00s/it]  7%|â–‹         | 393/6000 [13:06<3:07:43,  2.01s/it]                                                    {'loss': 0.0735, 'grad_norm': 15.644339561462402, 'learning_rate': 9.503389830508476e-06, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [13:06<3:07:43,  2.01s/it]  7%|â–‹         | 394/6000 [13:08<3:06:30,  2.00s/it]                                                    {'loss': 0.0868, 'grad_norm': 14.175725936889648, 'learning_rate': 9.501694915254239e-06, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [13:08<3:06:30,  2.00s/it]  7%|â–‹         | 395/6000 [13:10<3:06:01,  1.99s/it]                                                    {'loss': 0.0472, 'grad_norm': 14.984668731689453, 'learning_rate': 9.5e-06, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [13:10<3:06:01,  1.99s/it]  7%|â–‹         | 396/6000 [13:12<3:05:16,  1.98s/it]                                                    {'loss': 0.0179, 'grad_norm': 2.3888280391693115, 'learning_rate': 9.498305084745764e-06, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [13:12<3:05:16,  1.98s/it]  7%|â–‹         | 397/6000 [13:14<3:08:18,  2.02s/it]                                                    {'loss': 0.0795, 'grad_norm': 8.415657997131348, 'learning_rate': 9.496610169491525e-06, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [13:14<3:08:18,  2.02s/it]  7%|â–‹         | 398/6000 [13:16<3:05:08,  1.98s/it]                                                    {'loss': 0.0975, 'grad_norm': 15.51682186126709, 'learning_rate': 9.494915254237289e-06, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [13:16<3:05:08,  1.98s/it]  7%|â–‹         | 399/6000 [13:18<3:04:13,  1.97s/it]                                                    {'loss': 0.1953, 'grad_norm': 25.05312728881836, 'learning_rate': 9.493220338983052e-06, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [13:18<3:04:13,  1.97s/it]  7%|â–‹         | 400/6000 [13:20<3:04:37,  1.98s/it]                                                    {'loss': 0.0092, 'grad_norm': 2.8879053592681885, 'learning_rate': 9.491525423728815e-06, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [13:20<3:04:37,  1.98s/it][2025-11-18 10:03:17,526] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400
[2025-11-18 10:03:17,828] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-400/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  7%|â–‹         | 401/6000 [13:22<3:24:31,  2.19s/it]                                                    {'loss': 0.0251, 'grad_norm': 3.6426641941070557, 'learning_rate': 9.489830508474577e-06, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [13:22<3:24:31,  2.19s/it]  7%|â–‹         | 402/6000 [13:24<3:15:52,  2.10s/it]                                                    {'loss': 0.0849, 'grad_norm': 12.355988502502441, 'learning_rate': 9.48813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [13:24<3:15:52,  2.10s/it]  7%|â–‹         | 403/6000 [13:26<3:12:23,  2.06s/it]                                                    {'loss': 0.1304, 'grad_norm': 20.519813537597656, 'learning_rate': 9.486440677966102e-06, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [13:26<3:12:23,  2.06s/it]  7%|â–‹         | 404/6000 [13:28<3:09:15,  2.03s/it]                                                    {'loss': 0.2302, 'grad_norm': 21.480571746826172, 'learning_rate': 9.484745762711865e-06, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [13:28<3:09:15,  2.03s/it]  7%|â–‹         | 405/6000 [13:30<3:07:47,  2.01s/it]                                                    {'loss': 0.0879, 'grad_norm': 16.3477840423584, 'learning_rate': 9.483050847457628e-06, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [13:30<3:07:47,  2.01s/it]  7%|â–‹         | 406/6000 [13:32<3:06:12,  2.00s/it]                                                    {'loss': 0.0006, 'grad_norm': 0.14700733125209808, 'learning_rate': 9.481355932203391e-06, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [13:32<3:06:12,  2.00s/it]  7%|â–‹         | 407/6000 [13:34<3:04:08,  1.98s/it]                                                    {'loss': 0.0292, 'grad_norm': 5.421518802642822, 'learning_rate': 9.479661016949153e-06, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [13:34<3:04:08,  1.98s/it]  7%|â–‹         | 408/6000 [13:36<3:09:24,  2.03s/it]                                                    {'loss': 0.0512, 'grad_norm': 10.1393404006958, 'learning_rate': 9.477966101694916e-06, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [13:36<3:09:24,  2.03s/it]  7%|â–‹         | 409/6000 [13:38<3:08:30,  2.02s/it]                                                    {'loss': 0.051, 'grad_norm': 10.322701454162598, 'learning_rate': 9.476271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [13:38<3:08:30,  2.02s/it]  7%|â–‹         | 410/6000 [13:40<3:05:38,  1.99s/it]                                                    {'loss': 0.1961, 'grad_norm': 20.617958068847656, 'learning_rate': 9.474576271186441e-06, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [13:40<3:05:38,  1.99s/it]  7%|â–‹         | 411/6000 [13:42<3:12:10,  2.06s/it]                                                    {'loss': 0.0138, 'grad_norm': 2.0705671310424805, 'learning_rate': 9.472881355932204e-06, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [13:42<3:12:10,  2.06s/it]  7%|â–‹         | 412/6000 [13:44<3:07:50,  2.02s/it]                                                    {'loss': 0.081, 'grad_norm': 7.218044281005859, 'learning_rate': 9.471186440677966e-06, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [13:44<3:07:50,  2.02s/it]  7%|â–‹         | 413/6000 [13:46<3:09:49,  2.04s/it]                                                    {'loss': 0.315, 'grad_norm': 21.009765625, 'learning_rate': 9.46949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [13:47<3:09:49,  2.04s/it]  7%|â–‹         | 414/6000 [13:48<3:10:57,  2.05s/it]                                                    {'loss': 0.0588, 'grad_norm': 15.04892635345459, 'learning_rate': 9.467796610169493e-06, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [13:48<3:10:57,  2.05s/it]  7%|â–‹         | 415/6000 [13:50<3:09:16,  2.03s/it]                                                    {'loss': 0.0988, 'grad_norm': 19.442930221557617, 'learning_rate': 9.466101694915256e-06, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [13:50<3:09:16,  2.03s/it]  7%|â–‹         | 416/6000 [13:52<3:07:58,  2.02s/it]                                                    {'loss': 0.0635, 'grad_norm': 15.7544584274292, 'learning_rate': 9.464406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [13:52<3:07:58,  2.02s/it]  7%|â–‹         | 417/6000 [13:54<3:07:42,  2.02s/it]                                                    {'loss': 0.0229, 'grad_norm': 12.230534553527832, 'learning_rate': 9.46271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [13:54<3:07:42,  2.02s/it]  7%|â–‹         | 418/6000 [13:56<3:07:18,  2.01s/it]                                                    {'loss': 0.1109, 'grad_norm': 14.174103736877441, 'learning_rate': 9.461016949152542e-06, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [13:56<3:07:18,  2.01s/it]  7%|â–‹         | 419/6000 [13:59<3:09:36,  2.04s/it]                                                    {'loss': 0.0428, 'grad_norm': 7.350027084350586, 'learning_rate': 9.459322033898306e-06, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [13:59<3:09:36,  2.04s/it]  7%|â–‹         | 420/6000 [14:00<3:07:32,  2.02s/it]                                                    {'loss': 0.0041, 'grad_norm': 1.0207550525665283, 'learning_rate': 9.457627118644069e-06, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [14:00<3:07:32,  2.02s/it]  7%|â–‹         | 421/6000 [14:02<3:05:42,  2.00s/it]                                                    {'loss': 0.1262, 'grad_norm': 23.021129608154297, 'learning_rate': 9.455932203389832e-06, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [14:02<3:05:42,  2.00s/it]  7%|â–‹         | 422/6000 [14:04<3:04:39,  1.99s/it]                                                    {'loss': 0.1079, 'grad_norm': 17.402301788330078, 'learning_rate': 9.454237288135594e-06, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [14:04<3:04:39,  1.99s/it]  7%|â–‹         | 423/6000 [14:06<3:05:36,  2.00s/it]                                                    {'loss': 0.1037, 'grad_norm': 16.98293685913086, 'learning_rate': 9.452542372881357e-06, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [14:06<3:05:36,  2.00s/it]  7%|â–‹         | 424/6000 [14:08<3:03:04,  1.97s/it]                                                    {'loss': 0.0078, 'grad_norm': 2.465240001678467, 'learning_rate': 9.450847457627119e-06, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [14:08<3:03:04,  1.97s/it]  7%|â–‹         | 425/6000 [14:10<3:02:08,  1.96s/it]                                                    {'loss': 0.0771, 'grad_norm': 14.617260932922363, 'learning_rate': 9.449152542372882e-06, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [14:10<3:02:08,  1.96s/it]  7%|â–‹         | 426/6000 [14:12<3:01:56,  1.96s/it]                                                    {'loss': 0.0311, 'grad_norm': 6.495890140533447, 'learning_rate': 9.447457627118645e-06, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [14:12<3:01:56,  1.96s/it]  7%|â–‹         | 427/6000 [14:14<3:03:38,  1.98s/it]                                                    {'loss': 0.2289, 'grad_norm': 17.77524757385254, 'learning_rate': 9.445762711864408e-06, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [14:14<3:03:38,  1.98s/it]  7%|â–‹         | 428/6000 [14:16<3:03:13,  1.97s/it]                                                    {'loss': 0.0113, 'grad_norm': 3.021958351135254, 'learning_rate': 9.44406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [14:16<3:03:13,  1.97s/it]  7%|â–‹         | 429/6000 [14:18<3:02:10,  1.96s/it]                                                    {'loss': 0.021, 'grad_norm': 4.665284156799316, 'learning_rate': 9.442372881355933e-06, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [14:18<3:02:10,  1.96s/it]  7%|â–‹         | 430/6000 [14:20<3:02:04,  1.96s/it]                                                    {'loss': 0.2022, 'grad_norm': 24.196941375732422, 'learning_rate': 9.440677966101696e-06, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [14:20<3:02:04,  1.96s/it]  7%|â–‹         | 431/6000 [14:22<3:02:24,  1.97s/it]                                                    {'loss': 0.1625, 'grad_norm': 18.820772171020508, 'learning_rate': 9.43898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [14:22<3:02:24,  1.97s/it]  7%|â–‹         | 432/6000 [14:24<3:02:02,  1.96s/it]                                                    {'loss': 0.1085, 'grad_norm': 16.40564727783203, 'learning_rate': 9.437288135593221e-06, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [14:24<3:02:02,  1.96s/it]  7%|â–‹         | 433/6000 [14:26<3:01:19,  1.95s/it]                                                    {'loss': 0.0119, 'grad_norm': 3.7418041229248047, 'learning_rate': 9.435593220338983e-06, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [14:26<3:01:19,  1.95s/it]  7%|â–‹         | 434/6000 [14:28<3:01:54,  1.96s/it]                                                    {'loss': 0.0088, 'grad_norm': 2.693049430847168, 'learning_rate': 9.433898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [14:28<3:01:54,  1.96s/it]  7%|â–‹         | 435/6000 [14:30<3:02:39,  1.97s/it]                                                    {'loss': 0.0132, 'grad_norm': 2.5066475868225098, 'learning_rate': 9.43220338983051e-06, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [14:30<3:02:39,  1.97s/it]  7%|â–‹         | 436/6000 [14:32<3:02:37,  1.97s/it]                                                    {'loss': 0.0096, 'grad_norm': 2.5878474712371826, 'learning_rate': 9.430508474576273e-06, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [14:32<3:02:37,  1.97s/it]  7%|â–‹         | 437/6000 [14:34<3:04:03,  1.99s/it]                                                    {'loss': 0.0455, 'grad_norm': 10.691805839538574, 'learning_rate': 9.428813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [14:34<3:04:03,  1.99s/it]  7%|â–‹         | 438/6000 [14:36<3:03:40,  1.98s/it]                                                    {'loss': 0.0443, 'grad_norm': 10.689116477966309, 'learning_rate': 9.427118644067798e-06, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [14:36<3:03:40,  1.98s/it]  7%|â–‹         | 439/6000 [14:38<3:02:38,  1.97s/it]                                                    {'loss': 0.035, 'grad_norm': 7.696281909942627, 'learning_rate': 9.425423728813559e-06, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [14:38<3:02:38,  1.97s/it]  7%|â–‹         | 440/6000 [14:40<3:02:52,  1.97s/it]                                                    {'loss': 0.0103, 'grad_norm': 2.974402666091919, 'learning_rate': 9.423728813559322e-06, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [14:40<3:02:52,  1.97s/it]  7%|â–‹         | 441/6000 [14:42<3:05:13,  2.00s/it]                                                    {'loss': 0.0088, 'grad_norm': 3.2384979724884033, 'learning_rate': 9.422033898305086e-06, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [14:42<3:05:13,  2.00s/it]  7%|â–‹         | 442/6000 [14:44<3:05:39,  2.00s/it]                                                    {'loss': 0.1111, 'grad_norm': 19.63018798828125, 'learning_rate': 9.420338983050849e-06, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [14:44<3:05:39,  2.00s/it]  7%|â–‹         | 443/6000 [14:46<3:04:20,  1.99s/it]                                                    {'loss': 0.015, 'grad_norm': 3.016484022140503, 'learning_rate': 9.41864406779661e-06, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [14:46<3:04:20,  1.99s/it]  7%|â–‹         | 444/6000 [14:48<3:03:51,  1.99s/it]                                                    {'loss': 0.0211, 'grad_norm': 10.87714958190918, 'learning_rate': 9.416949152542374e-06, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [14:48<3:03:51,  1.99s/it]  7%|â–‹         | 445/6000 [14:50<3:02:19,  1.97s/it]                                                    {'loss': 0.1186, 'grad_norm': 21.391523361206055, 'learning_rate': 9.415254237288135e-06, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [14:50<3:02:19,  1.97s/it]  7%|â–‹         | 446/6000 [14:52<3:01:39,  1.96s/it]                                                    {'loss': 0.0072, 'grad_norm': 2.607496976852417, 'learning_rate': 9.413559322033899e-06, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [14:52<3:01:39,  1.96s/it]  7%|â–‹         | 447/6000 [14:54<3:02:13,  1.97s/it]                                                    {'loss': 0.1321, 'grad_norm': 24.511415481567383, 'learning_rate': 9.411864406779662e-06, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [14:54<3:02:13,  1.97s/it]  7%|â–‹         | 448/6000 [14:56<3:02:28,  1.97s/it]                                                    {'loss': 0.0153, 'grad_norm': 5.808385372161865, 'learning_rate': 9.410169491525425e-06, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [14:56<3:02:28,  1.97s/it]  7%|â–‹         | 449/6000 [14:58<3:04:03,  1.99s/it]                                                    {'loss': 0.0431, 'grad_norm': 13.89097785949707, 'learning_rate': 9.408474576271187e-06, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [14:58<3:04:03,  1.99s/it]  8%|â–Š         | 450/6000 [15:00<3:04:23,  1.99s/it]                                                    {'loss': 0.1211, 'grad_norm': 23.652690887451172, 'learning_rate': 9.40677966101695e-06, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [15:00<3:04:23,  1.99s/it][2025-11-18 10:04:57,557] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450
[2025-11-18 10:04:58,057] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-450/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 451/6000 [15:03<3:32:03,  2.29s/it]                                                    {'loss': 0.0078, 'grad_norm': 7.116686820983887, 'learning_rate': 9.405084745762713e-06, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [15:03<3:32:03,  2.29s/it]  8%|â–Š         | 452/6000 [15:05<3:24:18,  2.21s/it]                                                    {'loss': 0.128, 'grad_norm': 12.071915626525879, 'learning_rate': 9.403389830508477e-06, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [15:05<3:24:18,  2.21s/it]  8%|â–Š         | 453/6000 [15:07<3:17:04,  2.13s/it]                                                    {'loss': 0.1936, 'grad_norm': 21.143278121948242, 'learning_rate': 9.401694915254238e-06, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [15:07<3:17:04,  2.13s/it]  8%|â–Š         | 454/6000 [15:09<3:13:30,  2.09s/it]                                                    {'loss': 0.1469, 'grad_norm': 18.805208206176758, 'learning_rate': 9.4e-06, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [15:09<3:13:30,  2.09s/it]  8%|â–Š         | 455/6000 [15:11<3:07:56,  2.03s/it]                                                    {'loss': 0.0574, 'grad_norm': 9.901094436645508, 'learning_rate': 9.398305084745763e-06, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [15:11<3:07:56,  2.03s/it]  8%|â–Š         | 456/6000 [15:12<3:05:22,  2.01s/it]                                                    {'loss': 0.3709, 'grad_norm': 29.931814193725586, 'learning_rate': 9.396610169491526e-06, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [15:12<3:05:22,  2.01s/it]  8%|â–Š         | 457/6000 [15:14<3:03:19,  1.98s/it]                                                    {'loss': 0.0618, 'grad_norm': 14.463194847106934, 'learning_rate': 9.39491525423729e-06, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [15:14<3:03:19,  1.98s/it]  8%|â–Š         | 458/6000 [15:16<3:02:28,  1.98s/it]                                                    {'loss': 0.0618, 'grad_norm': 15.070459365844727, 'learning_rate': 9.393220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [15:16<3:02:28,  1.98s/it]  8%|â–Š         | 459/6000 [15:18<3:01:30,  1.97s/it]                                                    {'loss': 0.2145, 'grad_norm': 18.96016502380371, 'learning_rate': 9.391525423728814e-06, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [15:18<3:01:30,  1.97s/it]  8%|â–Š         | 460/6000 [15:20<3:01:08,  1.96s/it]                                                    {'loss': 0.0157, 'grad_norm': 4.304361820220947, 'learning_rate': 9.389830508474576e-06, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [15:20<3:01:08,  1.96s/it]  8%|â–Š         | 461/6000 [15:22<3:00:32,  1.96s/it]                                                    {'loss': 0.0736, 'grad_norm': 7.428755760192871, 'learning_rate': 9.38813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [15:22<3:00:32,  1.96s/it]  8%|â–Š         | 462/6000 [15:24<3:00:11,  1.95s/it]                                                    {'loss': 0.062, 'grad_norm': 6.145244121551514, 'learning_rate': 9.386440677966103e-06, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [15:24<3:00:11,  1.95s/it]  8%|â–Š         | 463/6000 [15:26<3:00:36,  1.96s/it]                                                    {'loss': 0.089, 'grad_norm': 14.565902709960938, 'learning_rate': 9.384745762711866e-06, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [15:26<3:00:36,  1.96s/it]  8%|â–Š         | 464/6000 [15:28<2:58:39,  1.94s/it]                                                    {'loss': 0.3377, 'grad_norm': 34.12130355834961, 'learning_rate': 9.383050847457627e-06, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [15:28<2:58:39,  1.94s/it]  8%|â–Š         | 465/6000 [15:30<3:00:48,  1.96s/it]                                                    {'loss': 0.3502, 'grad_norm': 28.87777328491211, 'learning_rate': 9.38135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [15:30<3:00:48,  1.96s/it]  8%|â–Š         | 466/6000 [15:32<2:59:58,  1.95s/it]                                                    {'loss': 0.0827, 'grad_norm': 11.21504020690918, 'learning_rate': 9.379661016949152e-06, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [15:32<2:59:58,  1.95s/it]  8%|â–Š         | 467/6000 [15:34<3:05:05,  2.01s/it]                                                    {'loss': 0.1121, 'grad_norm': 15.739686012268066, 'learning_rate': 9.377966101694916e-06, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [15:34<3:05:05,  2.01s/it]  8%|â–Š         | 468/6000 [15:36<3:07:09,  2.03s/it]                                                    {'loss': 0.0695, 'grad_norm': 8.862003326416016, 'learning_rate': 9.376271186440679e-06, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [15:36<3:07:09,  2.03s/it]  8%|â–Š         | 469/6000 [15:38<3:04:45,  2.00s/it]                                                    {'loss': 0.282, 'grad_norm': 24.759824752807617, 'learning_rate': 9.374576271186442e-06, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [15:38<3:04:45,  2.00s/it]  8%|â–Š         | 470/6000 [15:40<3:02:54,  1.98s/it]                                                    {'loss': 0.0511, 'grad_norm': 10.432427406311035, 'learning_rate': 9.372881355932204e-06, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [15:40<3:02:54,  1.98s/it]  8%|â–Š         | 471/6000 [15:42<3:03:55,  2.00s/it]                                                    {'loss': 0.2208, 'grad_norm': 23.513296127319336, 'learning_rate': 9.371186440677967e-06, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [15:42<3:03:55,  2.00s/it]  8%|â–Š         | 472/6000 [15:44<3:03:26,  1.99s/it]                                                    {'loss': 0.294, 'grad_norm': 18.430580139160156, 'learning_rate': 9.36949152542373e-06, 'epoch': 0.08}
  8%|â–Š         | 472/6000 [15:44<3:03:26,  1.99s/it]  8%|â–Š         | 473/6000 [15:46<3:02:25,  1.98s/it]                                                    {'loss': 0.1819, 'grad_norm': 18.732135772705078, 'learning_rate': 9.367796610169494e-06, 'epoch': 0.08}
  8%|â–Š         | 473/6000 [15:46<3:02:25,  1.98s/it]  8%|â–Š         | 474/6000 [15:48<3:01:16,  1.97s/it]                                                    {'loss': 0.0411, 'grad_norm': 5.577921390533447, 'learning_rate': 9.366101694915255e-06, 'epoch': 0.08}
  8%|â–Š         | 474/6000 [15:48<3:01:16,  1.97s/it]  8%|â–Š         | 475/6000 [15:50<3:01:19,  1.97s/it]                                                    {'loss': 0.0877, 'grad_norm': 8.373613357543945, 'learning_rate': 9.364406779661017e-06, 'epoch': 0.08}
  8%|â–Š         | 475/6000 [15:50<3:01:19,  1.97s/it]  8%|â–Š         | 476/6000 [15:52<3:00:01,  1.96s/it]                                                    {'loss': 0.0319, 'grad_norm': 8.02701473236084, 'learning_rate': 9.36271186440678e-06, 'epoch': 0.08}
  8%|â–Š         | 476/6000 [15:52<3:00:01,  1.96s/it]  8%|â–Š         | 477/6000 [15:54<2:59:24,  1.95s/it]                                                    {'loss': 0.035, 'grad_norm': 9.457246780395508, 'learning_rate': 9.361016949152543e-06, 'epoch': 0.08}
  8%|â–Š         | 477/6000 [15:54<2:59:24,  1.95s/it]  8%|â–Š         | 478/6000 [15:56<3:01:01,  1.97s/it]                                                    {'loss': 0.0342, 'grad_norm': 11.844700813293457, 'learning_rate': 9.359322033898306e-06, 'epoch': 0.08}
  8%|â–Š         | 478/6000 [15:56<3:01:01,  1.97s/it]  8%|â–Š         | 479/6000 [15:58<3:01:15,  1.97s/it]                                                    {'loss': 0.1067, 'grad_norm': 17.286659240722656, 'learning_rate': 9.357627118644068e-06, 'epoch': 0.08}
  8%|â–Š         | 479/6000 [15:58<3:01:15,  1.97s/it]  8%|â–Š         | 480/6000 [16:00<3:00:14,  1.96s/it]                                                    {'loss': 0.0309, 'grad_norm': 5.662011623382568, 'learning_rate': 9.355932203389831e-06, 'epoch': 0.08}
  8%|â–Š         | 480/6000 [16:00<3:00:14,  1.96s/it]  8%|â–Š         | 481/6000 [16:02<3:01:58,  1.98s/it]                                                    {'loss': 0.0346, 'grad_norm': 11.824665069580078, 'learning_rate': 9.354237288135593e-06, 'epoch': 0.08}
  8%|â–Š         | 481/6000 [16:02<3:01:58,  1.98s/it]  8%|â–Š         | 482/6000 [16:04<3:00:40,  1.96s/it]                                                    {'loss': 0.0176, 'grad_norm': 3.7262837886810303, 'learning_rate': 9.352542372881356e-06, 'epoch': 0.08}
  8%|â–Š         | 482/6000 [16:04<3:00:40,  1.96s/it]  8%|â–Š         | 483/6000 [16:06<3:01:20,  1.97s/it]                                                    {'loss': 0.0714, 'grad_norm': 11.319342613220215, 'learning_rate': 9.35084745762712e-06, 'epoch': 0.08}
  8%|â–Š         | 483/6000 [16:06<3:01:20,  1.97s/it]  8%|â–Š         | 484/6000 [16:08<3:04:03,  2.00s/it]                                                    {'loss': 0.1502, 'grad_norm': 25.492225646972656, 'learning_rate': 9.349152542372883e-06, 'epoch': 0.08}
  8%|â–Š         | 484/6000 [16:08<3:04:03,  2.00s/it]  8%|â–Š         | 485/6000 [16:10<3:07:43,  2.04s/it]                                                    {'loss': 0.0274, 'grad_norm': 4.965278148651123, 'learning_rate': 9.347457627118644e-06, 'epoch': 0.08}
  8%|â–Š         | 485/6000 [16:10<3:07:43,  2.04s/it]  8%|â–Š         | 486/6000 [16:12<3:04:57,  2.01s/it]                                                    {'loss': 0.004, 'grad_norm': 0.9719555974006653, 'learning_rate': 9.345762711864408e-06, 'epoch': 0.08}
  8%|â–Š         | 486/6000 [16:12<3:04:57,  2.01s/it]  8%|â–Š         | 487/6000 [16:14<3:04:40,  2.01s/it]                                                    {'loss': 0.2166, 'grad_norm': 17.51121711730957, 'learning_rate': 9.344067796610171e-06, 'epoch': 0.08}
  8%|â–Š         | 487/6000 [16:14<3:04:40,  2.01s/it]  8%|â–Š         | 488/6000 [16:16<3:06:31,  2.03s/it]                                                    {'loss': 0.369, 'grad_norm': 25.947534561157227, 'learning_rate': 9.342372881355934e-06, 'epoch': 0.08}
  8%|â–Š         | 488/6000 [16:16<3:06:31,  2.03s/it]  8%|â–Š         | 489/6000 [16:18<3:05:46,  2.02s/it]                                                    {'loss': 0.0577, 'grad_norm': 9.471129417419434, 'learning_rate': 9.340677966101696e-06, 'epoch': 0.08}
  8%|â–Š         | 489/6000 [16:18<3:05:46,  2.02s/it]  8%|â–Š         | 490/6000 [16:20<3:03:23,  2.00s/it]                                                    {'loss': 0.1251, 'grad_norm': 17.25726318359375, 'learning_rate': 9.338983050847459e-06, 'epoch': 0.08}
  8%|â–Š         | 490/6000 [16:20<3:03:23,  2.00s/it]  8%|â–Š         | 491/6000 [16:22<3:02:43,  1.99s/it]                                                    {'loss': 0.1933, 'grad_norm': 22.152254104614258, 'learning_rate': 9.33728813559322e-06, 'epoch': 0.08}
  8%|â–Š         | 491/6000 [16:22<3:02:43,  1.99s/it]  8%|â–Š         | 492/6000 [16:24<3:00:57,  1.97s/it]                                                    {'loss': 0.0372, 'grad_norm': 5.984915733337402, 'learning_rate': 9.335593220338984e-06, 'epoch': 0.08}
  8%|â–Š         | 492/6000 [16:24<3:00:57,  1.97s/it]  8%|â–Š         | 493/6000 [16:26<3:00:56,  1.97s/it]                                                    {'loss': 0.1554, 'grad_norm': 16.87651824951172, 'learning_rate': 9.333898305084747e-06, 'epoch': 0.08}
  8%|â–Š         | 493/6000 [16:26<3:00:56,  1.97s/it]  8%|â–Š         | 494/6000 [16:28<2:59:50,  1.96s/it]                                                    {'loss': 0.0481, 'grad_norm': 11.567218780517578, 'learning_rate': 9.33220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 494/6000 [16:28<2:59:50,  1.96s/it]  8%|â–Š         | 495/6000 [16:30<3:03:16,  2.00s/it]                                                    {'loss': 0.1528, 'grad_norm': 18.67441749572754, 'learning_rate': 9.330508474576272e-06, 'epoch': 0.08}
  8%|â–Š         | 495/6000 [16:30<3:03:16,  2.00s/it]  8%|â–Š         | 496/6000 [16:32<3:02:21,  1.99s/it]                                                    {'loss': 0.0259, 'grad_norm': 9.46820068359375, 'learning_rate': 9.328813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 496/6000 [16:32<3:02:21,  1.99s/it]  8%|â–Š         | 497/6000 [16:34<3:01:15,  1.98s/it]                                                    {'loss': 0.0074, 'grad_norm': 3.3225314617156982, 'learning_rate': 9.327118644067797e-06, 'epoch': 0.08}
  8%|â–Š         | 497/6000 [16:34<3:01:15,  1.98s/it]  8%|â–Š         | 498/6000 [16:36<3:07:11,  2.04s/it]                                                    {'loss': 0.0942, 'grad_norm': 12.738205909729004, 'learning_rate': 9.32542372881356e-06, 'epoch': 0.08}
  8%|â–Š         | 498/6000 [16:36<3:07:11,  2.04s/it]  8%|â–Š         | 499/6000 [16:38<3:04:03,  2.01s/it]                                                    {'loss': 0.0818, 'grad_norm': 9.073087692260742, 'learning_rate': 9.323728813559323e-06, 'epoch': 0.08}
  8%|â–Š         | 499/6000 [16:38<3:04:03,  2.01s/it]  8%|â–Š         | 500/6000 [16:40<3:03:12,  2.00s/it]                                                    {'loss': 0.1122, 'grad_norm': 10.173909187316895, 'learning_rate': 9.322033898305085e-06, 'epoch': 0.08}
  8%|â–Š         | 500/6000 [16:40<3:03:12,  2.00s/it][2025-11-18 10:06:37,599] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
[2025-11-18 10:06:37,901] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 501/6000 [16:42<3:21:38,  2.20s/it]                                                    {'loss': 0.1336, 'grad_norm': 16.374584197998047, 'learning_rate': 9.320338983050848e-06, 'epoch': 0.08}
  8%|â–Š         | 501/6000 [16:42<3:21:38,  2.20s/it]  8%|â–Š         | 502/6000 [16:44<3:16:15,  2.14s/it]                                                    {'loss': 0.032, 'grad_norm': 5.237454414367676, 'learning_rate': 9.31864406779661e-06, 'epoch': 0.08}
  8%|â–Š         | 502/6000 [16:44<3:16:15,  2.14s/it]  8%|â–Š         | 503/6000 [16:46<3:11:30,  2.09s/it]                                                    {'loss': 0.0444, 'grad_norm': 7.777744770050049, 'learning_rate': 9.316949152542373e-06, 'epoch': 0.08}
  8%|â–Š         | 503/6000 [16:46<3:11:30,  2.09s/it]  8%|â–Š         | 504/6000 [16:48<3:07:18,  2.04s/it]                                                    {'loss': 0.0768, 'grad_norm': 17.19315528869629, 'learning_rate': 9.315254237288136e-06, 'epoch': 0.08}
  8%|â–Š         | 504/6000 [16:48<3:07:18,  2.04s/it]  8%|â–Š         | 505/6000 [16:50<3:03:59,  2.01s/it]                                                    {'loss': 0.0372, 'grad_norm': 10.003898620605469, 'learning_rate': 9.3135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 505/6000 [16:50<3:03:59,  2.01s/it]  8%|â–Š         | 506/6000 [16:52<3:03:43,  2.01s/it]                                                    {'loss': 0.1278, 'grad_norm': 20.99114990234375, 'learning_rate': 9.311864406779661e-06, 'epoch': 0.08}
  8%|â–Š         | 506/6000 [16:52<3:03:43,  2.01s/it]  8%|â–Š         | 507/6000 [16:54<3:03:22,  2.00s/it]                                                    {'loss': 0.08, 'grad_norm': 14.601216316223145, 'learning_rate': 9.310169491525424e-06, 'epoch': 0.08}
  8%|â–Š         | 507/6000 [16:54<3:03:22,  2.00s/it]  8%|â–Š         | 508/6000 [16:56<3:00:37,  1.97s/it]                                                    {'loss': 0.0294, 'grad_norm': 6.708971977233887, 'learning_rate': 9.308474576271188e-06, 'epoch': 0.08}
  8%|â–Š         | 508/6000 [16:56<3:00:37,  1.97s/it]  8%|â–Š         | 509/6000 [16:58<2:59:44,  1.96s/it]                                                    {'loss': 0.0779, 'grad_norm': 11.529013633728027, 'learning_rate': 9.306779661016951e-06, 'epoch': 0.08}
  8%|â–Š         | 509/6000 [16:58<2:59:44,  1.96s/it]  8%|â–Š         | 510/6000 [17:00<3:00:59,  1.98s/it]                                                    {'loss': 0.0902, 'grad_norm': 21.57221031188965, 'learning_rate': 9.305084745762713e-06, 'epoch': 0.09}
  8%|â–Š         | 510/6000 [17:00<3:00:59,  1.98s/it]  9%|â–Š         | 511/6000 [17:02<3:01:54,  1.99s/it]                                                    {'loss': 0.1101, 'grad_norm': 12.173996925354004, 'learning_rate': 9.303389830508476e-06, 'epoch': 0.09}
  9%|â–Š         | 511/6000 [17:02<3:01:54,  1.99s/it]  9%|â–Š         | 512/6000 [17:04<3:00:20,  1.97s/it]                                                    {'loss': 0.082, 'grad_norm': 13.516196250915527, 'learning_rate': 9.301694915254237e-06, 'epoch': 0.09}
  9%|â–Š         | 512/6000 [17:04<3:00:20,  1.97s/it]  9%|â–Š         | 513/6000 [17:06<3:01:03,  1.98s/it]                                                    {'loss': 0.0111, 'grad_norm': 4.395260810852051, 'learning_rate': 9.3e-06, 'epoch': 0.09}
  9%|â–Š         | 513/6000 [17:06<3:01:03,  1.98s/it]  9%|â–Š         | 514/6000 [17:08<3:02:54,  2.00s/it]                                                    {'loss': 0.0022, 'grad_norm': 0.49821752309799194, 'learning_rate': 9.298305084745764e-06, 'epoch': 0.09}
  9%|â–Š         | 514/6000 [17:08<3:02:54,  2.00s/it]  9%|â–Š         | 515/6000 [17:10<3:01:09,  1.98s/it]                                                    {'loss': 0.0984, 'grad_norm': 11.956374168395996, 'learning_rate': 9.296610169491527e-06, 'epoch': 0.09}
  9%|â–Š         | 515/6000 [17:10<3:01:09,  1.98s/it]  9%|â–Š         | 516/6000 [17:12<3:02:42,  2.00s/it]                                                    {'loss': 0.0924, 'grad_norm': 18.256027221679688, 'learning_rate': 9.294915254237289e-06, 'epoch': 0.09}
  9%|â–Š         | 516/6000 [17:12<3:02:42,  2.00s/it]  9%|â–Š         | 517/6000 [17:14<3:02:11,  1.99s/it]                                                    {'loss': 0.0068, 'grad_norm': 3.8581595420837402, 'learning_rate': 9.29322033898305e-06, 'epoch': 0.09}
  9%|â–Š         | 517/6000 [17:14<3:02:11,  1.99s/it]  9%|â–Š         | 518/6000 [17:16<3:00:54,  1.98s/it]                                                    {'loss': 0.2154, 'grad_norm': 17.45442771911621, 'learning_rate': 9.291525423728814e-06, 'epoch': 0.09}
  9%|â–Š         | 518/6000 [17:16<3:00:54,  1.98s/it]  9%|â–Š         | 519/6000 [17:18<3:02:10,  1.99s/it]                                                    {'loss': 0.0232, 'grad_norm': 5.0395708084106445, 'learning_rate': 9.289830508474577e-06, 'epoch': 0.09}
  9%|â–Š         | 519/6000 [17:18<3:02:10,  1.99s/it]  9%|â–Š         | 520/6000 [17:20<3:04:31,  2.02s/it]                                                    {'loss': 0.099, 'grad_norm': 13.966712951660156, 'learning_rate': 9.28813559322034e-06, 'epoch': 0.09}
  9%|â–Š         | 520/6000 [17:20<3:04:31,  2.02s/it]  9%|â–Š         | 521/6000 [17:22<3:01:58,  1.99s/it]                                                    {'loss': 0.0648, 'grad_norm': 15.781152725219727, 'learning_rate': 9.286440677966102e-06, 'epoch': 0.09}
  9%|â–Š         | 521/6000 [17:22<3:01:58,  1.99s/it]  9%|â–Š         | 522/6000 [17:24<3:00:47,  1.98s/it]                                                    {'loss': 0.0093, 'grad_norm': 2.91094708442688, 'learning_rate': 9.284745762711865e-06, 'epoch': 0.09}
  9%|â–Š         | 522/6000 [17:24<3:00:47,  1.98s/it]  9%|â–Š         | 523/6000 [17:26<3:00:39,  1.98s/it]                                                    {'loss': 0.0119, 'grad_norm': 2.605938673019409, 'learning_rate': 9.283050847457627e-06, 'epoch': 0.09}
  9%|â–Š         | 523/6000 [17:26<3:00:39,  1.98s/it]  9%|â–Š         | 524/6000 [17:28<2:59:33,  1.97s/it]                                                    {'loss': 0.137, 'grad_norm': 26.646108627319336, 'learning_rate': 9.28135593220339e-06, 'epoch': 0.09}
  9%|â–Š         | 524/6000 [17:28<2:59:33,  1.97s/it]  9%|â–‰         | 525/6000 [17:30<2:59:16,  1.96s/it]                                                    {'loss': 0.086, 'grad_norm': 15.029571533203125, 'learning_rate': 9.279661016949153e-06, 'epoch': 0.09}
  9%|â–‰         | 525/6000 [17:30<2:59:16,  1.96s/it]  9%|â–‰         | 526/6000 [17:32<2:57:39,  1.95s/it]                                                    {'loss': 0.0258, 'grad_norm': 5.822390556335449, 'learning_rate': 9.277966101694917e-06, 'epoch': 0.09}
  9%|â–‰         | 526/6000 [17:32<2:57:39,  1.95s/it]  9%|â–‰         | 527/6000 [17:34<2:59:56,  1.97s/it]                                                    {'loss': 0.044, 'grad_norm': 12.878486633300781, 'learning_rate': 9.276271186440678e-06, 'epoch': 0.09}
  9%|â–‰         | 527/6000 [17:34<2:59:56,  1.97s/it]  9%|â–‰         | 528/6000 [17:36<3:02:22,  2.00s/it]                                                    {'loss': 0.1081, 'grad_norm': 14.829263687133789, 'learning_rate': 9.274576271186441e-06, 'epoch': 0.09}
  9%|â–‰         | 528/6000 [17:36<3:02:22,  2.00s/it]  9%|â–‰         | 529/6000 [17:38<3:01:02,  1.99s/it]                                                    {'loss': 0.0125, 'grad_norm': 5.002429008483887, 'learning_rate': 9.272881355932205e-06, 'epoch': 0.09}
  9%|â–‰         | 529/6000 [17:38<3:01:02,  1.99s/it]  9%|â–‰         | 530/6000 [17:40<3:00:03,  1.98s/it]                                                    {'loss': 0.0167, 'grad_norm': 4.281765460968018, 'learning_rate': 9.271186440677968e-06, 'epoch': 0.09}
  9%|â–‰         | 530/6000 [17:40<3:00:03,  1.98s/it]  9%|â–‰         | 531/6000 [17:42<2:58:08,  1.95s/it]                                                    {'loss': 0.1844, 'grad_norm': 20.390663146972656, 'learning_rate': 9.26949152542373e-06, 'epoch': 0.09}
  9%|â–‰         | 531/6000 [17:42<2:58:08,  1.95s/it]  9%|â–‰         | 532/6000 [17:44<2:57:55,  1.95s/it]                                                    {'loss': 0.0612, 'grad_norm': 11.889032363891602, 'learning_rate': 9.267796610169493e-06, 'epoch': 0.09}
  9%|â–‰         | 532/6000 [17:44<2:57:55,  1.95s/it]  9%|â–‰         | 533/6000 [17:45<2:56:03,  1.93s/it]                                                    {'loss': 0.0722, 'grad_norm': 15.946593284606934, 'learning_rate': 9.266101694915254e-06, 'epoch': 0.09}
  9%|â–‰         | 533/6000 [17:46<2:56:03,  1.93s/it]  9%|â–‰         | 534/6000 [17:47<2:56:12,  1.93s/it]                                                    {'loss': 0.0121, 'grad_norm': 3.6479010581970215, 'learning_rate': 9.264406779661018e-06, 'epoch': 0.09}
  9%|â–‰         | 534/6000 [17:47<2:56:12,  1.93s/it]  9%|â–‰         | 535/6000 [17:49<2:57:24,  1.95s/it]                                                    {'loss': 0.0057, 'grad_norm': 1.6443707942962646, 'learning_rate': 9.262711864406781e-06, 'epoch': 0.09}
  9%|â–‰         | 535/6000 [17:49<2:57:24,  1.95s/it]  9%|â–‰         | 536/6000 [17:51<2:58:27,  1.96s/it]                                                    {'loss': 0.1904, 'grad_norm': 19.159692764282227, 'learning_rate': 9.261016949152544e-06, 'epoch': 0.09}
  9%|â–‰         | 536/6000 [17:51<2:58:27,  1.96s/it]  9%|â–‰         | 537/6000 [17:53<2:58:21,  1.96s/it]                                                    {'loss': 0.0296, 'grad_norm': 7.131099700927734, 'learning_rate': 9.259322033898306e-06, 'epoch': 0.09}
  9%|â–‰         | 537/6000 [17:53<2:58:21,  1.96s/it]  9%|â–‰         | 538/6000 [17:55<2:56:28,  1.94s/it]                                                    {'loss': 0.0128, 'grad_norm': 4.70460319519043, 'learning_rate': 9.257627118644067e-06, 'epoch': 0.09}
  9%|â–‰         | 538/6000 [17:55<2:56:28,  1.94s/it]  9%|â–‰         | 539/6000 [17:57<2:55:09,  1.92s/it]                                                    {'loss': 0.0013, 'grad_norm': 0.22106918692588806, 'learning_rate': 9.25593220338983e-06, 'epoch': 0.09}
  9%|â–‰         | 539/6000 [17:57<2:55:09,  1.92s/it]  9%|â–‰         | 540/6000 [17:59<2:54:22,  1.92s/it]                                                    {'loss': 0.0924, 'grad_norm': 17.205350875854492, 'learning_rate': 9.254237288135594e-06, 'epoch': 0.09}
  9%|â–‰         | 540/6000 [17:59<2:54:22,  1.92s/it]  9%|â–‰         | 541/6000 [18:01<2:57:37,  1.95s/it]                                                    {'loss': 0.0211, 'grad_norm': 8.500590324401855, 'learning_rate': 9.252542372881357e-06, 'epoch': 0.09}
  9%|â–‰         | 541/6000 [18:01<2:57:37,  1.95s/it]  9%|â–‰         | 542/6000 [18:03<2:58:18,  1.96s/it]                                                    {'loss': 0.018, 'grad_norm': 3.618668556213379, 'learning_rate': 9.250847457627119e-06, 'epoch': 0.09}
  9%|â–‰         | 542/6000 [18:03<2:58:18,  1.96s/it]  9%|â–‰         | 543/6000 [18:05<2:57:34,  1.95s/it]                                                    {'loss': 0.2208, 'grad_norm': 22.662601470947266, 'learning_rate': 9.249152542372882e-06, 'epoch': 0.09}
  9%|â–‰         | 543/6000 [18:05<2:57:34,  1.95s/it]  9%|â–‰         | 544/6000 [18:07<3:03:34,  2.02s/it]                                                    {'loss': 0.2521, 'grad_norm': 24.702537536621094, 'learning_rate': 9.247457627118645e-06, 'epoch': 0.09}
  9%|â–‰         | 544/6000 [18:07<3:03:34,  2.02s/it]  9%|â–‰         | 545/6000 [18:09<3:01:23,  2.00s/it]                                                    {'loss': 0.216, 'grad_norm': 25.57280731201172, 'learning_rate': 9.245762711864409e-06, 'epoch': 0.09}
  9%|â–‰         | 545/6000 [18:09<3:01:23,  2.00s/it]  9%|â–‰         | 546/6000 [18:11<3:00:56,  1.99s/it]                                                    {'loss': 0.0323, 'grad_norm': 7.547780990600586, 'learning_rate': 9.24406779661017e-06, 'epoch': 0.09}
  9%|â–‰         | 546/6000 [18:11<3:00:56,  1.99s/it]  9%|â–‰         | 547/6000 [18:13<3:01:01,  1.99s/it]                                                    {'loss': 0.0056, 'grad_norm': 2.2494945526123047, 'learning_rate': 9.242372881355933e-06, 'epoch': 0.09}
  9%|â–‰         | 547/6000 [18:13<3:01:01,  1.99s/it]  9%|â–‰         | 548/6000 [18:15<3:01:51,  2.00s/it]                                                    {'loss': 0.0161, 'grad_norm': 3.781419277191162, 'learning_rate': 9.240677966101695e-06, 'epoch': 0.09}
  9%|â–‰         | 548/6000 [18:15<3:01:51,  2.00s/it]  9%|â–‰         | 549/6000 [18:17<3:00:04,  1.98s/it]                                                    {'loss': 0.0565, 'grad_norm': 8.992718696594238, 'learning_rate': 9.238983050847458e-06, 'epoch': 0.09}
  9%|â–‰         | 549/6000 [18:17<3:00:04,  1.98s/it]  9%|â–‰         | 550/6000 [18:19<2:59:07,  1.97s/it]                                                    {'loss': 0.1732, 'grad_norm': 13.184820175170898, 'learning_rate': 9.237288135593222e-06, 'epoch': 0.09}
  9%|â–‰         | 550/6000 [18:19<2:59:07,  1.97s/it][2025-11-18 10:08:16,857] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550
[2025-11-18 10:08:17,147] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-550/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  9%|â–‰         | 551/6000 [18:22<3:29:10,  2.30s/it]                                                    {'loss': 0.0605, 'grad_norm': 12.315461158752441, 'learning_rate': 9.235593220338985e-06, 'epoch': 0.09}
  9%|â–‰         | 551/6000 [18:22<3:29:10,  2.30s/it]  9%|â–‰         | 552/6000 [18:24<3:20:15,  2.21s/it]                                                    {'loss': 0.0028, 'grad_norm': 1.1443811655044556, 'learning_rate': 9.233898305084746e-06, 'epoch': 0.09}
  9%|â–‰         | 552/6000 [18:24<3:20:15,  2.21s/it]  9%|â–‰         | 553/6000 [18:26<3:13:26,  2.13s/it]                                                    {'loss': 0.0975, 'grad_norm': 16.8210391998291, 'learning_rate': 9.23220338983051e-06, 'epoch': 0.09}
  9%|â–‰         | 553/6000 [18:26<3:13:26,  2.13s/it]  9%|â–‰         | 554/6000 [18:28<3:07:47,  2.07s/it]                                                    {'loss': 0.4267, 'grad_norm': 20.82353973388672, 'learning_rate': 9.230508474576271e-06, 'epoch': 0.09}
  9%|â–‰         | 554/6000 [18:28<3:07:47,  2.07s/it]  9%|â–‰         | 555/6000 [18:30<3:02:59,  2.02s/it]                                                    {'loss': 0.0952, 'grad_norm': 14.9097261428833, 'learning_rate': 9.228813559322035e-06, 'epoch': 0.09}
  9%|â–‰         | 555/6000 [18:30<3:02:59,  2.02s/it]  9%|â–‰         | 556/6000 [18:32<3:01:19,  2.00s/it]                                                    {'loss': 0.0437, 'grad_norm': 5.775832653045654, 'learning_rate': 9.227118644067798e-06, 'epoch': 0.09}
  9%|â–‰         | 556/6000 [18:32<3:01:19,  2.00s/it]  9%|â–‰         | 557/6000 [18:34<2:59:44,  1.98s/it]                                                    {'loss': 0.0486, 'grad_norm': 5.4124956130981445, 'learning_rate': 9.225423728813561e-06, 'epoch': 0.09}
  9%|â–‰         | 557/6000 [18:34<2:59:44,  1.98s/it]  9%|â–‰         | 558/6000 [18:36<3:00:30,  1.99s/it]                                                    {'loss': 0.0598, 'grad_norm': 15.509045600891113, 'learning_rate': 9.223728813559323e-06, 'epoch': 0.09}
  9%|â–‰         | 558/6000 [18:36<3:00:30,  1.99s/it]  9%|â–‰         | 559/6000 [18:38<3:00:55,  2.00s/it]                                                    {'loss': 0.0666, 'grad_norm': 15.343050003051758, 'learning_rate': 9.222033898305084e-06, 'epoch': 0.09}
  9%|â–‰         | 559/6000 [18:38<3:00:55,  2.00s/it]  9%|â–‰         | 560/6000 [18:40<2:59:11,  1.98s/it]                                                    {'loss': 0.1977, 'grad_norm': 20.796939849853516, 'learning_rate': 9.220338983050847e-06, 'epoch': 0.09}
  9%|â–‰         | 560/6000 [18:40<2:59:11,  1.98s/it]  9%|â–‰         | 561/6000 [18:42<3:03:23,  2.02s/it]                                                    {'loss': 0.0221, 'grad_norm': 5.5672430992126465, 'learning_rate': 9.21864406779661e-06, 'epoch': 0.09}
  9%|â–‰         | 561/6000 [18:42<3:03:23,  2.02s/it]  9%|â–‰         | 562/6000 [18:44<2:59:55,  1.99s/it]                                                    {'loss': 0.2919, 'grad_norm': 20.882244110107422, 'learning_rate': 9.216949152542374e-06, 'epoch': 0.09}
  9%|â–‰         | 562/6000 [18:44<2:59:55,  1.99s/it]  9%|â–‰         | 563/6000 [18:46<2:59:07,  1.98s/it]                                                    {'loss': 0.0135, 'grad_norm': 3.1188552379608154, 'learning_rate': 9.215254237288136e-06, 'epoch': 0.09}
  9%|â–‰         | 563/6000 [18:46<2:59:07,  1.98s/it]  9%|â–‰         | 564/6000 [18:48<2:57:27,  1.96s/it]                                                    {'loss': 0.0141, 'grad_norm': 3.2695798873901367, 'learning_rate': 9.213559322033899e-06, 'epoch': 0.09}
  9%|â–‰         | 564/6000 [18:48<2:57:27,  1.96s/it]  9%|â–‰         | 565/6000 [18:50<3:00:08,  1.99s/it]                                                    {'loss': 0.0308, 'grad_norm': 7.447402000427246, 'learning_rate': 9.211864406779662e-06, 'epoch': 0.09}
  9%|â–‰         | 565/6000 [18:50<3:00:08,  1.99s/it]  9%|â–‰         | 566/6000 [18:52<2:59:13,  1.98s/it]                                                    {'loss': 0.0927, 'grad_norm': 18.167016983032227, 'learning_rate': 9.210169491525425e-06, 'epoch': 0.09}
  9%|â–‰         | 566/6000 [18:52<2:59:13,  1.98s/it]  9%|â–‰         | 567/6000 [18:54<3:04:14,  2.03s/it]                                                    {'loss': 0.0444, 'grad_norm': 6.552008628845215, 'learning_rate': 9.208474576271187e-06, 'epoch': 0.09}
  9%|â–‰         | 567/6000 [18:54<3:04:14,  2.03s/it]  9%|â–‰         | 568/6000 [18:56<3:03:30,  2.03s/it]                                                    {'loss': 0.0658, 'grad_norm': 5.971095561981201, 'learning_rate': 9.20677966101695e-06, 'epoch': 0.09}
  9%|â–‰         | 568/6000 [18:56<3:03:30,  2.03s/it]  9%|â–‰         | 569/6000 [18:58<3:03:12,  2.02s/it]                                                    {'loss': 0.0024, 'grad_norm': 0.647958517074585, 'learning_rate': 9.205084745762712e-06, 'epoch': 0.09}
  9%|â–‰         | 569/6000 [18:58<3:03:12,  2.02s/it] 10%|â–‰         | 570/6000 [19:00<3:01:43,  2.01s/it]                                                    {'loss': 0.0624, 'grad_norm': 7.671009063720703, 'learning_rate': 9.203389830508475e-06, 'epoch': 0.1}
 10%|â–‰         | 570/6000 [19:00<3:01:43,  2.01s/it] 10%|â–‰         | 571/6000 [19:02<3:14:05,  2.15s/it]                                                    {'loss': 0.0344, 'grad_norm': 6.831878185272217, 'learning_rate': 9.201694915254238e-06, 'epoch': 0.1}
 10%|â–‰         | 571/6000 [19:02<3:14:05,  2.15s/it] 10%|â–‰         | 572/6000 [19:04<3:10:19,  2.10s/it]                                                    {'loss': 0.3911, 'grad_norm': 20.30156707763672, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.1}
 10%|â–‰         | 572/6000 [19:04<3:10:19,  2.10s/it] 10%|â–‰         | 573/6000 [19:06<3:05:08,  2.05s/it]                                                    {'loss': 0.1824, 'grad_norm': 16.669174194335938, 'learning_rate': 9.198305084745763e-06, 'epoch': 0.1}
 10%|â–‰         | 573/6000 [19:06<3:05:08,  2.05s/it] 10%|â–‰         | 574/6000 [19:08<3:02:22,  2.02s/it]                                                    {'loss': 0.4302, 'grad_norm': 25.49115753173828, 'learning_rate': 9.196610169491527e-06, 'epoch': 0.1}
 10%|â–‰         | 574/6000 [19:08<3:02:22,  2.02s/it] 10%|â–‰         | 575/6000 [19:10<3:00:26,  2.00s/it]                                                    {'loss': 0.006, 'grad_norm': 1.2757729291915894, 'learning_rate': 9.194915254237288e-06, 'epoch': 0.1}
 10%|â–‰         | 575/6000 [19:10<3:00:26,  2.00s/it] 10%|â–‰         | 576/6000 [19:12<3:01:51,  2.01s/it]                                                    {'loss': 0.0718, 'grad_norm': 14.338651657104492, 'learning_rate': 9.193220338983051e-06, 'epoch': 0.1}
 10%|â–‰         | 576/6000 [19:12<3:01:51,  2.01s/it] 10%|â–‰         | 577/6000 [19:14<3:01:57,  2.01s/it]                                                    {'loss': 0.0786, 'grad_norm': 8.541083335876465, 'learning_rate': 9.191525423728815e-06, 'epoch': 0.1}
 10%|â–‰         | 577/6000 [19:14<3:01:57,  2.01s/it] 10%|â–‰         | 578/6000 [19:16<3:04:45,  2.04s/it]                                                    {'loss': 0.2119, 'grad_norm': 18.512462615966797, 'learning_rate': 9.189830508474576e-06, 'epoch': 0.1}
 10%|â–‰         | 578/6000 [19:16<3:04:45,  2.04s/it] 10%|â–‰         | 579/6000 [19:18<3:06:13,  2.06s/it]                                                    {'loss': 0.0191, 'grad_norm': 4.6284027099609375, 'learning_rate': 9.18813559322034e-06, 'epoch': 0.1}
 10%|â–‰         | 579/6000 [19:18<3:06:13,  2.06s/it] 10%|â–‰         | 580/6000 [19:20<3:04:50,  2.05s/it]                                                    {'loss': 0.1342, 'grad_norm': 15.620078086853027, 'learning_rate': 9.186440677966101e-06, 'epoch': 0.1}
 10%|â–‰         | 580/6000 [19:20<3:04:50,  2.05s/it] 10%|â–‰         | 581/6000 [19:23<3:12:00,  2.13s/it]                                                    {'loss': 0.3866, 'grad_norm': 20.260950088500977, 'learning_rate': 9.184745762711866e-06, 'epoch': 0.1}
 10%|â–‰         | 581/6000 [19:23<3:12:00,  2.13s/it] 10%|â–‰         | 582/6000 [19:25<3:24:46,  2.27s/it]                                                    {'loss': 0.1818, 'grad_norm': 23.200986862182617, 'learning_rate': 9.183050847457628e-06, 'epoch': 0.1}
 10%|â–‰         | 582/6000 [19:25<3:24:46,  2.27s/it] 10%|â–‰         | 583/6000 [19:27<3:15:54,  2.17s/it]                                                    {'loss': 0.0507, 'grad_norm': 10.736340522766113, 'learning_rate': 9.181355932203391e-06, 'epoch': 0.1}
 10%|â–‰         | 583/6000 [19:27<3:15:54,  2.17s/it] 10%|â–‰         | 584/6000 [19:29<3:10:45,  2.11s/it]                                                    {'loss': 0.4464, 'grad_norm': 24.00497817993164, 'learning_rate': 9.179661016949153e-06, 'epoch': 0.1}
 10%|â–‰         | 584/6000 [19:29<3:10:45,  2.11s/it] 10%|â–‰         | 585/6000 [19:31<3:13:20,  2.14s/it]                                                    {'loss': 0.0023, 'grad_norm': 0.49612879753112793, 'learning_rate': 9.177966101694916e-06, 'epoch': 0.1}
 10%|â–‰         | 585/6000 [19:31<3:13:20,  2.14s/it] 10%|â–‰         | 586/6000 [19:33<3:07:56,  2.08s/it]                                                    {'loss': 0.0333, 'grad_norm': 7.748125076293945, 'learning_rate': 9.176271186440679e-06, 'epoch': 0.1}
 10%|â–‰         | 586/6000 [19:33<3:07:56,  2.08s/it] 10%|â–‰         | 587/6000 [19:35<3:03:02,  2.03s/it]                                                    {'loss': 0.0693, 'grad_norm': 9.973235130310059, 'learning_rate': 9.174576271186442e-06, 'epoch': 0.1}
 10%|â–‰         | 587/6000 [19:35<3:03:02,  2.03s/it] 10%|â–‰         | 588/6000 [19:37<3:00:28,  2.00s/it]                                                    {'loss': 0.0084, 'grad_norm': 2.4909422397613525, 'learning_rate': 9.172881355932204e-06, 'epoch': 0.1}
 10%|â–‰         | 588/6000 [19:37<3:00:28,  2.00s/it] 10%|â–‰         | 589/6000 [19:39<3:00:00,  2.00s/it]                                                    {'loss': 0.0141, 'grad_norm': 3.0812594890594482, 'learning_rate': 9.171186440677967e-06, 'epoch': 0.1}
 10%|â–‰         | 589/6000 [19:39<3:00:00,  2.00s/it] 10%|â–‰         | 590/6000 [19:41<2:59:09,  1.99s/it]                                                    {'loss': 0.0431, 'grad_norm': 6.868009090423584, 'learning_rate': 9.169491525423729e-06, 'epoch': 0.1}
 10%|â–‰         | 590/6000 [19:41<2:59:09,  1.99s/it] 10%|â–‰         | 591/6000 [19:43<2:58:19,  1.98s/it]                                                    {'loss': 0.1005, 'grad_norm': 15.114609718322754, 'learning_rate': 9.167796610169492e-06, 'epoch': 0.1}
 10%|â–‰         | 591/6000 [19:43<2:58:19,  1.98s/it] 10%|â–‰         | 592/6000 [19:45<2:57:12,  1.97s/it]                                                    {'loss': 0.1585, 'grad_norm': 18.038515090942383, 'learning_rate': 9.166101694915255e-06, 'epoch': 0.1}
 10%|â–‰         | 592/6000 [19:45<2:57:12,  1.97s/it] 10%|â–‰         | 593/6000 [19:47<2:57:39,  1.97s/it]                                                    {'loss': 0.0254, 'grad_norm': 7.479465484619141, 'learning_rate': 9.164406779661019e-06, 'epoch': 0.1}
 10%|â–‰         | 593/6000 [19:47<2:57:39,  1.97s/it] 10%|â–‰         | 594/6000 [19:49<2:57:15,  1.97s/it]                                                    {'loss': 0.0837, 'grad_norm': 14.207656860351562, 'learning_rate': 9.16271186440678e-06, 'epoch': 0.1}
 10%|â–‰         | 594/6000 [19:49<2:57:15,  1.97s/it] 10%|â–‰         | 595/6000 [19:51<3:01:00,  2.01s/it]                                                    {'loss': 0.0599, 'grad_norm': 22.178403854370117, 'learning_rate': 9.161016949152543e-06, 'epoch': 0.1}
 10%|â–‰         | 595/6000 [19:51<3:01:00,  2.01s/it] 10%|â–‰         | 596/6000 [19:53<3:11:09,  2.12s/it]                                                    {'loss': 0.0741, 'grad_norm': 4.599849224090576, 'learning_rate': 9.159322033898305e-06, 'epoch': 0.1}
 10%|â–‰         | 596/6000 [19:53<3:11:09,  2.12s/it] 10%|â–‰         | 597/6000 [19:55<3:04:48,  2.05s/it]                                                    {'loss': 0.0431, 'grad_norm': 7.067619323730469, 'learning_rate': 9.157627118644068e-06, 'epoch': 0.1}
 10%|â–‰         | 597/6000 [19:55<3:04:48,  2.05s/it] 10%|â–‰         | 598/6000 [19:57<3:04:02,  2.04s/it]                                                    {'loss': 0.0706, 'grad_norm': 16.681333541870117, 'learning_rate': 9.155932203389832e-06, 'epoch': 0.1}
 10%|â–‰         | 598/6000 [19:57<3:04:02,  2.04s/it] 10%|â–‰         | 599/6000 [19:59<3:00:22,  2.00s/it]                                                    {'loss': 0.0241, 'grad_norm': 6.108974933624268, 'learning_rate': 9.154237288135593e-06, 'epoch': 0.1}
 10%|â–‰         | 599/6000 [19:59<3:00:22,  2.00s/it] 10%|â–ˆ         | 600/6000 [20:01<2:59:12,  1.99s/it]                                                    {'loss': 0.0378, 'grad_norm': 9.735820770263672, 'learning_rate': 9.152542372881356e-06, 'epoch': 0.1}
 10%|â–ˆ         | 600/6000 [20:01<2:59:12,  1.99s/it][2025-11-18 10:09:59,080] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600
[2025-11-18 10:09:59,447] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-600/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 10%|â–ˆ         | 601/6000 [20:04<3:21:03,  2.23s/it]                                                    {'loss': 0.1484, 'grad_norm': 17.751298904418945, 'learning_rate': 9.15084745762712e-06, 'epoch': 0.1}
 10%|â–ˆ         | 601/6000 [20:04<3:21:03,  2.23s/it] 10%|â–ˆ         | 602/6000 [20:06<3:13:45,  2.15s/it]                                                    {'loss': 0.0597, 'grad_norm': 17.219951629638672, 'learning_rate': 9.149152542372883e-06, 'epoch': 0.1}
 10%|â–ˆ         | 602/6000 [20:06<3:13:45,  2.15s/it] 10%|â–ˆ         | 603/6000 [20:08<3:10:47,  2.12s/it]                                                    {'loss': 0.2749, 'grad_norm': 28.31087875366211, 'learning_rate': 9.147457627118645e-06, 'epoch': 0.1}
 10%|â–ˆ         | 603/6000 [20:08<3:10:47,  2.12s/it] 10%|â–ˆ         | 604/6000 [20:10<3:10:31,  2.12s/it]                                                    {'loss': 0.0827, 'grad_norm': 14.409965515136719, 'learning_rate': 9.145762711864408e-06, 'epoch': 0.1}
 10%|â–ˆ         | 604/6000 [20:10<3:10:31,  2.12s/it] 10%|â–ˆ         | 605/6000 [20:12<3:04:54,  2.06s/it]                                                    {'loss': 0.1364, 'grad_norm': 16.2237491607666, 'learning_rate': 9.14406779661017e-06, 'epoch': 0.1}
 10%|â–ˆ         | 605/6000 [20:12<3:04:54,  2.06s/it] 10%|â–ˆ         | 606/6000 [20:14<3:04:34,  2.05s/it]                                                    {'loss': 0.0144, 'grad_norm': 3.738943099975586, 'learning_rate': 9.142372881355933e-06, 'epoch': 0.1}
 10%|â–ˆ         | 606/6000 [20:14<3:04:34,  2.05s/it] 10%|â–ˆ         | 607/6000 [20:16<3:00:41,  2.01s/it]                                                    {'loss': 0.0281, 'grad_norm': 7.876990795135498, 'learning_rate': 9.140677966101696e-06, 'epoch': 0.1}
 10%|â–ˆ         | 607/6000 [20:16<3:00:41,  2.01s/it] 10%|â–ˆ         | 608/6000 [20:18<2:59:54,  2.00s/it]                                                    {'loss': 0.1002, 'grad_norm': 15.828315734863281, 'learning_rate': 9.13898305084746e-06, 'epoch': 0.1}
 10%|â–ˆ         | 608/6000 [20:18<2:59:54,  2.00s/it] 10%|â–ˆ         | 609/6000 [20:20<2:59:59,  2.00s/it]                                                    {'loss': 0.1516, 'grad_norm': 12.20417308807373, 'learning_rate': 9.13728813559322e-06, 'epoch': 0.1}
 10%|â–ˆ         | 609/6000 [20:20<2:59:59,  2.00s/it] 10%|â–ˆ         | 610/6000 [20:22<2:58:25,  1.99s/it]                                                    {'loss': 0.0786, 'grad_norm': 19.40853500366211, 'learning_rate': 9.135593220338984e-06, 'epoch': 0.1}
 10%|â–ˆ         | 610/6000 [20:22<2:58:25,  1.99s/it] 10%|â–ˆ         | 611/6000 [20:24<2:57:06,  1.97s/it]                                                    {'loss': 0.2031, 'grad_norm': 24.38996124267578, 'learning_rate': 9.133898305084746e-06, 'epoch': 0.1}
 10%|â–ˆ         | 611/6000 [20:24<2:57:06,  1.97s/it] 10%|â–ˆ         | 612/6000 [20:26<2:58:24,  1.99s/it]                                                    {'loss': 0.0254, 'grad_norm': 3.520663261413574, 'learning_rate': 9.132203389830509e-06, 'epoch': 0.1}
 10%|â–ˆ         | 612/6000 [20:26<2:58:24,  1.99s/it] 10%|â–ˆ         | 613/6000 [20:28<2:59:55,  2.00s/it]                                                    {'loss': 0.0786, 'grad_norm': 9.278155326843262, 'learning_rate': 9.130508474576272e-06, 'epoch': 0.1}
 10%|â–ˆ         | 613/6000 [20:28<2:59:55,  2.00s/it] 10%|â–ˆ         | 614/6000 [20:30<2:57:36,  1.98s/it]                                                    {'loss': 0.0194, 'grad_norm': 4.839051246643066, 'learning_rate': 9.128813559322035e-06, 'epoch': 0.1}
 10%|â–ˆ         | 614/6000 [20:30<2:57:36,  1.98s/it] 10%|â–ˆ         | 615/6000 [20:32<2:58:40,  1.99s/it]                                                    {'loss': 0.0616, 'grad_norm': 6.418368339538574, 'learning_rate': 9.127118644067797e-06, 'epoch': 0.1}
 10%|â–ˆ         | 615/6000 [20:32<2:58:40,  1.99s/it] 10%|â–ˆ         | 616/6000 [20:34<2:56:33,  1.97s/it]                                                    {'loss': 0.0896, 'grad_norm': 9.119024276733398, 'learning_rate': 9.12542372881356e-06, 'epoch': 0.1}
 10%|â–ˆ         | 616/6000 [20:34<2:56:33,  1.97s/it] 10%|â–ˆ         | 617/6000 [20:36<2:56:15,  1.96s/it]                                                    {'loss': 0.003, 'grad_norm': 0.8428474068641663, 'learning_rate': 9.123728813559322e-06, 'epoch': 0.1}
 10%|â–ˆ         | 617/6000 [20:36<2:56:15,  1.96s/it] 10%|â–ˆ         | 618/6000 [20:38<2:55:58,  1.96s/it]                                                    {'loss': 0.0416, 'grad_norm': 7.7180094718933105, 'learning_rate': 9.122033898305085e-06, 'epoch': 0.1}
 10%|â–ˆ         | 618/6000 [20:38<2:55:58,  1.96s/it] 10%|â–ˆ         | 619/6000 [20:40<3:01:26,  2.02s/it]                                                    {'loss': 0.1382, 'grad_norm': 19.076528549194336, 'learning_rate': 9.120338983050848e-06, 'epoch': 0.1}
 10%|â–ˆ         | 619/6000 [20:40<3:01:26,  2.02s/it] 10%|â–ˆ         | 620/6000 [20:42<2:59:16,  2.00s/it]                                                    {'loss': 0.0401, 'grad_norm': 9.506568908691406, 'learning_rate': 9.11864406779661e-06, 'epoch': 0.1}
 10%|â–ˆ         | 620/6000 [20:42<2:59:16,  2.00s/it] 10%|â–ˆ         | 621/6000 [20:44<2:56:51,  1.97s/it]                                                    {'loss': 0.0862, 'grad_norm': 13.9124174118042, 'learning_rate': 9.116949152542373e-06, 'epoch': 0.1}
 10%|â–ˆ         | 621/6000 [20:44<2:56:51,  1.97s/it] 10%|â–ˆ         | 622/6000 [20:46<2:55:20,  1.96s/it]                                                    {'loss': 0.2285, 'grad_norm': 20.085453033447266, 'learning_rate': 9.115254237288137e-06, 'epoch': 0.1}
 10%|â–ˆ         | 622/6000 [20:46<2:55:20,  1.96s/it] 10%|â–ˆ         | 623/6000 [20:48<2:54:26,  1.95s/it]                                                    {'loss': 0.2618, 'grad_norm': 21.86199378967285, 'learning_rate': 9.1135593220339e-06, 'epoch': 0.1}
 10%|â–ˆ         | 623/6000 [20:48<2:54:26,  1.95s/it] 10%|â–ˆ         | 624/6000 [20:50<2:54:54,  1.95s/it]                                                    {'loss': 0.2047, 'grad_norm': 20.763484954833984, 'learning_rate': 9.111864406779661e-06, 'epoch': 0.1}
 10%|â–ˆ         | 624/6000 [20:50<2:54:54,  1.95s/it] 10%|â–ˆ         | 625/6000 [20:52<2:57:17,  1.98s/it]                                                    {'loss': 0.0298, 'grad_norm': 11.07534408569336, 'learning_rate': 9.110169491525425e-06, 'epoch': 0.1}
 10%|â–ˆ         | 625/6000 [20:52<2:57:17,  1.98s/it] 10%|â–ˆ         | 626/6000 [20:54<2:56:18,  1.97s/it]                                                    {'loss': 0.0576, 'grad_norm': 8.81428337097168, 'learning_rate': 9.108474576271186e-06, 'epoch': 0.1}
 10%|â–ˆ         | 626/6000 [20:54<2:56:18,  1.97s/it] 10%|â–ˆ         | 627/6000 [20:56<3:05:01,  2.07s/it]                                                    {'loss': 0.0188, 'grad_norm': 3.1869776248931885, 'learning_rate': 9.10677966101695e-06, 'epoch': 0.1}
 10%|â–ˆ         | 627/6000 [20:56<3:05:01,  2.07s/it] 10%|â–ˆ         | 628/6000 [20:58<3:02:26,  2.04s/it]                                                    {'loss': 0.2639, 'grad_norm': 29.743610382080078, 'learning_rate': 9.105084745762713e-06, 'epoch': 0.1}
 10%|â–ˆ         | 628/6000 [20:58<3:02:26,  2.04s/it] 10%|â–ˆ         | 629/6000 [21:00<3:00:55,  2.02s/it]                                                    {'loss': 0.0154, 'grad_norm': 3.117896795272827, 'learning_rate': 9.103389830508476e-06, 'epoch': 0.1}
 10%|â–ˆ         | 629/6000 [21:00<3:00:55,  2.02s/it] 10%|â–ˆ         | 630/6000 [21:02<2:58:20,  1.99s/it]                                                    {'loss': 0.5593, 'grad_norm': 25.929161071777344, 'learning_rate': 9.101694915254238e-06, 'epoch': 0.1}
 10%|â–ˆ         | 630/6000 [21:02<2:58:20,  1.99s/it] 11%|â–ˆ         | 631/6000 [21:04<2:58:15,  1.99s/it]                                                    {'loss': 0.1684, 'grad_norm': 20.072843551635742, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.11}
 11%|â–ˆ         | 631/6000 [21:04<2:58:15,  1.99s/it] 11%|â–ˆ         | 632/6000 [21:06<2:57:13,  1.98s/it]                                                    {'loss': 0.0838, 'grad_norm': 15.612696647644043, 'learning_rate': 9.098305084745763e-06, 'epoch': 0.11}
 11%|â–ˆ         | 632/6000 [21:06<2:57:13,  1.98s/it] 11%|â–ˆ         | 633/6000 [21:08<2:57:17,  1.98s/it]                                                    {'loss': 0.1069, 'grad_norm': 10.617608070373535, 'learning_rate': 9.096610169491526e-06, 'epoch': 0.11}
 11%|â–ˆ         | 633/6000 [21:08<2:57:17,  1.98s/it] 11%|â–ˆ         | 634/6000 [21:10<3:00:18,  2.02s/it]                                                    {'loss': 0.0194, 'grad_norm': 3.6915130615234375, 'learning_rate': 9.094915254237289e-06, 'epoch': 0.11}
 11%|â–ˆ         | 634/6000 [21:10<3:00:18,  2.02s/it] 11%|â–ˆ         | 635/6000 [21:12<2:59:53,  2.01s/it]                                                    {'loss': 0.022, 'grad_norm': 5.1228227615356445, 'learning_rate': 9.093220338983052e-06, 'epoch': 0.11}
 11%|â–ˆ         | 635/6000 [21:12<2:59:53,  2.01s/it] 11%|â–ˆ         | 636/6000 [21:14<2:58:47,  2.00s/it]                                                    {'loss': 0.0099, 'grad_norm': 3.1311864852905273, 'learning_rate': 9.091525423728814e-06, 'epoch': 0.11}
 11%|â–ˆ         | 636/6000 [21:14<2:58:47,  2.00s/it] 11%|â–ˆ         | 637/6000 [21:16<2:57:51,  1.99s/it]                                                    {'loss': 0.0248, 'grad_norm': 6.826132297515869, 'learning_rate': 9.089830508474577e-06, 'epoch': 0.11}
 11%|â–ˆ         | 637/6000 [21:16<2:57:51,  1.99s/it] 11%|â–ˆ         | 638/6000 [21:18<3:02:09,  2.04s/it]                                                    {'loss': 0.0447, 'grad_norm': 5.086575984954834, 'learning_rate': 9.08813559322034e-06, 'epoch': 0.11}
 11%|â–ˆ         | 638/6000 [21:18<3:02:09,  2.04s/it] 11%|â–ˆ         | 639/6000 [21:20<3:02:07,  2.04s/it]                                                    {'loss': 0.0469, 'grad_norm': 7.401865482330322, 'learning_rate': 9.086440677966104e-06, 'epoch': 0.11}
 11%|â–ˆ         | 639/6000 [21:20<3:02:07,  2.04s/it] 11%|â–ˆ         | 640/6000 [21:22<3:02:34,  2.04s/it]                                                    {'loss': 0.0175, 'grad_norm': 3.460941791534424, 'learning_rate': 9.084745762711865e-06, 'epoch': 0.11}
 11%|â–ˆ         | 640/6000 [21:22<3:02:34,  2.04s/it] 11%|â–ˆ         | 641/6000 [21:24<3:05:14,  2.07s/it]                                                    {'loss': 0.0075, 'grad_norm': 1.6610629558563232, 'learning_rate': 9.083050847457627e-06, 'epoch': 0.11}
 11%|â–ˆ         | 641/6000 [21:24<3:05:14,  2.07s/it] 11%|â–ˆ         | 642/6000 [21:26<3:00:51,  2.03s/it]                                                    {'loss': 0.0124, 'grad_norm': 3.8080251216888428, 'learning_rate': 9.08135593220339e-06, 'epoch': 0.11}
 11%|â–ˆ         | 642/6000 [21:26<3:00:51,  2.03s/it] 11%|â–ˆ         | 643/6000 [21:28<2:59:29,  2.01s/it]                                                    {'loss': 0.0855, 'grad_norm': 15.040749549865723, 'learning_rate': 9.079661016949153e-06, 'epoch': 0.11}
 11%|â–ˆ         | 643/6000 [21:28<2:59:29,  2.01s/it] 11%|â–ˆ         | 644/6000 [21:30<2:59:11,  2.01s/it]                                                    {'loss': 0.0058, 'grad_norm': 1.6411148309707642, 'learning_rate': 9.077966101694917e-06, 'epoch': 0.11}
 11%|â–ˆ         | 644/6000 [21:30<2:59:11,  2.01s/it] 11%|â–ˆ         | 645/6000 [21:32<2:58:53,  2.00s/it]                                                    {'loss': 0.0792, 'grad_norm': 8.193692207336426, 'learning_rate': 9.076271186440678e-06, 'epoch': 0.11}
 11%|â–ˆ         | 645/6000 [21:32<2:58:53,  2.00s/it] 11%|â–ˆ         | 646/6000 [21:34<3:00:52,  2.03s/it]                                                    {'loss': 0.093, 'grad_norm': 13.143814086914062, 'learning_rate': 9.074576271186442e-06, 'epoch': 0.11}
 11%|â–ˆ         | 646/6000 [21:34<3:00:52,  2.03s/it] 11%|â–ˆ         | 647/6000 [21:36<3:00:11,  2.02s/it]                                                    {'loss': 0.129, 'grad_norm': 16.892913818359375, 'learning_rate': 9.072881355932203e-06, 'epoch': 0.11}
 11%|â–ˆ         | 647/6000 [21:36<3:00:11,  2.02s/it] 11%|â–ˆ         | 648/6000 [21:38<3:02:56,  2.05s/it]                                                    {'loss': 0.0698, 'grad_norm': 10.235759735107422, 'learning_rate': 9.071186440677966e-06, 'epoch': 0.11}
 11%|â–ˆ         | 648/6000 [21:38<3:02:56,  2.05s/it] 11%|â–ˆ         | 649/6000 [21:40<3:00:16,  2.02s/it]                                                    {'loss': 0.0426, 'grad_norm': 11.020048141479492, 'learning_rate': 9.06949152542373e-06, 'epoch': 0.11}
 11%|â–ˆ         | 649/6000 [21:40<3:00:16,  2.02s/it] 11%|â–ˆ         | 650/6000 [21:42<2:57:04,  1.99s/it]                                                    {'loss': 0.1288, 'grad_norm': 18.012418746948242, 'learning_rate': 9.067796610169493e-06, 'epoch': 0.11}
 11%|â–ˆ         | 650/6000 [21:42<2:57:04,  1.99s/it][2025-11-18 10:11:39,868] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650
[2025-11-18 10:11:40,152] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-650/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 11%|â–ˆ         | 651/6000 [21:45<3:15:07,  2.19s/it]                                                    {'loss': 0.1575, 'grad_norm': 19.77835464477539, 'learning_rate': 9.066101694915255e-06, 'epoch': 0.11}
 11%|â–ˆ         | 651/6000 [21:45<3:15:07,  2.19s/it] 11%|â–ˆ         | 652/6000 [21:47<3:11:03,  2.14s/it]                                                    {'loss': 0.0358, 'grad_norm': 8.159102439880371, 'learning_rate': 9.064406779661018e-06, 'epoch': 0.11}
 11%|â–ˆ         | 652/6000 [21:47<3:11:03,  2.14s/it] 11%|â–ˆ         | 653/6000 [21:49<3:07:08,  2.10s/it]                                                    {'loss': 0.1423, 'grad_norm': 12.515315055847168, 'learning_rate': 9.06271186440678e-06, 'epoch': 0.11}
 11%|â–ˆ         | 653/6000 [21:49<3:07:08,  2.10s/it] 11%|â–ˆ         | 654/6000 [21:51<3:01:46,  2.04s/it]                                                    {'loss': 0.0577, 'grad_norm': 14.283598899841309, 'learning_rate': 9.061016949152543e-06, 'epoch': 0.11}
 11%|â–ˆ         | 654/6000 [21:51<3:01:46,  2.04s/it] 11%|â–ˆ         | 655/6000 [21:53<2:58:17,  2.00s/it]                                                    {'loss': 0.073, 'grad_norm': 14.67620849609375, 'learning_rate': 9.059322033898306e-06, 'epoch': 0.11}
 11%|â–ˆ         | 655/6000 [21:53<2:58:17,  2.00s/it] 11%|â–ˆ         | 656/6000 [21:54<2:57:57,  2.00s/it]                                                    {'loss': 0.131, 'grad_norm': 17.908954620361328, 'learning_rate': 9.05762711864407e-06, 'epoch': 0.11}
 11%|â–ˆ         | 656/6000 [21:55<2:57:57,  2.00s/it] 11%|â–ˆ         | 657/6000 [21:57<2:59:19,  2.01s/it]                                                    {'loss': 0.1314, 'grad_norm': 13.994410514831543, 'learning_rate': 9.05593220338983e-06, 'epoch': 0.11}
 11%|â–ˆ         | 657/6000 [21:57<2:59:19,  2.01s/it] 11%|â–ˆ         | 658/6000 [21:59<3:00:48,  2.03s/it]                                                    {'loss': 0.0061, 'grad_norm': 1.5431327819824219, 'learning_rate': 9.054237288135594e-06, 'epoch': 0.11}
 11%|â–ˆ         | 658/6000 [21:59<3:00:48,  2.03s/it] 11%|â–ˆ         | 659/6000 [22:01<2:59:07,  2.01s/it]                                                    {'loss': 0.0032, 'grad_norm': 0.8844045996665955, 'learning_rate': 9.052542372881357e-06, 'epoch': 0.11}
 11%|â–ˆ         | 659/6000 [22:01<2:59:07,  2.01s/it] 11%|â–ˆ         | 660/6000 [22:03<2:57:57,  2.00s/it]                                                    {'loss': 0.0105, 'grad_norm': 2.9094526767730713, 'learning_rate': 9.05084745762712e-06, 'epoch': 0.11}
 11%|â–ˆ         | 660/6000 [22:03<2:57:57,  2.00s/it] 11%|â–ˆ         | 661/6000 [22:04<2:55:58,  1.98s/it]                                                    {'loss': 0.0158, 'grad_norm': 5.331737995147705, 'learning_rate': 9.049152542372882e-06, 'epoch': 0.11}
 11%|â–ˆ         | 661/6000 [22:04<2:55:58,  1.98s/it] 11%|â–ˆ         | 662/6000 [22:06<2:55:42,  1.97s/it]                                                    {'loss': 0.0776, 'grad_norm': 27.08528709411621, 'learning_rate': 9.047457627118644e-06, 'epoch': 0.11}
 11%|â–ˆ         | 662/6000 [22:06<2:55:42,  1.97s/it] 11%|â–ˆ         | 663/6000 [22:08<2:53:39,  1.95s/it]                                                    {'loss': 0.094, 'grad_norm': 14.333802223205566, 'learning_rate': 9.045762711864407e-06, 'epoch': 0.11}
 11%|â–ˆ         | 663/6000 [22:08<2:53:39,  1.95s/it] 11%|â–ˆ         | 664/6000 [22:10<2:54:26,  1.96s/it]                                                    {'loss': 0.0118, 'grad_norm': 3.8130383491516113, 'learning_rate': 9.04406779661017e-06, 'epoch': 0.11}
 11%|â–ˆ         | 664/6000 [22:10<2:54:26,  1.96s/it] 11%|â–ˆ         | 665/6000 [22:12<2:53:58,  1.96s/it]                                                    {'loss': 0.0653, 'grad_norm': 7.963310718536377, 'learning_rate': 9.042372881355934e-06, 'epoch': 0.11}
 11%|â–ˆ         | 665/6000 [22:12<2:53:58,  1.96s/it] 11%|â–ˆ         | 666/6000 [22:14<2:59:41,  2.02s/it]                                                    {'loss': 0.0432, 'grad_norm': 4.850011348724365, 'learning_rate': 9.040677966101695e-06, 'epoch': 0.11}
 11%|â–ˆ         | 666/6000 [22:14<2:59:41,  2.02s/it] 11%|â–ˆ         | 667/6000 [22:16<2:59:29,  2.02s/it]                                                    {'loss': 0.0816, 'grad_norm': 10.506904602050781, 'learning_rate': 9.038983050847458e-06, 'epoch': 0.11}
 11%|â–ˆ         | 667/6000 [22:16<2:59:29,  2.02s/it] 11%|â–ˆ         | 668/6000 [22:18<2:57:18,  2.00s/it]                                                    {'loss': 0.0208, 'grad_norm': 6.23286247253418, 'learning_rate': 9.03728813559322e-06, 'epoch': 0.11}
 11%|â–ˆ         | 668/6000 [22:18<2:57:18,  2.00s/it] 11%|â–ˆ         | 669/6000 [22:20<2:55:30,  1.98s/it]                                                    {'loss': 0.0113, 'grad_norm': 2.6785454750061035, 'learning_rate': 9.035593220338983e-06, 'epoch': 0.11}
 11%|â–ˆ         | 669/6000 [22:20<2:55:30,  1.98s/it] 11%|â–ˆ         | 670/6000 [22:22<2:56:50,  1.99s/it]                                                    {'loss': 0.0531, 'grad_norm': 12.25223159790039, 'learning_rate': 9.033898305084747e-06, 'epoch': 0.11}
 11%|â–ˆ         | 670/6000 [22:22<2:56:50,  1.99s/it] 11%|â–ˆ         | 671/6000 [22:24<2:56:23,  1.99s/it]                                                    {'loss': 0.037, 'grad_norm': 5.579490661621094, 'learning_rate': 9.03220338983051e-06, 'epoch': 0.11}
 11%|â–ˆ         | 671/6000 [22:24<2:56:23,  1.99s/it] 11%|â–ˆ         | 672/6000 [22:26<2:55:02,  1.97s/it]                                                    {'loss': 0.0955, 'grad_norm': 18.9011173248291, 'learning_rate': 9.030508474576271e-06, 'epoch': 0.11}
 11%|â–ˆ         | 672/6000 [22:26<2:55:02,  1.97s/it] 11%|â–ˆ         | 673/6000 [22:28<2:55:02,  1.97s/it]                                                    {'loss': 0.0561, 'grad_norm': 10.392683029174805, 'learning_rate': 9.028813559322035e-06, 'epoch': 0.11}
 11%|â–ˆ         | 673/6000 [22:28<2:55:02,  1.97s/it] 11%|â–ˆ         | 674/6000 [22:30<2:53:56,  1.96s/it]                                                    {'loss': 0.0358, 'grad_norm': 5.540411472320557, 'learning_rate': 9.027118644067796e-06, 'epoch': 0.11}
 11%|â–ˆ         | 674/6000 [22:30<2:53:56,  1.96s/it] 11%|â–ˆâ–        | 675/6000 [22:32<2:52:37,  1.95s/it]                                                    {'loss': 0.1242, 'grad_norm': 19.087575912475586, 'learning_rate': 9.02542372881356e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 675/6000 [22:32<2:52:37,  1.95s/it] 11%|â–ˆâ–        | 676/6000 [22:34<2:52:27,  1.94s/it]                                                    {'loss': 0.0478, 'grad_norm': 6.497345447540283, 'learning_rate': 9.023728813559323e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 676/6000 [22:34<2:52:27,  1.94s/it] 11%|â–ˆâ–        | 677/6000 [22:36<2:51:33,  1.93s/it]                                                    {'loss': 0.0518, 'grad_norm': 10.568645477294922, 'learning_rate': 9.022033898305086e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 677/6000 [22:36<2:51:33,  1.93s/it] 11%|â–ˆâ–        | 678/6000 [22:38<2:53:30,  1.96s/it]                                                    {'loss': 0.0241, 'grad_norm': 5.8172478675842285, 'learning_rate': 9.020338983050848e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 678/6000 [22:38<2:53:30,  1.96s/it] 11%|â–ˆâ–        | 679/6000 [22:40<2:51:16,  1.93s/it]                                                    {'loss': 0.0409, 'grad_norm': 12.703024864196777, 'learning_rate': 9.018644067796611e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 679/6000 [22:40<2:51:16,  1.93s/it] 11%|â–ˆâ–        | 680/6000 [22:42<2:50:50,  1.93s/it]                                                    {'loss': 0.0699, 'grad_norm': 11.62154483795166, 'learning_rate': 9.016949152542374e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 680/6000 [22:42<2:50:50,  1.93s/it] 11%|â–ˆâ–        | 681/6000 [22:44<2:56:04,  1.99s/it]                                                    {'loss': 0.0232, 'grad_norm': 5.3986053466796875, 'learning_rate': 9.015254237288138e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 681/6000 [22:44<2:56:04,  1.99s/it] 11%|â–ˆâ–        | 682/6000 [22:46<2:56:05,  1.99s/it]                                                    {'loss': 0.0089, 'grad_norm': 3.0250589847564697, 'learning_rate': 9.013559322033899e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 682/6000 [22:46<2:56:05,  1.99s/it] 11%|â–ˆâ–        | 683/6000 [22:48<2:55:34,  1.98s/it]                                                    {'loss': 0.1167, 'grad_norm': 19.007984161376953, 'learning_rate': 9.01186440677966e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 683/6000 [22:48<2:55:34,  1.98s/it] 11%|â–ˆâ–        | 684/6000 [22:50<2:53:46,  1.96s/it]                                                    {'loss': 0.121, 'grad_norm': 15.556292533874512, 'learning_rate': 9.010169491525424e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 684/6000 [22:50<2:53:46,  1.96s/it] 11%|â–ˆâ–        | 685/6000 [22:52<2:53:19,  1.96s/it]                                                    {'loss': 0.0733, 'grad_norm': 16.795427322387695, 'learning_rate': 9.008474576271187e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 685/6000 [22:52<2:53:19,  1.96s/it] 11%|â–ˆâ–        | 686/6000 [22:54<2:57:16,  2.00s/it]                                                    {'loss': 0.1695, 'grad_norm': 16.041959762573242, 'learning_rate': 9.00677966101695e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 686/6000 [22:54<2:57:16,  2.00s/it] 11%|â–ˆâ–        | 687/6000 [22:56<2:55:05,  1.98s/it]                                                    {'loss': 0.0037, 'grad_norm': 2.191784620285034, 'learning_rate': 9.005084745762712e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 687/6000 [22:56<2:55:05,  1.98s/it] 11%|â–ˆâ–        | 688/6000 [22:58<2:54:46,  1.97s/it]                                                    {'loss': 0.027, 'grad_norm': 6.636477470397949, 'learning_rate': 9.003389830508475e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 688/6000 [22:58<2:54:46,  1.97s/it] 11%|â–ˆâ–        | 689/6000 [23:00<2:53:58,  1.97s/it]                                                    {'loss': 0.1835, 'grad_norm': 19.34090232849121, 'learning_rate': 9.001694915254237e-06, 'epoch': 0.11}
 11%|â–ˆâ–        | 689/6000 [23:00<2:53:58,  1.97s/it] 12%|â–ˆâ–        | 690/6000 [23:02<2:55:55,  1.99s/it]                                                    {'loss': 0.0794, 'grad_norm': 9.784820556640625, 'learning_rate': 9e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 690/6000 [23:02<2:55:55,  1.99s/it] 12%|â–ˆâ–        | 691/6000 [23:04<2:55:11,  1.98s/it]                                                    {'loss': 0.0474, 'grad_norm': 9.853870391845703, 'learning_rate': 8.998305084745764e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 691/6000 [23:04<2:55:11,  1.98s/it] 12%|â–ˆâ–        | 692/6000 [23:06<2:55:54,  1.99s/it]                                                    {'loss': 0.4374, 'grad_norm': 25.7891788482666, 'learning_rate': 8.996610169491527e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 692/6000 [23:06<2:55:54,  1.99s/it] 12%|â–ˆâ–        | 693/6000 [23:08<2:54:52,  1.98s/it]                                                    {'loss': 0.114, 'grad_norm': 15.929580688476562, 'learning_rate': 8.994915254237288e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 693/6000 [23:08<2:54:52,  1.98s/it] 12%|â–ˆâ–        | 694/6000 [23:10<2:54:33,  1.97s/it]                                                    {'loss': 0.0024, 'grad_norm': 0.7455386519432068, 'learning_rate': 8.993220338983052e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 694/6000 [23:10<2:54:33,  1.97s/it] 12%|â–ˆâ–        | 695/6000 [23:12<2:55:09,  1.98s/it]                                                    {'loss': 0.0142, 'grad_norm': 2.3469526767730713, 'learning_rate': 8.991525423728815e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 695/6000 [23:12<2:55:09,  1.98s/it] 12%|â–ˆâ–        | 696/6000 [23:14<2:57:13,  2.00s/it]                                                    {'loss': 0.0922, 'grad_norm': 17.0963191986084, 'learning_rate': 8.989830508474578e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 696/6000 [23:14<2:57:13,  2.00s/it] 12%|â–ˆâ–        | 697/6000 [23:16<2:57:25,  2.01s/it]                                                    {'loss': 0.0843, 'grad_norm': 15.58324909210205, 'learning_rate': 8.98813559322034e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 697/6000 [23:16<2:57:25,  2.01s/it] 12%|â–ˆâ–        | 698/6000 [23:18<2:56:04,  1.99s/it]                                                    {'loss': 0.1048, 'grad_norm': 18.129526138305664, 'learning_rate': 8.986440677966103e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 698/6000 [23:18<2:56:04,  1.99s/it] 12%|â–ˆâ–        | 699/6000 [23:20<2:55:34,  1.99s/it]                                                    {'loss': 0.0778, 'grad_norm': 13.895208358764648, 'learning_rate': 8.984745762711865e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 699/6000 [23:20<2:55:34,  1.99s/it] 12%|â–ˆâ–        | 700/6000 [23:22<3:01:31,  2.05s/it]                                                    {'loss': 0.0475, 'grad_norm': 11.130108833312988, 'learning_rate': 8.983050847457628e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 700/6000 [23:22<3:01:31,  2.05s/it][2025-11-18 10:13:19,630] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700
[2025-11-18 10:13:20,079] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-700/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 12%|â–ˆâ–        | 701/6000 [23:25<3:24:32,  2.32s/it]                                                    {'loss': 0.0148, 'grad_norm': 4.561741828918457, 'learning_rate': 8.981355932203391e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 701/6000 [23:25<3:24:32,  2.32s/it] 12%|â–ˆâ–        | 702/6000 [23:27<3:20:09,  2.27s/it]                                                    {'loss': 0.0457, 'grad_norm': 10.302095413208008, 'learning_rate': 8.979661016949154e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 702/6000 [23:27<3:20:09,  2.27s/it] 12%|â–ˆâ–        | 703/6000 [23:29<3:13:10,  2.19s/it]                                                    {'loss': 0.0264, 'grad_norm': 7.0466179847717285, 'learning_rate': 8.977966101694916e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 703/6000 [23:29<3:13:10,  2.19s/it] 12%|â–ˆâ–        | 704/6000 [23:31<3:11:17,  2.17s/it]                                                    {'loss': 0.0069, 'grad_norm': 0.9779147505760193, 'learning_rate': 8.976271186440678e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 704/6000 [23:31<3:11:17,  2.17s/it] 12%|â–ˆâ–        | 705/6000 [23:33<3:05:50,  2.11s/it]                                                    {'loss': 0.228, 'grad_norm': 18.592199325561523, 'learning_rate': 8.974576271186441e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 705/6000 [23:33<3:05:50,  2.11s/it] 12%|â–ˆâ–        | 706/6000 [23:35<3:01:51,  2.06s/it]                                                    {'loss': 0.1072, 'grad_norm': 11.41012191772461, 'learning_rate': 8.972881355932204e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 706/6000 [23:35<3:01:51,  2.06s/it] 12%|â–ˆâ–        | 707/6000 [23:37<2:59:42,  2.04s/it]                                                    {'loss': 0.0152, 'grad_norm': 3.758880853652954, 'learning_rate': 8.971186440677967e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 707/6000 [23:37<2:59:42,  2.04s/it] 12%|â–ˆâ–        | 708/6000 [23:39<2:59:08,  2.03s/it]                                                    {'loss': 0.0215, 'grad_norm': 4.410792350769043, 'learning_rate': 8.969491525423729e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 708/6000 [23:39<2:59:08,  2.03s/it] 12%|â–ˆâ–        | 709/6000 [23:41<2:59:06,  2.03s/it]                                                    {'loss': 0.1728, 'grad_norm': 17.089941024780273, 'learning_rate': 8.967796610169492e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 709/6000 [23:41<2:59:06,  2.03s/it] 12%|â–ˆâ–        | 710/6000 [23:43<2:58:42,  2.03s/it]                                                    {'loss': 0.0695, 'grad_norm': 11.387240409851074, 'learning_rate': 8.966101694915254e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 710/6000 [23:43<2:58:42,  2.03s/it] 12%|â–ˆâ–        | 711/6000 [23:45<2:56:42,  2.00s/it]                                                    {'loss': 0.0014, 'grad_norm': 0.37764644622802734, 'learning_rate': 8.964406779661017e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 711/6000 [23:45<2:56:42,  2.00s/it] 12%|â–ˆâ–        | 712/6000 [23:47<2:55:36,  1.99s/it]                                                    {'loss': 0.0202, 'grad_norm': 4.168824195861816, 'learning_rate': 8.96271186440678e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 712/6000 [23:47<2:55:36,  1.99s/it] 12%|â–ˆâ–        | 713/6000 [23:49<2:54:58,  1.99s/it]                                                    {'loss': 0.1246, 'grad_norm': 18.818532943725586, 'learning_rate': 8.961016949152544e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 713/6000 [23:49<2:54:58,  1.99s/it] 12%|â–ˆâ–        | 714/6000 [23:51<2:53:47,  1.97s/it]                                                    {'loss': 0.072, 'grad_norm': 14.801886558532715, 'learning_rate': 8.959322033898305e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 714/6000 [23:51<2:53:47,  1.97s/it] 12%|â–ˆâ–        | 715/6000 [23:53<2:52:44,  1.96s/it]                                                    {'loss': 0.0215, 'grad_norm': 5.431392192840576, 'learning_rate': 8.957627118644069e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 715/6000 [23:53<2:52:44,  1.96s/it] 12%|â–ˆâ–        | 716/6000 [23:55<2:51:55,  1.95s/it]                                                    {'loss': 0.0906, 'grad_norm': 18.122926712036133, 'learning_rate': 8.955932203389832e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 716/6000 [23:55<2:51:55,  1.95s/it] 12%|â–ˆâ–        | 717/6000 [23:57<2:53:29,  1.97s/it]                                                    {'loss': 0.2505, 'grad_norm': 19.568992614746094, 'learning_rate': 8.954237288135595e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 717/6000 [23:57<2:53:29,  1.97s/it] 12%|â–ˆâ–        | 718/6000 [23:59<2:52:13,  1.96s/it]                                                    {'loss': 0.0368, 'grad_norm': 9.147098541259766, 'learning_rate': 8.952542372881357e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 718/6000 [23:59<2:52:13,  1.96s/it] 12%|â–ˆâ–        | 719/6000 [24:01<2:51:47,  1.95s/it]                                                    {'loss': 0.1044, 'grad_norm': 21.551799774169922, 'learning_rate': 8.95084745762712e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 719/6000 [24:01<2:51:47,  1.95s/it] 12%|â–ˆâ–        | 720/6000 [24:02<2:51:06,  1.94s/it]                                                    {'loss': 0.005, 'grad_norm': 0.9839183688163757, 'learning_rate': 8.949152542372881e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 720/6000 [24:02<2:51:06,  1.94s/it] 12%|â–ˆâ–        | 721/6000 [24:04<2:52:27,  1.96s/it]                                                    {'loss': 0.0494, 'grad_norm': 5.882765293121338, 'learning_rate': 8.947457627118645e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 721/6000 [24:04<2:52:27,  1.96s/it] 12%|â–ˆâ–        | 722/6000 [24:06<2:51:53,  1.95s/it]                                                    {'loss': 0.0898, 'grad_norm': 11.661730766296387, 'learning_rate': 8.945762711864408e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 722/6000 [24:06<2:51:53,  1.95s/it] 12%|â–ˆâ–        | 723/6000 [24:09<2:56:40,  2.01s/it]                                                    {'loss': 0.0433, 'grad_norm': 8.527813911437988, 'learning_rate': 8.944067796610171e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 723/6000 [24:09<2:56:40,  2.01s/it] 12%|â–ˆâ–        | 724/6000 [24:10<2:54:38,  1.99s/it]                                                    {'loss': 0.0775, 'grad_norm': 12.280715942382812, 'learning_rate': 8.942372881355933e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 724/6000 [24:10<2:54:38,  1.99s/it] 12%|â–ˆâ–        | 725/6000 [24:12<2:52:53,  1.97s/it]                                                    {'loss': 0.1072, 'grad_norm': 16.665571212768555, 'learning_rate': 8.940677966101694e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 725/6000 [24:12<2:52:53,  1.97s/it] 12%|â–ˆâ–        | 726/6000 [24:14<2:52:39,  1.96s/it]                                                    {'loss': 0.0164, 'grad_norm': 3.9324629306793213, 'learning_rate': 8.938983050847458e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 726/6000 [24:14<2:52:39,  1.96s/it] 12%|â–ˆâ–        | 727/6000 [24:16<2:53:20,  1.97s/it]                                                    {'loss': 0.1921, 'grad_norm': 15.765120506286621, 'learning_rate': 8.937288135593221e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 727/6000 [24:16<2:53:20,  1.97s/it] 12%|â–ˆâ–        | 728/6000 [24:18<2:52:01,  1.96s/it]                                                    {'loss': 0.0565, 'grad_norm': 13.252524375915527, 'learning_rate': 8.935593220338984e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 728/6000 [24:18<2:52:01,  1.96s/it] 12%|â–ˆâ–        | 729/6000 [24:20<2:53:12,  1.97s/it]                                                    {'loss': 0.0094, 'grad_norm': 3.049698829650879, 'learning_rate': 8.933898305084746e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 729/6000 [24:20<2:53:12,  1.97s/it] 12%|â–ˆâ–        | 730/6000 [24:22<2:52:20,  1.96s/it]                                                    {'loss': 0.1465, 'grad_norm': 9.505202293395996, 'learning_rate': 8.932203389830509e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 730/6000 [24:22<2:52:20,  1.96s/it] 12%|â–ˆâ–        | 731/6000 [24:24<2:51:13,  1.95s/it]                                                    {'loss': 0.0079, 'grad_norm': 1.9595311880111694, 'learning_rate': 8.93050847457627e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 731/6000 [24:24<2:51:13,  1.95s/it] 12%|â–ˆâ–        | 732/6000 [24:26<2:52:03,  1.96s/it]                                                    {'loss': 0.1353, 'grad_norm': 13.423903465270996, 'learning_rate': 8.928813559322036e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 732/6000 [24:26<2:52:03,  1.96s/it] 12%|â–ˆâ–        | 733/6000 [24:28<2:54:43,  1.99s/it]                                                    {'loss': 0.0097, 'grad_norm': 3.1770098209381104, 'learning_rate': 8.927118644067797e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 733/6000 [24:28<2:54:43,  1.99s/it] 12%|â–ˆâ–        | 734/6000 [24:30<3:00:06,  2.05s/it]                                                    {'loss': 0.0027, 'grad_norm': 1.2022560834884644, 'learning_rate': 8.92542372881356e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 734/6000 [24:30<3:00:06,  2.05s/it] 12%|â–ˆâ–        | 735/6000 [24:32<2:58:32,  2.03s/it]                                                    {'loss': 0.006, 'grad_norm': 1.6719703674316406, 'learning_rate': 8.923728813559322e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 735/6000 [24:32<2:58:32,  2.03s/it] 12%|â–ˆâ–        | 736/6000 [24:34<2:54:30,  1.99s/it]                                                    {'loss': 0.045, 'grad_norm': 6.097073078155518, 'learning_rate': 8.922033898305085e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 736/6000 [24:34<2:54:30,  1.99s/it] 12%|â–ˆâ–        | 737/6000 [24:36<2:53:20,  1.98s/it]                                                    {'loss': 0.0428, 'grad_norm': 6.485086441040039, 'learning_rate': 8.920338983050849e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 737/6000 [24:36<2:53:20,  1.98s/it] 12%|â–ˆâ–        | 738/6000 [24:38<2:51:16,  1.95s/it]                                                    {'loss': 0.1776, 'grad_norm': 23.829601287841797, 'learning_rate': 8.918644067796612e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 738/6000 [24:38<2:51:16,  1.95s/it] 12%|â–ˆâ–        | 739/6000 [24:40<2:51:25,  1.96s/it]                                                    {'loss': 0.0291, 'grad_norm': 5.634993076324463, 'learning_rate': 8.916949152542374e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 739/6000 [24:40<2:51:25,  1.96s/it] 12%|â–ˆâ–        | 740/6000 [24:42<2:51:00,  1.95s/it]                                                    {'loss': 0.3522, 'grad_norm': 23.18308448791504, 'learning_rate': 8.915254237288137e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 740/6000 [24:42<2:51:00,  1.95s/it] 12%|â–ˆâ–        | 741/6000 [24:44<2:53:28,  1.98s/it]                                                    {'loss': 0.0858, 'grad_norm': 12.180708885192871, 'learning_rate': 8.913559322033898e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 741/6000 [24:44<2:53:28,  1.98s/it] 12%|â–ˆâ–        | 742/6000 [24:46<2:51:29,  1.96s/it]                                                    {'loss': 0.0123, 'grad_norm': 3.589167833328247, 'learning_rate': 8.911864406779662e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 742/6000 [24:46<2:51:29,  1.96s/it] 12%|â–ˆâ–        | 743/6000 [24:48<2:49:33,  1.94s/it]                                                    {'loss': 0.0541, 'grad_norm': 11.679800033569336, 'learning_rate': 8.910169491525425e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 743/6000 [24:48<2:49:33,  1.94s/it] 12%|â–ˆâ–        | 744/6000 [24:50<2:48:11,  1.92s/it]                                                    {'loss': 0.0252, 'grad_norm': 5.139801025390625, 'learning_rate': 8.908474576271188e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 744/6000 [24:50<2:48:11,  1.92s/it] 12%|â–ˆâ–        | 745/6000 [24:52<2:48:45,  1.93s/it]                                                    {'loss': 0.2196, 'grad_norm': 15.675190925598145, 'learning_rate': 8.90677966101695e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 745/6000 [24:52<2:48:45,  1.93s/it] 12%|â–ˆâ–        | 746/6000 [24:54<2:50:03,  1.94s/it]                                                    {'loss': 0.3032, 'grad_norm': 18.437061309814453, 'learning_rate': 8.905084745762711e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 746/6000 [24:54<2:50:03,  1.94s/it] 12%|â–ˆâ–        | 747/6000 [24:56<2:50:40,  1.95s/it]                                                    {'loss': 0.071, 'grad_norm': 12.810070037841797, 'learning_rate': 8.903389830508475e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 747/6000 [24:56<2:50:40,  1.95s/it] 12%|â–ˆâ–        | 748/6000 [24:58<2:57:06,  2.02s/it]                                                    {'loss': 0.0911, 'grad_norm': 18.614458084106445, 'learning_rate': 8.901694915254238e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 748/6000 [24:58<2:57:06,  2.02s/it] 12%|â–ˆâ–        | 749/6000 [25:00<3:04:13,  2.10s/it]                                                    {'loss': 0.0688, 'grad_norm': 15.225545883178711, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.12}
 12%|â–ˆâ–        | 749/6000 [25:00<3:04:13,  2.10s/it] 12%|â–ˆâ–Ž        | 750/6000 [25:02<3:02:06,  2.08s/it]                                                    {'loss': 0.1705, 'grad_norm': 21.223405838012695, 'learning_rate': 8.898305084745763e-06, 'epoch': 0.12}
 12%|â–ˆâ–Ž        | 750/6000 [25:02<3:02:06,  2.08s/it][2025-11-18 10:14:59,952] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750
[2025-11-18 10:15:00,263] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-750/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 751/6000 [25:05<3:19:09,  2.28s/it]                                                    {'loss': 0.0275, 'grad_norm': 4.525378704071045, 'learning_rate': 8.896610169491526e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 751/6000 [25:05<3:19:09,  2.28s/it] 13%|â–ˆâ–Ž        | 752/6000 [25:07<3:14:47,  2.23s/it]                                                    {'loss': 0.0498, 'grad_norm': 9.019014358520508, 'learning_rate': 8.89491525423729e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 752/6000 [25:07<3:14:47,  2.23s/it] 13%|â–ˆâ–Ž        | 753/6000 [25:09<3:07:46,  2.15s/it]                                                    {'loss': 0.0017, 'grad_norm': 0.6973345875740051, 'learning_rate': 8.893220338983053e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 753/6000 [25:09<3:07:46,  2.15s/it] 13%|â–ˆâ–Ž        | 754/6000 [25:11<3:03:10,  2.10s/it]                                                    {'loss': 0.0491, 'grad_norm': 5.824972629547119, 'learning_rate': 8.891525423728814e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 754/6000 [25:11<3:03:10,  2.10s/it] 13%|â–ˆâ–Ž        | 755/6000 [25:13<2:59:12,  2.05s/it]                                                    {'loss': 0.0094, 'grad_norm': 4.10178804397583, 'learning_rate': 8.889830508474577e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 755/6000 [25:13<2:59:12,  2.05s/it] 13%|â–ˆâ–Ž        | 756/6000 [25:15<2:55:54,  2.01s/it]                                                    {'loss': 0.0011, 'grad_norm': 0.44854164123535156, 'learning_rate': 8.888135593220339e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 756/6000 [25:15<2:55:54,  2.01s/it] 13%|â–ˆâ–Ž        | 757/6000 [25:17<2:56:10,  2.02s/it]                                                    {'loss': 0.0141, 'grad_norm': 4.390691757202148, 'learning_rate': 8.886440677966102e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 757/6000 [25:17<2:56:10,  2.02s/it] 13%|â–ˆâ–Ž        | 758/6000 [25:19<2:55:48,  2.01s/it]                                                    {'loss': 0.0999, 'grad_norm': 17.213979721069336, 'learning_rate': 8.884745762711866e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 758/6000 [25:19<2:55:48,  2.01s/it] 13%|â–ˆâ–Ž        | 759/6000 [25:21<2:53:01,  1.98s/it]                                                    {'loss': 0.322, 'grad_norm': 18.452436447143555, 'learning_rate': 8.883050847457629e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 759/6000 [25:21<2:53:01,  1.98s/it] 13%|â–ˆâ–Ž        | 760/6000 [25:23<2:51:36,  1.96s/it]                                                    {'loss': 0.0087, 'grad_norm': 2.8509860038757324, 'learning_rate': 8.88135593220339e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 760/6000 [25:23<2:51:36,  1.96s/it] 13%|â–ˆâ–Ž        | 761/6000 [25:25<2:52:39,  1.98s/it]                                                    {'loss': 0.0638, 'grad_norm': 6.776143550872803, 'learning_rate': 8.879661016949154e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 761/6000 [25:25<2:52:39,  1.98s/it] 13%|â–ˆâ–Ž        | 762/6000 [25:27<2:54:15,  2.00s/it]                                                    {'loss': 0.092, 'grad_norm': 16.235979080200195, 'learning_rate': 8.877966101694915e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 762/6000 [25:27<2:54:15,  2.00s/it] 13%|â–ˆâ–Ž        | 763/6000 [25:29<2:53:01,  1.98s/it]                                                    {'loss': 0.1925, 'grad_norm': 13.418537139892578, 'learning_rate': 8.876271186440679e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 763/6000 [25:29<2:53:01,  1.98s/it] 13%|â–ˆâ–Ž        | 764/6000 [25:31<2:52:15,  1.97s/it]                                                    {'loss': 0.061, 'grad_norm': 11.279677391052246, 'learning_rate': 8.874576271186442e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 764/6000 [25:31<2:52:15,  1.97s/it] 13%|â–ˆâ–Ž        | 765/6000 [25:32<2:50:56,  1.96s/it]                                                    {'loss': 0.0306, 'grad_norm': 6.195157527923584, 'learning_rate': 8.872881355932203e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 765/6000 [25:32<2:50:56,  1.96s/it] 13%|â–ˆâ–Ž        | 766/6000 [25:34<2:52:28,  1.98s/it]                                                    {'loss': 0.0014, 'grad_norm': 0.45650264620780945, 'learning_rate': 8.871186440677967e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 766/6000 [25:34<2:52:28,  1.98s/it] 13%|â–ˆâ–Ž        | 767/6000 [25:36<2:53:13,  1.99s/it]                                                    {'loss': 0.0097, 'grad_norm': 3.3237931728363037, 'learning_rate': 8.869491525423728e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 767/6000 [25:37<2:53:13,  1.99s/it] 13%|â–ˆâ–Ž        | 768/6000 [25:38<2:52:20,  1.98s/it]                                                    {'loss': 0.0136, 'grad_norm': 6.0499162673950195, 'learning_rate': 8.867796610169492e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 768/6000 [25:38<2:52:20,  1.98s/it] 13%|â–ˆâ–Ž        | 769/6000 [25:41<2:55:05,  2.01s/it]                                                    {'loss': 0.0398, 'grad_norm': 2.7992844581604004, 'learning_rate': 8.866101694915255e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 769/6000 [25:41<2:55:05,  2.01s/it] 13%|â–ˆâ–Ž        | 770/6000 [25:43<2:55:07,  2.01s/it]                                                    {'loss': 0.1963, 'grad_norm': 18.534120559692383, 'learning_rate': 8.864406779661018e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 770/6000 [25:43<2:55:07,  2.01s/it] 13%|â–ˆâ–Ž        | 771/6000 [25:45<2:55:29,  2.01s/it]                                                    {'loss': 0.0025, 'grad_norm': 0.5362980961799622, 'learning_rate': 8.86271186440678e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 771/6000 [25:45<2:55:29,  2.01s/it] 13%|â–ˆâ–Ž        | 772/6000 [25:46<2:52:21,  1.98s/it]                                                    {'loss': 0.0483, 'grad_norm': 14.031999588012695, 'learning_rate': 8.861016949152543e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 772/6000 [25:46<2:52:21,  1.98s/it] 13%|â–ˆâ–Ž        | 773/6000 [25:48<2:51:40,  1.97s/it]                                                    {'loss': 0.1551, 'grad_norm': 18.463293075561523, 'learning_rate': 8.859322033898306e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 773/6000 [25:48<2:51:40,  1.97s/it] 13%|â–ˆâ–Ž        | 774/6000 [25:50<2:52:40,  1.98s/it]                                                    {'loss': 0.009, 'grad_norm': 1.4968138933181763, 'learning_rate': 8.85762711864407e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 774/6000 [25:50<2:52:40,  1.98s/it] 13%|â–ˆâ–Ž        | 775/6000 [25:52<2:53:12,  1.99s/it]                                                    {'loss': 0.0093, 'grad_norm': 1.8861401081085205, 'learning_rate': 8.855932203389831e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 775/6000 [25:52<2:53:12,  1.99s/it] 13%|â–ˆâ–Ž        | 776/6000 [25:54<2:52:57,  1.99s/it]                                                    {'loss': 0.1202, 'grad_norm': 19.29998779296875, 'learning_rate': 8.854237288135594e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 776/6000 [25:54<2:52:57,  1.99s/it] 13%|â–ˆâ–Ž        | 777/6000 [25:56<2:54:32,  2.01s/it]                                                    {'loss': 0.0626, 'grad_norm': 13.925922393798828, 'learning_rate': 8.852542372881356e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 777/6000 [25:56<2:54:32,  2.01s/it] 13%|â–ˆâ–Ž        | 778/6000 [25:58<2:54:22,  2.00s/it]                                                    {'loss': 0.1996, 'grad_norm': 22.051071166992188, 'learning_rate': 8.85084745762712e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 778/6000 [25:58<2:54:22,  2.00s/it] 13%|â–ˆâ–Ž        | 779/6000 [26:01<2:57:39,  2.04s/it]                                                    {'loss': 0.0655, 'grad_norm': 11.737137794494629, 'learning_rate': 8.849152542372882e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 779/6000 [26:01<2:57:39,  2.04s/it] 13%|â–ˆâ–Ž        | 780/6000 [26:03<2:56:40,  2.03s/it]                                                    {'loss': 0.004, 'grad_norm': 0.792582094669342, 'learning_rate': 8.847457627118646e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 780/6000 [26:03<2:56:40,  2.03s/it] 13%|â–ˆâ–Ž        | 781/6000 [26:05<2:55:33,  2.02s/it]                                                    {'loss': 0.0138, 'grad_norm': 6.09708833694458, 'learning_rate': 8.845762711864407e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 781/6000 [26:05<2:55:33,  2.02s/it] 13%|â–ˆâ–Ž        | 782/6000 [26:07<2:53:28,  1.99s/it]                                                    {'loss': 0.0257, 'grad_norm': 8.417855262756348, 'learning_rate': 8.84406779661017e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 782/6000 [26:07<2:53:28,  1.99s/it] 13%|â–ˆâ–Ž        | 783/6000 [26:08<2:52:35,  1.98s/it]                                                    {'loss': 0.0577, 'grad_norm': 11.08335018157959, 'learning_rate': 8.842372881355932e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 783/6000 [26:08<2:52:35,  1.98s/it] 13%|â–ˆâ–Ž        | 784/6000 [26:10<2:50:32,  1.96s/it]                                                    {'loss': 0.0487, 'grad_norm': 6.261589050292969, 'learning_rate': 8.840677966101695e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 784/6000 [26:10<2:50:32,  1.96s/it] 13%|â–ˆâ–Ž        | 785/6000 [26:12<2:48:59,  1.94s/it]                                                    {'loss': 0.3405, 'grad_norm': 27.22369384765625, 'learning_rate': 8.838983050847459e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 785/6000 [26:12<2:48:59,  1.94s/it] 13%|â–ˆâ–Ž        | 786/6000 [26:14<2:47:59,  1.93s/it]                                                    {'loss': 0.0963, 'grad_norm': 18.257877349853516, 'learning_rate': 8.83728813559322e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 786/6000 [26:14<2:47:59,  1.93s/it] 13%|â–ˆâ–Ž        | 787/6000 [26:16<2:50:05,  1.96s/it]                                                    {'loss': 0.0319, 'grad_norm': 7.231712818145752, 'learning_rate': 8.835593220338984e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 787/6000 [26:16<2:50:05,  1.96s/it] 13%|â–ˆâ–Ž        | 788/6000 [26:18<2:48:55,  1.94s/it]                                                    {'loss': 0.0395, 'grad_norm': 6.147414207458496, 'learning_rate': 8.833898305084747e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 788/6000 [26:18<2:48:55,  1.94s/it] 13%|â–ˆâ–Ž        | 789/6000 [26:20<2:49:38,  1.95s/it]                                                    {'loss': 0.0451, 'grad_norm': 8.855287551879883, 'learning_rate': 8.83220338983051e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 789/6000 [26:20<2:49:38,  1.95s/it] 13%|â–ˆâ–Ž        | 790/6000 [26:22<2:49:54,  1.96s/it]                                                    {'loss': 0.1653, 'grad_norm': 18.06201934814453, 'learning_rate': 8.830508474576272e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 790/6000 [26:22<2:49:54,  1.96s/it] 13%|â–ˆâ–Ž        | 791/6000 [26:24<2:50:02,  1.96s/it]                                                    {'loss': 0.0273, 'grad_norm': 2.861952781677246, 'learning_rate': 8.828813559322035e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 791/6000 [26:24<2:50:02,  1.96s/it] 13%|â–ˆâ–Ž        | 792/6000 [26:26<2:51:52,  1.98s/it]                                                    {'loss': 0.1125, 'grad_norm': 18.215810775756836, 'learning_rate': 8.827118644067797e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 792/6000 [26:26<2:51:52,  1.98s/it] 13%|â–ˆâ–Ž        | 793/6000 [26:28<2:51:51,  1.98s/it]                                                    {'loss': 0.0046, 'grad_norm': 0.9242222309112549, 'learning_rate': 8.82542372881356e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 793/6000 [26:28<2:51:51,  1.98s/it] 13%|â–ˆâ–Ž        | 794/6000 [26:30<2:51:07,  1.97s/it]                                                    {'loss': 0.021, 'grad_norm': 5.463891983032227, 'learning_rate': 8.823728813559323e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 794/6000 [26:30<2:51:07,  1.97s/it] 13%|â–ˆâ–Ž        | 795/6000 [26:32<2:53:07,  2.00s/it]                                                    {'loss': 0.0076, 'grad_norm': 1.814847469329834, 'learning_rate': 8.822033898305086e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 795/6000 [26:32<2:53:07,  2.00s/it] 13%|â–ˆâ–Ž        | 796/6000 [26:34<2:50:57,  1.97s/it]                                                    {'loss': 0.1518, 'grad_norm': 16.75054359436035, 'learning_rate': 8.820338983050848e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 796/6000 [26:34<2:50:57,  1.97s/it] 13%|â–ˆâ–Ž        | 797/6000 [26:36<2:52:00,  1.98s/it]                                                    {'loss': 0.0309, 'grad_norm': 4.258938312530518, 'learning_rate': 8.818644067796611e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 797/6000 [26:36<2:52:00,  1.98s/it] 13%|â–ˆâ–Ž        | 798/6000 [26:38<2:51:37,  1.98s/it]                                                    {'loss': 0.0096, 'grad_norm': 4.208253383636475, 'learning_rate': 8.816949152542373e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 798/6000 [26:38<2:51:37,  1.98s/it] 13%|â–ˆâ–Ž        | 799/6000 [26:40<2:51:09,  1.97s/it]                                                    {'loss': 0.1775, 'grad_norm': 21.64958381652832, 'learning_rate': 8.815254237288136e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 799/6000 [26:40<2:51:09,  1.97s/it] 13%|â–ˆâ–Ž        | 800/6000 [26:42<2:52:10,  1.99s/it]                                                    {'loss': 0.0241, 'grad_norm': 6.274759292602539, 'learning_rate': 8.8135593220339e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 800/6000 [26:42<2:52:10,  1.99s/it][2025-11-18 10:16:39,794] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800
[2025-11-18 10:16:40,083] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-800/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 13%|â–ˆâ–Ž        | 801/6000 [26:45<3:11:54,  2.21s/it]                                                    {'loss': 0.004, 'grad_norm': 1.243270754814148, 'learning_rate': 8.811864406779663e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 801/6000 [26:45<3:11:54,  2.21s/it] 13%|â–ˆâ–Ž        | 802/6000 [26:47<3:05:44,  2.14s/it]                                                    {'loss': 0.0441, 'grad_norm': 12.55759048461914, 'learning_rate': 8.810169491525424e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 802/6000 [26:47<3:05:44,  2.14s/it] 13%|â–ˆâ–Ž        | 803/6000 [26:49<2:59:57,  2.08s/it]                                                    {'loss': 0.1503, 'grad_norm': 10.880847930908203, 'learning_rate': 8.808474576271187e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 803/6000 [26:49<2:59:57,  2.08s/it] 13%|â–ˆâ–Ž        | 804/6000 [26:51<2:56:20,  2.04s/it]                                                    {'loss': 0.1772, 'grad_norm': 22.143020629882812, 'learning_rate': 8.806779661016949e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 804/6000 [26:51<2:56:20,  2.04s/it] 13%|â–ˆâ–Ž        | 805/6000 [26:52<2:53:57,  2.01s/it]                                                    {'loss': 0.01, 'grad_norm': 2.49257230758667, 'learning_rate': 8.805084745762712e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 805/6000 [26:52<2:53:57,  2.01s/it] 13%|â–ˆâ–Ž        | 806/6000 [26:54<2:54:08,  2.01s/it]                                                    {'loss': 0.0013, 'grad_norm': 0.42719024419784546, 'learning_rate': 8.803389830508476e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 806/6000 [26:54<2:54:08,  2.01s/it] 13%|â–ˆâ–Ž        | 807/6000 [26:56<2:51:30,  1.98s/it]                                                    {'loss': 0.0216, 'grad_norm': 7.1343841552734375, 'learning_rate': 8.801694915254237e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 807/6000 [26:56<2:51:30,  1.98s/it] 13%|â–ˆâ–Ž        | 808/6000 [26:58<2:51:24,  1.98s/it]                                                    {'loss': 0.0715, 'grad_norm': 17.640466690063477, 'learning_rate': 8.8e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 808/6000 [26:58<2:51:24,  1.98s/it] 13%|â–ˆâ–Ž        | 809/6000 [27:00<2:50:23,  1.97s/it]                                                    {'loss': 0.0079, 'grad_norm': 2.222325086593628, 'learning_rate': 8.798305084745764e-06, 'epoch': 0.13}
 13%|â–ˆâ–Ž        | 809/6000 [27:00<2:50:23,  1.97s/it] 14%|â–ˆâ–Ž        | 810/6000 [27:02<2:50:40,  1.97s/it]                                                    {'loss': 0.3911, 'grad_norm': 22.370115280151367, 'learning_rate': 8.796610169491527e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 810/6000 [27:02<2:50:40,  1.97s/it] 14%|â–ˆâ–Ž        | 811/6000 [27:04<2:51:11,  1.98s/it]                                                    {'loss': 0.0198, 'grad_norm': 4.231164455413818, 'learning_rate': 8.794915254237289e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 811/6000 [27:04<2:51:11,  1.98s/it] 14%|â–ˆâ–Ž        | 812/6000 [27:06<2:50:06,  1.97s/it]                                                    {'loss': 0.0319, 'grad_norm': 7.633143424987793, 'learning_rate': 8.793220338983052e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 812/6000 [27:06<2:50:06,  1.97s/it] 14%|â–ˆâ–Ž        | 813/6000 [27:08<2:57:30,  2.05s/it]                                                    {'loss': 0.0836, 'grad_norm': 10.41333293914795, 'learning_rate': 8.791525423728813e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 813/6000 [27:08<2:57:30,  2.05s/it] 14%|â–ˆâ–Ž        | 814/6000 [27:10<2:54:15,  2.02s/it]                                                    {'loss': 0.105, 'grad_norm': 13.379752159118652, 'learning_rate': 8.789830508474577e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 814/6000 [27:10<2:54:15,  2.02s/it] 14%|â–ˆâ–Ž        | 815/6000 [27:12<2:52:05,  1.99s/it]                                                    {'loss': 0.0071, 'grad_norm': 1.719504475593567, 'learning_rate': 8.78813559322034e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 815/6000 [27:12<2:52:05,  1.99s/it] 14%|â–ˆâ–Ž        | 816/6000 [27:14<2:50:17,  1.97s/it]                                                    {'loss': 0.144, 'grad_norm': 19.061235427856445, 'learning_rate': 8.786440677966103e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 816/6000 [27:14<2:50:17,  1.97s/it] 14%|â–ˆâ–Ž        | 817/6000 [27:16<2:56:01,  2.04s/it]                                                    {'loss': 0.124, 'grad_norm': 14.360481262207031, 'learning_rate': 8.784745762711865e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 817/6000 [27:16<2:56:01,  2.04s/it] 14%|â–ˆâ–Ž        | 818/6000 [27:18<2:53:38,  2.01s/it]                                                    {'loss': 0.0492, 'grad_norm': 10.085716247558594, 'learning_rate': 8.783050847457628e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 818/6000 [27:18<2:53:38,  2.01s/it] 14%|â–ˆâ–Ž        | 819/6000 [27:20<2:54:28,  2.02s/it]                                                    {'loss': 0.0808, 'grad_norm': 8.82140827178955, 'learning_rate': 8.78135593220339e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 819/6000 [27:20<2:54:28,  2.02s/it] 14%|â–ˆâ–Ž        | 820/6000 [27:22<2:52:22,  2.00s/it]                                                    {'loss': 0.0371, 'grad_norm': 7.634756565093994, 'learning_rate': 8.779661016949153e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 820/6000 [27:22<2:52:22,  2.00s/it] 14%|â–ˆâ–Ž        | 821/6000 [27:24<2:51:17,  1.98s/it]                                                    {'loss': 0.0098, 'grad_norm': 2.592500686645508, 'learning_rate': 8.777966101694916e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 821/6000 [27:24<2:51:17,  1.98s/it] 14%|â–ˆâ–Ž        | 822/6000 [27:26<2:49:37,  1.97s/it]                                                    {'loss': 0.0045, 'grad_norm': 1.1103297472000122, 'learning_rate': 8.77627118644068e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 822/6000 [27:26<2:49:37,  1.97s/it] 14%|â–ˆâ–Ž        | 823/6000 [27:28<2:49:31,  1.96s/it]                                                    {'loss': 0.1273, 'grad_norm': 16.26176643371582, 'learning_rate': 8.774576271186441e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 823/6000 [27:28<2:49:31,  1.96s/it] 14%|â–ˆâ–Ž        | 824/6000 [27:30<2:48:00,  1.95s/it]                                                    {'loss': 0.0012, 'grad_norm': 0.33706650137901306, 'learning_rate': 8.772881355932204e-06, 'epoch': 0.14}
 14%|â–ˆâ–Ž        | 824/6000 [27:30<2:48:00,  1.95s/it] 14%|â–ˆâ–        | 825/6000 [27:32<2:49:26,  1.96s/it]                                                    {'loss': 0.0807, 'grad_norm': 12.967763900756836, 'learning_rate': 8.771186440677966e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 825/6000 [27:32<2:49:26,  1.96s/it] 14%|â–ˆâ–        | 826/6000 [27:34<2:48:33,  1.95s/it]                                                    {'loss': 0.0029, 'grad_norm': 0.6111103892326355, 'learning_rate': 8.76949152542373e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 826/6000 [27:34<2:48:33,  1.95s/it] 14%|â–ˆâ–        | 827/6000 [27:36<2:47:42,  1.95s/it]                                                    {'loss': 0.0337, 'grad_norm': 7.84744119644165, 'learning_rate': 8.767796610169492e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 827/6000 [27:36<2:47:42,  1.95s/it] 14%|â–ˆâ–        | 828/6000 [27:38<2:49:13,  1.96s/it]                                                    {'loss': 0.2603, 'grad_norm': 20.123844146728516, 'learning_rate': 8.766101694915254e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 828/6000 [27:38<2:49:13,  1.96s/it] 14%|â–ˆâ–        | 829/6000 [27:40<2:49:14,  1.96s/it]                                                    {'loss': 0.1025, 'grad_norm': 5.693819999694824, 'learning_rate': 8.764406779661017e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 829/6000 [27:40<2:49:14,  1.96s/it] 14%|â–ˆâ–        | 830/6000 [27:43<3:04:08,  2.14s/it]                                                    {'loss': 0.0011, 'grad_norm': 0.38578611612319946, 'learning_rate': 8.76271186440678e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 830/6000 [27:43<3:04:08,  2.14s/it] 14%|â–ˆâ–        | 831/6000 [27:45<3:00:47,  2.10s/it]                                                    {'loss': 0.1407, 'grad_norm': 14.101579666137695, 'learning_rate': 8.761016949152544e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 831/6000 [27:45<3:00:47,  2.10s/it] 14%|â–ˆâ–        | 832/6000 [27:46<2:57:25,  2.06s/it]                                                    {'loss': 0.0489, 'grad_norm': 7.2988057136535645, 'learning_rate': 8.759322033898305e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 832/6000 [27:46<2:57:25,  2.06s/it] 14%|â–ˆâ–        | 833/6000 [27:48<2:53:22,  2.01s/it]                                                    {'loss': 0.0072, 'grad_norm': 2.3982951641082764, 'learning_rate': 8.757627118644069e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 833/6000 [27:48<2:53:22,  2.01s/it] 14%|â–ˆâ–        | 834/6000 [27:50<2:51:10,  1.99s/it]                                                    {'loss': 0.0486, 'grad_norm': 7.300816059112549, 'learning_rate': 8.75593220338983e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 834/6000 [27:50<2:51:10,  1.99s/it] 14%|â–ˆâ–        | 835/6000 [27:52<2:54:12,  2.02s/it]                                                    {'loss': 0.0431, 'grad_norm': 7.030264377593994, 'learning_rate': 8.754237288135594e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 835/6000 [27:52<2:54:12,  2.02s/it] 14%|â–ˆâ–        | 836/6000 [27:54<2:51:02,  1.99s/it]                                                    {'loss': 0.1274, 'grad_norm': 17.76447105407715, 'learning_rate': 8.752542372881357e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 836/6000 [27:54<2:51:02,  1.99s/it] 14%|â–ˆâ–        | 837/6000 [27:57<2:59:05,  2.08s/it]                                                    {'loss': 0.024, 'grad_norm': 4.967413425445557, 'learning_rate': 8.75084745762712e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 837/6000 [27:57<2:59:05,  2.08s/it] 14%|â–ˆâ–        | 838/6000 [27:59<2:58:46,  2.08s/it]                                                    {'loss': 0.1458, 'grad_norm': 23.42178726196289, 'learning_rate': 8.749152542372882e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 838/6000 [27:59<2:58:46,  2.08s/it] 14%|â–ˆâ–        | 839/6000 [28:01<2:56:23,  2.05s/it]                                                    {'loss': 0.0068, 'grad_norm': 1.5398590564727783, 'learning_rate': 8.747457627118645e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 839/6000 [28:01<2:56:23,  2.05s/it] 14%|â–ˆâ–        | 840/6000 [28:03<2:54:14,  2.03s/it]                                                    {'loss': 0.001, 'grad_norm': 0.18939393758773804, 'learning_rate': 8.745762711864407e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 840/6000 [28:03<2:54:14,  2.03s/it] 14%|â–ˆâ–        | 841/6000 [28:05<2:55:59,  2.05s/it]                                                    {'loss': 0.0478, 'grad_norm': 5.973255157470703, 'learning_rate': 8.74406779661017e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 841/6000 [28:05<2:55:59,  2.05s/it] 14%|â–ˆâ–        | 842/6000 [28:07<2:54:03,  2.02s/it]                                                    {'loss': 0.1064, 'grad_norm': 13.36581039428711, 'learning_rate': 8.742372881355933e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 842/6000 [28:07<2:54:03,  2.02s/it] 14%|â–ˆâ–        | 843/6000 [28:09<2:50:59,  1.99s/it]                                                    {'loss': 0.01, 'grad_norm': 2.437018871307373, 'learning_rate': 8.740677966101696e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 843/6000 [28:09<2:50:59,  1.99s/it] 14%|â–ˆâ–        | 844/6000 [28:11<2:51:26,  2.00s/it]                                                    {'loss': 0.016, 'grad_norm': 4.633407115936279, 'learning_rate': 8.738983050847458e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 844/6000 [28:11<2:51:26,  2.00s/it] 14%|â–ˆâ–        | 845/6000 [28:13<2:50:27,  1.98s/it]                                                    {'loss': 0.0312, 'grad_norm': 5.352300643920898, 'learning_rate': 8.737288135593221e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 845/6000 [28:13<2:50:27,  1.98s/it] 14%|â–ˆâ–        | 846/6000 [28:15<2:50:07,  1.98s/it]                                                    {'loss': 0.0095, 'grad_norm': 3.2753677368164062, 'learning_rate': 8.735593220338985e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 846/6000 [28:15<2:50:07,  1.98s/it] 14%|â–ˆâ–        | 847/6000 [28:17<2:49:09,  1.97s/it]                                                    {'loss': 0.0485, 'grad_norm': 3.8994712829589844, 'learning_rate': 8.733898305084748e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 847/6000 [28:17<2:49:09,  1.97s/it] 14%|â–ˆâ–        | 848/6000 [28:19<2:50:49,  1.99s/it]                                                    {'loss': 0.0903, 'grad_norm': 11.471386909484863, 'learning_rate': 8.73220338983051e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 848/6000 [28:19<2:50:49,  1.99s/it] 14%|â–ˆâ–        | 849/6000 [28:21<2:50:48,  1.99s/it]                                                    {'loss': 0.0102, 'grad_norm': 2.9209136962890625, 'learning_rate': 8.730508474576271e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 849/6000 [28:21<2:50:48,  1.99s/it] 14%|â–ˆâ–        | 850/6000 [28:23<2:51:00,  1.99s/it]                                                    {'loss': 0.046, 'grad_norm': 9.053889274597168, 'learning_rate': 8.728813559322034e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 850/6000 [28:23<2:51:00,  1.99s/it][2025-11-18 10:18:20,407] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850
[2025-11-18 10:18:20,691] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-850/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 14%|â–ˆâ–        | 851/6000 [28:25<3:08:36,  2.20s/it]                                                    {'loss': 0.0876, 'grad_norm': 13.182080268859863, 'learning_rate': 8.727118644067797e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 851/6000 [28:25<3:08:36,  2.20s/it] 14%|â–ˆâ–        | 852/6000 [28:27<3:02:04,  2.12s/it]                                                    {'loss': 0.1248, 'grad_norm': 8.278839111328125, 'learning_rate': 8.72542372881356e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 852/6000 [28:27<3:02:04,  2.12s/it] 14%|â–ˆâ–        | 853/6000 [28:29<2:58:07,  2.08s/it]                                                    {'loss': 0.0329, 'grad_norm': 5.335455894470215, 'learning_rate': 8.723728813559322e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 853/6000 [28:29<2:58:07,  2.08s/it] 14%|â–ˆâ–        | 854/6000 [28:31<2:54:58,  2.04s/it]                                                    {'loss': 0.028, 'grad_norm': 7.332456111907959, 'learning_rate': 8.722033898305086e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 854/6000 [28:31<2:54:58,  2.04s/it] 14%|â–ˆâ–        | 855/6000 [28:33<2:52:55,  2.02s/it]                                                    {'loss': 0.0088, 'grad_norm': 3.531287431716919, 'learning_rate': 8.720338983050847e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 855/6000 [28:33<2:52:55,  2.02s/it] 14%|â–ˆâ–        | 856/6000 [28:35<2:56:49,  2.06s/it]                                                    {'loss': 0.2907, 'grad_norm': 23.244140625, 'learning_rate': 8.71864406779661e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 856/6000 [28:35<2:56:49,  2.06s/it] 14%|â–ˆâ–        | 857/6000 [28:37<2:52:48,  2.02s/it]                                                    {'loss': 0.0062, 'grad_norm': 1.5123264789581299, 'learning_rate': 8.716949152542374e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 857/6000 [28:37<2:52:48,  2.02s/it] 14%|â–ˆâ–        | 858/6000 [28:39<2:51:24,  2.00s/it]                                                    {'loss': 0.0247, 'grad_norm': 3.944875955581665, 'learning_rate': 8.715254237288137e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 858/6000 [28:39<2:51:24,  2.00s/it] 14%|â–ˆâ–        | 859/6000 [28:41<2:50:18,  1.99s/it]                                                    {'loss': 0.0368, 'grad_norm': 8.168440818786621, 'learning_rate': 8.713559322033899e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 859/6000 [28:41<2:50:18,  1.99s/it] 14%|â–ˆâ–        | 860/6000 [28:43<2:51:57,  2.01s/it]                                                    {'loss': 0.1206, 'grad_norm': 19.401376724243164, 'learning_rate': 8.711864406779662e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 860/6000 [28:43<2:51:57,  2.01s/it] 14%|â–ˆâ–        | 861/6000 [28:45<2:49:31,  1.98s/it]                                                    {'loss': 0.0067, 'grad_norm': 2.1763863563537598, 'learning_rate': 8.710169491525423e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 861/6000 [28:45<2:49:31,  1.98s/it] 14%|â–ˆâ–        | 862/6000 [28:47<2:50:05,  1.99s/it]                                                    {'loss': 0.1545, 'grad_norm': 18.566587448120117, 'learning_rate': 8.708474576271187e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 862/6000 [28:47<2:50:05,  1.99s/it] 14%|â–ˆâ–        | 863/6000 [28:49<2:49:15,  1.98s/it]                                                    {'loss': 0.0345, 'grad_norm': 8.818846702575684, 'learning_rate': 8.70677966101695e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 863/6000 [28:49<2:49:15,  1.98s/it] 14%|â–ˆâ–        | 864/6000 [28:51<2:49:14,  1.98s/it]                                                    {'loss': 0.0004, 'grad_norm': 0.0915374606847763, 'learning_rate': 8.705084745762713e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 864/6000 [28:51<2:49:14,  1.98s/it] 14%|â–ˆâ–        | 865/6000 [28:53<2:50:58,  2.00s/it]                                                    {'loss': 0.2344, 'grad_norm': 20.54838752746582, 'learning_rate': 8.703389830508475e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 865/6000 [28:53<2:50:58,  2.00s/it] 14%|â–ˆâ–        | 866/6000 [28:55<2:48:42,  1.97s/it]                                                    {'loss': 0.0981, 'grad_norm': 13.434279441833496, 'learning_rate': 8.701694915254238e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 866/6000 [28:55<2:48:42,  1.97s/it] 14%|â–ˆâ–        | 867/6000 [28:57<2:49:21,  1.98s/it]                                                    {'loss': 0.1388, 'grad_norm': 17.666948318481445, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 867/6000 [28:57<2:49:21,  1.98s/it] 14%|â–ˆâ–        | 868/6000 [28:59<2:46:21,  1.95s/it]                                                    {'loss': 0.2305, 'grad_norm': 17.707273483276367, 'learning_rate': 8.698305084745765e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 868/6000 [28:59<2:46:21,  1.95s/it] 14%|â–ˆâ–        | 869/6000 [29:01<2:46:38,  1.95s/it]                                                    {'loss': 0.0032, 'grad_norm': 0.53534996509552, 'learning_rate': 8.696610169491526e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 869/6000 [29:01<2:46:38,  1.95s/it] 14%|â–ˆâ–        | 870/6000 [29:03<2:46:28,  1.95s/it]                                                    {'loss': 0.0987, 'grad_norm': 14.085777282714844, 'learning_rate': 8.694915254237288e-06, 'epoch': 0.14}
 14%|â–ˆâ–        | 870/6000 [29:03<2:46:28,  1.95s/it] 15%|â–ˆâ–        | 871/6000 [29:05<2:46:40,  1.95s/it]                                                    {'loss': 0.0158, 'grad_norm': 4.222603797912598, 'learning_rate': 8.693220338983051e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 871/6000 [29:05<2:46:40,  1.95s/it] 15%|â–ˆâ–        | 872/6000 [29:07<2:47:40,  1.96s/it]                                                    {'loss': 0.1755, 'grad_norm': 15.69112777709961, 'learning_rate': 8.691525423728814e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 872/6000 [29:07<2:47:40,  1.96s/it] 15%|â–ˆâ–        | 873/6000 [29:09<2:47:59,  1.97s/it]                                                    {'loss': 0.0438, 'grad_norm': 7.0591230392456055, 'learning_rate': 8.689830508474578e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 873/6000 [29:09<2:47:59,  1.97s/it] 15%|â–ˆâ–        | 874/6000 [29:11<2:47:53,  1.97s/it]                                                    {'loss': 0.0946, 'grad_norm': 12.70257568359375, 'learning_rate': 8.68813559322034e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 874/6000 [29:11<2:47:53,  1.97s/it] 15%|â–ˆâ–        | 875/6000 [29:13<2:50:01,  1.99s/it]                                                    {'loss': 0.054, 'grad_norm': 7.430942535400391, 'learning_rate': 8.686440677966103e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 875/6000 [29:13<2:50:01,  1.99s/it] 15%|â–ˆâ–        | 876/6000 [29:15<2:50:44,  2.00s/it]                                                    {'loss': 0.0613, 'grad_norm': 12.598427772521973, 'learning_rate': 8.684745762711864e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 876/6000 [29:15<2:50:44,  2.00s/it] 15%|â–ˆâ–        | 877/6000 [29:17<2:50:10,  1.99s/it]                                                    {'loss': 0.186, 'grad_norm': 22.27202033996582, 'learning_rate': 8.683050847457627e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 877/6000 [29:17<2:50:10,  1.99s/it] 15%|â–ˆâ–        | 878/6000 [29:19<2:49:18,  1.98s/it]                                                    {'loss': 0.1042, 'grad_norm': 12.824836730957031, 'learning_rate': 8.68135593220339e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 878/6000 [29:19<2:49:18,  1.98s/it] 15%|â–ˆâ–        | 879/6000 [29:21<2:53:15,  2.03s/it]                                                    {'loss': 0.1432, 'grad_norm': 9.754899978637695, 'learning_rate': 8.679661016949154e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 879/6000 [29:21<2:53:15,  2.03s/it] 15%|â–ˆâ–        | 880/6000 [29:23<2:51:56,  2.01s/it]                                                    {'loss': 0.4906, 'grad_norm': 27.44489288330078, 'learning_rate': 8.677966101694915e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 880/6000 [29:23<2:51:56,  2.01s/it] 15%|â–ˆâ–        | 881/6000 [29:25<2:51:09,  2.01s/it]                                                    {'loss': 0.0009, 'grad_norm': 0.20533733069896698, 'learning_rate': 8.676271186440679e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 881/6000 [29:25<2:51:09,  2.01s/it] 15%|â–ˆâ–        | 882/6000 [29:27<2:52:44,  2.03s/it]                                                    {'loss': 0.0327, 'grad_norm': 8.168274879455566, 'learning_rate': 8.67457627118644e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 882/6000 [29:27<2:52:44,  2.03s/it] 15%|â–ˆâ–        | 883/6000 [29:29<2:50:31,  2.00s/it]                                                    {'loss': 0.023, 'grad_norm': 4.852738380432129, 'learning_rate': 8.672881355932205e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 883/6000 [29:29<2:50:31,  2.00s/it] 15%|â–ˆâ–        | 884/6000 [29:31<2:51:52,  2.02s/it]                                                    {'loss': 0.1629, 'grad_norm': 15.540237426757812, 'learning_rate': 8.671186440677967e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 884/6000 [29:31<2:51:52,  2.02s/it] 15%|â–ˆâ–        | 885/6000 [29:33<2:50:36,  2.00s/it]                                                    {'loss': 0.0141, 'grad_norm': 3.4755802154541016, 'learning_rate': 8.66949152542373e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 885/6000 [29:33<2:50:36,  2.00s/it] 15%|â–ˆâ–        | 886/6000 [29:35<2:50:42,  2.00s/it]                                                    {'loss': 0.0485, 'grad_norm': 8.713743209838867, 'learning_rate': 8.667796610169492e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 886/6000 [29:35<2:50:42,  2.00s/it] 15%|â–ˆâ–        | 887/6000 [29:37<2:49:45,  1.99s/it]                                                    {'loss': 0.0064, 'grad_norm': 2.930227279663086, 'learning_rate': 8.666101694915255e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 887/6000 [29:37<2:49:45,  1.99s/it] 15%|â–ˆâ–        | 888/6000 [29:39<2:49:44,  1.99s/it]                                                    {'loss': 0.0242, 'grad_norm': 6.572662830352783, 'learning_rate': 8.664406779661018e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 888/6000 [29:39<2:49:44,  1.99s/it] 15%|â–ˆâ–        | 889/6000 [29:41<2:48:07,  1.97s/it]                                                    {'loss': 0.0981, 'grad_norm': 13.977631568908691, 'learning_rate': 8.662711864406782e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 889/6000 [29:41<2:48:07,  1.97s/it] 15%|â–ˆâ–        | 890/6000 [29:43<2:46:44,  1.96s/it]                                                    {'loss': 0.0104, 'grad_norm': 2.1092748641967773, 'learning_rate': 8.661016949152543e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 890/6000 [29:43<2:46:44,  1.96s/it] 15%|â–ˆâ–        | 891/6000 [29:44<2:44:41,  1.93s/it]                                                    {'loss': 0.0421, 'grad_norm': 6.819429397583008, 'learning_rate': 8.659322033898305e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 891/6000 [29:44<2:44:41,  1.93s/it] 15%|â–ˆâ–        | 892/6000 [29:46<2:45:16,  1.94s/it]                                                    {'loss': 0.0706, 'grad_norm': 11.741866111755371, 'learning_rate': 8.657627118644068e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 892/6000 [29:46<2:45:16,  1.94s/it] 15%|â–ˆâ–        | 893/6000 [29:48<2:44:29,  1.93s/it]                                                    {'loss': 0.1574, 'grad_norm': 18.94334602355957, 'learning_rate': 8.655932203389831e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 893/6000 [29:48<2:44:29,  1.93s/it] 15%|â–ˆâ–        | 894/6000 [29:50<2:44:40,  1.94s/it]                                                    {'loss': 0.0595, 'grad_norm': 9.873873710632324, 'learning_rate': 8.654237288135595e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 894/6000 [29:50<2:44:40,  1.94s/it] 15%|â–ˆâ–        | 895/6000 [29:52<2:44:12,  1.93s/it]                                                    {'loss': 0.0509, 'grad_norm': 8.925782203674316, 'learning_rate': 8.652542372881356e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 895/6000 [29:52<2:44:12,  1.93s/it] 15%|â–ˆâ–        | 896/6000 [29:54<2:44:45,  1.94s/it]                                                    {'loss': 0.0031, 'grad_norm': 0.9076195359230042, 'learning_rate': 8.65084745762712e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 896/6000 [29:54<2:44:45,  1.94s/it] 15%|â–ˆâ–        | 897/6000 [29:56<2:44:16,  1.93s/it]                                                    {'loss': 0.0172, 'grad_norm': 3.5054657459259033, 'learning_rate': 8.649152542372881e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 897/6000 [29:56<2:44:16,  1.93s/it] 15%|â–ˆâ–        | 898/6000 [29:58<2:44:29,  1.93s/it]                                                    {'loss': 0.1013, 'grad_norm': 8.909224510192871, 'learning_rate': 8.647457627118644e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 898/6000 [29:58<2:44:29,  1.93s/it] 15%|â–ˆâ–        | 899/6000 [30:00<2:52:30,  2.03s/it]                                                    {'loss': 0.0036, 'grad_norm': 1.2351579666137695, 'learning_rate': 8.645762711864408e-06, 'epoch': 0.15}
 15%|â–ˆâ–        | 899/6000 [30:00<2:52:30,  2.03s/it] 15%|â–ˆâ–Œ        | 900/6000 [30:02<2:50:24,  2.00s/it]                                                    {'loss': 0.1448, 'grad_norm': 16.00769805908203, 'learning_rate': 8.64406779661017e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 900/6000 [30:02<2:50:24,  2.00s/it][2025-11-18 10:20:00,001] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-900
[2025-11-18 10:20:00,360] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-900/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 15%|â–ˆâ–Œ        | 901/6000 [30:05<3:13:04,  2.27s/it]                                                    {'loss': 0.0185, 'grad_norm': 4.230609893798828, 'learning_rate': 8.642372881355932e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 901/6000 [30:05<3:13:04,  2.27s/it] 15%|â–ˆâ–Œ        | 902/6000 [30:07<3:05:39,  2.19s/it]                                                    {'loss': 0.0083, 'grad_norm': 1.7174408435821533, 'learning_rate': 8.640677966101696e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 902/6000 [30:07<3:05:39,  2.19s/it] 15%|â–ˆâ–Œ        | 903/6000 [30:09<3:00:30,  2.12s/it]                                                    {'loss': 0.0661, 'grad_norm': 5.7972612380981445, 'learning_rate': 8.638983050847459e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 903/6000 [30:09<3:00:30,  2.12s/it] 15%|â–ˆâ–Œ        | 904/6000 [30:11<2:55:55,  2.07s/it]                                                    {'loss': 0.2002, 'grad_norm': 13.844076156616211, 'learning_rate': 8.637288135593222e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 904/6000 [30:11<2:55:55,  2.07s/it] 15%|â–ˆâ–Œ        | 905/6000 [30:13<2:58:06,  2.10s/it]                                                    {'loss': 0.0865, 'grad_norm': 14.886656761169434, 'learning_rate': 8.635593220338984e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 905/6000 [30:13<2:58:06,  2.10s/it] 15%|â–ˆâ–Œ        | 906/6000 [30:15<2:56:04,  2.07s/it]                                                    {'loss': 0.0128, 'grad_norm': 3.5946872234344482, 'learning_rate': 8.633898305084747e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 906/6000 [30:15<2:56:04,  2.07s/it] 15%|â–ˆâ–Œ        | 907/6000 [30:17<2:55:41,  2.07s/it]                                                    {'loss': 0.1029, 'grad_norm': 5.650485992431641, 'learning_rate': 8.632203389830509e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 907/6000 [30:17<2:55:41,  2.07s/it] 15%|â–ˆâ–Œ        | 908/6000 [30:19<2:55:10,  2.06s/it]                                                    {'loss': 0.1649, 'grad_norm': 14.364906311035156, 'learning_rate': 8.630508474576272e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 908/6000 [30:19<2:55:10,  2.06s/it] 15%|â–ˆâ–Œ        | 909/6000 [30:21<2:52:12,  2.03s/it]                                                    {'loss': 0.0496, 'grad_norm': 9.697866439819336, 'learning_rate': 8.628813559322035e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 909/6000 [30:21<2:52:12,  2.03s/it] 15%|â–ˆâ–Œ        | 910/6000 [30:23<2:53:09,  2.04s/it]                                                    {'loss': 0.019, 'grad_norm': 5.547674179077148, 'learning_rate': 8.627118644067798e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 910/6000 [30:23<2:53:09,  2.04s/it] 15%|â–ˆâ–Œ        | 911/6000 [30:25<2:51:56,  2.03s/it]                                                    {'loss': 0.057, 'grad_norm': 13.638910293579102, 'learning_rate': 8.62542372881356e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 911/6000 [30:25<2:51:56,  2.03s/it] 15%|â–ˆâ–Œ        | 912/6000 [30:27<2:49:35,  2.00s/it]                                                    {'loss': 0.0352, 'grad_norm': 8.354982376098633, 'learning_rate': 8.623728813559322e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 912/6000 [30:27<2:49:35,  2.00s/it] 15%|â–ˆâ–Œ        | 913/6000 [30:29<2:51:51,  2.03s/it]                                                    {'loss': 0.0426, 'grad_norm': 6.463664531707764, 'learning_rate': 8.622033898305085e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 913/6000 [30:29<2:51:51,  2.03s/it] 15%|â–ˆâ–Œ        | 914/6000 [30:31<2:55:11,  2.07s/it]                                                    {'loss': 0.0005, 'grad_norm': 0.07717286050319672, 'learning_rate': 8.620338983050848e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 914/6000 [30:31<2:55:11,  2.07s/it] 15%|â–ˆâ–Œ        | 915/6000 [30:34<3:00:18,  2.13s/it]                                                    {'loss': 0.0718, 'grad_norm': 8.262834548950195, 'learning_rate': 8.618644067796611e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 915/6000 [30:34<3:00:18,  2.13s/it] 15%|â–ˆâ–Œ        | 916/6000 [30:36<2:56:34,  2.08s/it]                                                    {'loss': 0.0246, 'grad_norm': 4.2898969650268555, 'learning_rate': 8.616949152542373e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 916/6000 [30:36<2:56:34,  2.08s/it] 15%|â–ˆâ–Œ        | 917/6000 [30:38<2:52:56,  2.04s/it]                                                    {'loss': 0.0017, 'grad_norm': 0.47795918583869934, 'learning_rate': 8.615254237288136e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 917/6000 [30:38<2:52:56,  2.04s/it] 15%|â–ˆâ–Œ        | 918/6000 [30:40<2:51:43,  2.03s/it]                                                    {'loss': 0.0153, 'grad_norm': 6.948526382446289, 'learning_rate': 8.613559322033898e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 918/6000 [30:40<2:51:43,  2.03s/it] 15%|â–ˆâ–Œ        | 919/6000 [30:42<2:49:34,  2.00s/it]                                                    {'loss': 0.0887, 'grad_norm': 7.014545917510986, 'learning_rate': 8.611864406779661e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 919/6000 [30:42<2:49:34,  2.00s/it] 15%|â–ˆâ–Œ        | 920/6000 [30:44<2:50:15,  2.01s/it]                                                    {'loss': 0.0335, 'grad_norm': 7.71464729309082, 'learning_rate': 8.610169491525424e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 920/6000 [30:44<2:50:15,  2.01s/it] 15%|â–ˆâ–Œ        | 921/6000 [30:46<2:49:52,  2.01s/it]                                                    {'loss': 0.1403, 'grad_norm': 13.725118637084961, 'learning_rate': 8.608474576271188e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 921/6000 [30:46<2:49:52,  2.01s/it] 15%|â–ˆâ–Œ        | 922/6000 [30:48<2:48:54,  2.00s/it]                                                    {'loss': 0.0074, 'grad_norm': 1.8568087816238403, 'learning_rate': 8.60677966101695e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 922/6000 [30:48<2:48:54,  2.00s/it] 15%|â–ˆâ–Œ        | 923/6000 [30:50<2:49:24,  2.00s/it]                                                    {'loss': 0.1188, 'grad_norm': 11.881356239318848, 'learning_rate': 8.605084745762713e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 923/6000 [30:50<2:49:24,  2.00s/it] 15%|â–ˆâ–Œ        | 924/6000 [30:51<2:47:18,  1.98s/it]                                                    {'loss': 0.0014, 'grad_norm': 0.2562996447086334, 'learning_rate': 8.603389830508476e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 924/6000 [30:52<2:47:18,  1.98s/it] 15%|â–ˆâ–Œ        | 925/6000 [30:53<2:46:55,  1.97s/it]                                                    {'loss': 0.0926, 'grad_norm': 11.118766784667969, 'learning_rate': 8.601694915254239e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 925/6000 [30:53<2:46:55,  1.97s/it] 15%|â–ˆâ–Œ        | 926/6000 [30:55<2:48:15,  1.99s/it]                                                    {'loss': 0.1152, 'grad_norm': 15.19479751586914, 'learning_rate': 8.6e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 926/6000 [30:55<2:48:15,  1.99s/it] 15%|â–ˆâ–Œ        | 927/6000 [30:57<2:46:50,  1.97s/it]                                                    {'loss': 0.2441, 'grad_norm': 16.666664123535156, 'learning_rate': 8.598305084745764e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 927/6000 [30:57<2:46:50,  1.97s/it] 15%|â–ˆâ–Œ        | 928/6000 [30:59<2:45:21,  1.96s/it]                                                    {'loss': 0.0575, 'grad_norm': 8.374016761779785, 'learning_rate': 8.596610169491526e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 928/6000 [30:59<2:45:21,  1.96s/it] 15%|â–ˆâ–Œ        | 929/6000 [31:01<2:45:14,  1.96s/it]                                                    {'loss': 0.0943, 'grad_norm': 11.488749504089355, 'learning_rate': 8.594915254237289e-06, 'epoch': 0.15}
 15%|â–ˆâ–Œ        | 929/6000 [31:01<2:45:14,  1.96s/it] 16%|â–ˆâ–Œ        | 930/6000 [31:03<2:45:09,  1.95s/it]                                                    {'loss': 0.2823, 'grad_norm': 17.994626998901367, 'learning_rate': 8.593220338983052e-06, 'epoch': 0.15}
 16%|â–ˆâ–Œ        | 930/6000 [31:03<2:45:09,  1.95s/it] 16%|â–ˆâ–Œ        | 931/6000 [31:05<2:44:47,  1.95s/it]                                                    {'loss': 0.0355, 'grad_norm': 6.823879241943359, 'learning_rate': 8.591525423728814e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 931/6000 [31:05<2:44:47,  1.95s/it] 16%|â–ˆâ–Œ        | 932/6000 [31:07<2:44:31,  1.95s/it]                                                    {'loss': 0.0313, 'grad_norm': 8.732234954833984, 'learning_rate': 8.589830508474577e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 932/6000 [31:07<2:44:31,  1.95s/it] 16%|â–ˆâ–Œ        | 933/6000 [31:09<2:43:37,  1.94s/it]                                                    {'loss': 0.0102, 'grad_norm': 3.8185973167419434, 'learning_rate': 8.588135593220339e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 933/6000 [31:09<2:43:37,  1.94s/it] 16%|â–ˆâ–Œ        | 934/6000 [31:11<2:46:25,  1.97s/it]                                                    {'loss': 0.0529, 'grad_norm': 6.8431396484375, 'learning_rate': 8.586440677966102e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 934/6000 [31:11<2:46:25,  1.97s/it] 16%|â–ˆâ–Œ        | 935/6000 [31:13<2:45:32,  1.96s/it]                                                    {'loss': 0.0165, 'grad_norm': 2.032957077026367, 'learning_rate': 8.584745762711865e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 935/6000 [31:13<2:45:32,  1.96s/it] 16%|â–ˆâ–Œ        | 936/6000 [31:15<2:44:48,  1.95s/it]                                                    {'loss': 0.0593, 'grad_norm': 10.937788963317871, 'learning_rate': 8.583050847457628e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 936/6000 [31:15<2:44:48,  1.95s/it] 16%|â–ˆâ–Œ        | 937/6000 [31:17<2:51:21,  2.03s/it]                                                    {'loss': 0.1343, 'grad_norm': 15.202922821044922, 'learning_rate': 8.58135593220339e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 937/6000 [31:17<2:51:21,  2.03s/it] 16%|â–ˆâ–Œ        | 938/6000 [31:19<2:50:27,  2.02s/it]                                                    {'loss': 0.0148, 'grad_norm': 6.510083198547363, 'learning_rate': 8.579661016949153e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 938/6000 [31:19<2:50:27,  2.02s/it] 16%|â–ˆâ–Œ        | 939/6000 [31:21<2:51:41,  2.04s/it]                                                    {'loss': 0.0258, 'grad_norm': 9.29171085357666, 'learning_rate': 8.577966101694916e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 939/6000 [31:21<2:51:41,  2.04s/it] 16%|â–ˆâ–Œ        | 940/6000 [31:23<2:49:24,  2.01s/it]                                                    {'loss': 0.002, 'grad_norm': 0.6226810812950134, 'learning_rate': 8.57627118644068e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 940/6000 [31:23<2:49:24,  2.01s/it] 16%|â–ˆâ–Œ        | 941/6000 [31:25<2:53:42,  2.06s/it]                                                    {'loss': 0.0275, 'grad_norm': 8.332372665405273, 'learning_rate': 8.574576271186441e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 941/6000 [31:25<2:53:42,  2.06s/it] 16%|â–ˆâ–Œ        | 942/6000 [31:27<2:51:23,  2.03s/it]                                                    {'loss': 0.1006, 'grad_norm': 28.93755531311035, 'learning_rate': 8.572881355932205e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 942/6000 [31:27<2:51:23,  2.03s/it] 16%|â–ˆâ–Œ        | 943/6000 [31:29<2:50:08,  2.02s/it]                                                    {'loss': 0.0068, 'grad_norm': 1.512891173362732, 'learning_rate': 8.571186440677966e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 943/6000 [31:29<2:50:08,  2.02s/it] 16%|â–ˆâ–Œ        | 944/6000 [31:31<2:49:24,  2.01s/it]                                                    {'loss': 0.096, 'grad_norm': 10.977622032165527, 'learning_rate': 8.56949152542373e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 944/6000 [31:31<2:49:24,  2.01s/it] 16%|â–ˆâ–Œ        | 945/6000 [31:33<2:50:33,  2.02s/it]                                                    {'loss': 0.1825, 'grad_norm': 17.370901107788086, 'learning_rate': 8.567796610169493e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 945/6000 [31:33<2:50:33,  2.02s/it] 16%|â–ˆâ–Œ        | 946/6000 [31:35<2:49:23,  2.01s/it]                                                    {'loss': 0.0101, 'grad_norm': 3.216609239578247, 'learning_rate': 8.566101694915256e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 946/6000 [31:35<2:49:23,  2.01s/it] 16%|â–ˆâ–Œ        | 947/6000 [31:37<2:47:44,  1.99s/it]                                                    {'loss': 0.1078, 'grad_norm': 11.76124095916748, 'learning_rate': 8.564406779661018e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 947/6000 [31:37<2:47:44,  1.99s/it] 16%|â–ˆâ–Œ        | 948/6000 [31:39<2:49:03,  2.01s/it]                                                    {'loss': 0.0966, 'grad_norm': 12.646719932556152, 'learning_rate': 8.56271186440678e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 948/6000 [31:39<2:49:03,  2.01s/it] 16%|â–ˆâ–Œ        | 949/6000 [31:41<2:51:31,  2.04s/it]                                                    {'loss': 0.1203, 'grad_norm': 20.00420570373535, 'learning_rate': 8.561016949152542e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 949/6000 [31:41<2:51:31,  2.04s/it] 16%|â–ˆâ–Œ        | 950/6000 [31:44<2:52:18,  2.05s/it]                                                    {'loss': 0.1234, 'grad_norm': 17.262468338012695, 'learning_rate': 8.559322033898306e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 950/6000 [31:44<2:52:18,  2.05s/it][2025-11-18 10:21:41,386] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-950
[2025-11-18 10:21:41,684] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-950/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 16%|â–ˆâ–Œ        | 951/6000 [31:46<3:10:22,  2.26s/it]                                                    {'loss': 0.0903, 'grad_norm': 9.493419647216797, 'learning_rate': 8.557627118644069e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 951/6000 [31:46<3:10:22,  2.26s/it] 16%|â–ˆâ–Œ        | 952/6000 [31:48<3:02:04,  2.16s/it]                                                    {'loss': 0.1422, 'grad_norm': 14.989127159118652, 'learning_rate': 8.55593220338983e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 952/6000 [31:48<3:02:04,  2.16s/it] 16%|â–ˆâ–Œ        | 953/6000 [31:50<2:59:13,  2.13s/it]                                                    {'loss': 0.0013, 'grad_norm': 0.22605568170547485, 'learning_rate': 8.554237288135594e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 953/6000 [31:50<2:59:13,  2.13s/it] 16%|â–ˆâ–Œ        | 954/6000 [31:52<2:53:48,  2.07s/it]                                                    {'loss': 0.007, 'grad_norm': 1.764507532119751, 'learning_rate': 8.552542372881355e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 954/6000 [31:52<2:53:48,  2.07s/it] 16%|â–ˆâ–Œ        | 955/6000 [31:54<2:50:51,  2.03s/it]                                                    {'loss': 0.1057, 'grad_norm': 14.22418212890625, 'learning_rate': 8.550847457627119e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 955/6000 [31:54<2:50:51,  2.03s/it] 16%|â–ˆâ–Œ        | 956/6000 [31:56<2:49:52,  2.02s/it]                                                    {'loss': 0.1124, 'grad_norm': 16.653268814086914, 'learning_rate': 8.549152542372882e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 956/6000 [31:56<2:49:52,  2.02s/it] 16%|â–ˆâ–Œ        | 957/6000 [31:58<2:48:08,  2.00s/it]                                                    {'loss': 0.0343, 'grad_norm': 5.364099979400635, 'learning_rate': 8.547457627118645e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 957/6000 [31:58<2:48:08,  2.00s/it] 16%|â–ˆâ–Œ        | 958/6000 [32:00<2:47:13,  1.99s/it]                                                    {'loss': 0.1694, 'grad_norm': 14.268698692321777, 'learning_rate': 8.545762711864407e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 958/6000 [32:00<2:47:13,  1.99s/it] 16%|â–ˆâ–Œ        | 959/6000 [32:02<2:45:33,  1.97s/it]                                                    {'loss': 0.0441, 'grad_norm': 4.981091499328613, 'learning_rate': 8.54406779661017e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 959/6000 [32:02<2:45:33,  1.97s/it] 16%|â–ˆâ–Œ        | 960/6000 [32:04<2:46:02,  1.98s/it]                                                    {'loss': 0.3989, 'grad_norm': 20.629413604736328, 'learning_rate': 8.542372881355933e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 960/6000 [32:04<2:46:02,  1.98s/it] 16%|â–ˆâ–Œ        | 961/6000 [32:06<2:46:56,  1.99s/it]                                                    {'loss': 0.024, 'grad_norm': 3.546414852142334, 'learning_rate': 8.540677966101697e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 961/6000 [32:06<2:46:56,  1.99s/it] 16%|â–ˆâ–Œ        | 962/6000 [32:08<2:49:06,  2.01s/it]                                                    {'loss': 0.1544, 'grad_norm': 13.972503662109375, 'learning_rate': 8.538983050847458e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 962/6000 [32:08<2:49:06,  2.01s/it] 16%|â–ˆâ–Œ        | 963/6000 [32:10<2:46:28,  1.98s/it]                                                    {'loss': 0.1067, 'grad_norm': 10.135196685791016, 'learning_rate': 8.537288135593221e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 963/6000 [32:10<2:46:28,  1.98s/it] 16%|â–ˆâ–Œ        | 964/6000 [32:12<2:46:18,  1.98s/it]                                                    {'loss': 0.1731, 'grad_norm': 17.486753463745117, 'learning_rate': 8.535593220338983e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 964/6000 [32:12<2:46:18,  1.98s/it] 16%|â–ˆâ–Œ        | 965/6000 [32:14<2:45:26,  1.97s/it]                                                    {'loss': 0.0861, 'grad_norm': 12.754217147827148, 'learning_rate': 8.533898305084746e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 965/6000 [32:14<2:45:26,  1.97s/it] 16%|â–ˆâ–Œ        | 966/6000 [32:16<2:46:33,  1.99s/it]                                                    {'loss': 0.002, 'grad_norm': 0.6839156150817871, 'learning_rate': 8.53220338983051e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 966/6000 [32:16<2:46:33,  1.99s/it] 16%|â–ˆâ–Œ        | 967/6000 [32:18<2:46:22,  1.98s/it]                                                    {'loss': 0.0048, 'grad_norm': 0.8808407783508301, 'learning_rate': 8.530508474576273e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 967/6000 [32:18<2:46:22,  1.98s/it] 16%|â–ˆâ–Œ        | 968/6000 [32:20<2:47:30,  2.00s/it]                                                    {'loss': 0.0046, 'grad_norm': 1.2050178050994873, 'learning_rate': 8.528813559322034e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 968/6000 [32:20<2:47:30,  2.00s/it] 16%|â–ˆâ–Œ        | 969/6000 [32:22<2:46:57,  1.99s/it]                                                    {'loss': 0.0232, 'grad_norm': 4.821930408477783, 'learning_rate': 8.527118644067798e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 969/6000 [32:22<2:46:57,  1.99s/it] 16%|â–ˆâ–Œ        | 970/6000 [32:24<2:47:32,  2.00s/it]                                                    {'loss': 0.0368, 'grad_norm': 7.589064598083496, 'learning_rate': 8.52542372881356e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 970/6000 [32:24<2:47:32,  2.00s/it] 16%|â–ˆâ–Œ        | 971/6000 [32:26<2:45:55,  1.98s/it]                                                    {'loss': 0.2091, 'grad_norm': 15.580077171325684, 'learning_rate': 8.523728813559323e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 971/6000 [32:26<2:45:55,  1.98s/it] 16%|â–ˆâ–Œ        | 972/6000 [32:28<2:45:08,  1.97s/it]                                                    {'loss': 0.073, 'grad_norm': 11.808694839477539, 'learning_rate': 8.522033898305086e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 972/6000 [32:28<2:45:08,  1.97s/it] 16%|â–ˆâ–Œ        | 973/6000 [32:30<2:43:40,  1.95s/it]                                                    {'loss': 0.1285, 'grad_norm': 14.814590454101562, 'learning_rate': 8.520338983050847e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 973/6000 [32:30<2:43:40,  1.95s/it] 16%|â–ˆâ–Œ        | 974/6000 [32:32<2:44:57,  1.97s/it]                                                    {'loss': 0.0035, 'grad_norm': 1.0340055227279663, 'learning_rate': 8.51864406779661e-06, 'epoch': 0.16}
 16%|â–ˆâ–Œ        | 974/6000 [32:32<2:44:57,  1.97s/it] 16%|â–ˆâ–‹        | 975/6000 [32:34<2:43:53,  1.96s/it]                                                    {'loss': 0.1012, 'grad_norm': 9.896893501281738, 'learning_rate': 8.516949152542372e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 975/6000 [32:34<2:43:53,  1.96s/it] 16%|â–ˆâ–‹        | 976/6000 [32:36<2:43:50,  1.96s/it]                                                    {'loss': 0.0018, 'grad_norm': 0.44520121812820435, 'learning_rate': 8.515254237288136e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 976/6000 [32:36<2:43:50,  1.96s/it] 16%|â–ˆâ–‹        | 977/6000 [32:38<2:43:41,  1.96s/it]                                                    {'loss': 0.0793, 'grad_norm': 11.874862670898438, 'learning_rate': 8.513559322033899e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 977/6000 [32:38<2:43:41,  1.96s/it] 16%|â–ˆâ–‹        | 978/6000 [32:39<2:42:57,  1.95s/it]                                                    {'loss': 0.0796, 'grad_norm': 18.135345458984375, 'learning_rate': 8.511864406779662e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 978/6000 [32:39<2:42:57,  1.95s/it] 16%|â–ˆâ–‹        | 979/6000 [32:41<2:42:55,  1.95s/it]                                                    {'loss': 0.0246, 'grad_norm': 3.700024127960205, 'learning_rate': 8.510169491525424e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 979/6000 [32:41<2:42:55,  1.95s/it] 16%|â–ˆâ–‹        | 980/6000 [32:43<2:44:17,  1.96s/it]                                                    {'loss': 0.1086, 'grad_norm': 11.349364280700684, 'learning_rate': 8.508474576271187e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 980/6000 [32:43<2:44:17,  1.96s/it] 16%|â–ˆâ–‹        | 981/6000 [32:45<2:43:07,  1.95s/it]                                                    {'loss': 0.0452, 'grad_norm': 3.813143491744995, 'learning_rate': 8.50677966101695e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 981/6000 [32:45<2:43:07,  1.95s/it] 16%|â–ˆâ–‹        | 982/6000 [32:47<2:45:21,  1.98s/it]                                                    {'loss': 0.111, 'grad_norm': 11.682912826538086, 'learning_rate': 8.505084745762714e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 982/6000 [32:47<2:45:21,  1.98s/it] 16%|â–ˆâ–‹        | 983/6000 [32:49<2:45:59,  1.99s/it]                                                    {'loss': 0.0176, 'grad_norm': 2.4250741004943848, 'learning_rate': 8.503389830508475e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 983/6000 [32:49<2:45:59,  1.99s/it] 16%|â–ˆâ–‹        | 984/6000 [32:51<2:44:31,  1.97s/it]                                                    {'loss': 0.1034, 'grad_norm': 9.44100570678711, 'learning_rate': 8.501694915254238e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 984/6000 [32:51<2:44:31,  1.97s/it] 16%|â–ˆâ–‹        | 985/6000 [32:53<2:45:17,  1.98s/it]                                                    {'loss': 0.0768, 'grad_norm': 7.583032131195068, 'learning_rate': 8.5e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 985/6000 [32:53<2:45:17,  1.98s/it] 16%|â–ˆâ–‹        | 986/6000 [32:55<2:43:22,  1.95s/it]                                                    {'loss': 0.1066, 'grad_norm': 8.776201248168945, 'learning_rate': 8.498305084745763e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 986/6000 [32:55<2:43:22,  1.95s/it] 16%|â–ˆâ–‹        | 987/6000 [32:57<2:43:49,  1.96s/it]                                                    {'loss': 0.0006, 'grad_norm': 0.16054248809814453, 'learning_rate': 8.496610169491526e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 987/6000 [32:57<2:43:49,  1.96s/it] 16%|â–ˆâ–‹        | 988/6000 [32:59<2:43:17,  1.95s/it]                                                    {'loss': 0.0482, 'grad_norm': 4.61094331741333, 'learning_rate': 8.49491525423729e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 988/6000 [32:59<2:43:17,  1.95s/it] 16%|â–ˆâ–‹        | 989/6000 [33:01<2:42:46,  1.95s/it]                                                    {'loss': 0.1601, 'grad_norm': 14.961088180541992, 'learning_rate': 8.493220338983051e-06, 'epoch': 0.16}
 16%|â–ˆâ–‹        | 989/6000 [33:01<2:42:46,  1.95s/it] 16%|â–ˆâ–‹        | 990/6000 [33:03<2:44:28,  1.97s/it]                                                    {'loss': 0.0303, 'grad_norm': 6.816399097442627, 'learning_rate': 8.491525423728815e-06, 'epoch': 0.17}
 16%|â–ˆâ–‹        | 990/6000 [33:03<2:44:28,  1.97s/it] 17%|â–ˆâ–‹        | 991/6000 [33:05<2:47:13,  2.00s/it]                                                    {'loss': 0.0139, 'grad_norm': 2.279637098312378, 'learning_rate': 8.489830508474576e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 991/6000 [33:05<2:47:13,  2.00s/it] 17%|â–ˆâ–‹        | 992/6000 [33:07<2:46:46,  2.00s/it]                                                    {'loss': 0.0011, 'grad_norm': 0.35627052187919617, 'learning_rate': 8.48813559322034e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 992/6000 [33:07<2:46:46,  2.00s/it] 17%|â–ˆâ–‹        | 993/6000 [33:09<2:46:11,  1.99s/it]                                                    {'loss': 0.0166, 'grad_norm': 3.3215508460998535, 'learning_rate': 8.486440677966103e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 993/6000 [33:09<2:46:11,  1.99s/it] 17%|â–ˆâ–‹        | 994/6000 [33:11<2:46:24,  1.99s/it]                                                    {'loss': 0.0131, 'grad_norm': 3.8614628314971924, 'learning_rate': 8.484745762711864e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 994/6000 [33:11<2:46:24,  1.99s/it] 17%|â–ˆâ–‹        | 995/6000 [33:13<2:45:22,  1.98s/it]                                                    {'loss': 0.1414, 'grad_norm': 18.24080467224121, 'learning_rate': 8.483050847457628e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 995/6000 [33:13<2:45:22,  1.98s/it] 17%|â–ˆâ–‹        | 996/6000 [33:15<2:45:54,  1.99s/it]                                                    {'loss': 0.0099, 'grad_norm': 2.671827554702759, 'learning_rate': 8.481355932203391e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 996/6000 [33:15<2:45:54,  1.99s/it] 17%|â–ˆâ–‹        | 997/6000 [33:17<2:49:45,  2.04s/it]                                                    {'loss': 0.1881, 'grad_norm': 12.76059627532959, 'learning_rate': 8.479661016949154e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 997/6000 [33:17<2:49:45,  2.04s/it] 17%|â–ˆâ–‹        | 998/6000 [33:19<2:46:08,  1.99s/it]                                                    {'loss': 0.1802, 'grad_norm': 17.53424072265625, 'learning_rate': 8.477966101694916e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 998/6000 [33:19<2:46:08,  1.99s/it] 17%|â–ˆâ–‹        | 999/6000 [33:21<2:43:51,  1.97s/it]                                                    {'loss': 0.0685, 'grad_norm': 13.146698951721191, 'learning_rate': 8.476271186440679e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 999/6000 [33:21<2:43:51,  1.97s/it] 17%|â–ˆâ–‹        | 1000/6000 [33:23<2:46:17,  2.00s/it]                                                     {'loss': 0.0033, 'grad_norm': 1.1471226215362549, 'learning_rate': 8.47457627118644e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1000/6000 [33:23<2:46:17,  2.00s/it][2025-11-18 10:23:20,971] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1000
[2025-11-18 10:23:21,271] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1000/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 17%|â–ˆâ–‹        | 1001/6000 [33:26<3:03:34,  2.20s/it]                                                     {'loss': 0.355, 'grad_norm': 16.464595794677734, 'learning_rate': 8.472881355932204e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1001/6000 [33:26<3:03:34,  2.20s/it] 17%|â–ˆâ–‹        | 1002/6000 [33:28<2:57:25,  2.13s/it]                                                     {'loss': 0.0074, 'grad_norm': 2.5795438289642334, 'learning_rate': 8.471186440677967e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1002/6000 [33:28<2:57:25,  2.13s/it] 17%|â–ˆâ–‹        | 1003/6000 [33:30<2:51:22,  2.06s/it]                                                     {'loss': 0.1679, 'grad_norm': 18.398386001586914, 'learning_rate': 8.46949152542373e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1003/6000 [33:30<2:51:22,  2.06s/it] 17%|â–ˆâ–‹        | 1004/6000 [33:32<2:48:17,  2.02s/it]                                                     {'loss': 0.0211, 'grad_norm': 4.655160427093506, 'learning_rate': 8.467796610169492e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1004/6000 [33:32<2:48:17,  2.02s/it] 17%|â–ˆâ–‹        | 1005/6000 [33:34<2:46:08,  2.00s/it]                                                     {'loss': 0.0848, 'grad_norm': 14.99942398071289, 'learning_rate': 8.466101694915255e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1005/6000 [33:34<2:46:08,  2.00s/it] 17%|â–ˆâ–‹        | 1006/6000 [33:35<2:43:23,  1.96s/it]                                                     {'loss': 0.106, 'grad_norm': 14.699877738952637, 'learning_rate': 8.464406779661017e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1006/6000 [33:35<2:43:23,  1.96s/it] 17%|â–ˆâ–‹        | 1007/6000 [33:37<2:44:25,  1.98s/it]                                                     {'loss': 0.0466, 'grad_norm': 13.2544527053833, 'learning_rate': 8.46271186440678e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1007/6000 [33:37<2:44:25,  1.98s/it] 17%|â–ˆâ–‹        | 1008/6000 [33:39<2:44:44,  1.98s/it]                                                     {'loss': 0.005, 'grad_norm': 1.2881025075912476, 'learning_rate': 8.461016949152543e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1008/6000 [33:39<2:44:44,  1.98s/it] 17%|â–ˆâ–‹        | 1009/6000 [33:41<2:43:47,  1.97s/it]                                                     {'loss': 0.0004, 'grad_norm': 0.14063161611557007, 'learning_rate': 8.459322033898307e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1009/6000 [33:41<2:43:47,  1.97s/it] 17%|â–ˆâ–‹        | 1010/6000 [33:43<2:42:02,  1.95s/it]                                                     {'loss': 0.0713, 'grad_norm': 6.950913429260254, 'learning_rate': 8.457627118644068e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1010/6000 [33:43<2:42:02,  1.95s/it] 17%|â–ˆâ–‹        | 1011/6000 [33:45<2:43:35,  1.97s/it]                                                     {'loss': 0.0862, 'grad_norm': 8.12349796295166, 'learning_rate': 8.455932203389831e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1011/6000 [33:45<2:43:35,  1.97s/it] 17%|â–ˆâ–‹        | 1012/6000 [33:47<2:49:39,  2.04s/it]                                                     {'loss': 0.0225, 'grad_norm': 5.441495418548584, 'learning_rate': 8.454237288135593e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1012/6000 [33:47<2:49:39,  2.04s/it] 17%|â–ˆâ–‹        | 1013/6000 [33:49<2:46:56,  2.01s/it]                                                     {'loss': 0.1102, 'grad_norm': 15.23160171508789, 'learning_rate': 8.452542372881356e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1013/6000 [33:49<2:46:56,  2.01s/it] 17%|â–ˆâ–‹        | 1014/6000 [33:51<2:45:33,  1.99s/it]                                                     {'loss': 0.2804, 'grad_norm': 17.823640823364258, 'learning_rate': 8.45084745762712e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1014/6000 [33:51<2:45:33,  1.99s/it] 17%|â–ˆâ–‹        | 1015/6000 [33:53<2:43:41,  1.97s/it]                                                     {'loss': 0.0838, 'grad_norm': 15.401938438415527, 'learning_rate': 8.449152542372881e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1015/6000 [33:53<2:43:41,  1.97s/it] 17%|â–ˆâ–‹        | 1016/6000 [33:55<2:43:44,  1.97s/it]                                                     {'loss': 0.0745, 'grad_norm': 12.052674293518066, 'learning_rate': 8.447457627118644e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1016/6000 [33:55<2:43:44,  1.97s/it] 17%|â–ˆâ–‹        | 1017/6000 [33:57<2:43:49,  1.97s/it]                                                     {'loss': 0.0896, 'grad_norm': 7.495887279510498, 'learning_rate': 8.445762711864408e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1017/6000 [33:57<2:43:49,  1.97s/it] 17%|â–ˆâ–‹        | 1018/6000 [33:59<2:45:46,  2.00s/it]                                                     {'loss': 0.0158, 'grad_norm': 2.744494676589966, 'learning_rate': 8.444067796610171e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1018/6000 [33:59<2:45:46,  2.00s/it] 17%|â–ˆâ–‹        | 1019/6000 [34:01<2:44:18,  1.98s/it]                                                     {'loss': 0.0425, 'grad_norm': 6.651789665222168, 'learning_rate': 8.442372881355933e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1019/6000 [34:01<2:44:18,  1.98s/it] 17%|â–ˆâ–‹        | 1020/6000 [34:03<2:44:10,  1.98s/it]                                                     {'loss': 0.2417, 'grad_norm': 15.992709159851074, 'learning_rate': 8.440677966101696e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1020/6000 [34:03<2:44:10,  1.98s/it] 17%|â–ˆâ–‹        | 1021/6000 [34:05<2:44:29,  1.98s/it]                                                     {'loss': 0.0426, 'grad_norm': 4.510143756866455, 'learning_rate': 8.438983050847457e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1021/6000 [34:05<2:44:29,  1.98s/it] 17%|â–ˆâ–‹        | 1022/6000 [34:07<2:46:30,  2.01s/it]                                                     {'loss': 0.156, 'grad_norm': 22.97954750061035, 'learning_rate': 8.43728813559322e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1022/6000 [34:07<2:46:30,  2.01s/it] 17%|â–ˆâ–‹        | 1023/6000 [34:09<2:45:22,  1.99s/it]                                                     {'loss': 0.1252, 'grad_norm': 14.19189739227295, 'learning_rate': 8.435593220338984e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1023/6000 [34:09<2:45:22,  1.99s/it] 17%|â–ˆâ–‹        | 1024/6000 [34:11<2:44:37,  1.99s/it]                                                     {'loss': 0.0133, 'grad_norm': 9.750434875488281, 'learning_rate': 8.433898305084747e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1024/6000 [34:11<2:44:37,  1.99s/it] 17%|â–ˆâ–‹        | 1025/6000 [34:13<2:43:48,  1.98s/it]                                                     {'loss': 0.0002, 'grad_norm': 0.04964723065495491, 'learning_rate': 8.432203389830509e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1025/6000 [34:13<2:43:48,  1.98s/it] 17%|â–ˆâ–‹        | 1026/6000 [34:15<2:42:21,  1.96s/it]                                                     {'loss': 0.1169, 'grad_norm': 20.159900665283203, 'learning_rate': 8.430508474576272e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1026/6000 [34:15<2:42:21,  1.96s/it] 17%|â–ˆâ–‹        | 1027/6000 [34:17<2:43:19,  1.97s/it]                                                     {'loss': 0.0304, 'grad_norm': 7.678555011749268, 'learning_rate': 8.428813559322034e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1027/6000 [34:17<2:43:19,  1.97s/it] 17%|â–ˆâ–‹        | 1028/6000 [34:19<2:42:07,  1.96s/it]                                                     {'loss': 0.002, 'grad_norm': 0.5173463821411133, 'learning_rate': 8.427118644067797e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1028/6000 [34:19<2:42:07,  1.96s/it] 17%|â–ˆâ–‹        | 1029/6000 [34:21<2:41:15,  1.95s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.3640671372413635, 'learning_rate': 8.42542372881356e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1029/6000 [34:21<2:41:15,  1.95s/it] 17%|â–ˆâ–‹        | 1030/6000 [34:23<2:39:59,  1.93s/it]                                                     {'loss': 0.0435, 'grad_norm': 14.118781089782715, 'learning_rate': 8.423728813559324e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1030/6000 [34:23<2:39:59,  1.93s/it] 17%|â–ˆâ–‹        | 1031/6000 [34:25<2:40:44,  1.94s/it]                                                     {'loss': 0.0287, 'grad_norm': 6.757256507873535, 'learning_rate': 8.422033898305085e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1031/6000 [34:25<2:40:44,  1.94s/it] 17%|â–ˆâ–‹        | 1032/6000 [34:27<2:40:14,  1.94s/it]                                                     {'loss': 0.0668, 'grad_norm': 11.501306533813477, 'learning_rate': 8.420338983050848e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1032/6000 [34:27<2:40:14,  1.94s/it] 17%|â–ˆâ–‹        | 1033/6000 [34:29<2:40:22,  1.94s/it]                                                     {'loss': 0.0046, 'grad_norm': 2.184126138687134, 'learning_rate': 8.41864406779661e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1033/6000 [34:29<2:40:22,  1.94s/it] 17%|â–ˆâ–‹        | 1034/6000 [34:31<2:39:51,  1.93s/it]                                                     {'loss': 0.0774, 'grad_norm': 6.876898765563965, 'learning_rate': 8.416949152542375e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1034/6000 [34:31<2:39:51,  1.93s/it] 17%|â–ˆâ–‹        | 1035/6000 [34:32<2:40:16,  1.94s/it]                                                     {'loss': 0.0408, 'grad_norm': 5.770735263824463, 'learning_rate': 8.415254237288137e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1035/6000 [34:32<2:40:16,  1.94s/it] 17%|â–ˆâ–‹        | 1036/6000 [34:34<2:41:21,  1.95s/it]                                                     {'loss': 0.1203, 'grad_norm': 11.897048950195312, 'learning_rate': 8.413559322033898e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1036/6000 [34:34<2:41:21,  1.95s/it] 17%|â–ˆâ–‹        | 1037/6000 [34:36<2:43:23,  1.98s/it]                                                     {'loss': 0.0648, 'grad_norm': 13.776217460632324, 'learning_rate': 8.411864406779661e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1037/6000 [34:36<2:43:23,  1.98s/it] 17%|â–ˆâ–‹        | 1038/6000 [34:39<2:46:16,  2.01s/it]                                                     {'loss': 0.0402, 'grad_norm': 11.685260772705078, 'learning_rate': 8.410169491525425e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1038/6000 [34:39<2:46:16,  2.01s/it] 17%|â–ˆâ–‹        | 1039/6000 [34:41<2:45:41,  2.00s/it]                                                     {'loss': 0.0459, 'grad_norm': 5.882702350616455, 'learning_rate': 8.408474576271188e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1039/6000 [34:41<2:45:41,  2.00s/it] 17%|â–ˆâ–‹        | 1040/6000 [34:43<2:45:07,  2.00s/it]                                                     {'loss': 0.0479, 'grad_norm': 6.183849334716797, 'learning_rate': 8.40677966101695e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1040/6000 [34:43<2:45:07,  2.00s/it] 17%|â–ˆâ–‹        | 1041/6000 [34:45<2:44:53,  2.00s/it]                                                     {'loss': 0.1145, 'grad_norm': 16.189895629882812, 'learning_rate': 8.405084745762713e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1041/6000 [34:45<2:44:53,  2.00s/it] 17%|â–ˆâ–‹        | 1042/6000 [34:47<2:44:29,  1.99s/it]                                                     {'loss': 0.006, 'grad_norm': 1.2755142450332642, 'learning_rate': 8.403389830508474e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1042/6000 [34:47<2:44:29,  1.99s/it] 17%|â–ˆâ–‹        | 1043/6000 [34:48<2:42:33,  1.97s/it]                                                     {'loss': 0.091, 'grad_norm': 15.813079833984375, 'learning_rate': 8.401694915254238e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1043/6000 [34:48<2:42:33,  1.97s/it] 17%|â–ˆâ–‹        | 1044/6000 [34:50<2:44:09,  1.99s/it]                                                     {'loss': 0.1177, 'grad_norm': 16.169166564941406, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1044/6000 [34:50<2:44:09,  1.99s/it] 17%|â–ˆâ–‹        | 1045/6000 [34:52<2:43:25,  1.98s/it]                                                     {'loss': 0.2072, 'grad_norm': 15.090639114379883, 'learning_rate': 8.398305084745764e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1045/6000 [34:52<2:43:25,  1.98s/it] 17%|â–ˆâ–‹        | 1046/6000 [34:54<2:44:22,  1.99s/it]                                                     {'loss': 0.2098, 'grad_norm': 20.244657516479492, 'learning_rate': 8.396610169491526e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1046/6000 [34:54<2:44:22,  1.99s/it] 17%|â–ˆâ–‹        | 1047/6000 [34:56<2:45:43,  2.01s/it]                                                     {'loss': 0.3043, 'grad_norm': 19.298858642578125, 'learning_rate': 8.394915254237289e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1047/6000 [34:56<2:45:43,  2.01s/it] 17%|â–ˆâ–‹        | 1048/6000 [34:58<2:44:33,  1.99s/it]                                                     {'loss': 0.0197, 'grad_norm': 7.124427318572998, 'learning_rate': 8.39322033898305e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1048/6000 [34:58<2:44:33,  1.99s/it] 17%|â–ˆâ–‹        | 1049/6000 [35:00<2:43:27,  1.98s/it]                                                     {'loss': 0.162, 'grad_norm': 11.312501907348633, 'learning_rate': 8.391525423728814e-06, 'epoch': 0.17}
 17%|â–ˆâ–‹        | 1049/6000 [35:00<2:43:27,  1.98s/it] 18%|â–ˆâ–Š        | 1050/6000 [35:03<2:49:47,  2.06s/it]                                                     {'loss': 0.0125, 'grad_norm': 1.8569635152816772, 'learning_rate': 8.389830508474577e-06, 'epoch': 0.17}
 18%|â–ˆâ–Š        | 1050/6000 [35:03<2:49:47,  2.06s/it][2025-11-18 10:25:00,513] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1050
[2025-11-18 10:25:00,848] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1050/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1051/6000 [35:05<3:06:59,  2.27s/it]                                                     {'loss': 0.1056, 'grad_norm': 10.425435066223145, 'learning_rate': 8.38813559322034e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1051/6000 [35:05<3:06:59,  2.27s/it] 18%|â–ˆâ–Š        | 1052/6000 [35:07<2:59:56,  2.18s/it]                                                     {'loss': 0.0128, 'grad_norm': 4.428313732147217, 'learning_rate': 8.386440677966102e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1052/6000 [35:07<2:59:56,  2.18s/it] 18%|â–ˆâ–Š        | 1053/6000 [35:09<2:54:45,  2.12s/it]                                                     {'loss': 0.0573, 'grad_norm': 7.079534530639648, 'learning_rate': 8.384745762711865e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1053/6000 [35:09<2:54:45,  2.12s/it] 18%|â–ˆâ–Š        | 1054/6000 [35:11<2:51:06,  2.08s/it]                                                     {'loss': 0.0523, 'grad_norm': 11.275286674499512, 'learning_rate': 8.383050847457629e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1054/6000 [35:11<2:51:06,  2.08s/it] 18%|â–ˆâ–Š        | 1055/6000 [35:13<2:48:28,  2.04s/it]                                                     {'loss': 0.0633, 'grad_norm': 8.571845054626465, 'learning_rate': 8.381355932203392e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1055/6000 [35:13<2:48:28,  2.04s/it] 18%|â–ˆâ–Š        | 1056/6000 [35:15<2:46:43,  2.02s/it]                                                     {'loss': 0.0226, 'grad_norm': 4.5701093673706055, 'learning_rate': 8.379661016949153e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1056/6000 [35:15<2:46:43,  2.02s/it] 18%|â–ˆâ–Š        | 1057/6000 [35:17<2:44:09,  1.99s/it]                                                     {'loss': 0.0019, 'grad_norm': 0.409428209066391, 'learning_rate': 8.377966101694915e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1057/6000 [35:17<2:44:09,  1.99s/it] 18%|â–ˆâ–Š        | 1058/6000 [35:19<2:41:56,  1.97s/it]                                                     {'loss': 0.0095, 'grad_norm': 3.9447743892669678, 'learning_rate': 8.376271186440678e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1058/6000 [35:19<2:41:56,  1.97s/it] 18%|â–ˆâ–Š        | 1059/6000 [35:21<2:42:06,  1.97s/it]                                                     {'loss': 0.1008, 'grad_norm': 15.582052230834961, 'learning_rate': 8.374576271186442e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1059/6000 [35:21<2:42:06,  1.97s/it] 18%|â–ˆâ–Š        | 1060/6000 [35:23<2:41:26,  1.96s/it]                                                     {'loss': 0.0319, 'grad_norm': 6.514023303985596, 'learning_rate': 8.372881355932205e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1060/6000 [35:23<2:41:26,  1.96s/it] 18%|â–ˆâ–Š        | 1061/6000 [35:25<2:43:24,  1.99s/it]                                                     {'loss': 0.0194, 'grad_norm': 5.525745868682861, 'learning_rate': 8.371186440677966e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1061/6000 [35:25<2:43:24,  1.99s/it] 18%|â–ˆâ–Š        | 1062/6000 [35:27<2:42:19,  1.97s/it]                                                     {'loss': 0.0249, 'grad_norm': 5.306639194488525, 'learning_rate': 8.36949152542373e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1062/6000 [35:27<2:42:19,  1.97s/it] 18%|â–ˆâ–Š        | 1063/6000 [35:29<2:40:57,  1.96s/it]                                                     {'loss': 0.0002, 'grad_norm': 0.03339189291000366, 'learning_rate': 8.367796610169491e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1063/6000 [35:29<2:40:57,  1.96s/it] 18%|â–ˆâ–Š        | 1064/6000 [35:31<2:41:24,  1.96s/it]                                                     {'loss': 0.0267, 'grad_norm': 10.182218551635742, 'learning_rate': 8.366101694915255e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1064/6000 [35:31<2:41:24,  1.96s/it] 18%|â–ˆâ–Š        | 1065/6000 [35:33<2:39:50,  1.94s/it]                                                     {'loss': 0.0625, 'grad_norm': 8.62316608428955, 'learning_rate': 8.364406779661018e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1065/6000 [35:33<2:39:50,  1.94s/it] 18%|â–ˆâ–Š        | 1066/6000 [35:35<2:39:44,  1.94s/it]                                                     {'loss': 0.0564, 'grad_norm': 10.950241088867188, 'learning_rate': 8.362711864406781e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1066/6000 [35:35<2:39:44,  1.94s/it] 18%|â–ˆâ–Š        | 1067/6000 [35:37<2:41:00,  1.96s/it]                                                     {'loss': 0.0853, 'grad_norm': 12.604351043701172, 'learning_rate': 8.361016949152543e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1067/6000 [35:37<2:41:00,  1.96s/it] 18%|â–ˆâ–Š        | 1068/6000 [35:39<2:38:55,  1.93s/it]                                                     {'loss': 0.0212, 'grad_norm': 8.063921928405762, 'learning_rate': 8.359322033898306e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1068/6000 [35:39<2:38:55,  1.93s/it] 18%|â–ˆâ–Š        | 1069/6000 [35:41<2:38:26,  1.93s/it]                                                     {'loss': 0.0113, 'grad_norm': 2.611163377761841, 'learning_rate': 8.357627118644067e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1069/6000 [35:41<2:38:26,  1.93s/it] 18%|â–ˆâ–Š        | 1070/6000 [35:43<2:41:28,  1.97s/it]                                                     {'loss': 0.0828, 'grad_norm': 13.977557182312012, 'learning_rate': 8.35593220338983e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1070/6000 [35:43<2:41:28,  1.97s/it] 18%|â–ˆâ–Š        | 1071/6000 [35:45<2:41:54,  1.97s/it]                                                     {'loss': 0.0019, 'grad_norm': 0.6760914325714111, 'learning_rate': 8.354237288135594e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1071/6000 [35:45<2:41:54,  1.97s/it] 18%|â–ˆâ–Š        | 1072/6000 [35:46<2:40:30,  1.95s/it]                                                     {'loss': 0.0595, 'grad_norm': 9.432819366455078, 'learning_rate': 8.352542372881357e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1072/6000 [35:46<2:40:30,  1.95s/it] 18%|â–ˆâ–Š        | 1073/6000 [35:48<2:41:05,  1.96s/it]                                                     {'loss': 0.0073, 'grad_norm': 3.1672279834747314, 'learning_rate': 8.350847457627119e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1073/6000 [35:48<2:41:05,  1.96s/it] 18%|â–ˆâ–Š        | 1074/6000 [35:50<2:40:54,  1.96s/it]                                                     {'loss': 0.0083, 'grad_norm': 1.6786969900131226, 'learning_rate': 8.349152542372882e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1074/6000 [35:50<2:40:54,  1.96s/it] 18%|â–ˆâ–Š        | 1075/6000 [35:52<2:42:00,  1.97s/it]                                                     {'loss': 0.0826, 'grad_norm': 10.791389465332031, 'learning_rate': 8.347457627118645e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1075/6000 [35:52<2:42:00,  1.97s/it] 18%|â–ˆâ–Š        | 1076/6000 [35:54<2:41:24,  1.97s/it]                                                     {'loss': 0.0116, 'grad_norm': 4.575576305389404, 'learning_rate': 8.345762711864409e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1076/6000 [35:54<2:41:24,  1.97s/it] 18%|â–ˆâ–Š        | 1077/6000 [35:56<2:40:58,  1.96s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.5347860455513, 'learning_rate': 8.34406779661017e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1077/6000 [35:56<2:40:58,  1.96s/it] 18%|â–ˆâ–Š        | 1078/6000 [35:58<2:41:06,  1.96s/it]                                                     {'loss': 0.0174, 'grad_norm': 3.626141309738159, 'learning_rate': 8.342372881355932e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1078/6000 [35:58<2:41:06,  1.96s/it] 18%|â–ˆâ–Š        | 1079/6000 [36:00<2:43:15,  1.99s/it]                                                     {'loss': 0.1339, 'grad_norm': 14.483565330505371, 'learning_rate': 8.340677966101695e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1079/6000 [36:00<2:43:15,  1.99s/it] 18%|â–ˆâ–Š        | 1080/6000 [36:02<2:42:57,  1.99s/it]                                                     {'loss': 0.11, 'grad_norm': 43.918914794921875, 'learning_rate': 8.338983050847458e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1080/6000 [36:02<2:42:57,  1.99s/it] 18%|â–ˆâ–Š        | 1081/6000 [36:04<2:47:01,  2.04s/it]                                                     {'loss': 0.0554, 'grad_norm': 17.651355743408203, 'learning_rate': 8.337288135593222e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1081/6000 [36:04<2:47:01,  2.04s/it] 18%|â–ˆâ–Š        | 1082/6000 [36:06<2:45:34,  2.02s/it]                                                     {'loss': 0.0561, 'grad_norm': 11.570906639099121, 'learning_rate': 8.335593220338983e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1082/6000 [36:06<2:45:34,  2.02s/it] 18%|â–ˆâ–Š        | 1083/6000 [36:08<2:44:22,  2.01s/it]                                                     {'loss': 0.0254, 'grad_norm': 8.849896430969238, 'learning_rate': 8.333898305084747e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1083/6000 [36:08<2:44:22,  2.01s/it] 18%|â–ˆâ–Š        | 1084/6000 [36:10<2:45:12,  2.02s/it]                                                     {'loss': 0.1574, 'grad_norm': 22.203857421875, 'learning_rate': 8.332203389830508e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1084/6000 [36:10<2:45:12,  2.02s/it] 18%|â–ˆâ–Š        | 1085/6000 [36:12<2:43:57,  2.00s/it]                                                     {'loss': 0.035, 'grad_norm': 4.34930944442749, 'learning_rate': 8.330508474576271e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1085/6000 [36:12<2:43:57,  2.00s/it] 18%|â–ˆâ–Š        | 1086/6000 [36:14<2:44:03,  2.00s/it]                                                     {'loss': 0.0821, 'grad_norm': 10.863903045654297, 'learning_rate': 8.328813559322035e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1086/6000 [36:14<2:44:03,  2.00s/it] 18%|â–ˆâ–Š        | 1087/6000 [36:16<2:44:55,  2.01s/it]                                                     {'loss': 0.0222, 'grad_norm': 5.057217121124268, 'learning_rate': 8.327118644067798e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1087/6000 [36:16<2:44:55,  2.01s/it] 18%|â–ˆâ–Š        | 1088/6000 [36:18<2:43:28,  2.00s/it]                                                     {'loss': 0.091, 'grad_norm': 13.293524742126465, 'learning_rate': 8.32542372881356e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1088/6000 [36:18<2:43:28,  2.00s/it] 18%|â–ˆâ–Š        | 1089/6000 [36:21<2:51:47,  2.10s/it]                                                     {'loss': 0.2348, 'grad_norm': 21.026437759399414, 'learning_rate': 8.323728813559323e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1089/6000 [36:21<2:51:47,  2.10s/it] 18%|â–ˆâ–Š        | 1090/6000 [36:23<2:48:19,  2.06s/it]                                                     {'loss': 0.115, 'grad_norm': 11.940138816833496, 'learning_rate': 8.322033898305086e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1090/6000 [36:23<2:48:19,  2.06s/it] 18%|â–ˆâ–Š        | 1091/6000 [36:25<2:46:01,  2.03s/it]                                                     {'loss': 0.005, 'grad_norm': 1.0587742328643799, 'learning_rate': 8.32033898305085e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1091/6000 [36:25<2:46:01,  2.03s/it] 18%|â–ˆâ–Š        | 1092/6000 [36:27<2:44:45,  2.01s/it]                                                     {'loss': 0.2158, 'grad_norm': 21.615324020385742, 'learning_rate': 8.318644067796611e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1092/6000 [36:27<2:44:45,  2.01s/it] 18%|â–ˆâ–Š        | 1093/6000 [36:29<2:42:22,  1.99s/it]                                                     {'loss': 0.0006, 'grad_norm': 0.14200037717819214, 'learning_rate': 8.316949152542374e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1093/6000 [36:29<2:42:22,  1.99s/it] 18%|â–ˆâ–Š        | 1094/6000 [36:30<2:40:15,  1.96s/it]                                                     {'loss': 0.1008, 'grad_norm': 6.7243804931640625, 'learning_rate': 8.315254237288136e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1094/6000 [36:30<2:40:15,  1.96s/it] 18%|â–ˆâ–Š        | 1095/6000 [36:32<2:40:49,  1.97s/it]                                                     {'loss': 0.0074, 'grad_norm': 0.8649752140045166, 'learning_rate': 8.313559322033899e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1095/6000 [36:32<2:40:49,  1.97s/it] 18%|â–ˆâ–Š        | 1096/6000 [36:35<2:47:37,  2.05s/it]                                                     {'loss': 0.0445, 'grad_norm': 7.2629170417785645, 'learning_rate': 8.311864406779662e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1096/6000 [36:35<2:47:37,  2.05s/it] 18%|â–ˆâ–Š        | 1097/6000 [36:37<2:46:00,  2.03s/it]                                                     {'loss': 0.0295, 'grad_norm': 6.596240043640137, 'learning_rate': 8.310169491525426e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1097/6000 [36:37<2:46:00,  2.03s/it] 18%|â–ˆâ–Š        | 1098/6000 [36:39<2:49:03,  2.07s/it]                                                     {'loss': 0.1236, 'grad_norm': 17.03481101989746, 'learning_rate': 8.308474576271187e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1098/6000 [36:39<2:49:03,  2.07s/it] 18%|â–ˆâ–Š        | 1099/6000 [36:41<2:52:20,  2.11s/it]                                                     {'loss': 0.029, 'grad_norm': 9.663030624389648, 'learning_rate': 8.306779661016949e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1099/6000 [36:41<2:52:20,  2.11s/it] 18%|â–ˆâ–Š        | 1100/6000 [36:43<2:47:26,  2.05s/it]                                                     {'loss': 0.1239, 'grad_norm': 16.0170955657959, 'learning_rate': 8.305084745762712e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1100/6000 [36:43<2:47:26,  2.05s/it][2025-11-18 10:26:40,851] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1100
[2025-11-18 10:26:41,127] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1100/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 18%|â–ˆâ–Š        | 1101/6000 [36:46<3:03:16,  2.24s/it]                                                     {'loss': 0.0975, 'grad_norm': 16.608036041259766, 'learning_rate': 8.303389830508475e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1101/6000 [36:46<3:03:16,  2.24s/it] 18%|â–ˆâ–Š        | 1102/6000 [36:48<2:55:36,  2.15s/it]                                                     {'loss': 0.0355, 'grad_norm': 8.57229995727539, 'learning_rate': 8.301694915254239e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1102/6000 [36:48<2:55:36,  2.15s/it] 18%|â–ˆâ–Š        | 1103/6000 [36:50<2:50:25,  2.09s/it]                                                     {'loss': 0.0717, 'grad_norm': 8.677226066589355, 'learning_rate': 8.3e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1103/6000 [36:50<2:50:25,  2.09s/it] 18%|â–ˆâ–Š        | 1104/6000 [36:52<2:47:25,  2.05s/it]                                                     {'loss': 0.0177, 'grad_norm': 5.7591872215271, 'learning_rate': 8.298305084745763e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1104/6000 [36:52<2:47:25,  2.05s/it] 18%|â–ˆâ–Š        | 1105/6000 [36:53<2:44:25,  2.02s/it]                                                     {'loss': 0.0024, 'grad_norm': 0.513542652130127, 'learning_rate': 8.296610169491525e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1105/6000 [36:53<2:44:25,  2.02s/it] 18%|â–ˆâ–Š        | 1106/6000 [36:55<2:43:01,  2.00s/it]                                                     {'loss': 0.0533, 'grad_norm': 8.626096725463867, 'learning_rate': 8.294915254237288e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1106/6000 [36:55<2:43:01,  2.00s/it] 18%|â–ˆâ–Š        | 1107/6000 [36:57<2:41:51,  1.98s/it]                                                     {'loss': 0.0057, 'grad_norm': 1.1705348491668701, 'learning_rate': 8.293220338983052e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1107/6000 [36:57<2:41:51,  1.98s/it] 18%|â–ˆâ–Š        | 1108/6000 [36:59<2:42:37,  1.99s/it]                                                     {'loss': 0.0248, 'grad_norm': 5.697868347167969, 'learning_rate': 8.291525423728815e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1108/6000 [36:59<2:42:37,  1.99s/it] 18%|â–ˆâ–Š        | 1109/6000 [37:01<2:41:58,  1.99s/it]                                                     {'loss': 0.029, 'grad_norm': 3.380903959274292, 'learning_rate': 8.289830508474576e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1109/6000 [37:01<2:41:58,  1.99s/it] 18%|â–ˆâ–Š        | 1110/6000 [37:03<2:40:45,  1.97s/it]                                                     {'loss': 0.1805, 'grad_norm': 14.939471244812012, 'learning_rate': 8.28813559322034e-06, 'epoch': 0.18}
 18%|â–ˆâ–Š        | 1110/6000 [37:03<2:40:45,  1.97s/it] 19%|â–ˆâ–Š        | 1111/6000 [37:05<2:39:26,  1.96s/it]                                                     {'loss': 0.0999, 'grad_norm': 10.61513900756836, 'learning_rate': 8.286440677966103e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1111/6000 [37:05<2:39:26,  1.96s/it] 19%|â–ˆâ–Š        | 1112/6000 [37:07<2:39:04,  1.95s/it]                                                     {'loss': 0.0041, 'grad_norm': 0.9870720505714417, 'learning_rate': 8.284745762711866e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1112/6000 [37:07<2:39:04,  1.95s/it] 19%|â–ˆâ–Š        | 1113/6000 [37:09<2:38:32,  1.95s/it]                                                     {'loss': 0.023, 'grad_norm': 4.705002784729004, 'learning_rate': 8.283050847457628e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1113/6000 [37:09<2:38:32,  1.95s/it] 19%|â–ˆâ–Š        | 1114/6000 [37:11<2:37:42,  1.94s/it]                                                     {'loss': 0.1176, 'grad_norm': 11.27000904083252, 'learning_rate': 8.281355932203391e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1114/6000 [37:11<2:37:42,  1.94s/it] 19%|â–ˆâ–Š        | 1115/6000 [37:13<2:39:02,  1.95s/it]                                                     {'loss': 0.1, 'grad_norm': 10.193633079528809, 'learning_rate': 8.279661016949153e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1115/6000 [37:13<2:39:02,  1.95s/it] 19%|â–ˆâ–Š        | 1116/6000 [37:15<2:39:28,  1.96s/it]                                                     {'loss': 0.2424, 'grad_norm': 22.471094131469727, 'learning_rate': 8.277966101694916e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1116/6000 [37:15<2:39:28,  1.96s/it] 19%|â–ˆâ–Š        | 1117/6000 [37:17<2:40:30,  1.97s/it]                                                     {'loss': 0.0054, 'grad_norm': 1.578442931175232, 'learning_rate': 8.27627118644068e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1117/6000 [37:17<2:40:30,  1.97s/it] 19%|â–ˆâ–Š        | 1118/6000 [37:19<2:40:40,  1.97s/it]                                                     {'loss': 0.4881, 'grad_norm': 25.229339599609375, 'learning_rate': 8.27457627118644e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1118/6000 [37:19<2:40:40,  1.97s/it] 19%|â–ˆâ–Š        | 1119/6000 [37:21<2:41:46,  1.99s/it]                                                     {'loss': 0.049, 'grad_norm': 10.642366409301758, 'learning_rate': 8.272881355932204e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1119/6000 [37:21<2:41:46,  1.99s/it] 19%|â–ˆâ–Š        | 1120/6000 [37:23<2:40:57,  1.98s/it]                                                     {'loss': 0.0065, 'grad_norm': 2.0956389904022217, 'learning_rate': 8.271186440677966e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1120/6000 [37:23<2:40:57,  1.98s/it] 19%|â–ˆâ–Š        | 1121/6000 [37:25<2:40:44,  1.98s/it]                                                     {'loss': 0.0404, 'grad_norm': 5.828619956970215, 'learning_rate': 8.269491525423729e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1121/6000 [37:25<2:40:44,  1.98s/it] 19%|â–ˆâ–Š        | 1122/6000 [37:27<2:39:45,  1.97s/it]                                                     {'loss': 0.0038, 'grad_norm': 1.421580195426941, 'learning_rate': 8.267796610169492e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1122/6000 [37:27<2:39:45,  1.97s/it] 19%|â–ˆâ–Š        | 1123/6000 [37:29<2:40:48,  1.98s/it]                                                     {'loss': 0.0026, 'grad_norm': 0.4814796447753906, 'learning_rate': 8.266101694915255e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1123/6000 [37:29<2:40:48,  1.98s/it] 19%|â–ˆâ–Š        | 1124/6000 [37:31<2:42:35,  2.00s/it]                                                     {'loss': 0.0766, 'grad_norm': 16.57442283630371, 'learning_rate': 8.264406779661017e-06, 'epoch': 0.19}
 19%|â–ˆâ–Š        | 1124/6000 [37:31<2:42:35,  2.00s/it] 19%|â–ˆâ–‰        | 1125/6000 [37:33<2:42:46,  2.00s/it]                                                     {'loss': 0.1005, 'grad_norm': 11.450223922729492, 'learning_rate': 8.26271186440678e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1125/6000 [37:33<2:42:46,  2.00s/it] 19%|â–ˆâ–‰        | 1126/6000 [37:35<2:41:16,  1.99s/it]                                                     {'loss': 0.4163, 'grad_norm': 20.65308380126953, 'learning_rate': 8.261016949152542e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1126/6000 [37:35<2:41:16,  1.99s/it] 19%|â–ˆâ–‰        | 1127/6000 [37:37<2:40:02,  1.97s/it]                                                     {'loss': 0.1296, 'grad_norm': 19.93459701538086, 'learning_rate': 8.259322033898305e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1127/6000 [37:37<2:40:02,  1.97s/it] 19%|â–ˆâ–‰        | 1128/6000 [37:39<2:39:41,  1.97s/it]                                                     {'loss': 0.001, 'grad_norm': 0.25770658254623413, 'learning_rate': 8.257627118644068e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1128/6000 [37:39<2:39:41,  1.97s/it] 19%|â–ˆâ–‰        | 1129/6000 [37:41<2:38:22,  1.95s/it]                                                     {'loss': 0.0381, 'grad_norm': 5.43861722946167, 'learning_rate': 8.255932203389832e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1129/6000 [37:41<2:38:22,  1.95s/it] 19%|â–ˆâ–‰        | 1130/6000 [37:43<2:38:50,  1.96s/it]                                                     {'loss': 0.0647, 'grad_norm': 6.0888237953186035, 'learning_rate': 8.254237288135593e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1130/6000 [37:43<2:38:50,  1.96s/it] 19%|â–ˆâ–‰        | 1131/6000 [37:45<2:37:11,  1.94s/it]                                                     {'loss': 0.2114, 'grad_norm': 17.60530662536621, 'learning_rate': 8.252542372881357e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1131/6000 [37:45<2:37:11,  1.94s/it] 19%|â–ˆâ–‰        | 1132/6000 [37:46<2:37:06,  1.94s/it]                                                     {'loss': 0.0192, 'grad_norm': 2.3596808910369873, 'learning_rate': 8.25084745762712e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1132/6000 [37:46<2:37:06,  1.94s/it] 19%|â–ˆâ–‰        | 1133/6000 [37:49<2:41:29,  1.99s/it]                                                     {'loss': 0.0111, 'grad_norm': 3.2102999687194824, 'learning_rate': 8.249152542372883e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1133/6000 [37:49<2:41:29,  1.99s/it] 19%|â–ˆâ–‰        | 1134/6000 [37:51<2:43:58,  2.02s/it]                                                     {'loss': 0.0121, 'grad_norm': 2.763705015182495, 'learning_rate': 8.247457627118645e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1134/6000 [37:51<2:43:58,  2.02s/it] 19%|â–ˆâ–‰        | 1135/6000 [37:53<2:42:51,  2.01s/it]                                                     {'loss': 0.0649, 'grad_norm': 12.390478134155273, 'learning_rate': 8.245762711864408e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1135/6000 [37:53<2:42:51,  2.01s/it] 19%|â–ˆâ–‰        | 1136/6000 [37:55<2:41:50,  2.00s/it]                                                     {'loss': 0.0061, 'grad_norm': 2.2596094608306885, 'learning_rate': 8.24406779661017e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1136/6000 [37:55<2:41:50,  2.00s/it] 19%|â–ˆâ–‰        | 1137/6000 [37:57<2:41:20,  1.99s/it]                                                     {'loss': 0.0036, 'grad_norm': 0.6765555143356323, 'learning_rate': 8.242372881355933e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1137/6000 [37:57<2:41:20,  1.99s/it] 19%|â–ˆâ–‰        | 1138/6000 [37:59<2:40:12,  1.98s/it]                                                     {'loss': 0.047, 'grad_norm': 10.878971099853516, 'learning_rate': 8.240677966101696e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1138/6000 [37:59<2:40:12,  1.98s/it] 19%|â–ˆâ–‰        | 1139/6000 [38:01<2:40:35,  1.98s/it]                                                     {'loss': 0.0081, 'grad_norm': 1.4416354894638062, 'learning_rate': 8.238983050847458e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1139/6000 [38:01<2:40:35,  1.98s/it] 19%|â–ˆâ–‰        | 1140/6000 [38:03<2:42:20,  2.00s/it]                                                     {'loss': 0.1281, 'grad_norm': 18.632001876831055, 'learning_rate': 8.237288135593221e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1140/6000 [38:03<2:42:20,  2.00s/it] 19%|â–ˆâ–‰        | 1141/6000 [38:05<2:41:23,  1.99s/it]                                                     {'loss': 0.2573, 'grad_norm': 17.15337371826172, 'learning_rate': 8.235593220338983e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1141/6000 [38:05<2:41:23,  1.99s/it] 19%|â–ˆâ–‰        | 1142/6000 [38:07<2:41:24,  1.99s/it]                                                     {'loss': 0.05, 'grad_norm': 11.032102584838867, 'learning_rate': 8.233898305084746e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1142/6000 [38:07<2:41:24,  1.99s/it] 19%|â–ˆâ–‰        | 1143/6000 [38:08<2:39:57,  1.98s/it]                                                     {'loss': 0.2457, 'grad_norm': 16.499788284301758, 'learning_rate': 8.232203389830509e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1143/6000 [38:08<2:39:57,  1.98s/it] 19%|â–ˆâ–‰        | 1144/6000 [38:10<2:39:41,  1.97s/it]                                                     {'loss': 0.0229, 'grad_norm': 3.1659998893737793, 'learning_rate': 8.230508474576272e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1144/6000 [38:10<2:39:41,  1.97s/it] 19%|â–ˆâ–‰        | 1145/6000 [38:12<2:39:56,  1.98s/it]                                                     {'loss': 0.0722, 'grad_norm': 9.758248329162598, 'learning_rate': 8.228813559322034e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1145/6000 [38:12<2:39:56,  1.98s/it] 19%|â–ˆâ–‰        | 1146/6000 [38:14<2:41:03,  1.99s/it]                                                     {'loss': 0.0006, 'grad_norm': 0.21072223782539368, 'learning_rate': 8.227118644067797e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1146/6000 [38:14<2:41:03,  1.99s/it] 19%|â–ˆâ–‰        | 1147/6000 [38:16<2:39:33,  1.97s/it]                                                     {'loss': 0.0726, 'grad_norm': 11.369832992553711, 'learning_rate': 8.22542372881356e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1147/6000 [38:16<2:39:33,  1.97s/it] 19%|â–ˆâ–‰        | 1148/6000 [38:18<2:38:30,  1.96s/it]                                                     {'loss': 0.2382, 'grad_norm': 24.017894744873047, 'learning_rate': 8.223728813559324e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1148/6000 [38:18<2:38:30,  1.96s/it] 19%|â–ˆâ–‰        | 1149/6000 [38:20<2:40:41,  1.99s/it]                                                     {'loss': 0.1223, 'grad_norm': 10.594854354858398, 'learning_rate': 8.222033898305085e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1149/6000 [38:20<2:40:41,  1.99s/it] 19%|â–ˆâ–‰        | 1150/6000 [38:22<2:39:45,  1.98s/it]                                                     {'loss': 0.0328, 'grad_norm': 7.161163330078125, 'learning_rate': 8.220338983050849e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1150/6000 [38:22<2:39:45,  1.98s/it][2025-11-18 10:28:20,189] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1150
[2025-11-18 10:28:20,487] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1150/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 19%|â–ˆâ–‰        | 1151/6000 [38:25<2:57:34,  2.20s/it]                                                     {'loss': 0.0778, 'grad_norm': 9.324281692504883, 'learning_rate': 8.21864406779661e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1151/6000 [38:25<2:57:34,  2.20s/it] 19%|â–ˆâ–‰        | 1152/6000 [38:27<2:51:13,  2.12s/it]                                                     {'loss': 0.001, 'grad_norm': 0.47333791851997375, 'learning_rate': 8.216949152542373e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1152/6000 [38:27<2:51:13,  2.12s/it] 19%|â–ˆâ–‰        | 1153/6000 [38:29<2:48:06,  2.08s/it]                                                     {'loss': 0.0005, 'grad_norm': 0.13075590133666992, 'learning_rate': 8.215254237288137e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1153/6000 [38:29<2:48:06,  2.08s/it] 19%|â–ˆâ–‰        | 1154/6000 [38:31<2:44:52,  2.04s/it]                                                     {'loss': 0.0322, 'grad_norm': 5.94097900390625, 'learning_rate': 8.2135593220339e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1154/6000 [38:31<2:44:52,  2.04s/it] 19%|â–ˆâ–‰        | 1155/6000 [38:33<2:43:21,  2.02s/it]                                                     {'loss': 0.0082, 'grad_norm': 1.6844980716705322, 'learning_rate': 8.211864406779662e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1155/6000 [38:33<2:43:21,  2.02s/it] 19%|â–ˆâ–‰        | 1156/6000 [38:35<2:41:37,  2.00s/it]                                                     {'loss': 0.2205, 'grad_norm': 19.05209732055664, 'learning_rate': 8.210169491525425e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1156/6000 [38:35<2:41:37,  2.00s/it] 19%|â–ˆâ–‰        | 1157/6000 [38:37<2:41:13,  2.00s/it]                                                     {'loss': 0.0705, 'grad_norm': 10.72884464263916, 'learning_rate': 8.208474576271186e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1157/6000 [38:37<2:41:13,  2.00s/it] 19%|â–ˆâ–‰        | 1158/6000 [38:39<2:40:26,  1.99s/it]                                                     {'loss': 0.0151, 'grad_norm': 4.938472747802734, 'learning_rate': 8.20677966101695e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1158/6000 [38:39<2:40:26,  1.99s/it] 19%|â–ˆâ–‰        | 1159/6000 [38:41<2:40:31,  1.99s/it]                                                     {'loss': 0.1073, 'grad_norm': 15.462769508361816, 'learning_rate': 8.205084745762713e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1159/6000 [38:41<2:40:31,  1.99s/it] 19%|â–ˆâ–‰        | 1160/6000 [38:43<2:39:16,  1.97s/it]                                                     {'loss': 0.0003, 'grad_norm': 0.07464484125375748, 'learning_rate': 8.203389830508475e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1160/6000 [38:43<2:39:16,  1.97s/it] 19%|â–ˆâ–‰        | 1161/6000 [38:45<2:37:56,  1.96s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.3634810745716095, 'learning_rate': 8.201694915254238e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1161/6000 [38:45<2:37:56,  1.96s/it] 19%|â–ˆâ–‰        | 1162/6000 [38:47<2:37:19,  1.95s/it]                                                     {'loss': 0.0068, 'grad_norm': 0.9186376333236694, 'learning_rate': 8.2e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1162/6000 [38:47<2:37:19,  1.95s/it] 19%|â–ˆâ–‰        | 1163/6000 [38:49<2:37:51,  1.96s/it]                                                     {'loss': 0.0149, 'grad_norm': 4.210061550140381, 'learning_rate': 8.198305084745763e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1163/6000 [38:49<2:37:51,  1.96s/it] 19%|â–ˆâ–‰        | 1164/6000 [38:51<2:41:11,  2.00s/it]                                                     {'loss': 0.1569, 'grad_norm': 14.284306526184082, 'learning_rate': 8.196610169491526e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1164/6000 [38:51<2:41:11,  2.00s/it] 19%|â–ˆâ–‰        | 1165/6000 [38:53<2:42:52,  2.02s/it]                                                     {'loss': 0.0343, 'grad_norm': 5.570511817932129, 'learning_rate': 8.19491525423729e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1165/6000 [38:53<2:42:52,  2.02s/it] 19%|â–ˆâ–‰        | 1166/6000 [38:55<2:40:46,  2.00s/it]                                                     {'loss': 0.0764, 'grad_norm': 8.847259521484375, 'learning_rate': 8.19322033898305e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1166/6000 [38:55<2:40:46,  2.00s/it] 19%|â–ˆâ–‰        | 1167/6000 [38:57<2:43:52,  2.03s/it]                                                     {'loss': 0.185, 'grad_norm': 13.699352264404297, 'learning_rate': 8.191525423728814e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1167/6000 [38:57<2:43:52,  2.03s/it] 19%|â–ˆâ–‰        | 1168/6000 [38:59<2:41:17,  2.00s/it]                                                     {'loss': 0.0624, 'grad_norm': 12.027331352233887, 'learning_rate': 8.189830508474577e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1168/6000 [38:59<2:41:17,  2.00s/it] 19%|â–ˆâ–‰        | 1169/6000 [39:01<2:39:53,  1.99s/it]                                                     {'loss': 0.1092, 'grad_norm': 13.163174629211426, 'learning_rate': 8.18813559322034e-06, 'epoch': 0.19}
 19%|â–ˆâ–‰        | 1169/6000 [39:01<2:39:53,  1.99s/it] 20%|â–ˆâ–‰        | 1170/6000 [39:03<2:40:19,  1.99s/it]                                                     {'loss': 0.0102, 'grad_norm': 2.9258265495300293, 'learning_rate': 8.186440677966102e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1170/6000 [39:03<2:40:19,  1.99s/it] 20%|â–ˆâ–‰        | 1171/6000 [39:05<2:41:49,  2.01s/it]                                                     {'loss': 0.0286, 'grad_norm': 4.690229892730713, 'learning_rate': 8.184745762711865e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1171/6000 [39:05<2:41:49,  2.01s/it] 20%|â–ˆâ–‰        | 1172/6000 [39:07<2:41:45,  2.01s/it]                                                     {'loss': 0.0337, 'grad_norm': 10.531656265258789, 'learning_rate': 8.183050847457627e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1172/6000 [39:07<2:41:45,  2.01s/it] 20%|â–ˆâ–‰        | 1173/6000 [39:09<2:41:17,  2.00s/it]                                                     {'loss': 0.0479, 'grad_norm': 6.967640399932861, 'learning_rate': 8.18135593220339e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1173/6000 [39:09<2:41:17,  2.00s/it] 20%|â–ˆâ–‰        | 1174/6000 [39:11<2:40:42,  2.00s/it]                                                     {'loss': 0.1986, 'grad_norm': 17.486310958862305, 'learning_rate': 8.179661016949154e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1174/6000 [39:11<2:40:42,  2.00s/it] 20%|â–ˆâ–‰        | 1175/6000 [39:13<2:43:20,  2.03s/it]                                                     {'loss': 0.0126, 'grad_norm': 3.727965831756592, 'learning_rate': 8.177966101694917e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1175/6000 [39:13<2:43:20,  2.03s/it] 20%|â–ˆâ–‰        | 1176/6000 [39:15<2:40:16,  1.99s/it]                                                     {'loss': 0.004, 'grad_norm': 0.8831183910369873, 'learning_rate': 8.176271186440678e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1176/6000 [39:15<2:40:16,  1.99s/it] 20%|â–ˆâ–‰        | 1177/6000 [39:17<2:41:35,  2.01s/it]                                                     {'loss': 0.0795, 'grad_norm': 10.783467292785645, 'learning_rate': 8.174576271186442e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1177/6000 [39:17<2:41:35,  2.01s/it] 20%|â–ˆâ–‰        | 1178/6000 [39:19<2:43:33,  2.04s/it]                                                     {'loss': 0.0498, 'grad_norm': 3.833599805831909, 'learning_rate': 8.172881355932203e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1178/6000 [39:19<2:43:33,  2.04s/it] 20%|â–ˆâ–‰        | 1179/6000 [39:21<2:42:28,  2.02s/it]                                                     {'loss': 0.0595, 'grad_norm': 10.0455322265625, 'learning_rate': 8.171186440677967e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1179/6000 [39:21<2:42:28,  2.02s/it] 20%|â–ˆâ–‰        | 1180/6000 [39:23<2:43:52,  2.04s/it]                                                     {'loss': 0.0429, 'grad_norm': 4.118849754333496, 'learning_rate': 8.16949152542373e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1180/6000 [39:23<2:43:52,  2.04s/it] 20%|â–ˆâ–‰        | 1181/6000 [39:25<2:42:23,  2.02s/it]                                                     {'loss': 0.0228, 'grad_norm': 4.807918548583984, 'learning_rate': 8.167796610169491e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1181/6000 [39:25<2:42:23,  2.02s/it] 20%|â–ˆâ–‰        | 1182/6000 [39:27<2:49:29,  2.11s/it]                                                     {'loss': 0.0055, 'grad_norm': 1.1885775327682495, 'learning_rate': 8.166101694915255e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1182/6000 [39:27<2:49:29,  2.11s/it] 20%|â–ˆâ–‰        | 1183/6000 [39:29<2:44:12,  2.05s/it]                                                     {'loss': 0.0073, 'grad_norm': 2.646259069442749, 'learning_rate': 8.164406779661016e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1183/6000 [39:29<2:44:12,  2.05s/it] 20%|â–ˆâ–‰        | 1184/6000 [39:31<2:41:08,  2.01s/it]                                                     {'loss': 0.1797, 'grad_norm': 12.133389472961426, 'learning_rate': 8.162711864406781e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1184/6000 [39:31<2:41:08,  2.01s/it] 20%|â–ˆâ–‰        | 1185/6000 [39:33<2:38:58,  1.98s/it]                                                     {'loss': 0.1416, 'grad_norm': 15.640451431274414, 'learning_rate': 8.161016949152543e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1185/6000 [39:33<2:38:58,  1.98s/it] 20%|â–ˆâ–‰        | 1186/6000 [39:35<2:38:32,  1.98s/it]                                                     {'loss': 0.1911, 'grad_norm': 10.206672668457031, 'learning_rate': 8.159322033898306e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1186/6000 [39:35<2:38:32,  1.98s/it] 20%|â–ˆâ–‰        | 1187/6000 [39:37<2:39:57,  1.99s/it]                                                     {'loss': 0.0147, 'grad_norm': 1.7186357975006104, 'learning_rate': 8.157627118644068e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1187/6000 [39:37<2:39:57,  1.99s/it] 20%|â–ˆâ–‰        | 1188/6000 [39:39<2:37:23,  1.96s/it]                                                     {'loss': 0.0255, 'grad_norm': 5.496838092803955, 'learning_rate': 8.155932203389831e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1188/6000 [39:39<2:37:23,  1.96s/it] 20%|â–ˆâ–‰        | 1189/6000 [39:41<2:36:54,  1.96s/it]                                                     {'loss': 0.0699, 'grad_norm': 12.696117401123047, 'learning_rate': 8.154237288135594e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1189/6000 [39:41<2:36:54,  1.96s/it] 20%|â–ˆâ–‰        | 1190/6000 [39:43<2:39:49,  1.99s/it]                                                     {'loss': 0.0082, 'grad_norm': 1.559119701385498, 'learning_rate': 8.152542372881358e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1190/6000 [39:43<2:39:49,  1.99s/it] 20%|â–ˆâ–‰        | 1191/6000 [39:45<2:38:06,  1.97s/it]                                                     {'loss': 0.0086, 'grad_norm': 1.7683212757110596, 'learning_rate': 8.150847457627119e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1191/6000 [39:45<2:38:06,  1.97s/it] 20%|â–ˆâ–‰        | 1192/6000 [39:47<2:38:24,  1.98s/it]                                                     {'loss': 0.0198, 'grad_norm': 4.404068470001221, 'learning_rate': 8.149152542372882e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1192/6000 [39:47<2:38:24,  1.98s/it] 20%|â–ˆâ–‰        | 1193/6000 [39:49<2:37:13,  1.96s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.6444098949432373, 'learning_rate': 8.147457627118644e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1193/6000 [39:49<2:37:13,  1.96s/it] 20%|â–ˆâ–‰        | 1194/6000 [39:51<2:38:46,  1.98s/it]                                                     {'loss': 0.0585, 'grad_norm': 9.948025703430176, 'learning_rate': 8.145762711864407e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1194/6000 [39:51<2:38:46,  1.98s/it] 20%|â–ˆâ–‰        | 1195/6000 [39:53<2:37:51,  1.97s/it]                                                     {'loss': 0.0035, 'grad_norm': 1.1528890132904053, 'learning_rate': 8.14406779661017e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1195/6000 [39:53<2:37:51,  1.97s/it] 20%|â–ˆâ–‰        | 1196/6000 [39:55<2:36:17,  1.95s/it]                                                     {'loss': 0.0013, 'grad_norm': 0.29762354493141174, 'learning_rate': 8.142372881355934e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1196/6000 [39:55<2:36:17,  1.95s/it] 20%|â–ˆâ–‰        | 1197/6000 [39:57<2:37:28,  1.97s/it]                                                     {'loss': 0.0232, 'grad_norm': 4.781428813934326, 'learning_rate': 8.140677966101695e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1197/6000 [39:57<2:37:28,  1.97s/it] 20%|â–ˆâ–‰        | 1198/6000 [39:59<2:36:38,  1.96s/it]                                                     {'loss': 0.078, 'grad_norm': 11.916569709777832, 'learning_rate': 8.138983050847459e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1198/6000 [39:59<2:36:38,  1.96s/it] 20%|â–ˆâ–‰        | 1199/6000 [40:00<2:36:13,  1.95s/it]                                                     {'loss': 0.0939, 'grad_norm': 12.071870803833008, 'learning_rate': 8.13728813559322e-06, 'epoch': 0.2}
 20%|â–ˆâ–‰        | 1199/6000 [40:00<2:36:13,  1.95s/it] 20%|â–ˆâ–ˆ        | 1200/6000 [40:02<2:35:30,  1.94s/it]                                                     {'loss': 0.199, 'grad_norm': 17.092147827148438, 'learning_rate': 8.135593220338983e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1200/6000 [40:02<2:35:30,  1.94s/it][2025-11-18 10:30:00,268] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1200
[2025-11-18 10:30:00,568] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1200/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 20%|â–ˆâ–ˆ        | 1201/6000 [40:05<2:55:20,  2.19s/it]                                                     {'loss': 0.2084, 'grad_norm': 11.305964469909668, 'learning_rate': 8.133898305084747e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1201/6000 [40:05<2:55:20,  2.19s/it] 20%|â–ˆâ–ˆ        | 1202/6000 [40:07<2:50:04,  2.13s/it]                                                     {'loss': 0.0273, 'grad_norm': 9.627575874328613, 'learning_rate': 8.132203389830508e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1202/6000 [40:07<2:50:04,  2.13s/it] 20%|â–ˆâ–ˆ        | 1203/6000 [40:09<2:45:48,  2.07s/it]                                                     {'loss': 0.0806, 'grad_norm': 11.788065910339355, 'learning_rate': 8.130508474576272e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1203/6000 [40:09<2:45:48,  2.07s/it] 20%|â–ˆâ–ˆ        | 1204/6000 [40:11<2:45:52,  2.08s/it]                                                     {'loss': 0.0027, 'grad_norm': 0.42247509956359863, 'learning_rate': 8.128813559322035e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1204/6000 [40:11<2:45:52,  2.08s/it] 20%|â–ˆâ–ˆ        | 1205/6000 [40:13<2:44:11,  2.05s/it]                                                     {'loss': 0.1565, 'grad_norm': 11.581912994384766, 'learning_rate': 8.127118644067798e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1205/6000 [40:13<2:44:11,  2.05s/it] 20%|â–ˆâ–ˆ        | 1206/6000 [40:15<2:42:37,  2.04s/it]                                                     {'loss': 0.0286, 'grad_norm': 5.715898513793945, 'learning_rate': 8.12542372881356e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1206/6000 [40:15<2:42:37,  2.04s/it] 20%|â–ˆâ–ˆ        | 1207/6000 [40:17<2:41:36,  2.02s/it]                                                     {'loss': 0.0629, 'grad_norm': 6.324131488800049, 'learning_rate': 8.123728813559323e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1207/6000 [40:17<2:41:36,  2.02s/it] 20%|â–ˆâ–ˆ        | 1208/6000 [40:19<2:40:15,  2.01s/it]                                                     {'loss': 0.0531, 'grad_norm': 4.222176551818848, 'learning_rate': 8.122033898305085e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1208/6000 [40:19<2:40:15,  2.01s/it] 20%|â–ˆâ–ˆ        | 1209/6000 [40:21<2:38:28,  1.98s/it]                                                     {'loss': 0.0116, 'grad_norm': 2.10034441947937, 'learning_rate': 8.120338983050848e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1209/6000 [40:21<2:38:28,  1.98s/it] 20%|â–ˆâ–ˆ        | 1210/6000 [40:23<2:38:10,  1.98s/it]                                                     {'loss': 0.2155, 'grad_norm': 18.8541316986084, 'learning_rate': 8.118644067796611e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1210/6000 [40:23<2:38:10,  1.98s/it] 20%|â–ˆâ–ˆ        | 1211/6000 [40:25<2:37:32,  1.97s/it]                                                     {'loss': 0.0167, 'grad_norm': 2.7303521633148193, 'learning_rate': 8.116949152542374e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1211/6000 [40:25<2:37:32,  1.97s/it] 20%|â–ˆâ–ˆ        | 1212/6000 [40:27<2:37:11,  1.97s/it]                                                     {'loss': 0.0873, 'grad_norm': 11.963428497314453, 'learning_rate': 8.115254237288136e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1212/6000 [40:27<2:37:11,  1.97s/it] 20%|â–ˆâ–ˆ        | 1213/6000 [40:29<2:36:16,  1.96s/it]                                                     {'loss': 0.1259, 'grad_norm': 15.589301109313965, 'learning_rate': 8.1135593220339e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1213/6000 [40:29<2:36:16,  1.96s/it] 20%|â–ˆâ–ˆ        | 1214/6000 [40:31<2:38:35,  1.99s/it]                                                     {'loss': 0.0146, 'grad_norm': 2.4701318740844727, 'learning_rate': 8.111864406779661e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1214/6000 [40:31<2:38:35,  1.99s/it] 20%|â–ˆâ–ˆ        | 1215/6000 [40:33<2:38:43,  1.99s/it]                                                     {'loss': 0.0365, 'grad_norm': 7.514610290527344, 'learning_rate': 8.110169491525424e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1215/6000 [40:33<2:38:43,  1.99s/it] 20%|â–ˆâ–ˆ        | 1216/6000 [40:35<2:37:31,  1.98s/it]                                                     {'loss': 0.0447, 'grad_norm': 5.7545013427734375, 'learning_rate': 8.108474576271187e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1216/6000 [40:35<2:37:31,  1.98s/it] 20%|â–ˆâ–ˆ        | 1217/6000 [40:37<2:39:33,  2.00s/it]                                                     {'loss': 0.1288, 'grad_norm': 10.764464378356934, 'learning_rate': 8.10677966101695e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1217/6000 [40:37<2:39:33,  2.00s/it] 20%|â–ˆâ–ˆ        | 1218/6000 [40:39<2:39:33,  2.00s/it]                                                     {'loss': 0.0035, 'grad_norm': 0.7587947845458984, 'learning_rate': 8.105084745762712e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1218/6000 [40:39<2:39:33,  2.00s/it] 20%|â–ˆâ–ˆ        | 1219/6000 [40:41<2:39:23,  2.00s/it]                                                     {'loss': 0.0244, 'grad_norm': 4.2238383293151855, 'learning_rate': 8.103389830508476e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1219/6000 [40:41<2:39:23,  2.00s/it] 20%|â–ˆâ–ˆ        | 1220/6000 [40:43<2:38:14,  1.99s/it]                                                     {'loss': 0.0211, 'grad_norm': 4.209766387939453, 'learning_rate': 8.101694915254237e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1220/6000 [40:43<2:38:14,  1.99s/it] 20%|â–ˆâ–ˆ        | 1221/6000 [40:45<2:37:18,  1.98s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.2448345273733139, 'learning_rate': 8.1e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1221/6000 [40:45<2:37:18,  1.98s/it] 20%|â–ˆâ–ˆ        | 1222/6000 [40:47<2:37:27,  1.98s/it]                                                     {'loss': 0.0817, 'grad_norm': 10.117317199707031, 'learning_rate': 8.098305084745764e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1222/6000 [40:47<2:37:27,  1.98s/it] 20%|â–ˆâ–ˆ        | 1223/6000 [40:49<2:35:56,  1.96s/it]                                                     {'loss': 0.0391, 'grad_norm': 8.908638954162598, 'learning_rate': 8.096610169491525e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1223/6000 [40:49<2:35:56,  1.96s/it] 20%|â–ˆâ–ˆ        | 1224/6000 [40:51<2:35:26,  1.95s/it]                                                     {'loss': 0.0051, 'grad_norm': 1.3215450048446655, 'learning_rate': 8.094915254237289e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1224/6000 [40:51<2:35:26,  1.95s/it] 20%|â–ˆâ–ˆ        | 1225/6000 [40:53<2:37:51,  1.98s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.3233714699745178, 'learning_rate': 8.093220338983052e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1225/6000 [40:53<2:37:51,  1.98s/it] 20%|â–ˆâ–ˆ        | 1226/6000 [40:55<2:38:08,  1.99s/it]                                                     {'loss': 0.1624, 'grad_norm': 12.385700225830078, 'learning_rate': 8.091525423728815e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1226/6000 [40:55<2:38:08,  1.99s/it] 20%|â–ˆâ–ˆ        | 1227/6000 [40:57<2:39:50,  2.01s/it]                                                     {'loss': 0.0126, 'grad_norm': 3.397434949874878, 'learning_rate': 8.089830508474577e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1227/6000 [40:57<2:39:50,  2.01s/it] 20%|â–ˆâ–ˆ        | 1228/6000 [40:59<2:38:30,  1.99s/it]                                                     {'loss': 0.0538, 'grad_norm': 3.3395659923553467, 'learning_rate': 8.08813559322034e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1228/6000 [40:59<2:38:30,  1.99s/it] 20%|â–ˆâ–ˆ        | 1229/6000 [41:01<2:36:52,  1.97s/it]                                                     {'loss': 0.0041, 'grad_norm': 0.8401458859443665, 'learning_rate': 8.086440677966101e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1229/6000 [41:01<2:36:52,  1.97s/it] 20%|â–ˆâ–ˆ        | 1230/6000 [41:03<2:36:11,  1.96s/it]                                                     {'loss': 0.1091, 'grad_norm': 10.903238296508789, 'learning_rate': 8.084745762711865e-06, 'epoch': 0.2}
 20%|â–ˆâ–ˆ        | 1230/6000 [41:03<2:36:11,  1.96s/it] 21%|â–ˆâ–ˆ        | 1231/6000 [41:05<2:36:19,  1.97s/it]                                                     {'loss': 0.1417, 'grad_norm': 13.685834884643555, 'learning_rate': 8.083050847457628e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1231/6000 [41:05<2:36:19,  1.97s/it] 21%|â–ˆâ–ˆ        | 1232/6000 [41:07<2:36:05,  1.96s/it]                                                     {'loss': 0.0026, 'grad_norm': 0.7916390895843506, 'learning_rate': 8.081355932203391e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1232/6000 [41:07<2:36:05,  1.96s/it] 21%|â–ˆâ–ˆ        | 1233/6000 [41:09<2:37:19,  1.98s/it]                                                     {'loss': 0.0243, 'grad_norm': 6.816484451293945, 'learning_rate': 8.079661016949153e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1233/6000 [41:09<2:37:19,  1.98s/it] 21%|â–ˆâ–ˆ        | 1234/6000 [41:11<2:40:03,  2.01s/it]                                                     {'loss': 0.2405, 'grad_norm': 20.07556915283203, 'learning_rate': 8.077966101694916e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1234/6000 [41:11<2:40:03,  2.01s/it] 21%|â–ˆâ–ˆ        | 1235/6000 [41:13<2:38:00,  1.99s/it]                                                     {'loss': 0.0112, 'grad_norm': 4.450156211853027, 'learning_rate': 8.076271186440678e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1235/6000 [41:13<2:38:00,  1.99s/it] 21%|â–ˆâ–ˆ        | 1236/6000 [41:15<2:36:49,  1.98s/it]                                                     {'loss': 0.0845, 'grad_norm': 10.405440330505371, 'learning_rate': 8.074576271186441e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1236/6000 [41:15<2:36:49,  1.98s/it] 21%|â–ˆâ–ˆ        | 1237/6000 [41:16<2:35:39,  1.96s/it]                                                     {'loss': 0.0142, 'grad_norm': 2.2421326637268066, 'learning_rate': 8.072881355932204e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1237/6000 [41:16<2:35:39,  1.96s/it] 21%|â–ˆâ–ˆ        | 1238/6000 [41:18<2:36:13,  1.97s/it]                                                     {'loss': 0.0081, 'grad_norm': 1.930692195892334, 'learning_rate': 8.071186440677968e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1238/6000 [41:18<2:36:13,  1.97s/it] 21%|â–ˆâ–ˆ        | 1239/6000 [41:20<2:36:58,  1.98s/it]                                                     {'loss': 0.0345, 'grad_norm': 4.786054611206055, 'learning_rate': 8.069491525423729e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1239/6000 [41:20<2:36:58,  1.98s/it] 21%|â–ˆâ–ˆ        | 1240/6000 [41:22<2:36:26,  1.97s/it]                                                     {'loss': 0.0356, 'grad_norm': 4.666996479034424, 'learning_rate': 8.067796610169492e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1240/6000 [41:22<2:36:26,  1.97s/it] 21%|â–ˆâ–ˆ        | 1241/6000 [41:24<2:34:58,  1.95s/it]                                                     {'loss': 0.008, 'grad_norm': 2.6682803630828857, 'learning_rate': 8.066101694915256e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1241/6000 [41:24<2:34:58,  1.95s/it] 21%|â–ˆâ–ˆ        | 1242/6000 [41:26<2:36:25,  1.97s/it]                                                     {'loss': 0.2109, 'grad_norm': 21.006196975708008, 'learning_rate': 8.064406779661019e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1242/6000 [41:26<2:36:25,  1.97s/it] 21%|â–ˆâ–ˆ        | 1243/6000 [41:28<2:36:29,  1.97s/it]                                                     {'loss': 0.0073, 'grad_norm': 1.7083003520965576, 'learning_rate': 8.06271186440678e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1243/6000 [41:28<2:36:29,  1.97s/it] 21%|â–ˆâ–ˆ        | 1244/6000 [41:30<2:36:14,  1.97s/it]                                                     {'loss': 0.1044, 'grad_norm': 12.468271255493164, 'learning_rate': 8.061016949152542e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1244/6000 [41:30<2:36:14,  1.97s/it] 21%|â–ˆâ–ˆ        | 1245/6000 [41:32<2:35:12,  1.96s/it]                                                     {'loss': 0.0038, 'grad_norm': 0.7220926880836487, 'learning_rate': 8.059322033898305e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1245/6000 [41:32<2:35:12,  1.96s/it] 21%|â–ˆâ–ˆ        | 1246/6000 [41:34<2:36:11,  1.97s/it]                                                     {'loss': 0.018, 'grad_norm': 4.9234771728515625, 'learning_rate': 8.057627118644069e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1246/6000 [41:34<2:36:11,  1.97s/it] 21%|â–ˆâ–ˆ        | 1247/6000 [41:36<2:35:53,  1.97s/it]                                                     {'loss': 0.0557, 'grad_norm': 4.658834934234619, 'learning_rate': 8.055932203389832e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1247/6000 [41:36<2:35:53,  1.97s/it] 21%|â–ˆâ–ˆ        | 1248/6000 [41:38<2:40:21,  2.02s/it]                                                     {'loss': 0.0007, 'grad_norm': 0.16316409409046173, 'learning_rate': 8.054237288135594e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1248/6000 [41:38<2:40:21,  2.02s/it] 21%|â–ˆâ–ˆ        | 1249/6000 [41:40<2:39:21,  2.01s/it]                                                     {'loss': 0.1293, 'grad_norm': 10.66942024230957, 'learning_rate': 8.052542372881357e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1249/6000 [41:40<2:39:21,  2.01s/it] 21%|â–ˆâ–ˆ        | 1250/6000 [41:42<2:37:11,  1.99s/it]                                                     {'loss': 0.165, 'grad_norm': 14.332473754882812, 'learning_rate': 8.050847457627118e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1250/6000 [41:42<2:37:11,  1.99s/it][2025-11-18 10:31:40,102] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1250
[2025-11-18 10:31:40,377] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1250/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 21%|â–ˆâ–ˆ        | 1251/6000 [41:45<2:57:29,  2.24s/it]                                                     {'loss': 0.0006, 'grad_norm': 0.1361411064863205, 'learning_rate': 8.049152542372882e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1251/6000 [41:45<2:57:29,  2.24s/it] 21%|â–ˆâ–ˆ        | 1252/6000 [41:47<2:51:33,  2.17s/it]                                                     {'loss': 0.2931, 'grad_norm': 20.525358200073242, 'learning_rate': 8.047457627118645e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1252/6000 [41:47<2:51:33,  2.17s/it] 21%|â–ˆâ–ˆ        | 1253/6000 [41:49<2:49:55,  2.15s/it]                                                     {'loss': 0.1474, 'grad_norm': 15.183191299438477, 'learning_rate': 8.045762711864408e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1253/6000 [41:49<2:49:55,  2.15s/it] 21%|â–ˆâ–ˆ        | 1254/6000 [41:51<2:43:56,  2.07s/it]                                                     {'loss': 0.0323, 'grad_norm': 9.212827682495117, 'learning_rate': 8.04406779661017e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1254/6000 [41:51<2:43:56,  2.07s/it] 21%|â–ˆâ–ˆ        | 1255/6000 [41:53<2:40:46,  2.03s/it]                                                     {'loss': 0.0377, 'grad_norm': 11.989558219909668, 'learning_rate': 8.042372881355933e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1255/6000 [41:53<2:40:46,  2.03s/it] 21%|â–ˆâ–ˆ        | 1256/6000 [41:55<2:40:11,  2.03s/it]                                                     {'loss': 0.0129, 'grad_norm': 2.4797182083129883, 'learning_rate': 8.040677966101695e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1256/6000 [41:55<2:40:11,  2.03s/it] 21%|â–ˆâ–ˆ        | 1257/6000 [41:57<2:36:56,  1.99s/it]                                                     {'loss': 0.0186, 'grad_norm': 6.5086541175842285, 'learning_rate': 8.038983050847458e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1257/6000 [41:57<2:36:56,  1.99s/it] 21%|â–ˆâ–ˆ        | 1258/6000 [41:59<2:38:37,  2.01s/it]                                                     {'loss': 0.057, 'grad_norm': 12.402971267700195, 'learning_rate': 8.037288135593221e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1258/6000 [41:59<2:38:37,  2.01s/it] 21%|â–ˆâ–ˆ        | 1259/6000 [42:01<2:37:33,  1.99s/it]                                                     {'loss': 0.5369, 'grad_norm': 21.97843360900879, 'learning_rate': 8.035593220338984e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1259/6000 [42:01<2:37:33,  1.99s/it] 21%|â–ˆâ–ˆ        | 1260/6000 [42:03<2:36:43,  1.98s/it]                                                     {'loss': 0.0642, 'grad_norm': 7.589386463165283, 'learning_rate': 8.033898305084746e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1260/6000 [42:03<2:36:43,  1.98s/it] 21%|â–ˆâ–ˆ        | 1261/6000 [42:05<2:36:51,  1.99s/it]                                                     {'loss': 0.1199, 'grad_norm': 15.216235160827637, 'learning_rate': 8.03220338983051e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1261/6000 [42:05<2:36:51,  1.99s/it] 21%|â–ˆâ–ˆ        | 1262/6000 [42:07<2:38:32,  2.01s/it]                                                     {'loss': 0.1148, 'grad_norm': 9.87251091003418, 'learning_rate': 8.030508474576273e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1262/6000 [42:07<2:38:32,  2.01s/it] 21%|â–ˆâ–ˆ        | 1263/6000 [42:09<2:39:18,  2.02s/it]                                                     {'loss': 0.0086, 'grad_norm': 2.9247162342071533, 'learning_rate': 8.028813559322036e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1263/6000 [42:09<2:39:18,  2.02s/it] 21%|â–ˆâ–ˆ        | 1264/6000 [42:11<2:37:00,  1.99s/it]                                                     {'loss': 0.0115, 'grad_norm': 4.786344528198242, 'learning_rate': 8.027118644067797e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1264/6000 [42:11<2:37:00,  1.99s/it] 21%|â–ˆâ–ˆ        | 1265/6000 [42:13<2:37:34,  2.00s/it]                                                     {'loss': 0.1083, 'grad_norm': 13.769187927246094, 'learning_rate': 8.025423728813559e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1265/6000 [42:13<2:37:34,  2.00s/it] 21%|â–ˆâ–ˆ        | 1266/6000 [42:15<2:36:10,  1.98s/it]                                                     {'loss': 0.0018, 'grad_norm': 0.45850807428359985, 'learning_rate': 8.023728813559322e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1266/6000 [42:15<2:36:10,  1.98s/it] 21%|â–ˆâ–ˆ        | 1267/6000 [42:17<2:39:06,  2.02s/it]                                                     {'loss': 0.0457, 'grad_norm': 10.20606517791748, 'learning_rate': 8.022033898305086e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1267/6000 [42:17<2:39:06,  2.02s/it] 21%|â–ˆâ–ˆ        | 1268/6000 [42:19<2:37:59,  2.00s/it]                                                     {'loss': 0.0566, 'grad_norm': 9.698838233947754, 'learning_rate': 8.020338983050849e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1268/6000 [42:19<2:37:59,  2.00s/it] 21%|â–ˆâ–ˆ        | 1269/6000 [42:21<2:37:40,  2.00s/it]                                                     {'loss': 0.1303, 'grad_norm': 18.676965713500977, 'learning_rate': 8.01864406779661e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1269/6000 [42:21<2:37:40,  2.00s/it] 21%|â–ˆâ–ˆ        | 1270/6000 [42:23<2:37:50,  2.00s/it]                                                     {'loss': 0.0029, 'grad_norm': 0.5145072340965271, 'learning_rate': 8.016949152542374e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1270/6000 [42:23<2:37:50,  2.00s/it] 21%|â–ˆâ–ˆ        | 1271/6000 [42:25<2:37:15,  2.00s/it]                                                     {'loss': 0.1141, 'grad_norm': 13.559897422790527, 'learning_rate': 8.015254237288135e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1271/6000 [42:25<2:37:15,  2.00s/it] 21%|â–ˆâ–ˆ        | 1272/6000 [42:27<2:39:38,  2.03s/it]                                                     {'loss': 0.0193, 'grad_norm': 3.9589784145355225, 'learning_rate': 8.013559322033899e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1272/6000 [42:27<2:39:38,  2.03s/it] 21%|â–ˆâ–ˆ        | 1273/6000 [42:29<2:37:52,  2.00s/it]                                                     {'loss': 0.0257, 'grad_norm': 3.4537878036499023, 'learning_rate': 8.011864406779662e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1273/6000 [42:29<2:37:52,  2.00s/it] 21%|â–ˆâ–ˆ        | 1274/6000 [42:31<2:35:42,  1.98s/it]                                                     {'loss': 0.0066, 'grad_norm': 2.013061285018921, 'learning_rate': 8.010169491525425e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆ        | 1274/6000 [42:31<2:35:42,  1.98s/it] 21%|â–ˆâ–ˆâ–       | 1275/6000 [42:33<2:34:46,  1.97s/it]                                                     {'loss': 0.1923, 'grad_norm': 17.043479919433594, 'learning_rate': 8.008474576271187e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1275/6000 [42:33<2:34:46,  1.97s/it] 21%|â–ˆâ–ˆâ–       | 1276/6000 [42:35<2:34:22,  1.96s/it]                                                     {'loss': 0.3155, 'grad_norm': 21.520368576049805, 'learning_rate': 8.00677966101695e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1276/6000 [42:35<2:34:22,  1.96s/it] 21%|â–ˆâ–ˆâ–       | 1277/6000 [42:37<2:35:48,  1.98s/it]                                                     {'loss': 0.3033, 'grad_norm': 25.528335571289062, 'learning_rate': 8.005084745762712e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1277/6000 [42:37<2:35:48,  1.98s/it] 21%|â–ˆâ–ˆâ–       | 1278/6000 [42:39<2:37:17,  2.00s/it]                                                     {'loss': 0.0913, 'grad_norm': 8.230881690979004, 'learning_rate': 8.003389830508475e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1278/6000 [42:39<2:37:17,  2.00s/it] 21%|â–ˆâ–ˆâ–       | 1279/6000 [42:41<2:36:20,  1.99s/it]                                                     {'loss': 0.1966, 'grad_norm': 15.773641586303711, 'learning_rate': 8.001694915254238e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1279/6000 [42:41<2:36:20,  1.99s/it] 21%|â–ˆâ–ˆâ–       | 1280/6000 [42:43<2:36:00,  1.98s/it]                                                     {'loss': 0.0103, 'grad_norm': 3.3200953006744385, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1280/6000 [42:43<2:36:00,  1.98s/it] 21%|â–ˆâ–ˆâ–       | 1281/6000 [42:45<2:37:52,  2.01s/it]                                                     {'loss': 0.0118, 'grad_norm': 2.4326393604278564, 'learning_rate': 7.998305084745763e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1281/6000 [42:45<2:37:52,  2.01s/it] 21%|â–ˆâ–ˆâ–       | 1282/6000 [42:47<2:36:30,  1.99s/it]                                                     {'loss': 0.0691, 'grad_norm': 8.59438705444336, 'learning_rate': 7.996610169491526e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1282/6000 [42:47<2:36:30,  1.99s/it] 21%|â–ˆâ–ˆâ–       | 1283/6000 [42:49<2:35:10,  1.97s/it]                                                     {'loss': 0.04, 'grad_norm': 5.927712917327881, 'learning_rate': 7.99491525423729e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1283/6000 [42:49<2:35:10,  1.97s/it] 21%|â–ˆâ–ˆâ–       | 1284/6000 [42:51<2:35:00,  1.97s/it]                                                     {'loss': 0.0695, 'grad_norm': 8.337729454040527, 'learning_rate': 7.993220338983053e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1284/6000 [42:51<2:35:00,  1.97s/it] 21%|â–ˆâ–ˆâ–       | 1285/6000 [42:53<2:34:38,  1.97s/it]                                                     {'loss': 0.0431, 'grad_norm': 4.022339344024658, 'learning_rate': 7.991525423728814e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1285/6000 [42:53<2:34:38,  1.97s/it] 21%|â–ˆâ–ˆâ–       | 1286/6000 [42:55<2:36:13,  1.99s/it]                                                     {'loss': 0.046, 'grad_norm': 7.8591203689575195, 'learning_rate': 7.989830508474576e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1286/6000 [42:55<2:36:13,  1.99s/it] 21%|â–ˆâ–ˆâ–       | 1287/6000 [42:57<2:37:25,  2.00s/it]                                                     {'loss': 0.0059, 'grad_norm': 1.6547690629959106, 'learning_rate': 7.98813559322034e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1287/6000 [42:57<2:37:25,  2.00s/it] 21%|â–ˆâ–ˆâ–       | 1288/6000 [42:59<2:37:14,  2.00s/it]                                                     {'loss': 0.0968, 'grad_norm': 11.74282455444336, 'learning_rate': 7.986440677966102e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1288/6000 [42:59<2:37:14,  2.00s/it] 21%|â–ˆâ–ˆâ–       | 1289/6000 [43:01<2:35:11,  1.98s/it]                                                     {'loss': 0.0234, 'grad_norm': 5.118658542633057, 'learning_rate': 7.984745762711866e-06, 'epoch': 0.21}
 21%|â–ˆâ–ˆâ–       | 1289/6000 [43:01<2:35:11,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1290/6000 [43:03<2:35:56,  1.99s/it]                                                     {'loss': 0.0008, 'grad_norm': 0.3292214274406433, 'learning_rate': 7.983050847457627e-06, 'epoch': 0.21}
 22%|â–ˆâ–ˆâ–       | 1290/6000 [43:03<2:35:56,  1.99s/it] 22%|â–ˆâ–ˆâ–       | 1291/6000 [43:05<2:34:41,  1.97s/it]                                                     {'loss': 0.0322, 'grad_norm': 5.798618316650391, 'learning_rate': 7.98135593220339e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1291/6000 [43:05<2:34:41,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1292/6000 [43:07<2:33:42,  1.96s/it]                                                     {'loss': 0.0443, 'grad_norm': 10.914674758911133, 'learning_rate': 7.979661016949152e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1292/6000 [43:07<2:33:42,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1293/6000 [43:08<2:33:11,  1.95s/it]                                                     {'loss': 0.0077, 'grad_norm': 1.6478725671768188, 'learning_rate': 7.977966101694915e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1293/6000 [43:08<2:33:11,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1294/6000 [43:10<2:32:12,  1.94s/it]                                                     {'loss': 0.0595, 'grad_norm': 7.950933456420898, 'learning_rate': 7.976271186440679e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1294/6000 [43:10<2:32:12,  1.94s/it] 22%|â–ˆâ–ˆâ–       | 1295/6000 [43:12<2:30:58,  1.93s/it]                                                     {'loss': 0.0543, 'grad_norm': 13.687432289123535, 'learning_rate': 7.974576271186442e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1295/6000 [43:12<2:30:58,  1.93s/it] 22%|â–ˆâ–ˆâ–       | 1296/6000 [43:14<2:32:05,  1.94s/it]                                                     {'loss': 0.0034, 'grad_norm': 0.49986499547958374, 'learning_rate': 7.972881355932204e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1296/6000 [43:14<2:32:05,  1.94s/it] 22%|â–ˆâ–ˆâ–       | 1297/6000 [43:16<2:33:46,  1.96s/it]                                                     {'loss': 0.0073, 'grad_norm': 0.9947072267532349, 'learning_rate': 7.971186440677967e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1297/6000 [43:16<2:33:46,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1298/6000 [43:18<2:33:14,  1.96s/it]                                                     {'loss': 0.1193, 'grad_norm': 12.028658866882324, 'learning_rate': 7.96949152542373e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1298/6000 [43:18<2:33:14,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1299/6000 [43:20<2:33:14,  1.96s/it]                                                     {'loss': 0.012, 'grad_norm': 3.13250994682312, 'learning_rate': 7.967796610169493e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1299/6000 [43:20<2:33:14,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1300/6000 [43:22<2:33:39,  1.96s/it]                                                     {'loss': 0.2017, 'grad_norm': 16.476768493652344, 'learning_rate': 7.966101694915255e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1300/6000 [43:22<2:33:39,  1.96s/it][2025-11-18 10:33:19,974] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1300
[2025-11-18 10:33:20,255] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1300/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 22%|â–ˆâ–ˆâ–       | 1301/6000 [43:25<2:50:32,  2.18s/it]                                                     {'loss': 0.0037, 'grad_norm': 0.6337918639183044, 'learning_rate': 7.964406779661018e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1301/6000 [43:25<2:50:32,  2.18s/it] 22%|â–ˆâ–ˆâ–       | 1302/6000 [43:27<2:45:10,  2.11s/it]                                                     {'loss': 0.0618, 'grad_norm': 8.095274925231934, 'learning_rate': 7.96271186440678e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1302/6000 [43:27<2:45:10,  2.11s/it] 22%|â–ˆâ–ˆâ–       | 1303/6000 [43:29<2:41:31,  2.06s/it]                                                     {'loss': 0.0104, 'grad_norm': 1.8230600357055664, 'learning_rate': 7.961016949152543e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1303/6000 [43:29<2:41:31,  2.06s/it] 22%|â–ˆâ–ˆâ–       | 1304/6000 [43:31<2:39:13,  2.03s/it]                                                     {'loss': 0.0806, 'grad_norm': 11.979012489318848, 'learning_rate': 7.959322033898306e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1304/6000 [43:31<2:39:13,  2.03s/it] 22%|â–ˆâ–ˆâ–       | 1305/6000 [43:33<2:36:32,  2.00s/it]                                                     {'loss': 0.0924, 'grad_norm': 12.736249923706055, 'learning_rate': 7.957627118644068e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1305/6000 [43:33<2:36:32,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1306/6000 [43:35<2:36:05,  2.00s/it]                                                     {'loss': 0.0335, 'grad_norm': 6.271603584289551, 'learning_rate': 7.955932203389831e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1306/6000 [43:35<2:36:05,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1307/6000 [43:37<2:35:04,  1.98s/it]                                                     {'loss': 0.0031, 'grad_norm': 0.7285322546958923, 'learning_rate': 7.954237288135593e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1307/6000 [43:37<2:35:04,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1308/6000 [43:39<2:36:22,  2.00s/it]                                                     {'loss': 0.0867, 'grad_norm': 15.346402168273926, 'learning_rate': 7.952542372881356e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1308/6000 [43:39<2:36:22,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1309/6000 [43:41<2:36:29,  2.00s/it]                                                     {'loss': 0.011, 'grad_norm': 3.566699981689453, 'learning_rate': 7.95084745762712e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1309/6000 [43:41<2:36:29,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1310/6000 [43:43<2:35:43,  1.99s/it]                                                     {'loss': 0.0726, 'grad_norm': 9.323285102844238, 'learning_rate': 7.949152542372883e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1310/6000 [43:43<2:35:43,  1.99s/it] 22%|â–ˆâ–ˆâ–       | 1311/6000 [43:44<2:34:01,  1.97s/it]                                                     {'loss': 0.0135, 'grad_norm': 2.670698404312134, 'learning_rate': 7.947457627118644e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1311/6000 [43:44<2:34:01,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1312/6000 [43:46<2:32:53,  1.96s/it]                                                     {'loss': 0.4724, 'grad_norm': 21.77036476135254, 'learning_rate': 7.945762711864407e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1312/6000 [43:46<2:32:53,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1313/6000 [43:48<2:33:38,  1.97s/it]                                                     {'loss': 0.0859, 'grad_norm': 10.185373306274414, 'learning_rate': 7.944067796610169e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1313/6000 [43:48<2:33:38,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1314/6000 [43:50<2:32:46,  1.96s/it]                                                     {'loss': 0.006, 'grad_norm': 1.4734877347946167, 'learning_rate': 7.942372881355932e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1314/6000 [43:50<2:32:46,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1315/6000 [43:52<2:32:29,  1.95s/it]                                                     {'loss': 0.1943, 'grad_norm': 12.58891487121582, 'learning_rate': 7.940677966101696e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1315/6000 [43:52<2:32:29,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1316/6000 [43:54<2:32:19,  1.95s/it]                                                     {'loss': 0.3095, 'grad_norm': 12.808552742004395, 'learning_rate': 7.938983050847459e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1316/6000 [43:54<2:32:19,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1317/6000 [43:56<2:32:05,  1.95s/it]                                                     {'loss': 0.0062, 'grad_norm': 1.9502942562103271, 'learning_rate': 7.93728813559322e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1317/6000 [43:56<2:32:05,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1318/6000 [43:58<2:32:33,  1.95s/it]                                                     {'loss': 0.0849, 'grad_norm': 10.806977272033691, 'learning_rate': 7.935593220338984e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1318/6000 [43:58<2:32:33,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1319/6000 [44:00<2:32:31,  1.95s/it]                                                     {'loss': 0.2704, 'grad_norm': 17.301170349121094, 'learning_rate': 7.933898305084747e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1319/6000 [44:00<2:32:31,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1320/6000 [44:02<2:32:38,  1.96s/it]                                                     {'loss': 0.0975, 'grad_norm': 20.376155853271484, 'learning_rate': 7.93220338983051e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1320/6000 [44:02<2:32:38,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1321/6000 [44:04<2:33:12,  1.96s/it]                                                     {'loss': 0.249, 'grad_norm': 24.29721450805664, 'learning_rate': 7.930508474576272e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1321/6000 [44:04<2:33:12,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1322/6000 [44:06<2:37:35,  2.02s/it]                                                     {'loss': 0.0813, 'grad_norm': 18.280132293701172, 'learning_rate': 7.928813559322035e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1322/6000 [44:06<2:37:35,  2.02s/it] 22%|â–ˆâ–ˆâ–       | 1323/6000 [44:08<2:37:03,  2.01s/it]                                                     {'loss': 0.0079, 'grad_norm': 1.8549002408981323, 'learning_rate': 7.927118644067797e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1323/6000 [44:08<2:37:03,  2.01s/it] 22%|â–ˆâ–ˆâ–       | 1324/6000 [44:10<2:35:27,  1.99s/it]                                                     {'loss': 0.0332, 'grad_norm': 6.990255832672119, 'learning_rate': 7.92542372881356e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1324/6000 [44:10<2:35:27,  1.99s/it] 22%|â–ˆâ–ˆâ–       | 1325/6000 [44:12<2:34:38,  1.98s/it]                                                     {'loss': 0.023, 'grad_norm': 3.958439350128174, 'learning_rate': 7.923728813559323e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1325/6000 [44:12<2:34:38,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1326/6000 [44:14<2:33:28,  1.97s/it]                                                     {'loss': 0.0946, 'grad_norm': 17.702835083007812, 'learning_rate': 7.922033898305085e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1326/6000 [44:14<2:33:28,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1327/6000 [44:16<2:33:18,  1.97s/it]                                                     {'loss': 0.0185, 'grad_norm': 5.09202241897583, 'learning_rate': 7.920338983050848e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1327/6000 [44:16<2:33:18,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1328/6000 [44:18<2:33:19,  1.97s/it]                                                     {'loss': 0.0571, 'grad_norm': 11.530383110046387, 'learning_rate': 7.91864406779661e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1328/6000 [44:18<2:33:19,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1329/6000 [44:20<2:35:23,  2.00s/it]                                                     {'loss': 0.0167, 'grad_norm': 4.9209675788879395, 'learning_rate': 7.916949152542373e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1329/6000 [44:20<2:35:23,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1330/6000 [44:22<2:40:15,  2.06s/it]                                                     {'loss': 0.078, 'grad_norm': 10.357781410217285, 'learning_rate': 7.915254237288136e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1330/6000 [44:22<2:40:15,  2.06s/it] 22%|â–ˆâ–ˆâ–       | 1331/6000 [44:24<2:39:32,  2.05s/it]                                                     {'loss': 0.148, 'grad_norm': 16.88973617553711, 'learning_rate': 7.9135593220339e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1331/6000 [44:24<2:39:32,  2.05s/it] 22%|â–ˆâ–ˆâ–       | 1332/6000 [44:26<2:42:10,  2.08s/it]                                                     {'loss': 0.0878, 'grad_norm': 11.506645202636719, 'learning_rate': 7.911864406779661e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1332/6000 [44:26<2:42:10,  2.08s/it] 22%|â–ˆâ–ˆâ–       | 1333/6000 [44:28<2:40:20,  2.06s/it]                                                     {'loss': 0.0312, 'grad_norm': 6.968096733093262, 'learning_rate': 7.910169491525424e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1333/6000 [44:28<2:40:20,  2.06s/it] 22%|â–ˆâ–ˆâ–       | 1334/6000 [44:30<2:38:29,  2.04s/it]                                                     {'loss': 0.002, 'grad_norm': 0.6432834267616272, 'learning_rate': 7.908474576271186e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1334/6000 [44:30<2:38:29,  2.04s/it] 22%|â–ˆâ–ˆâ–       | 1335/6000 [44:32<2:35:26,  2.00s/it]                                                     {'loss': 0.0861, 'grad_norm': 8.643099784851074, 'learning_rate': 7.906779661016951e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1335/6000 [44:32<2:35:26,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1336/6000 [44:34<2:34:00,  1.98s/it]                                                     {'loss': 0.0232, 'grad_norm': 6.8929762840271, 'learning_rate': 7.905084745762712e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1336/6000 [44:34<2:34:00,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1337/6000 [44:36<2:31:52,  1.95s/it]                                                     {'loss': 0.0553, 'grad_norm': 7.202635288238525, 'learning_rate': 7.903389830508476e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1337/6000 [44:36<2:31:52,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1338/6000 [44:38<2:31:56,  1.96s/it]                                                     {'loss': 0.0135, 'grad_norm': 3.122912883758545, 'learning_rate': 7.901694915254237e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1338/6000 [44:38<2:31:56,  1.96s/it] 22%|â–ˆâ–ˆâ–       | 1339/6000 [44:40<2:31:45,  1.95s/it]                                                     {'loss': 0.0278, 'grad_norm': 7.006589412689209, 'learning_rate': 7.9e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1339/6000 [44:40<2:31:45,  1.95s/it] 22%|â–ˆâ–ˆâ–       | 1340/6000 [44:42<2:33:34,  1.98s/it]                                                     {'loss': 0.0893, 'grad_norm': 13.022100448608398, 'learning_rate': 7.898305084745764e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1340/6000 [44:42<2:33:34,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1341/6000 [44:44<2:33:03,  1.97s/it]                                                     {'loss': 0.1689, 'grad_norm': 14.765727996826172, 'learning_rate': 7.896610169491527e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1341/6000 [44:44<2:33:03,  1.97s/it] 22%|â–ˆâ–ˆâ–       | 1342/6000 [44:46<2:35:42,  2.01s/it]                                                     {'loss': 0.1876, 'grad_norm': 17.646081924438477, 'learning_rate': 7.894915254237289e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1342/6000 [44:46<2:35:42,  2.01s/it] 22%|â–ˆâ–ˆâ–       | 1343/6000 [44:48<2:36:01,  2.01s/it]                                                     {'loss': 0.1171, 'grad_norm': 11.738770484924316, 'learning_rate': 7.893220338983052e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1343/6000 [44:48<2:36:01,  2.01s/it] 22%|â–ˆâ–ˆâ–       | 1344/6000 [44:50<2:37:40,  2.03s/it]                                                     {'loss': 0.008, 'grad_norm': 3.192204236984253, 'learning_rate': 7.891525423728814e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1344/6000 [44:50<2:37:40,  2.03s/it] 22%|â–ˆâ–ˆâ–       | 1345/6000 [44:52<2:37:04,  2.02s/it]                                                     {'loss': 0.1169, 'grad_norm': 11.58769702911377, 'learning_rate': 7.889830508474577e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1345/6000 [44:52<2:37:04,  2.02s/it] 22%|â–ˆâ–ˆâ–       | 1346/6000 [44:54<2:35:06,  2.00s/it]                                                     {'loss': 0.0017, 'grad_norm': 0.3356725573539734, 'learning_rate': 7.88813559322034e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1346/6000 [44:54<2:35:06,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1347/6000 [44:56<2:35:16,  2.00s/it]                                                     {'loss': 0.0022, 'grad_norm': 0.4777151644229889, 'learning_rate': 7.886440677966102e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1347/6000 [44:56<2:35:16,  2.00s/it] 22%|â–ˆâ–ˆâ–       | 1348/6000 [44:58<2:33:44,  1.98s/it]                                                     {'loss': 0.0064, 'grad_norm': 1.2037369012832642, 'learning_rate': 7.884745762711865e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1348/6000 [44:58<2:33:44,  1.98s/it] 22%|â–ˆâ–ˆâ–       | 1349/6000 [45:00<2:35:03,  2.00s/it]                                                     {'loss': 0.0014, 'grad_norm': 0.4444500505924225, 'learning_rate': 7.883050847457627e-06, 'epoch': 0.22}
 22%|â–ˆâ–ˆâ–       | 1349/6000 [45:00<2:35:03,  2.00s/it] 22%|â–ˆâ–ˆâ–Ž       | 1350/6000 [45:02<2:36:31,  2.02s/it]                                                     {'loss': 0.0557, 'grad_norm': 10.90001106262207, 'learning_rate': 7.88135593220339e-06, 'epoch': 0.23}
 22%|â–ˆâ–ˆâ–Ž       | 1350/6000 [45:02<2:36:31,  2.02s/it][2025-11-18 10:35:00,081] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1350
[2025-11-18 10:35:00,391] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1350/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 23%|â–ˆâ–ˆâ–Ž       | 1351/6000 [45:05<2:53:34,  2.24s/it]                                                     {'loss': 0.0, 'grad_norm': 0.011726079508662224, 'learning_rate': 7.879661016949153e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1351/6000 [45:05<2:53:34,  2.24s/it] 23%|â–ˆâ–ˆâ–Ž       | 1352/6000 [45:07<2:47:37,  2.16s/it]                                                     {'loss': 0.0557, 'grad_norm': 8.589859962463379, 'learning_rate': 7.877966101694916e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1352/6000 [45:07<2:47:37,  2.16s/it] 23%|â–ˆâ–ˆâ–Ž       | 1353/6000 [45:09<2:43:56,  2.12s/it]                                                     {'loss': 0.0462, 'grad_norm': 10.685775756835938, 'learning_rate': 7.876271186440678e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1353/6000 [45:09<2:43:56,  2.12s/it] 23%|â–ˆâ–ˆâ–Ž       | 1354/6000 [45:11<2:40:27,  2.07s/it]                                                     {'loss': 0.0222, 'grad_norm': 3.8145973682403564, 'learning_rate': 7.874576271186441e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1354/6000 [45:11<2:40:27,  2.07s/it] 23%|â–ˆâ–ˆâ–Ž       | 1355/6000 [45:13<2:37:17,  2.03s/it]                                                     {'loss': 0.0016, 'grad_norm': 0.3878220021724701, 'learning_rate': 7.872881355932205e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1355/6000 [45:13<2:37:17,  2.03s/it] 23%|â–ˆâ–ˆâ–Ž       | 1356/6000 [45:15<2:40:29,  2.07s/it]                                                     {'loss': 0.0223, 'grad_norm': 4.99007511138916, 'learning_rate': 7.871186440677968e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1356/6000 [45:15<2:40:29,  2.07s/it] 23%|â–ˆâ–ˆâ–Ž       | 1357/6000 [45:17<2:38:41,  2.05s/it]                                                     {'loss': 0.0117, 'grad_norm': 3.620551586151123, 'learning_rate': 7.86949152542373e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1357/6000 [45:17<2:38:41,  2.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 1358/6000 [45:19<2:38:25,  2.05s/it]                                                     {'loss': 0.1263, 'grad_norm': 14.708974838256836, 'learning_rate': 7.867796610169493e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1358/6000 [45:19<2:38:25,  2.05s/it] 23%|â–ˆâ–ˆâ–Ž       | 1359/6000 [45:21<2:37:41,  2.04s/it]                                                     {'loss': 0.0138, 'grad_norm': 2.9373741149902344, 'learning_rate': 7.866101694915254e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1359/6000 [45:21<2:37:41,  2.04s/it] 23%|â–ˆâ–ˆâ–Ž       | 1360/6000 [45:23<2:34:35,  2.00s/it]                                                     {'loss': 0.1139, 'grad_norm': 7.886022090911865, 'learning_rate': 7.864406779661017e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1360/6000 [45:23<2:34:35,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1361/6000 [45:25<2:32:17,  1.97s/it]                                                     {'loss': 0.1575, 'grad_norm': 18.91692543029785, 'learning_rate': 7.86271186440678e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1361/6000 [45:25<2:32:17,  1.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 1362/6000 [45:27<2:33:40,  1.99s/it]                                                     {'loss': 0.0871, 'grad_norm': 13.68372631072998, 'learning_rate': 7.861016949152544e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1362/6000 [45:27<2:33:40,  1.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1363/6000 [45:29<2:34:18,  2.00s/it]                                                     {'loss': 0.0005, 'grad_norm': 0.13873887062072754, 'learning_rate': 7.859322033898306e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1363/6000 [45:29<2:34:18,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1364/6000 [45:31<2:32:42,  1.98s/it]                                                     {'loss': 0.0441, 'grad_norm': 7.783438682556152, 'learning_rate': 7.857627118644069e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1364/6000 [45:31<2:32:42,  1.98s/it] 23%|â–ˆâ–ˆâ–Ž       | 1365/6000 [45:33<2:34:04,  1.99s/it]                                                     {'loss': 0.0235, 'grad_norm': 7.966278076171875, 'learning_rate': 7.85593220338983e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1365/6000 [45:33<2:34:04,  1.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1366/6000 [45:35<2:36:33,  2.03s/it]                                                     {'loss': 0.0034, 'grad_norm': 0.788263738155365, 'learning_rate': 7.854237288135594e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1366/6000 [45:35<2:36:33,  2.03s/it] 23%|â–ˆâ–ˆâ–Ž       | 1367/6000 [45:37<2:37:08,  2.04s/it]                                                     {'loss': 0.0525, 'grad_norm': 8.983809471130371, 'learning_rate': 7.852542372881357e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1367/6000 [45:37<2:37:08,  2.04s/it] 23%|â–ˆâ–ˆâ–Ž       | 1368/6000 [45:39<2:35:25,  2.01s/it]                                                     {'loss': 0.0136, 'grad_norm': 6.130199909210205, 'learning_rate': 7.850847457627119e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1368/6000 [45:39<2:35:25,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1369/6000 [45:41<2:34:37,  2.00s/it]                                                     {'loss': 0.0245, 'grad_norm': 5.164398670196533, 'learning_rate': 7.849152542372882e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1369/6000 [45:41<2:34:37,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1370/6000 [45:43<2:42:09,  2.10s/it]                                                     {'loss': 0.1649, 'grad_norm': 10.803548812866211, 'learning_rate': 7.847457627118643e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1370/6000 [45:43<2:42:09,  2.10s/it] 23%|â–ˆâ–ˆâ–Ž       | 1371/6000 [45:45<2:40:15,  2.08s/it]                                                     {'loss': 0.0513, 'grad_norm': 9.0963716506958, 'learning_rate': 7.845762711864407e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1371/6000 [45:45<2:40:15,  2.08s/it] 23%|â–ˆâ–ˆâ–Ž       | 1372/6000 [45:47<2:37:32,  2.04s/it]                                                     {'loss': 0.0299, 'grad_norm': 4.836065292358398, 'learning_rate': 7.84406779661017e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1372/6000 [45:47<2:37:32,  2.04s/it] 23%|â–ˆâ–ˆâ–Ž       | 1373/6000 [45:49<2:36:37,  2.03s/it]                                                     {'loss': 0.0198, 'grad_norm': 7.319488525390625, 'learning_rate': 7.842372881355933e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1373/6000 [45:49<2:36:37,  2.03s/it] 23%|â–ˆâ–ˆâ–Ž       | 1374/6000 [45:51<2:35:45,  2.02s/it]                                                     {'loss': 0.0032, 'grad_norm': 0.4123899042606354, 'learning_rate': 7.840677966101695e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1374/6000 [45:51<2:35:45,  2.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1375/6000 [45:53<2:34:34,  2.01s/it]                                                     {'loss': 0.0111, 'grad_norm': 2.927243709564209, 'learning_rate': 7.838983050847458e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1375/6000 [45:53<2:34:34,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1376/6000 [45:55<2:34:51,  2.01s/it]                                                     {'loss': 0.169, 'grad_norm': 23.837581634521484, 'learning_rate': 7.837288135593221e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1376/6000 [45:55<2:34:51,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1377/6000 [45:57<2:32:06,  1.97s/it]                                                     {'loss': 0.1731, 'grad_norm': 8.442323684692383, 'learning_rate': 7.835593220338985e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1377/6000 [45:57<2:32:06,  1.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 1378/6000 [45:59<2:31:42,  1.97s/it]                                                     {'loss': 0.0166, 'grad_norm': 4.576904773712158, 'learning_rate': 7.833898305084746e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1378/6000 [45:59<2:31:42,  1.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 1379/6000 [46:01<2:34:09,  2.00s/it]                                                     {'loss': 0.0359, 'grad_norm': 8.766400337219238, 'learning_rate': 7.83220338983051e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1379/6000 [46:01<2:34:09,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1380/6000 [46:03<2:32:24,  1.98s/it]                                                     {'loss': 0.0693, 'grad_norm': 8.606870651245117, 'learning_rate': 7.830508474576271e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1380/6000 [46:03<2:32:24,  1.98s/it] 23%|â–ˆâ–ˆâ–Ž       | 1381/6000 [46:05<2:34:55,  2.01s/it]                                                     {'loss': 0.2222, 'grad_norm': 24.310819625854492, 'learning_rate': 7.828813559322034e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1381/6000 [46:05<2:34:55,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1382/6000 [46:07<2:34:17,  2.00s/it]                                                     {'loss': 0.239, 'grad_norm': 14.650177955627441, 'learning_rate': 7.827118644067798e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1382/6000 [46:07<2:34:17,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1383/6000 [46:09<2:34:41,  2.01s/it]                                                     {'loss': 0.0136, 'grad_norm': 3.240600109100342, 'learning_rate': 7.825423728813561e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1383/6000 [46:09<2:34:41,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1384/6000 [46:11<2:34:17,  2.01s/it]                                                     {'loss': 0.1736, 'grad_norm': 8.850522994995117, 'learning_rate': 7.823728813559322e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1384/6000 [46:11<2:34:17,  2.01s/it] 23%|â–ˆâ–ˆâ–Ž       | 1385/6000 [46:13<2:33:52,  2.00s/it]                                                     {'loss': 0.0631, 'grad_norm': 10.091779708862305, 'learning_rate': 7.822033898305086e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1385/6000 [46:13<2:33:52,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1386/6000 [46:15<2:33:09,  1.99s/it]                                                     {'loss': 0.076, 'grad_norm': 13.642145156860352, 'learning_rate': 7.820338983050847e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1386/6000 [46:15<2:33:09,  1.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1387/6000 [46:17<2:32:54,  1.99s/it]                                                     {'loss': 0.0213, 'grad_norm': 4.9450764656066895, 'learning_rate': 7.81864406779661e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1387/6000 [46:17<2:32:54,  1.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1388/6000 [46:19<2:35:08,  2.02s/it]                                                     {'loss': 0.1164, 'grad_norm': 11.604986190795898, 'learning_rate': 7.816949152542374e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1388/6000 [46:19<2:35:08,  2.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1389/6000 [46:22<2:41:16,  2.10s/it]                                                     {'loss': 0.0009, 'grad_norm': 0.1684974730014801, 'learning_rate': 7.815254237288135e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1389/6000 [46:22<2:41:16,  2.10s/it] 23%|â–ˆâ–ˆâ–Ž       | 1390/6000 [46:24<2:37:56,  2.06s/it]                                                     {'loss': 0.0047, 'grad_norm': 1.0965886116027832, 'learning_rate': 7.813559322033899e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1390/6000 [46:24<2:37:56,  2.06s/it] 23%|â–ˆâ–ˆâ–Ž       | 1391/6000 [46:25<2:35:00,  2.02s/it]                                                     {'loss': 0.014, 'grad_norm': 2.190014600753784, 'learning_rate': 7.811864406779662e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1391/6000 [46:25<2:35:00,  2.02s/it] 23%|â–ˆâ–ˆâ–Ž       | 1392/6000 [46:27<2:33:47,  2.00s/it]                                                     {'loss': 0.0724, 'grad_norm': 12.95981216430664, 'learning_rate': 7.810169491525425e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1392/6000 [46:27<2:33:47,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1393/6000 [46:29<2:33:19,  2.00s/it]                                                     {'loss': 0.0308, 'grad_norm': 6.444193363189697, 'learning_rate': 7.808474576271187e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1393/6000 [46:29<2:33:19,  2.00s/it] 23%|â–ˆâ–ˆâ–Ž       | 1394/6000 [46:31<2:32:23,  1.99s/it]                                                     {'loss': 0.0668, 'grad_norm': 6.642226219177246, 'learning_rate': 7.80677966101695e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1394/6000 [46:31<2:32:23,  1.99s/it] 23%|â–ˆâ–ˆâ–Ž       | 1395/6000 [46:33<2:32:01,  1.98s/it]                                                     {'loss': 0.2415, 'grad_norm': 14.618758201599121, 'learning_rate': 7.805084745762712e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1395/6000 [46:33<2:32:01,  1.98s/it] 23%|â–ˆâ–ˆâ–Ž       | 1396/6000 [46:35<2:31:31,  1.97s/it]                                                     {'loss': 0.002, 'grad_norm': 0.396975576877594, 'learning_rate': 7.803389830508475e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1396/6000 [46:35<2:31:31,  1.97s/it] 23%|â–ˆâ–ˆâ–Ž       | 1397/6000 [46:37<2:30:17,  1.96s/it]                                                     {'loss': 0.0053, 'grad_norm': 1.1569442749023438, 'learning_rate': 7.801694915254238e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1397/6000 [46:37<2:30:17,  1.96s/it] 23%|â–ˆâ–ˆâ–Ž       | 1398/6000 [46:39<2:29:21,  1.95s/it]                                                     {'loss': 0.1067, 'grad_norm': 15.60141658782959, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1398/6000 [46:39<2:29:21,  1.95s/it] 23%|â–ˆâ–ˆâ–Ž       | 1399/6000 [46:41<2:29:20,  1.95s/it]                                                     {'loss': 0.2018, 'grad_norm': 12.867973327636719, 'learning_rate': 7.798305084745763e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1399/6000 [46:41<2:29:20,  1.95s/it] 23%|â–ˆâ–ˆâ–Ž       | 1400/6000 [46:43<2:29:57,  1.96s/it]                                                     {'loss': 0.0231, 'grad_norm': 9.215635299682617, 'learning_rate': 7.796610169491526e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1400/6000 [46:43<2:29:57,  1.96s/it][2025-11-18 10:36:40,938] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1400
[2025-11-18 10:36:41,395] INFO [src.utils:19] Saved tail token to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/1Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-1400/tail_token.pt
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
 23%|â–ˆâ–ˆâ–Ž       | 1401/6000 [46:46<2:52:29,  2.25s/it]                                                     {'loss': 0.0382, 'grad_norm': 7.878344535827637, 'learning_rate': 7.794915254237288e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1401/6000 [46:46<2:52:29,  2.25s/it] 23%|â–ˆâ–ˆâ–Ž       | 1402/6000 [46:48<2:46:38,  2.17s/it]                                                     {'loss': 0.1756, 'grad_norm': 14.874715805053711, 'learning_rate': 7.793220338983051e-06, 'epoch': 0.23}
 23%|â–ˆâ–ˆâ–Ž       | 1402/6000 [46:48<2:46:38,  2.17s/it]