==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 18Nov_1-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 09:17:40.547000 137617950525248 torch/distributed/run.py:779] 
W1118 09:17:40.547000 137617950525248 torch/distributed/run.py:779] *****************************************
W1118 09:17:40.547000 137617950525248 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 09:17:40.547000 137617950525248 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 09:17:47,719] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.45it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.48it/s]
wandb: setting up run nfeqeex5
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_091748-nfeqeex5
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 18Nov_1-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/nfeqeex5
[2025-11-18 09:17:49,180] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.95it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.36it/s]
[2025-11-18 09:17:49,683] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 09:17:56,337] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 09:17:57,134] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 09:17:57,135] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 09:18:00,655] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 09:18:00,656] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 09:18:01,440] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 09:18:01,440] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 09:18:01,440] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 09:18:01,441] INFO [src.utils:19] ==================================================
[2025-11-18 09:18:01,441] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 09:18:01,442] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:18:01,442] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:18:01,442] INFO [src.utils:19] ==================================================
[2025-11-18 09:18:03,176] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:18:03,177] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:18:03,177] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:18:03,177] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:18:03,177] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:18:03,177] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:18:03,177] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:18:03,177] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:18:03,177] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:18:03,177] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:18:03,178] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:18:03,178] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:18:03,178] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:18:03,178] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:18:03,180] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 09:18:03,181] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 09:18:03,183] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 09:18:03,184] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank1]:[W1118 09:18:05.384688605 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1118 09:18:05.385421301 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:03<5:36:55,  3.37s/it]                                                  {'loss': 21.1848, 'grad_norm': 1917.591552734375, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:36:55,  3.37s/it]  0%|          | 2/6000 [00:05<4:14:43,  2.55s/it]                                                  {'loss': 20.7669, 'grad_norm': 1534.9815673828125, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:14:43,  2.55s/it]  0%|          | 3/6000 [00:07<3:49:46,  2.30s/it]                                                  {'loss': 21.2372, 'grad_norm': 1592.984130859375, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:49:46,  2.30s/it]  0%|          | 4/6000 [00:09<3:37:21,  2.18s/it]                                                  {'loss': 20.4559, 'grad_norm': 1526.044677734375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:37:21,  2.18s/it]  0%|          | 5/6000 [00:11<3:26:59,  2.07s/it]                                                  {'loss': 21.3952, 'grad_norm': 1752.6177978515625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:26:59,  2.07s/it]  0%|          | 6/6000 [00:13<3:25:48,  2.06s/it]                                                  {'loss': 20.684, 'grad_norm': 1666.05419921875, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:25:48,  2.06s/it]  0%|          | 7/6000 [00:15<3:20:58,  2.01s/it]                                                  {'loss': 20.7458, 'grad_norm': 1636.135009765625, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:20:58,  2.01s/it]  0%|          | 8/6000 [00:17<3:16:47,  1.97s/it]                                                  {'loss': 20.7631, 'grad_norm': 1821.81884765625, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:17<3:16:47,  1.97s/it]  0%|          | 9/6000 [00:19<3:17:57,  1.98s/it]                                                  {'loss': 20.571, 'grad_norm': 1425.2952880859375, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:19<3:17:57,  1.98s/it]  0%|          | 10/6000 [00:20<3:15:39,  1.96s/it]                                                   {'loss': 20.6087, 'grad_norm': 1464.4361572265625, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:15:39,  1.96s/it]  0%|          | 11/6000 [00:22<3:16:51,  1.97s/it]                                                   {'loss': 18.7265, 'grad_norm': 1711.49169921875, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:16:51,  1.97s/it]  0%|          | 12/6000 [00:24<3:16:52,  1.97s/it]                                                   {'loss': 19.1053, 'grad_norm': 1410.7403564453125, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:16:52,  1.97s/it]  0%|          | 13/6000 [00:26<3:15:27,  1.96s/it]                                                   {'loss': 18.8807, 'grad_norm': 1326.8228759765625, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:15:27,  1.96s/it]  0%|          | 14/6000 [00:28<3:15:03,  1.96s/it]                                                   {'loss': 17.5891, 'grad_norm': 1358.795654296875, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:15:03,  1.96s/it]  0%|          | 15/6000 [00:30<3:15:33,  1.96s/it]                                                   {'loss': 18.0617, 'grad_norm': 1281.9049072265625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:15:33,  1.96s/it]  0%|          | 16/6000 [00:32<3:15:57,  1.96s/it]                                                   {'loss': 16.9816, 'grad_norm': 1294.2803955078125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:15:57,  1.96s/it]  0%|          | 17/6000 [00:34<3:16:18,  1.97s/it]                                                   {'loss': 17.2339, 'grad_norm': 1237.8897705078125, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:16:18,  1.97s/it]  0%|          | 18/6000 [00:36<3:15:55,  1.97s/it]                                                   {'loss': 16.7769, 'grad_norm': 1106.5091552734375, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:15:55,  1.97s/it]  0%|          | 19/6000 [00:38<3:15:02,  1.96s/it]                                                   {'loss': 15.1373, 'grad_norm': 1081.2371826171875, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:15:02,  1.96s/it]  0%|          | 20/6000 [00:40<3:14:42,  1.95s/it]                                                   {'loss': 14.5392, 'grad_norm': 1077.441650390625, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:14:42,  1.95s/it]  0%|          | 21/6000 [00:42<3:16:32,  1.97s/it]                                                   {'loss': 14.7455, 'grad_norm': 970.4342651367188, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:16:32,  1.97s/it]  0%|          | 22/6000 [00:44<3:17:38,  1.98s/it]                                                   {'loss': 14.7381, 'grad_norm': 932.5046997070312, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:17:38,  1.98s/it]  0%|          | 23/6000 [00:46<3:15:54,  1.97s/it]                                                   {'loss': 13.9972, 'grad_norm': 800.91259765625, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:15:54,  1.97s/it]  0%|          | 24/6000 [00:48<3:17:00,  1.98s/it]                                                   {'loss': 14.8858, 'grad_norm': 757.1310424804688, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:17:00,  1.98s/it]  0%|          | 25/6000 [00:50<3:16:31,  1.97s/it]                                                   {'loss': 12.9037, 'grad_norm': 741.9248657226562, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:16:31,  1.97s/it]  0%|          | 26/6000 [00:52<3:15:51,  1.97s/it]                                                   {'loss': 11.7359, 'grad_norm': 712.3858642578125, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:15:51,  1.97s/it]  0%|          | 27/6000 [00:54<3:14:08,  1.95s/it]                                                   {'loss': 10.733, 'grad_norm': 669.071533203125, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:14:08,  1.95s/it]  0%|          | 28/6000 [00:56<3:24:42,  2.06s/it]                                                   {'loss': 10.926, 'grad_norm': 600.4411010742188, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:24:42,  2.06s/it]  0%|          | 29/6000 [00:58<3:21:13,  2.02s/it]                                                   {'loss': 11.0345, 'grad_norm': 583.284423828125, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:21:13,  2.02s/it]  0%|          | 30/6000 [01:00<3:18:53,  2.00s/it]                                                   {'loss': 10.4896, 'grad_norm': 523.101806640625, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:18:53,  2.00s/it]  1%|          | 31/6000 [01:02<3:17:02,  1.98s/it]                                                   {'loss': 9.7846, 'grad_norm': 482.32891845703125, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:17:02,  1.98s/it]  1%|          | 32/6000 [01:04<3:19:33,  2.01s/it]                                                   {'loss': 9.2503, 'grad_norm': 439.43670654296875, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:19:33,  2.01s/it]  1%|          | 33/6000 [01:06<3:19:07,  2.00s/it]                                                   {'loss': 9.055, 'grad_norm': 422.767333984375, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:19:07,  2.00s/it]  1%|          | 34/6000 [01:08<3:17:17,  1.98s/it]                                                   {'loss': 7.276, 'grad_norm': 318.2183532714844, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:17:17,  1.98s/it]  1%|          | 35/6000 [01:10<3:16:22,  1.98s/it]                                                   {'loss': 8.6486, 'grad_norm': 340.4044494628906, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:16:22,  1.98s/it]  1%|          | 36/6000 [01:12<3:15:38,  1.97s/it]                                                   {'loss': 7.1362, 'grad_norm': 282.3948669433594, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:15:38,  1.97s/it]  1%|          | 37/6000 [01:14<3:14:09,  1.95s/it]                                                   {'loss': 7.633, 'grad_norm': 281.19671630859375, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:14:09,  1.95s/it]  1%|          | 38/6000 [01:16<3:13:42,  1.95s/it]                                                   {'loss': 6.9847, 'grad_norm': 279.2882995605469, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:13:42,  1.95s/it]  1%|          | 39/6000 [01:18<3:13:35,  1.95s/it]                                                   {'loss': 7.3875, 'grad_norm': 324.3250427246094, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:13:35,  1.95s/it]  1%|          | 40/6000 [01:20<3:13:01,  1.94s/it]                                                   {'loss': 6.6086, 'grad_norm': 222.30482482910156, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:13:01,  1.94s/it]  1%|          | 41/6000 [01:22<3:11:44,  1.93s/it]                                                   {'loss': 6.3956, 'grad_norm': 206.51950073242188, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:22<3:11:44,  1.93s/it]  1%|          | 42/6000 [01:23<3:10:46,  1.92s/it]                                                   {'loss': 6.2176, 'grad_norm': 189.9168701171875, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:23<3:10:46,  1.92s/it]  1%|          | 43/6000 [01:26<3:23:31,  2.05s/it]                                                   {'loss': 5.8458, 'grad_norm': 170.44461059570312, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:23:31,  2.05s/it]  1%|          | 44/6000 [01:28<3:22:57,  2.04s/it]                                                   {'loss': 5.8157, 'grad_norm': 169.40328979492188, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:22:57,  2.04s/it]  1%|          | 45/6000 [01:30<3:21:26,  2.03s/it]                                                   {'loss': 5.2823, 'grad_norm': 132.16436767578125, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:21:26,  2.03s/it]  1%|          | 46/6000 [01:32<3:23:55,  2.05s/it]                                                   {'loss': 5.3006, 'grad_norm': 117.32447052001953, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:23:55,  2.05s/it]  1%|          | 47/6000 [01:34<3:20:01,  2.02s/it]                                                   {'loss': 5.2213, 'grad_norm': 109.91114807128906, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:20:01,  2.02s/it]  1%|          | 48/6000 [01:36<3:18:19,  2.00s/it]                                                   {'loss': 5.1045, 'grad_norm': 109.79032897949219, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:18:19,  2.00s/it]  1%|          | 49/6000 [01:38<3:16:13,  1.98s/it]                                                   {'loss': 4.8048, 'grad_norm': 88.67752075195312, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:16:13,  1.98s/it]  1%|          | 50/6000 [01:40<3:16:13,  1.98s/it]                                                   {'loss': 4.6864, 'grad_norm': 80.47383117675781, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:16:13,  1.98s/it]  1%|          | 51/6000 [01:42<3:15:38,  1.97s/it]                                                   {'loss': 4.6359, 'grad_norm': 82.30879211425781, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:42<3:15:38,  1.97s/it]  1%|          | 52/6000 [01:44<3:13:48,  1.95s/it]                                                   {'loss': 4.6744, 'grad_norm': 85.00897979736328, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:44<3:13:48,  1.95s/it]  1%|          | 53/6000 [01:46<3:15:54,  1.98s/it]                                                   {'loss': 4.2009, 'grad_norm': 57.41492462158203, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:46<3:15:54,  1.98s/it]  1%|          | 54/6000 [01:48<3:15:06,  1.97s/it]                                                   {'loss': 4.2132, 'grad_norm': 44.244102478027344, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:48<3:15:06,  1.97s/it]  1%|          | 55/6000 [01:49<3:13:34,  1.95s/it]                                                   {'loss': 4.0285, 'grad_norm': 41.292449951171875, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:50<3:13:34,  1.95s/it]  1%|          | 56/6000 [01:51<3:11:37,  1.93s/it]                                                   {'loss': 4.086, 'grad_norm': 38.95792770385742, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:51<3:11:37,  1.93s/it]  1%|          | 57/6000 [01:53<3:10:07,  1.92s/it]                                                   {'loss': 3.94, 'grad_norm': 33.95147705078125, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:53<3:10:07,  1.92s/it]  1%|          | 58/6000 [01:55<3:11:22,  1.93s/it]                                                   {'loss': 3.9889, 'grad_norm': 36.56464767456055, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:55<3:11:22,  1.93s/it]  1%|          | 59/6000 [01:57<3:10:08,  1.92s/it]                                                   {'loss': 3.9037, 'grad_norm': 26.69195556640625, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:57<3:10:08,  1.92s/it]  1%|          | 60/6000 [01:59<3:12:20,  1.94s/it]                                                   {'loss': 3.8428, 'grad_norm': 28.725278854370117, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [01:59<3:12:20,  1.94s/it]  1%|          | 61/6000 [02:01<3:10:54,  1.93s/it]                                                   {'loss': 3.7825, 'grad_norm': 24.001453399658203, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:01<3:10:54,  1.93s/it]  1%|          | 62/6000 [02:03<3:10:15,  1.92s/it]                                                   {'loss': 3.7283, 'grad_norm': 18.318134307861328, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:03<3:10:15,  1.92s/it]  1%|          | 63/6000 [02:05<3:10:15,  1.92s/it]                                                   {'loss': 3.7246, 'grad_norm': 19.67304801940918, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:05<3:10:15,  1.92s/it]  1%|          | 64/6000 [02:07<3:10:05,  1.92s/it]                                                   {'loss': 3.671, 'grad_norm': 17.447391510009766, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:07<3:10:05,  1.92s/it]  1%|          | 65/6000 [02:09<3:10:44,  1.93s/it]                                                   {'loss': 3.6326, 'grad_norm': 13.555088996887207, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:09<3:10:44,  1.93s/it]  1%|          | 66/6000 [02:11<3:13:31,  1.96s/it]                                                   {'loss': 3.6196, 'grad_norm': 10.458394050598145, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:11<3:13:31,  1.96s/it]  1%|          | 67/6000 [02:13<3:14:46,  1.97s/it]                                                   {'loss': 3.5994, 'grad_norm': 11.480873107910156, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:13<3:14:46,  1.97s/it]  1%|          | 68/6000 [02:15<3:11:38,  1.94s/it]                                                   {'loss': 3.5817, 'grad_norm': 9.773046493530273, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:15<3:11:38,  1.94s/it]  1%|          | 69/6000 [02:17<3:13:16,  1.96s/it]                                                   {'loss': 3.5532, 'grad_norm': 6.898473739624023, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:17<3:13:16,  1.96s/it]  1%|          | 70/6000 [02:19<3:13:17,  1.96s/it]                                                   {'loss': 3.5542, 'grad_norm': 7.381536960601807, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:19<3:13:17,  1.96s/it]  1%|          | 71/6000 [02:20<3:11:16,  1.94s/it]                                                   {'loss': 3.5244, 'grad_norm': 6.041563510894775, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:20<3:11:16,  1.94s/it]  1%|          | 72/6000 [02:22<3:13:08,  1.95s/it]                                                   {'loss': 3.5407, 'grad_norm': 9.76709270477295, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:22<3:13:08,  1.95s/it]  1%|          | 73/6000 [02:24<3:12:20,  1.95s/it]                                                   {'loss': 3.5314, 'grad_norm': 5.045592784881592, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:24<3:12:20,  1.95s/it]  1%|          | 74/6000 [02:26<3:10:34,  1.93s/it]                                                   {'loss': 3.5223, 'grad_norm': 5.4705634117126465, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:26<3:10:34,  1.93s/it]  1%|â–         | 75/6000 [02:28<3:17:49,  2.00s/it]                                                   {'loss': 3.5051, 'grad_norm': 4.314574241638184, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:28<3:17:49,  2.00s/it]  1%|â–         | 76/6000 [02:30<3:17:14,  2.00s/it]                                                   {'loss': 3.5059, 'grad_norm': 3.6010689735412598, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:30<3:17:14,  2.00s/it]  1%|â–         | 77/6000 [02:32<3:16:01,  1.99s/it]                                                   {'loss': 3.4936, 'grad_norm': 3.113528251647949, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:32<3:16:01,  1.99s/it]  1%|â–         | 78/6000 [02:35<3:22:26,  2.05s/it]                                                   {'loss': 3.4875, 'grad_norm': 4.650039196014404, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:35<3:22:26,  2.05s/it]  1%|â–         | 79/6000 [02:36<3:18:26,  2.01s/it]                                                   {'loss': 3.5031, 'grad_norm': 3.6547508239746094, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:36<3:18:26,  2.01s/it]  1%|â–         | 80/6000 [02:38<3:15:18,  1.98s/it]                                                   {'loss': 3.4849, 'grad_norm': 3.14534330368042, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:38<3:15:18,  1.98s/it]  1%|â–         | 81/6000 [02:40<3:13:02,  1.96s/it]                                                   {'loss': 3.4724, 'grad_norm': 1.7226088047027588, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:40<3:13:02,  1.96s/it]  1%|â–         | 82/6000 [02:42<3:12:07,  1.95s/it]                                                   {'loss': 3.4784, 'grad_norm': 1.668766975402832, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:42<3:12:07,  1.95s/it]  1%|â–         | 83/6000 [02:44<3:12:04,  1.95s/it]                                                   {'loss': 3.4819, 'grad_norm': 1.534834384918213, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:44<3:12:04,  1.95s/it]  1%|â–         | 84/6000 [02:46<3:11:49,  1.95s/it]                                                   {'loss': 3.4669, 'grad_norm': 1.418184518814087, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:46<3:11:49,  1.95s/it]  1%|â–         | 85/6000 [02:48<3:15:48,  1.99s/it]                                                   {'loss': 3.468, 'grad_norm': 1.8790266513824463, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:48<3:15:48,  1.99s/it]  1%|â–         | 86/6000 [02:50<3:14:19,  1.97s/it]                                                   {'loss': 3.4754, 'grad_norm': 2.6787264347076416, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:50<3:14:19,  1.97s/it]  1%|â–         | 87/6000 [02:52<3:12:37,  1.95s/it]                                                   {'loss': 3.4776, 'grad_norm': 1.2295364141464233, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:52<3:12:37,  1.95s/it]  1%|â–         | 88/6000 [02:54<3:11:38,  1.94s/it]                                                   {'loss': 3.5323, 'grad_norm': 26.105836868286133, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:54<3:11:38,  1.94s/it]  1%|â–         | 89/6000 [02:56<3:11:36,  1.94s/it]                                                   {'loss': 3.4747, 'grad_norm': 1.0287147760391235, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:56<3:11:36,  1.94s/it]  2%|â–         | 90/6000 [02:58<3:11:51,  1.95s/it]                                                   {'loss': 3.4791, 'grad_norm': 1.2424101829528809, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [02:58<3:11:51,  1.95s/it]  2%|â–         | 91/6000 [03:00<3:11:33,  1.94s/it]                                                   {'loss': 3.4595, 'grad_norm': 1.4286977052688599, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:00<3:11:33,  1.94s/it]  2%|â–         | 92/6000 [03:02<3:12:40,  1.96s/it]                                                   {'loss': 3.4688, 'grad_norm': 1.111067771911621, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:02<3:12:40,  1.96s/it]  2%|â–         | 93/6000 [03:04<3:12:37,  1.96s/it]                                                   {'loss': 3.4716, 'grad_norm': 1.7688771486282349, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:04<3:12:37,  1.96s/it]  2%|â–         | 94/6000 [03:06<3:11:30,  1.95s/it]                                                   {'loss': 3.4606, 'grad_norm': 1.2157566547393799, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:06<3:11:30,  1.95s/it]  2%|â–         | 95/6000 [03:08<3:11:24,  1.94s/it]                                                   {'loss': 3.4647, 'grad_norm': 1.048258900642395, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:08<3:11:24,  1.94s/it]  2%|â–         | 96/6000 [03:10<3:10:38,  1.94s/it]                                                   {'loss': 3.4562, 'grad_norm': 1.0564132928848267, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:10<3:10:38,  1.94s/it]  2%|â–         | 97/6000 [03:11<3:10:29,  1.94s/it]                                                   {'loss': 3.4673, 'grad_norm': 1.683285117149353, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:11<3:10:29,  1.94s/it]  2%|â–         | 98/6000 [03:13<3:12:12,  1.95s/it]                                                   {'loss': 3.4536, 'grad_norm': 1.4106688499450684, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:13<3:12:12,  1.95s/it]  2%|â–         | 99/6000 [03:15<3:12:22,  1.96s/it]                                                   {'loss': 3.4596, 'grad_norm': 1.3934242725372314, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:15<3:12:22,  1.96s/it]  2%|â–         | 100/6000 [03:17<3:10:19,  1.94s/it]                                                    {'loss': 3.455, 'grad_norm': 1.8327966928482056, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:17<3:10:19,  1.94s/it]  2%|â–         | 101/6000 [03:20<3:22:54,  2.06s/it]                                                    {'loss': 3.4646, 'grad_norm': 1.5318405628204346, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:20<3:22:54,  2.06s/it]  2%|â–         | 102/6000 [03:22<3:20:09,  2.04s/it]                                                    {'loss': 3.4466, 'grad_norm': 2.4302916526794434, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:22<3:20:09,  2.04s/it]  2%|â–         | 103/6000 [03:24<3:16:55,  2.00s/it]                                                    {'loss': 3.4616, 'grad_norm': 1.5192832946777344, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:24<3:16:55,  2.00s/it]  2%|â–         | 104/6000 [03:26<3:17:30,  2.01s/it]                                                    {'loss': 3.4398, 'grad_norm': 2.6644036769866943, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:26<3:17:30,  2.01s/it]  2%|â–         | 105/6000 [03:28<3:15:44,  1.99s/it]                                                    {'loss': 3.4507, 'grad_norm': 2.1159920692443848, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:28<3:15:44,  1.99s/it]  2%|â–         | 106/6000 [03:30<3:14:37,  1.98s/it]                                                    {'loss': 3.4327, 'grad_norm': 3.4220938682556152, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:30<3:14:37,  1.98s/it]  2%|â–         | 107/6000 [03:32<3:15:22,  1.99s/it]                                                    {'loss': 3.4491, 'grad_norm': 2.550039291381836, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:32<3:15:22,  1.99s/it]  2%|â–         | 108/6000 [03:34<3:16:53,  2.01s/it]                                                    {'loss': 3.4192, 'grad_norm': 3.3681559562683105, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:34<3:16:53,  2.01s/it]  2%|â–         | 109/6000 [03:36<3:18:46,  2.02s/it]                                                    {'loss': 3.4057, 'grad_norm': 6.005209445953369, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:36<3:18:46,  2.02s/it]  2%|â–         | 110/6000 [03:38<3:17:10,  2.01s/it]                                                    {'loss': 3.429, 'grad_norm': 4.252404689788818, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:38<3:17:10,  2.01s/it]  2%|â–         | 111/6000 [03:40<3:14:15,  1.98s/it]                                                    {'loss': 3.4091, 'grad_norm': 4.1650800704956055, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:40<3:14:15,  1.98s/it]  2%|â–         | 112/6000 [03:42<3:17:31,  2.01s/it]                                                    {'loss': 3.4029, 'grad_norm': 5.184529781341553, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:42<3:17:31,  2.01s/it]  2%|â–         | 113/6000 [03:44<3:15:12,  1.99s/it]                                                    {'loss': 3.3763, 'grad_norm': 6.773614406585693, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:44<3:15:12,  1.99s/it]  2%|â–         | 114/6000 [03:45<3:14:14,  1.98s/it]                                                    {'loss': 3.3396, 'grad_norm': 9.335491180419922, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:45<3:14:14,  1.98s/it]  2%|â–         | 115/6000 [03:47<3:13:47,  1.98s/it]                                                    {'loss': 3.3978, 'grad_norm': 5.843255043029785, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:47<3:13:47,  1.98s/it]  2%|â–         | 116/6000 [03:49<3:12:52,  1.97s/it]                                                    {'loss': 3.3827, 'grad_norm': 6.907261371612549, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:49<3:12:52,  1.97s/it]  2%|â–         | 117/6000 [03:51<3:11:22,  1.95s/it]                                                    {'loss': 3.4094, 'grad_norm': 7.301300048828125, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:51<3:11:22,  1.95s/it]  2%|â–         | 118/6000 [03:53<3:15:26,  1.99s/it]                                                    {'loss': 3.3477, 'grad_norm': 5.407309532165527, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:53<3:15:26,  1.99s/it]  2%|â–         | 119/6000 [03:55<3:13:31,  1.97s/it]                                                    {'loss': 3.2999, 'grad_norm': 7.368941307067871, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:55<3:13:31,  1.97s/it]  2%|â–         | 120/6000 [03:57<3:13:09,  1.97s/it]                                                    {'loss': 3.3674, 'grad_norm': 13.885461807250977, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [03:57<3:13:09,  1.97s/it]  2%|â–         | 121/6000 [03:59<3:12:40,  1.97s/it]                                                    {'loss': 3.2906, 'grad_norm': 5.881619453430176, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [03:59<3:12:40,  1.97s/it]  2%|â–         | 122/6000 [04:01<3:12:08,  1.96s/it]                                                    {'loss': 3.3022, 'grad_norm': 15.356557846069336, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:01<3:12:08,  1.96s/it]  2%|â–         | 123/6000 [04:03<3:11:34,  1.96s/it]                                                    {'loss': 3.2695, 'grad_norm': 9.855935096740723, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:03<3:11:34,  1.96s/it]  2%|â–         | 124/6000 [04:05<3:10:51,  1.95s/it]                                                    {'loss': 3.5485, 'grad_norm': 60.94734573364258, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:05<3:10:51,  1.95s/it]  2%|â–         | 125/6000 [04:07<3:17:06,  2.01s/it]                                                    {'loss': 3.4812, 'grad_norm': 22.982772827148438, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:07<3:17:06,  2.01s/it]  2%|â–         | 126/6000 [04:09<3:15:10,  1.99s/it]                                                    {'loss': 3.3523, 'grad_norm': 29.507978439331055, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:09<3:15:10,  1.99s/it]  2%|â–         | 127/6000 [04:11<3:16:19,  2.01s/it]                                                    {'loss': 3.2496, 'grad_norm': 17.274675369262695, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:11<3:16:19,  2.01s/it]  2%|â–         | 128/6000 [04:13<3:15:24,  2.00s/it]                                                    {'loss': 3.3691, 'grad_norm': 18.386493682861328, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:13<3:15:24,  2.00s/it]  2%|â–         | 129/6000 [04:15<3:15:14,  2.00s/it]                                                    {'loss': 3.3211, 'grad_norm': 23.381738662719727, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:15<3:15:14,  2.00s/it]  2%|â–         | 130/6000 [04:17<3:17:17,  2.02s/it]                                                    {'loss': 3.2757, 'grad_norm': 21.448925018310547, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:17<3:17:17,  2.02s/it]  2%|â–         | 131/6000 [04:19<3:16:04,  2.00s/it]                                                    {'loss': 3.2819, 'grad_norm': 9.765814781188965, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:19<3:16:04,  2.00s/it]  2%|â–         | 132/6000 [04:21<3:12:52,  1.97s/it]                                                    {'loss': 3.2735, 'grad_norm': 16.154573440551758, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:21<3:12:52,  1.97s/it]  2%|â–         | 133/6000 [04:23<3:12:38,  1.97s/it]                                                    {'loss': 3.2268, 'grad_norm': 8.79541301727295, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:23<3:12:38,  1.97s/it]  2%|â–         | 134/6000 [04:25<3:15:06,  2.00s/it]                                                    {'loss': 3.2209, 'grad_norm': 8.198596954345703, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:25<3:15:06,  2.00s/it]  2%|â–         | 135/6000 [04:27<3:14:04,  1.99s/it]                                                    {'loss': 3.2929, 'grad_norm': 13.301533699035645, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:27<3:14:04,  1.99s/it]  2%|â–         | 136/6000 [04:29<3:12:31,  1.97s/it]                                                    {'loss': 3.2261, 'grad_norm': 8.960368156433105, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:29<3:12:31,  1.97s/it]  2%|â–         | 137/6000 [04:31<3:16:59,  2.02s/it]                                                    {'loss': 3.2231, 'grad_norm': 7.713418960571289, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:31<3:16:59,  2.02s/it]  2%|â–         | 138/6000 [04:33<3:14:14,  1.99s/it]                                                    {'loss': 3.2203, 'grad_norm': 9.94666862487793, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:33<3:14:14,  1.99s/it]  2%|â–         | 139/6000 [04:35<3:15:09,  2.00s/it]                                                    {'loss': 3.2898, 'grad_norm': 12.957703590393066, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:35<3:15:09,  2.00s/it]  2%|â–         | 140/6000 [04:37<3:20:07,  2.05s/it]                                                    {'loss': 3.2439, 'grad_norm': 11.094611167907715, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:37<3:20:07,  2.05s/it]  2%|â–         | 141/6000 [04:39<3:17:53,  2.03s/it]                                                    {'loss': 3.2204, 'grad_norm': 11.403244018554688, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:39<3:17:53,  2.03s/it]  2%|â–         | 142/6000 [04:41<3:17:07,  2.02s/it]                                                    {'loss': 3.2368, 'grad_norm': 9.767391204833984, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:41<3:17:07,  2.02s/it]  2%|â–         | 143/6000 [04:43<3:13:45,  1.98s/it]                                                    {'loss': 3.3539, 'grad_norm': 37.83207321166992, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:43<3:13:45,  1.98s/it]  2%|â–         | 144/6000 [04:45<3:11:06,  1.96s/it]                                                    {'loss': 3.2406, 'grad_norm': 11.77627182006836, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:45<3:11:06,  1.96s/it]  2%|â–         | 145/6000 [04:47<3:10:45,  1.95s/it]                                                    {'loss': 3.206, 'grad_norm': 6.411644458770752, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:47<3:10:45,  1.95s/it]  2%|â–         | 146/6000 [04:49<3:12:13,  1.97s/it]                                                    {'loss': 3.2824, 'grad_norm': 9.655976295471191, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:49<3:12:13,  1.97s/it]  2%|â–         | 147/6000 [04:51<3:09:29,  1.94s/it]                                                    {'loss': 3.2409, 'grad_norm': 8.121254920959473, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:51<3:09:29,  1.94s/it]  2%|â–         | 148/6000 [04:53<3:09:07,  1.94s/it]                                                    {'loss': 3.3193, 'grad_norm': 9.210518836975098, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:53<3:09:07,  1.94s/it]  2%|â–         | 149/6000 [04:55<3:08:32,  1.93s/it]                                                    {'loss': 3.1887, 'grad_norm': 12.056373596191406, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:55<3:08:32,  1.93s/it]  2%|â–Ž         | 150/6000 [04:57<3:07:39,  1.92s/it]                                                    {'loss': 3.2504, 'grad_norm': 7.702236175537109, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [04:57<3:07:39,  1.92s/it]  3%|â–Ž         | 151/6000 [04:59<3:08:01,  1.93s/it]                                                    {'loss': 3.2344, 'grad_norm': 8.89988899230957, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [04:59<3:08:01,  1.93s/it]  3%|â–Ž         | 152/6000 [05:01<3:11:17,  1.96s/it]                                                    {'loss': 3.1994, 'grad_norm': 7.785332679748535, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:01<3:11:17,  1.96s/it]  3%|â–Ž         | 153/6000 [05:02<3:08:06,  1.93s/it]                                                    {'loss': 3.2487, 'grad_norm': 14.06251049041748, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:02<3:08:06,  1.93s/it]  3%|â–Ž         | 154/6000 [05:05<3:15:58,  2.01s/it]                                                    {'loss': 3.2413, 'grad_norm': 9.957636833190918, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:05<3:15:58,  2.01s/it]  3%|â–Ž         | 155/6000 [05:07<3:13:07,  1.98s/it]                                                    {'loss': 3.1895, 'grad_norm': 9.595985412597656, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:07<3:13:07,  1.98s/it]  3%|â–Ž         | 156/6000 [05:09<3:14:28,  2.00s/it]                                                    {'loss': 3.2234, 'grad_norm': 8.82118034362793, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:09<3:14:28,  2.00s/it]  3%|â–Ž         | 157/6000 [05:11<3:16:27,  2.02s/it]                                                    {'loss': 3.2136, 'grad_norm': 9.237605094909668, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:11<3:16:27,  2.02s/it]  3%|â–Ž         | 158/6000 [05:13<3:14:40,  2.00s/it]                                                    {'loss': 3.1609, 'grad_norm': 8.927536010742188, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:13<3:14:40,  2.00s/it]  3%|â–Ž         | 159/6000 [05:15<3:12:05,  1.97s/it]                                                    {'loss': 3.1942, 'grad_norm': 10.572927474975586, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:15<3:12:05,  1.97s/it]  3%|â–Ž         | 160/6000 [05:17<3:18:32,  2.04s/it]                                                    {'loss': 3.1766, 'grad_norm': 12.096471786499023, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:17<3:18:32,  2.04s/it]  3%|â–Ž         | 161/6000 [05:19<3:17:38,  2.03s/it]                                                    {'loss': 3.5165, 'grad_norm': 149.4774932861328, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:19<3:17:38,  2.03s/it]  3%|â–Ž         | 162/6000 [05:21<3:20:13,  2.06s/it]                                                    {'loss': 3.2049, 'grad_norm': 18.42618179321289, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:21<3:20:13,  2.06s/it]  3%|â–Ž         | 163/6000 [05:23<3:19:15,  2.05s/it]                                                    {'loss': 3.1779, 'grad_norm': 9.837032318115234, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:23<3:19:15,  2.05s/it]  3%|â–Ž         | 164/6000 [05:25<3:16:23,  2.02s/it]                                                    {'loss': 3.2512, 'grad_norm': 9.46338939666748, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:25<3:16:23,  2.02s/it]  3%|â–Ž         | 165/6000 [05:27<3:15:11,  2.01s/it]                                                    {'loss': 3.1646, 'grad_norm': 12.636479377746582, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:27<3:15:11,  2.01s/it]  3%|â–Ž         | 166/6000 [05:29<3:11:30,  1.97s/it]                                                    {'loss': 3.1378, 'grad_norm': 8.832127571105957, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:29<3:11:30,  1.97s/it]  3%|â–Ž         | 167/6000 [05:31<3:10:24,  1.96s/it]                                                    {'loss': 3.1689, 'grad_norm': 11.890908241271973, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:31<3:10:24,  1.96s/it]  3%|â–Ž         | 168/6000 [05:33<3:08:57,  1.94s/it]                                                    {'loss': 3.1324, 'grad_norm': 12.983922958374023, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:33<3:08:57,  1.94s/it]  3%|â–Ž         | 169/6000 [05:35<3:13:44,  1.99s/it]                                                    {'loss': 3.1598, 'grad_norm': 9.050699234008789, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:35<3:13:44,  1.99s/it]  3%|â–Ž         | 170/6000 [05:37<3:12:18,  1.98s/it]                                                    {'loss': 3.1671, 'grad_norm': 12.300901412963867, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:37<3:12:18,  1.98s/it]  3%|â–Ž         | 171/6000 [05:39<3:10:18,  1.96s/it]                                                    {'loss': 3.1039, 'grad_norm': 9.92776870727539, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:39<3:10:18,  1.96s/it]  3%|â–Ž         | 172/6000 [05:41<3:11:06,  1.97s/it]                                                    {'loss': 3.2009, 'grad_norm': 14.055658340454102, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:41<3:11:06,  1.97s/it]  3%|â–Ž         | 173/6000 [05:43<3:11:51,  1.98s/it]                                                    {'loss': 3.2027, 'grad_norm': 24.378503799438477, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:43<3:11:51,  1.98s/it]  3%|â–Ž         | 174/6000 [05:45<3:17:29,  2.03s/it]                                                    {'loss': 3.168, 'grad_norm': 11.987319946289062, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:45<3:17:29,  2.03s/it]  3%|â–Ž         | 175/6000 [05:47<3:15:13,  2.01s/it]                                                    {'loss': 3.107, 'grad_norm': 11.351300239562988, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:47<3:15:13,  2.01s/it]  3%|â–Ž         | 176/6000 [05:49<3:15:13,  2.01s/it]                                                    {'loss': 3.1445, 'grad_norm': 16.910337448120117, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:49<3:15:13,  2.01s/it]  3%|â–Ž         | 177/6000 [05:51<3:12:05,  1.98s/it]                                                    {'loss': 3.1517, 'grad_norm': 15.991601943969727, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:51<3:12:05,  1.98s/it]  3%|â–Ž         | 178/6000 [05:53<3:11:18,  1.97s/it]                                                    {'loss': 3.112, 'grad_norm': 10.541770935058594, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:53<3:11:18,  1.97s/it]  3%|â–Ž         | 179/6000 [05:54<3:10:43,  1.97s/it]                                                    {'loss': 3.0898, 'grad_norm': 12.709869384765625, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [05:54<3:10:43,  1.97s/it]  3%|â–Ž         | 180/6000 [05:56<3:10:59,  1.97s/it]                                                    {'loss': 3.0834, 'grad_norm': 8.526251792907715, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [05:56<3:10:59,  1.97s/it]  3%|â–Ž         | 181/6000 [05:58<3:12:33,  1.99s/it]                                                    {'loss': 3.0749, 'grad_norm': 9.367177963256836, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [05:58<3:12:33,  1.99s/it]  3%|â–Ž         | 182/6000 [06:00<3:11:44,  1.98s/it]                                                    {'loss': 3.0927, 'grad_norm': 11.333550453186035, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [06:00<3:11:44,  1.98s/it]  3%|â–Ž         | 183/6000 [06:02<3:11:22,  1.97s/it]                                                    {'loss': 3.0547, 'grad_norm': 12.809920310974121, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:02<3:11:22,  1.97s/it]  3%|â–Ž         | 184/6000 [06:04<3:11:23,  1.97s/it]                                                    {'loss': 3.0754, 'grad_norm': 20.61473846435547, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:04<3:11:23,  1.97s/it]  3%|â–Ž         | 185/6000 [06:06<3:10:19,  1.96s/it]                                                    {'loss': 3.0485, 'grad_norm': 17.213407516479492, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:06<3:10:19,  1.96s/it]  3%|â–Ž         | 186/6000 [06:08<3:11:22,  1.98s/it]                                                    {'loss': 3.1654, 'grad_norm': 37.23551559448242, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:08<3:11:22,  1.98s/it]  3%|â–Ž         | 187/6000 [06:10<3:12:32,  1.99s/it]                                                    {'loss': 3.0617, 'grad_norm': 22.99738121032715, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:10<3:12:32,  1.99s/it]  3%|â–Ž         | 188/6000 [06:12<3:13:09,  1.99s/it]                                                    {'loss': 3.074, 'grad_norm': 21.107908248901367, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:12<3:13:09,  1.99s/it]  3%|â–Ž         | 189/6000 [06:14<3:11:21,  1.98s/it]                                                    {'loss': 3.0576, 'grad_norm': 16.77656364440918, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:14<3:11:21,  1.98s/it]  3%|â–Ž         | 190/6000 [06:16<3:08:13,  1.94s/it]                                                    {'loss': 3.0395, 'grad_norm': 27.73296356201172, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:16<3:08:13,  1.94s/it]  3%|â–Ž         | 191/6000 [06:18<3:07:53,  1.94s/it]                                                    {'loss': 3.0071, 'grad_norm': 88.71905517578125, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:18<3:07:53,  1.94s/it]  3%|â–Ž         | 192/6000 [06:20<3:14:40,  2.01s/it]                                                    {'loss': 3.1898, 'grad_norm': 190.1819610595703, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:20<3:14:40,  2.01s/it]  3%|â–Ž         | 193/6000 [06:22<3:15:30,  2.02s/it]                                                    {'loss': 3.0469, 'grad_norm': 15.952381134033203, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:22<3:15:30,  2.02s/it]  3%|â–Ž         | 194/6000 [06:24<3:11:16,  1.98s/it]                                                    {'loss': 3.0058, 'grad_norm': 30.926239013671875, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:24<3:11:16,  1.98s/it]  3%|â–Ž         | 195/6000 [06:26<3:11:22,  1.98s/it]                                                    {'loss': 3.0542, 'grad_norm': 18.330873489379883, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:26<3:11:22,  1.98s/it]  3%|â–Ž         | 196/6000 [06:28<3:12:08,  1.99s/it]                                                    {'loss': 3.1046, 'grad_norm': 35.352455139160156, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:28<3:12:08,  1.99s/it]  3%|â–Ž         | 197/6000 [06:30<3:15:38,  2.02s/it]                                                    {'loss': 3.1001, 'grad_norm': 121.59858703613281, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:30<3:15:38,  2.02s/it]  3%|â–Ž         | 198/6000 [06:32<3:14:41,  2.01s/it]                                                    {'loss': 2.9338, 'grad_norm': 55.043701171875, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:32<3:14:41,  2.01s/it]  3%|â–Ž         | 199/6000 [06:34<3:14:15,  2.01s/it]                                                    {'loss': 3.0251, 'grad_norm': 24.175312042236328, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:34<3:14:15,  2.01s/it]  3%|â–Ž         | 200/6000 [06:36<3:15:35,  2.02s/it]                                                    {'loss': 2.9245, 'grad_norm': 23.679561614990234, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:36<3:15:35,  2.02s/it]  3%|â–Ž         | 201/6000 [06:38<3:12:48,  1.99s/it]                                                    {'loss': 3.0888, 'grad_norm': 33.49613571166992, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:38<3:12:48,  1.99s/it]  3%|â–Ž         | 202/6000 [06:40<3:10:33,  1.97s/it]                                                    {'loss': 3.008, 'grad_norm': 37.438262939453125, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:40<3:10:33,  1.97s/it]  3%|â–Ž         | 203/6000 [06:42<3:10:03,  1.97s/it]                                                    {'loss': 3.0347, 'grad_norm': 24.196903228759766, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:42<3:10:03,  1.97s/it]  3%|â–Ž         | 204/6000 [06:44<3:09:00,  1.96s/it]                                                    {'loss': 3.1208, 'grad_norm': 51.36685562133789, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:44<3:09:00,  1.96s/it]  3%|â–Ž         | 205/6000 [06:46<3:08:31,  1.95s/it]                                                    {'loss': 3.2013, 'grad_norm': 94.3034896850586, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:46<3:08:31,  1.95s/it]  3%|â–Ž         | 206/6000 [06:48<3:06:27,  1.93s/it]                                                    {'loss': 3.0986, 'grad_norm': 93.25550079345703, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:48<3:06:27,  1.93s/it]  3%|â–Ž         | 207/6000 [06:50<3:08:28,  1.95s/it]                                                    {'loss': 3.1204, 'grad_norm': 38.9352912902832, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:50<3:08:28,  1.95s/it]  3%|â–Ž         | 208/6000 [06:52<3:06:49,  1.94s/it]                                                    {'loss': 2.8621, 'grad_norm': 20.766096115112305, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [06:52<3:06:49,  1.94s/it]  3%|â–Ž         | 209/6000 [06:54<3:05:49,  1.93s/it]                                                    {'loss': 2.9613, 'grad_norm': 38.852630615234375, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [06:54<3:05:49,  1.93s/it]  4%|â–Ž         | 210/6000 [06:56<3:08:24,  1.95s/it]                                                    {'loss': 3.0147, 'grad_norm': 21.384292602539062, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [06:56<3:08:24,  1.95s/it]  4%|â–Ž         | 211/6000 [06:58<3:11:32,  1.99s/it]                                                    {'loss': 3.1402, 'grad_norm': 58.49186325073242, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [06:58<3:11:32,  1.99s/it]  4%|â–Ž         | 212/6000 [07:00<3:13:22,  2.00s/it]                                                    {'loss': 3.0321, 'grad_norm': 26.33176040649414, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [07:00<3:13:22,  2.00s/it]  4%|â–Ž         | 213/6000 [07:02<3:14:02,  2.01s/it]                                                    {'loss': 2.9231, 'grad_norm': 21.161155700683594, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:02<3:14:02,  2.01s/it]  4%|â–Ž         | 214/6000 [07:04<3:11:07,  1.98s/it]                                                    {'loss': 3.0543, 'grad_norm': 25.101306915283203, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:04<3:11:07,  1.98s/it]  4%|â–Ž         | 215/6000 [07:06<3:09:40,  1.97s/it]                                                    {'loss': 3.0826, 'grad_norm': 31.693553924560547, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:06<3:09:40,  1.97s/it]  4%|â–Ž         | 216/6000 [07:08<3:09:02,  1.96s/it]                                                    {'loss': 3.0013, 'grad_norm': 23.911623001098633, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:08<3:09:02,  1.96s/it]  4%|â–Ž         | 217/6000 [07:10<3:10:08,  1.97s/it]                                                    {'loss': 2.9879, 'grad_norm': 31.19443702697754, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:10<3:10:08,  1.97s/it]  4%|â–Ž         | 218/6000 [07:12<3:09:45,  1.97s/it]                                                    {'loss': 2.9349, 'grad_norm': 18.99003028869629, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:12<3:09:45,  1.97s/it]  4%|â–Ž         | 219/6000 [07:13<3:07:43,  1.95s/it]                                                    {'loss': 3.0099, 'grad_norm': 23.818639755249023, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:13<3:07:43,  1.95s/it]  4%|â–Ž         | 220/6000 [07:15<3:08:14,  1.95s/it]                                                    {'loss': 2.9941, 'grad_norm': 43.60285568237305, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:15<3:08:14,  1.95s/it]  4%|â–Ž         | 221/6000 [07:17<3:08:30,  1.96s/it]                                                    {'loss': 2.9884, 'grad_norm': 24.091257095336914, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:17<3:08:30,  1.96s/it]  4%|â–Ž         | 222/6000 [07:19<3:05:58,  1.93s/it]                                                    {'loss': 2.956, 'grad_norm': 28.49643325805664, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:19<3:05:58,  1.93s/it]  4%|â–Ž         | 223/6000 [07:21<3:07:23,  1.95s/it]                                                    {'loss': 3.0198, 'grad_norm': 31.156707763671875, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:21<3:07:23,  1.95s/it]  4%|â–Ž         | 224/6000 [07:23<3:05:18,  1.92s/it]                                                    {'loss': 3.0097, 'grad_norm': 54.69707107543945, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:23<3:05:18,  1.92s/it]  4%|â–         | 225/6000 [07:25<3:07:59,  1.95s/it]                                                    {'loss': 2.8561, 'grad_norm': 18.865400314331055, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:25<3:07:59,  1.95s/it]  4%|â–         | 226/6000 [07:27<3:12:41,  2.00s/it]                                                    {'loss': 2.9724, 'grad_norm': 46.52974319458008, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:27<3:12:41,  2.00s/it]  4%|â–         | 227/6000 [07:29<3:10:01,  1.98s/it]                                                    {'loss': 3.0151, 'grad_norm': 54.80455017089844, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:29<3:10:01,  1.98s/it]  4%|â–         | 228/6000 [07:31<3:10:36,  1.98s/it]                                                    {'loss': 3.0728, 'grad_norm': 170.4313507080078, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:31<3:10:36,  1.98s/it]  4%|â–         | 229/6000 [07:33<3:08:01,  1.95s/it]                                                    {'loss': 3.0463, 'grad_norm': 112.19734191894531, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:33<3:08:01,  1.95s/it]  4%|â–         | 230/6000 [07:35<3:09:50,  1.97s/it]                                                    {'loss': 3.1153, 'grad_norm': 69.50786590576172, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:35<3:09:50,  1.97s/it]  4%|â–         | 231/6000 [07:37<3:07:21,  1.95s/it]                                                    {'loss': 2.997, 'grad_norm': 55.475154876708984, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:37<3:07:21,  1.95s/it]  4%|â–         | 232/6000 [07:39<3:07:21,  1.95s/it]                                                    {'loss': 3.0866, 'grad_norm': 123.7680892944336, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:39<3:07:21,  1.95s/it]  4%|â–         | 233/6000 [07:41<3:07:57,  1.96s/it]                                                    {'loss': 3.0577, 'grad_norm': 163.76707458496094, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:41<3:07:57,  1.96s/it]  4%|â–         | 234/6000 [07:43<3:13:35,  2.01s/it]                                                    {'loss': 3.0698, 'grad_norm': 171.96434020996094, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:43<3:13:35,  2.01s/it]  4%|â–         | 235/6000 [07:45<3:10:42,  1.98s/it]                                                    {'loss': 3.0048, 'grad_norm': 53.992919921875, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:45<3:10:42,  1.98s/it]  4%|â–         | 236/6000 [07:47<3:10:37,  1.98s/it]                                                    {'loss': 3.162, 'grad_norm': 123.3662338256836, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:47<3:10:37,  1.98s/it]  4%|â–         | 237/6000 [07:49<3:10:53,  1.99s/it]                                                    {'loss': 3.1047, 'grad_norm': 142.99693298339844, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:49<3:10:53,  1.99s/it]  4%|â–         | 238/6000 [07:51<3:13:10,  2.01s/it]                                                    {'loss': 3.0042, 'grad_norm': 91.95805358886719, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [07:51<3:13:10,  2.01s/it]  4%|â–         | 239/6000 [07:53<3:10:54,  1.99s/it]                                                    {'loss': 3.0514, 'grad_norm': 65.39896392822266, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [07:53<3:10:54,  1.99s/it]  4%|â–         | 240/6000 [07:55<3:09:00,  1.97s/it]                                                    {'loss': 2.8532, 'grad_norm': 32.01544189453125, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [07:55<3:09:00,  1.97s/it]  4%|â–         | 241/6000 [07:57<3:11:34,  2.00s/it]                                                    {'loss': 2.9647, 'grad_norm': 84.91220092773438, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [07:57<3:11:34,  2.00s/it]  4%|â–         | 242/6000 [07:59<3:08:34,  1.97s/it]                                                    {'loss': 3.0223, 'grad_norm': 56.70146942138672, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [07:59<3:08:34,  1.97s/it]  4%|â–         | 243/6000 [08:01<3:10:27,  1.98s/it]                                                    {'loss': 2.9418, 'grad_norm': 73.08485412597656, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [08:01<3:10:27,  1.98s/it]  4%|â–         | 244/6000 [08:03<3:12:45,  2.01s/it]                                                    {'loss': 3.0382, 'grad_norm': 53.80295181274414, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:03<3:12:45,  2.01s/it]  4%|â–         | 245/6000 [08:05<3:10:32,  1.99s/it]                                                    {'loss': 2.9395, 'grad_norm': 25.122331619262695, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:05<3:10:32,  1.99s/it]  4%|â–         | 246/6000 [08:07<3:10:30,  1.99s/it]                                                    {'loss': 3.0539, 'grad_norm': 29.26955795288086, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:07<3:10:30,  1.99s/it]  4%|â–         | 247/6000 [08:09<3:08:33,  1.97s/it]                                                    {'loss': 2.9767, 'grad_norm': 27.699501037597656, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:09<3:08:33,  1.97s/it]  4%|â–         | 248/6000 [08:11<3:07:51,  1.96s/it]                                                    {'loss': 2.9768, 'grad_norm': 35.325740814208984, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:11<3:07:51,  1.96s/it]  4%|â–         | 249/6000 [08:13<3:05:19,  1.93s/it]                                                    {'loss': 2.9824, 'grad_norm': 24.03049659729004, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:13<3:05:19,  1.93s/it]  4%|â–         | 250/6000 [08:15<3:05:25,  1.93s/it]                                                    {'loss': 2.9067, 'grad_norm': 26.06386947631836, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:15<3:05:25,  1.93s/it]  4%|â–         | 251/6000 [08:16<3:03:07,  1.91s/it]                                                    {'loss': 2.9428, 'grad_norm': 17.849952697753906, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:16<3:03:07,  1.91s/it]  4%|â–         | 252/6000 [08:18<3:09:06,  1.97s/it]                                                    {'loss': 2.9484, 'grad_norm': 58.45292282104492, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:18<3:09:06,  1.97s/it]  4%|â–         | 253/6000 [08:20<3:06:30,  1.95s/it]                                                    {'loss': 2.9072, 'grad_norm': 32.67805862426758, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:20<3:06:30,  1.95s/it]  4%|â–         | 254/6000 [08:22<3:07:14,  1.96s/it]                                                    {'loss': 2.9647, 'grad_norm': 18.783510208129883, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:22<3:07:14,  1.96s/it]  4%|â–         | 255/6000 [08:24<3:06:14,  1.95s/it]                                                    {'loss': 2.9072, 'grad_norm': 24.955615997314453, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:24<3:06:14,  1.95s/it]  4%|â–         | 256/6000 [08:26<3:10:44,  1.99s/it]                                                    {'loss': 2.9695, 'grad_norm': 23.35994529724121, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:26<3:10:44,  1.99s/it]  4%|â–         | 257/6000 [08:28<3:08:21,  1.97s/it]                                                    {'loss': 2.869, 'grad_norm': 71.01178741455078, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:28<3:08:21,  1.97s/it]  4%|â–         | 258/6000 [08:30<3:06:32,  1.95s/it]                                                    {'loss': 2.955, 'grad_norm': 23.684274673461914, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:30<3:06:32,  1.95s/it]  4%|â–         | 259/6000 [08:32<3:04:40,  1.93s/it]                                                    {'loss': 2.9346, 'grad_norm': 54.87455749511719, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:32<3:04:40,  1.93s/it]  4%|â–         | 260/6000 [08:34<3:03:43,  1.92s/it]                                                    {'loss': 2.9136, 'grad_norm': 55.72020721435547, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:34<3:03:43,  1.92s/it]  4%|â–         | 261/6000 [08:36<3:04:56,  1.93s/it]                                                    {'loss': 2.9628, 'grad_norm': 19.596393585205078, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:36<3:04:56,  1.93s/it]  4%|â–         | 262/6000 [08:38<3:08:31,  1.97s/it]                                                    {'loss': 2.9692, 'grad_norm': 29.553987503051758, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:38<3:08:31,  1.97s/it]  4%|â–         | 263/6000 [08:40<3:10:40,  1.99s/it]                                                    {'loss': 2.9325, 'grad_norm': 33.61716079711914, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:40<3:10:40,  1.99s/it]  4%|â–         | 264/6000 [08:42<3:09:44,  1.98s/it]                                                    {'loss': 2.8352, 'grad_norm': 35.784332275390625, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:42<3:09:44,  1.98s/it]  4%|â–         | 265/6000 [08:44<3:09:43,  1.98s/it]                                                    {'loss': 2.8854, 'grad_norm': 19.553749084472656, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:44<3:09:43,  1.98s/it]  4%|â–         | 266/6000 [08:46<3:09:02,  1.98s/it]                                                    {'loss': 2.8749, 'grad_norm': 18.593523025512695, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:46<3:09:02,  1.98s/it]  4%|â–         | 267/6000 [08:48<3:07:55,  1.97s/it]                                                    {'loss': 2.8502, 'grad_norm': 31.51117706298828, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [08:48<3:07:55,  1.97s/it]  4%|â–         | 268/6000 [08:50<3:06:17,  1.95s/it]                                                    {'loss': 2.9683, 'grad_norm': 51.30217361450195, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [08:50<3:06:17,  1.95s/it]  4%|â–         | 269/6000 [08:52<3:06:30,  1.95s/it]                                                    {'loss': 2.9079, 'grad_norm': 34.0352668762207, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [08:52<3:06:30,  1.95s/it]  4%|â–         | 270/6000 [08:54<3:06:04,  1.95s/it]                                                    {'loss': 3.0142, 'grad_norm': 62.559181213378906, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [08:54<3:06:04,  1.95s/it]  5%|â–         | 271/6000 [08:56<3:04:45,  1.93s/it]                                                    {'loss': 2.8914, 'grad_norm': 34.417659759521484, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [08:56<3:04:45,  1.93s/it]  5%|â–         | 272/6000 [08:57<3:03:21,  1.92s/it]                                                    {'loss': 2.9723, 'grad_norm': 94.94686889648438, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [08:57<3:03:21,  1.92s/it]  5%|â–         | 273/6000 [08:59<3:04:05,  1.93s/it]                                                    {'loss': 2.8915, 'grad_norm': 79.37739562988281, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [08:59<3:04:05,  1.93s/it]  5%|â–         | 274/6000 [09:01<3:04:49,  1.94s/it]                                                    {'loss': 2.863, 'grad_norm': 77.1578369140625, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:01<3:04:49,  1.94s/it]  5%|â–         | 275/6000 [09:03<3:03:23,  1.92s/it]                                                    {'loss': 2.9337, 'grad_norm': 28.403446197509766, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:03<3:03:23,  1.92s/it]  5%|â–         | 276/6000 [09:05<3:03:51,  1.93s/it]                                                    {'loss': 2.929, 'grad_norm': 65.9578857421875, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:05<3:03:51,  1.93s/it]  5%|â–         | 277/6000 [09:07<3:02:59,  1.92s/it]                                                    {'loss': 2.8955, 'grad_norm': 40.440879821777344, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:07<3:02:59,  1.92s/it]  5%|â–         | 278/6000 [09:09<3:04:25,  1.93s/it]                                                    {'loss': 2.9843, 'grad_norm': 63.74264144897461, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:09<3:04:25,  1.93s/it]  5%|â–         | 279/6000 [09:11<3:06:53,  1.96s/it]                                                    {'loss': 2.8288, 'grad_norm': 45.714111328125, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:11<3:06:53,  1.96s/it]  5%|â–         | 280/6000 [09:13<3:06:06,  1.95s/it]                                                    {'loss': 2.912, 'grad_norm': 40.64666748046875, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:13<3:06:06,  1.95s/it]  5%|â–         | 281/6000 [09:15<3:06:10,  1.95s/it]                                                    {'loss': 2.8538, 'grad_norm': 42.61921310424805, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:15<3:06:10,  1.95s/it]  5%|â–         | 282/6000 [09:17<3:10:29,  2.00s/it]                                                    {'loss': 2.8228, 'grad_norm': 25.19896125793457, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:17<3:10:29,  2.00s/it]  5%|â–         | 283/6000 [09:19<3:11:29,  2.01s/it]                                                    {'loss': 2.9163, 'grad_norm': 48.24334716796875, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:19<3:11:29,  2.01s/it]  5%|â–         | 284/6000 [09:21<3:17:22,  2.07s/it]                                                    {'loss': 2.8236, 'grad_norm': 41.102272033691406, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:21<3:17:22,  2.07s/it]  5%|â–         | 285/6000 [09:23<3:14:30,  2.04s/it]                                                    {'loss': 2.9126, 'grad_norm': 74.47234344482422, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:23<3:14:30,  2.04s/it]  5%|â–         | 286/6000 [09:25<3:10:59,  2.01s/it]                                                    {'loss': 2.8007, 'grad_norm': 43.26259231567383, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:25<3:10:59,  2.01s/it]  5%|â–         | 287/6000 [09:27<3:09:37,  1.99s/it]                                                    {'loss': 2.9451, 'grad_norm': 75.73153686523438, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:27<3:09:37,  1.99s/it]  5%|â–         | 288/6000 [09:29<3:08:10,  1.98s/it]                                                    {'loss': 2.866, 'grad_norm': 31.406391143798828, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:29<3:08:10,  1.98s/it]  5%|â–         | 289/6000 [09:31<3:11:07,  2.01s/it]                                                    {'loss': 2.8553, 'grad_norm': 47.36623764038086, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:31<3:11:07,  2.01s/it]  5%|â–         | 290/6000 [09:33<3:08:57,  1.99s/it]                                                    {'loss': 2.8915, 'grad_norm': 60.31596374511719, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:33<3:08:57,  1.99s/it]  5%|â–         | 291/6000 [09:35<3:05:53,  1.95s/it]                                                    {'loss': 3.0464, 'grad_norm': 114.53004455566406, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:35<3:05:53,  1.95s/it]  5%|â–         | 292/6000 [09:37<3:04:30,  1.94s/it]                                                    {'loss': 2.956, 'grad_norm': 49.34947204589844, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:37<3:04:30,  1.94s/it]  5%|â–         | 293/6000 [09:39<3:04:56,  1.94s/it]                                                    {'loss': 2.8986, 'grad_norm': 25.002235412597656, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:39<3:04:56,  1.94s/it]  5%|â–         | 294/6000 [09:41<3:05:46,  1.95s/it]                                                    {'loss': 2.7978, 'grad_norm': 33.02447509765625, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:41<3:05:46,  1.95s/it]  5%|â–         | 295/6000 [09:43<3:04:17,  1.94s/it]                                                    {'loss': 2.8357, 'grad_norm': 29.398822784423828, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:43<3:04:17,  1.94s/it]  5%|â–         | 296/6000 [09:45<3:06:01,  1.96s/it]                                                    {'loss': 2.8068, 'grad_norm': 81.08053588867188, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [09:45<3:06:01,  1.96s/it]  5%|â–         | 297/6000 [09:47<3:05:04,  1.95s/it]                                                    {'loss': 3.0099, 'grad_norm': 45.82510757446289, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [09:47<3:05:04,  1.95s/it]  5%|â–         | 298/6000 [09:49<3:06:46,  1.97s/it]                                                    {'loss': 2.8403, 'grad_norm': 34.11832809448242, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [09:49<3:06:46,  1.97s/it]  5%|â–         | 299/6000 [09:51<3:16:52,  2.07s/it]                                                    {'loss': 2.8899, 'grad_norm': 64.76156616210938, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [09:51<3:16:52,  2.07s/it]  5%|â–Œ         | 300/6000 [09:53<3:16:57,  2.07s/it]                                                    {'loss': 2.8582, 'grad_norm': 135.73585510253906, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [09:53<3:16:57,  2.07s/it]  5%|â–Œ         | 301/6000 [09:55<3:12:49,  2.03s/it]                                                    {'loss': 2.9933, 'grad_norm': 70.69631958007812, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [09:55<3:12:49,  2.03s/it]  5%|â–Œ         | 302/6000 [09:57<3:10:48,  2.01s/it]                                                    {'loss': 2.918, 'grad_norm': 57.58420181274414, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [09:57<3:10:48,  2.01s/it]  5%|â–Œ         | 303/6000 [09:59<3:11:24,  2.02s/it]                                                    {'loss': 2.9439, 'grad_norm': 113.56039428710938, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [09:59<3:11:24,  2.02s/it]  5%|â–Œ         | 304/6000 [10:01<3:10:16,  2.00s/it]                                                    {'loss': 2.9601, 'grad_norm': 68.80805969238281, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:01<3:10:16,  2.00s/it]  5%|â–Œ         | 305/6000 [10:03<3:06:39,  1.97s/it]                                                    {'loss': 2.8069, 'grad_norm': 28.40643310546875, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:03<3:06:39,  1.97s/it]  5%|â–Œ         | 306/6000 [10:05<3:05:50,  1.96s/it]                                                    {'loss': 2.9187, 'grad_norm': 35.59429168701172, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:05<3:05:50,  1.96s/it]  5%|â–Œ         | 307/6000 [10:07<3:06:35,  1.97s/it]                                                    {'loss': 2.9666, 'grad_norm': 28.1414737701416, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:07<3:06:35,  1.97s/it]  5%|â–Œ         | 308/6000 [10:09<3:04:39,  1.95s/it]                                                    {'loss': 2.9035, 'grad_norm': 42.3314208984375, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:09<3:04:39,  1.95s/it]  5%|â–Œ         | 309/6000 [10:11<3:07:35,  1.98s/it]                                                    {'loss': 2.863, 'grad_norm': 67.95938873291016, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:11<3:07:35,  1.98s/it]  5%|â–Œ         | 310/6000 [10:13<3:05:53,  1.96s/it]                                                    {'loss': 2.9108, 'grad_norm': 23.61705780029297, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:13<3:05:53,  1.96s/it]  5%|â–Œ         | 311/6000 [10:15<3:05:12,  1.95s/it]                                                    {'loss': 2.9087, 'grad_norm': 25.212575912475586, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:15<3:05:12,  1.95s/it]  5%|â–Œ         | 312/6000 [10:17<3:05:41,  1.96s/it]                                                    {'loss': 2.989, 'grad_norm': 101.79336547851562, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:17<3:05:41,  1.96s/it]  5%|â–Œ         | 313/6000 [10:19<3:05:32,  1.96s/it]                                                    {'loss': 2.8381, 'grad_norm': 89.6441421508789, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:19<3:05:32,  1.96s/it]  5%|â–Œ         | 314/6000 [10:20<3:03:56,  1.94s/it]                                                    {'loss': 2.9591, 'grad_norm': 30.299856185913086, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:20<3:03:56,  1.94s/it]  5%|â–Œ         | 315/6000 [10:22<3:05:40,  1.96s/it]                                                    {'loss': 2.899, 'grad_norm': 34.91279983520508, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:22<3:05:40,  1.96s/it]  5%|â–Œ         | 316/6000 [10:25<3:09:06,  2.00s/it]                                                    {'loss': 2.9336, 'grad_norm': 54.0966682434082, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:25<3:09:06,  2.00s/it]  5%|â–Œ         | 317/6000 [10:27<3:08:31,  1.99s/it]                                                    {'loss': 3.0343, 'grad_norm': 102.17208099365234, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:27<3:08:31,  1.99s/it]  5%|â–Œ         | 318/6000 [10:29<3:08:44,  1.99s/it]                                                    {'loss': 2.7738, 'grad_norm': 31.482627868652344, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:29<3:08:44,  1.99s/it]  5%|â–Œ         | 319/6000 [10:30<3:05:49,  1.96s/it]                                                    {'loss': 2.7917, 'grad_norm': 30.9224853515625, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:30<3:05:49,  1.96s/it]  5%|â–Œ         | 320/6000 [10:32<3:04:13,  1.95s/it]                                                    {'loss': 2.9106, 'grad_norm': 83.3774642944336, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:32<3:04:13,  1.95s/it]  5%|â–Œ         | 321/6000 [10:34<3:07:20,  1.98s/it]                                                    {'loss': 2.918, 'grad_norm': 267.3001403808594, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:34<3:07:20,  1.98s/it]  5%|â–Œ         | 322/6000 [10:36<3:08:15,  1.99s/it]                                                    {'loss': 2.8769, 'grad_norm': 40.14320373535156, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:36<3:08:15,  1.99s/it]  5%|â–Œ         | 323/6000 [10:39<3:13:03,  2.04s/it]                                                    {'loss': 2.8623, 'grad_norm': 409.1979064941406, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:39<3:13:03,  2.04s/it]  5%|â–Œ         | 324/6000 [10:40<3:10:11,  2.01s/it]                                                    {'loss': 2.8932, 'grad_norm': 23.554410934448242, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:40<3:10:11,  2.01s/it]  5%|â–Œ         | 325/6000 [10:42<3:07:32,  1.98s/it]                                                    {'loss': 2.87, 'grad_norm': 38.51298141479492, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:42<3:07:32,  1.98s/it]  5%|â–Œ         | 326/6000 [10:44<3:08:09,  1.99s/it]                                                    {'loss': 2.9435, 'grad_norm': 53.74885559082031, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [10:44<3:08:09,  1.99s/it]  5%|â–Œ         | 327/6000 [10:46<3:05:50,  1.97s/it]                                                    {'loss': 2.9183, 'grad_norm': 35.65134048461914, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [10:46<3:05:50,  1.97s/it]  5%|â–Œ         | 328/6000 [10:48<3:06:06,  1.97s/it]                                                    {'loss': 2.7869, 'grad_norm': 122.89153289794922, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [10:48<3:06:06,  1.97s/it]  5%|â–Œ         | 329/6000 [10:50<3:10:58,  2.02s/it]                                                    {'loss': 2.8318, 'grad_norm': 35.20167541503906, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [10:50<3:10:58,  2.02s/it]  6%|â–Œ         | 330/6000 [10:52<3:08:17,  1.99s/it]                                                    {'loss': 2.8379, 'grad_norm': 20.31801986694336, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [10:52<3:08:17,  1.99s/it]  6%|â–Œ         | 331/6000 [10:54<3:04:54,  1.96s/it]                                                    {'loss': 2.8423, 'grad_norm': 156.93951416015625, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [10:54<3:04:54,  1.96s/it]  6%|â–Œ         | 332/6000 [10:56<3:02:47,  1.93s/it]                                                    {'loss': 2.8155, 'grad_norm': 33.99882888793945, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [10:56<3:02:47,  1.93s/it]  6%|â–Œ         | 333/6000 [10:58<3:02:00,  1.93s/it]                                                    {'loss': 2.8222, 'grad_norm': 34.63416290283203, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [10:58<3:02:00,  1.93s/it]  6%|â–Œ         | 334/6000 [11:00<3:04:27,  1.95s/it]                                                    {'loss': 2.8642, 'grad_norm': 25.08719253540039, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [11:00<3:04:27,  1.95s/it]  6%|â–Œ         | 335/6000 [11:02<3:08:00,  1.99s/it]                                                    {'loss': 2.8176, 'grad_norm': 27.468544006347656, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:02<3:08:00,  1.99s/it]  6%|â–Œ         | 336/6000 [11:04<3:04:18,  1.95s/it]                                                    {'loss': 2.8039, 'grad_norm': 42.24891662597656, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:04<3:04:18,  1.95s/it]