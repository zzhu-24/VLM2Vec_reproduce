==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/train.log
W1112 19:23:00.851000 137996598626112 torch/distributed/run.py:779] 
W1112 19:23:00.851000 137996598626112 torch/distributed/run.py:779] *****************************************
W1112 19:23:00.851000 137996598626112 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1112 19:23:00.851000 137996598626112 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-12 19:23:10,896] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.18it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  8.06it/s]
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251112_192311-clo58v0r
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 12Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/clo58v0r
[2025-11-12 19:23:12,469] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  4.16it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00,  7.98it/s]
[2025-11-12 19:23:13,162] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-12 19:23:22,165] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-12 19:23:23,332] INFO [src.utils:19] PeftModel(
  (base_model): LoraModel(
    (model): Qwen2VLForConditionalGeneration(
      (visual): Qwen2VisionTransformerPretrainedModel(
        (patch_embed): PatchEmbed(
          (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
        )
        (rotary_pos_emb): VisionRotaryEmbedding()
        (blocks): ModuleList(
          (0-31): 32 x Qwen2VLVisionBlock(
            (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
            (attn): VisionFlashAttention2(
              (qkv): Linear(in_features=1280, out_features=3840, bias=True)
              (proj): Linear(in_features=1280, out_features=1280, bias=True)
            )
            (mlp): VisionMlp(
              (fc1): Linear(in_features=1280, out_features=5120, bias=True)
              (act): QuickGELUActivation()
              (fc2): Linear(in_features=5120, out_features=1280, bias=True)
            )
          )
        )
        (merger): PatchMerger(
          (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
          (mlp): Sequential(
            (0): Linear(in_features=5120, out_features=5120, bias=True)
            (1): GELU(approximate='none')
            (2): Linear(in_features=5120, out_features=1536, bias=True)
          )
        )
      )
      (model): Qwen2VLModel(
        (embed_tokens): Embedding(151936, 1536)
        (layers): ModuleList(
          (0-27): 28 x Qwen2VLDecoderLayer(
            (self_attn): Qwen2VLFlashAttention2(
              (q_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (k_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (v_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=256, bias=True)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=256, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (o_proj): lora.Linear(
                (base_layer): Linear(in_features=1536, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=1536, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (rotary_emb): Qwen2VLRotaryEmbedding()
            )
            (mlp): Qwen2MLP(
              (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
              (down_proj): lora.Linear(
                (base_layer): Linear(in_features=8960, out_features=1536, bias=False)
                (lora_dropout): ModuleDict(
                  (default): Dropout(p=0.1, inplace=False)
                )
                (lora_A): ModuleDict(
                  (default): Linear(in_features=8960, out_features=16, bias=False)
                )
                (lora_B): ModuleDict(
                  (default): Linear(in_features=16, out_features=1536, bias=False)
                )
                (lora_embedding_A): ParameterDict()
                (lora_embedding_B): ParameterDict()
                (lora_magnitude_vector): ModuleDict(
                  (default): lora.dora.DoraLinearLayer()
                )
              )
              (act_fn): SiLU()
            )
            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
          )
        )
        (norm): Qwen2RMSNorm((1536,), eps=1e-06)
        (rotary_emb): Qwen2VLRotaryEmbedding()
      )
      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
    )
  )
)
[2025-11-12 19:23:23,342] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-12 19:23:23,343] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-12 19:23:27,762] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-12 19:23:27,763] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-12 19:23:28,602] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-12 19:23:28,603] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-12 19:23:28,603] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-12 19:23:28,604] INFO [src.utils:19] ==================================================
[2025-11-12 19:23:28,605] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-12 19:23:28,606] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-12 19:23:28,606] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-12 19:23:28,607] INFO [src.utils:19] ==================================================
[2025-11-12 19:23:30,632] INFO [src.trainer:350] ***** Running training *****
[2025-11-12 19:23:30,633] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-12 19:23:30,633] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-12 19:23:30,634] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-12 19:23:30,634] INFO [src.trainer:350] ***** Running training *****
[2025-11-12 19:23:30,634] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-12 19:23:30,634] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-12 19:23:30,634] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-12 19:23:30,634] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-12 19:23:30,634] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-12 19:23:30,634] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-12 19:23:30,634] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-12 19:23:30,634] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-12 19:23:30,634] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-12 19:23:30,642] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-12 19:23:30,643] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-12 19:23:30,649] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-12 19:23:30,651] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.tail_token', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank0]:[W1112 19:23:33.338781874 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W1112 19:23:33.376856720 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:04<6:44:31,  4.05s/it]                                                  {'loss': 12.8489, 'grad_norm': 3637.894775390625, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:04<6:44:31,  4.05s/it]  0%|          | 2/6000 [00:06<5:21:16,  3.21s/it]                                                  {'loss': 10.2829, 'grad_norm': 3164.3837890625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:06<5:21:16,  3.21s/it]  0%|          | 3/6000 [00:09<5:02:30,  3.03s/it]                                                  {'loss': 9.9795, 'grad_norm': 2895.57861328125, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:09<5:02:30,  3.03s/it]  0%|          | 4/6000 [00:12<4:50:53,  2.91s/it]                                                  {'loss': 10.3571, 'grad_norm': 2546.370361328125, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:12<4:50:53,  2.91s/it]  0%|          | 5/6000 [00:14<4:44:25,  2.85s/it]                                                  {'loss': 11.138, 'grad_norm': 3182.4658203125, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:14<4:44:25,  2.85s/it]  0%|          | 6/6000 [00:17<4:39:27,  2.80s/it]                                                  {'loss': 11.1871, 'grad_norm': 2895.75927734375, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:17<4:39:27,  2.80s/it]  0%|          | 7/6000 [00:20<4:35:26,  2.76s/it]                                                  {'loss': 11.5521, 'grad_norm': 2875.697021484375, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:20<4:35:26,  2.76s/it]  0%|          | 8/6000 [00:22<4:32:35,  2.73s/it]                                                  {'loss': 11.2022, 'grad_norm': 2488.245361328125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:23<4:32:35,  2.73s/it]  0%|          | 9/6000 [00:25<4:33:47,  2.74s/it]                                                  {'loss': 7.9384, 'grad_norm': 1733.7479248046875, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:25<4:33:47,  2.74s/it]  0%|          | 10/6000 [00:28<4:31:28,  2.72s/it]                                                   {'loss': 9.9694, 'grad_norm': 1400.759033203125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:28<4:31:28,  2.72s/it]  0%|          | 11/6000 [00:31<4:41:00,  2.82s/it]                                                   {'loss': 11.7234, 'grad_norm': 1796.5794677734375, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:31<4:41:00,  2.82s/it]  0%|          | 12/6000 [00:34<4:45:23,  2.86s/it]                                                   {'loss': 9.3217, 'grad_norm': 1258.70166015625, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:34<4:45:23,  2.86s/it]  0%|          | 13/6000 [00:37<4:42:44,  2.83s/it]                                                   {'loss': 9.9899, 'grad_norm': 1116.4849853515625, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:37<4:42:44,  2.83s/it]  0%|          | 14/6000 [00:40<4:42:10,  2.83s/it]                                                   {'loss': 10.0423, 'grad_norm': 1166.0301513671875, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:40<4:42:10,  2.83s/it]  0%|          | 15/6000 [00:42<4:37:35,  2.78s/it]                                                   {'loss': 7.6534, 'grad_norm': 656.046630859375, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:42<4:37:35,  2.78s/it]  0%|          | 16/6000 [00:45<4:35:41,  2.76s/it]                                                   {'loss': 8.7663, 'grad_norm': 760.6834716796875, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:45<4:35:41,  2.76s/it]  0%|          | 17/6000 [00:48<4:34:11,  2.75s/it]                                                   {'loss': 7.5933, 'grad_norm': 554.1441040039062, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:48<4:34:11,  2.75s/it]  0%|          | 18/6000 [00:50<4:33:26,  2.74s/it]                                                   {'loss': 6.7682, 'grad_norm': 485.7388000488281, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:50<4:33:26,  2.74s/it]  0%|          | 19/6000 [00:53<4:33:10,  2.74s/it]                                                   {'loss': 7.8129, 'grad_norm': 586.6536254882812, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:53<4:33:10,  2.74s/it]  0%|          | 20/6000 [00:56<4:31:43,  2.73s/it]                                                   {'loss': 7.5438, 'grad_norm': 581.4738159179688, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:56<4:31:43,  2.73s/it]  0%|          | 21/6000 [00:59<4:35:14,  2.76s/it]                                                   {'loss': 6.202, 'grad_norm': 390.8455810546875, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:59<4:35:14,  2.76s/it]  0%|          | 22/6000 [01:01<4:35:54,  2.77s/it]                                                   {'loss': 7.2186, 'grad_norm': 517.6235961914062, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [01:01<4:35:54,  2.77s/it]  0%|          | 23/6000 [01:04<4:33:31,  2.75s/it]                                                   {'loss': 6.0664, 'grad_norm': 452.4148864746094, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [01:04<4:33:31,  2.75s/it]  0%|          | 24/6000 [01:07<4:34:55,  2.76s/it]                                                   {'loss': 4.7356, 'grad_norm': 213.955322265625, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [01:07<4:34:55,  2.76s/it]  0%|          | 25/6000 [01:10<4:34:52,  2.76s/it]                                                   {'loss': 5.7688, 'grad_norm': 464.72662353515625, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [01:10<4:34:52,  2.76s/it]  0%|          | 26/6000 [01:12<4:35:38,  2.77s/it]                                                   {'loss': 6.0684, 'grad_norm': 439.0755310058594, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [01:12<4:35:38,  2.77s/it]  0%|          | 27/6000 [01:15<4:34:34,  2.76s/it]                                                   {'loss': 5.6138, 'grad_norm': 445.5446472167969, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [01:15<4:34:34,  2.76s/it]  0%|          | 28/6000 [01:19<5:02:54,  3.04s/it]                                                   {'loss': 4.7752, 'grad_norm': 345.75238037109375, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [01:19<5:02:54,  3.04s/it]  0%|          | 29/6000 [01:22<4:51:09,  2.93s/it]                                                   {'loss': 4.6009, 'grad_norm': 321.67962646484375, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [01:22<4:51:09,  2.93s/it]  0%|          | 30/6000 [01:24<4:45:29,  2.87s/it]                                                   {'loss': 4.5565, 'grad_norm': 322.36029052734375, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:24<4:45:29,  2.87s/it]  1%|          | 31/6000 [01:27<4:41:09,  2.83s/it]                                                   {'loss': 4.014, 'grad_norm': 266.1317138671875, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:27<4:41:09,  2.83s/it]  1%|          | 32/6000 [01:30<4:37:22,  2.79s/it]                                                   {'loss': 4.2879, 'grad_norm': 340.0263366699219, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:30<4:37:22,  2.79s/it]  1%|          | 33/6000 [01:32<4:37:24,  2.79s/it]                                                   {'loss': 3.9791, 'grad_norm': 368.90533447265625, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:33<4:37:24,  2.79s/it]  1%|          | 34/6000 [01:35<4:35:48,  2.77s/it]                                                   {'loss': 3.9035, 'grad_norm': 344.434326171875, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:35<4:35:48,  2.77s/it]  1%|          | 35/6000 [01:38<4:36:24,  2.78s/it]                                                   {'loss': 3.6121, 'grad_norm': 275.05303955078125, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:38<4:36:24,  2.78s/it]  1%|          | 36/6000 [01:41<4:33:19,  2.75s/it]                                                   {'loss': 3.6586, 'grad_norm': 269.9792175292969, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:41<4:33:19,  2.75s/it]  1%|          | 37/6000 [01:43<4:33:56,  2.76s/it]                                                   {'loss': 3.2525, 'grad_norm': 174.4952850341797, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:43<4:33:56,  2.76s/it]  1%|          | 38/6000 [01:46<4:31:25,  2.73s/it]                                                   {'loss': 3.1504, 'grad_norm': 125.50125122070312, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:46<4:31:25,  2.73s/it]  1%|          | 39/6000 [01:49<4:31:03,  2.73s/it]                                                   {'loss': 3.0873, 'grad_norm': 75.86493682861328, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:49<4:31:03,  2.73s/it]  1%|          | 40/6000 [01:52<4:30:41,  2.73s/it]                                                   {'loss': 3.2574, 'grad_norm': 124.4648208618164, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:52<4:30:41,  2.73s/it]  1%|          | 41/6000 [01:54<4:30:28,  2.72s/it]                                                   {'loss': 3.1744, 'grad_norm': 97.31904602050781, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:54<4:30:28,  2.72s/it]  1%|          | 42/6000 [01:57<4:31:15,  2.73s/it]                                                   {'loss': 2.9334, 'grad_norm': 84.27971649169922, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:57<4:31:15,  2.73s/it]  1%|          | 43/6000 [02:01<5:03:22,  3.06s/it]                                                   {'loss': 2.9272, 'grad_norm': 39.87232208251953, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [02:01<5:03:22,  3.06s/it]  1%|          | 44/6000 [02:04<5:09:24,  3.12s/it]                                                   {'loss': 2.8458, 'grad_norm': 44.5139045715332, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [02:04<5:09:24,  3.12s/it]  1%|          | 45/6000 [02:07<4:59:01,  3.01s/it]                                                   {'loss': 2.8717, 'grad_norm': 76.02787017822266, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [02:07<4:59:01,  3.01s/it]  1%|          | 46/6000 [02:10<4:54:26,  2.97s/it]                                                   {'loss': 2.853, 'grad_norm': 73.36096954345703, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [02:10<4:54:26,  2.97s/it]  1%|          | 47/6000 [02:13<4:48:37,  2.91s/it]                                                   {'loss': 2.833, 'grad_norm': 51.8665771484375, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [02:13<4:48:37,  2.91s/it]  1%|          | 48/6000 [02:15<4:46:09,  2.88s/it]                                                   {'loss': 2.9252, 'grad_norm': 109.20504760742188, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [02:15<4:46:09,  2.88s/it]  1%|          | 49/6000 [02:18<4:40:50,  2.83s/it]                                                   {'loss': 2.8596, 'grad_norm': 43.3344841003418, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [02:18<4:40:50,  2.83s/it]  1%|          | 50/6000 [02:21<4:43:34,  2.86s/it]                                                   {'loss': 2.8601, 'grad_norm': 33.03749465942383, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [02:21<4:43:34,  2.86s/it]  1%|          | 51/6000 [02:24<4:40:04,  2.82s/it]                                                   {'loss': 2.8287, 'grad_norm': 43.0816764831543, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [02:24<4:40:04,  2.82s/it]  1%|          | 52/6000 [02:26<4:36:51,  2.79s/it]                                                   {'loss': 2.8282, 'grad_norm': 34.70443344116211, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [02:26<4:36:51,  2.79s/it]  1%|          | 53/6000 [02:29<4:38:00,  2.80s/it]                                                   {'loss': 3.0823, 'grad_norm': 102.26472473144531, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [02:29<4:38:00,  2.80s/it]  1%|          | 54/6000 [02:32<4:35:12,  2.78s/it]                                                   {'loss': 2.8294, 'grad_norm': 29.088428497314453, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [02:32<4:35:12,  2.78s/it]  1%|          | 55/6000 [02:35<4:33:53,  2.76s/it]                                                   {'loss': 2.8507, 'grad_norm': 28.479909896850586, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [02:35<4:33:53,  2.76s/it]  1%|          | 56/6000 [02:38<4:33:43,  2.76s/it]                                                   {'loss': 2.8044, 'grad_norm': 25.941661834716797, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [02:38<4:33:43,  2.76s/it]  1%|          | 57/6000 [02:40<4:33:10,  2.76s/it]                                                   {'loss': 2.8053, 'grad_norm': 20.764179229736328, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [02:40<4:33:10,  2.76s/it]  1%|          | 58/6000 [02:43<4:32:39,  2.75s/it]                                                   {'loss': 2.8325, 'grad_norm': 47.79952621459961, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [02:43<4:32:39,  2.75s/it]  1%|          | 59/6000 [02:46<4:29:43,  2.72s/it]                                                   {'loss': 2.8177, 'grad_norm': 36.42922592163086, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [02:46<4:29:43,  2.72s/it]  1%|          | 60/6000 [02:48<4:30:39,  2.73s/it]                                                   {'loss': 2.7904, 'grad_norm': 16.612335205078125, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [02:48<4:30:39,  2.73s/it]  1%|          | 61/6000 [02:51<4:28:43,  2.71s/it]                                                   {'loss': 2.8067, 'grad_norm': 15.938286781311035, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:51<4:28:43,  2.71s/it]  1%|          | 62/6000 [02:54<4:30:10,  2.73s/it]                                                   {'loss': 2.8182, 'grad_norm': 27.51541519165039, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:54<4:30:10,  2.73s/it]  1%|          | 63/6000 [02:57<4:30:22,  2.73s/it]                                                   {'loss': 2.7842, 'grad_norm': 23.36798858642578, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:57<4:30:22,  2.73s/it]  1%|          | 64/6000 [02:59<4:29:08,  2.72s/it]                                                   {'loss': 2.7957, 'grad_norm': 34.102699279785156, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:59<4:29:08,  2.72s/it]  1%|          | 65/6000 [03:02<4:28:37,  2.72s/it]                                                   {'loss': 2.8267, 'grad_norm': 14.527702331542969, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [03:02<4:28:37,  2.72s/it]  1%|          | 66/6000 [03:05<4:41:58,  2.85s/it]                                                   {'loss': 2.794, 'grad_norm': 22.706518173217773, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [03:05<4:41:58,  2.85s/it]  1%|          | 67/6000 [03:08<4:38:49,  2.82s/it]                                                   {'loss': 2.8468, 'grad_norm': 37.31147384643555, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [03:08<4:38:49,  2.82s/it]  1%|          | 68/6000 [03:11<4:35:29,  2.79s/it]                                                   {'loss': 2.8347, 'grad_norm': 25.7833194732666, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [03:11<4:35:29,  2.79s/it]  1%|          | 69/6000 [03:13<4:33:42,  2.77s/it]                                                   {'loss': 2.8431, 'grad_norm': 36.63591384887695, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [03:13<4:33:42,  2.77s/it]  1%|          | 70/6000 [03:16<4:34:41,  2.78s/it]                                                   {'loss': 2.7955, 'grad_norm': 9.834806442260742, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [03:16<4:34:41,  2.78s/it]  1%|          | 71/6000 [03:19<4:33:09,  2.76s/it]                                                   {'loss': 2.8064, 'grad_norm': 26.871082305908203, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [03:19<4:33:09,  2.76s/it]  1%|          | 72/6000 [03:22<4:44:42,  2.88s/it]                                                   {'loss': 2.79, 'grad_norm': 21.33159828186035, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [03:22<4:44:42,  2.88s/it]  1%|          | 73/6000 [03:25<4:41:54,  2.85s/it]                                                   {'loss': 2.8232, 'grad_norm': 12.467724800109863, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [03:25<4:41:54,  2.85s/it]  1%|          | 74/6000 [03:28<4:37:18,  2.81s/it]                                                   {'loss': 2.7793, 'grad_norm': 9.798771858215332, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [03:28<4:37:18,  2.81s/it]  1%|â–         | 75/6000 [03:30<4:42:48,  2.86s/it]                                                   {'loss': 2.801, 'grad_norm': 11.99463939666748, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [03:31<4:42:48,  2.86s/it]  1%|â–         | 76/6000 [03:33<4:43:06,  2.87s/it]                                                   {'loss': 2.8165, 'grad_norm': 13.177752494812012, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [03:33<4:43:06,  2.87s/it]  1%|â–         | 77/6000 [03:36<4:41:07,  2.85s/it]                                                   {'loss': 2.892, 'grad_norm': 28.892499923706055, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [03:36<4:41:07,  2.85s/it]  1%|â–         | 78/6000 [03:39<4:50:21,  2.94s/it]                                                   {'loss': 2.7863, 'grad_norm': 14.754598617553711, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [03:39<4:50:21,  2.94s/it]  1%|â–         | 79/6000 [03:42<4:53:41,  2.98s/it]                                                   {'loss': 2.7823, 'grad_norm': 13.957902908325195, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [03:42<4:53:41,  2.98s/it]  1%|â–         | 80/6000 [03:45<4:47:37,  2.92s/it]                                                   {'loss': 2.8051, 'grad_norm': 25.63140869140625, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [03:45<4:47:37,  2.92s/it]  1%|â–         | 81/6000 [03:48<4:42:33,  2.86s/it]                                                   {'loss': 2.7859, 'grad_norm': 8.740941047668457, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [03:48<4:42:33,  2.86s/it]  1%|â–         | 82/6000 [03:51<4:41:37,  2.86s/it]                                                   {'loss': 2.7765, 'grad_norm': 8.792569160461426, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [03:51<4:41:37,  2.86s/it]  1%|â–         | 83/6000 [03:53<4:38:16,  2.82s/it]                                                   {'loss': 2.7968, 'grad_norm': 7.376758575439453, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [03:54<4:38:16,  2.82s/it]  1%|â–         | 84/6000 [03:56<4:39:19,  2.83s/it]                                                   {'loss': 2.7817, 'grad_norm': 19.670948028564453, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [03:56<4:39:19,  2.83s/it]  1%|â–         | 85/6000 [03:59<4:47:51,  2.92s/it]                                                   {'loss': 2.7865, 'grad_norm': 19.02682113647461, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [03:59<4:47:51,  2.92s/it]  1%|â–         | 86/6000 [04:02<4:42:19,  2.86s/it]                                                   {'loss': 2.7959, 'grad_norm': 8.78785228729248, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [04:02<4:42:19,  2.86s/it]  1%|â–         | 87/6000 [04:05<4:38:07,  2.82s/it]                                                   {'loss': 2.8032, 'grad_norm': 10.605839729309082, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [04:05<4:38:07,  2.82s/it]  1%|â–         | 88/6000 [04:08<4:36:40,  2.81s/it]                                                   {'loss': 2.9074, 'grad_norm': 43.85819625854492, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [04:08<4:36:40,  2.81s/it]  1%|â–         | 89/6000 [04:10<4:32:55,  2.77s/it]                                                   {'loss': 2.783, 'grad_norm': 10.455320358276367, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [04:10<4:32:55,  2.77s/it]  2%|â–         | 90/6000 [04:13<4:32:13,  2.76s/it]                                                   {'loss': 2.8221, 'grad_norm': 10.972980499267578, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [04:13<4:32:13,  2.76s/it]  2%|â–         | 91/6000 [04:16<4:32:52,  2.77s/it]                                                   {'loss': 2.7906, 'grad_norm': 8.29973316192627, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [04:16<4:32:52,  2.77s/it]  2%|â–         | 92/6000 [04:19<4:43:52,  2.88s/it]                                                   {'loss': 2.7766, 'grad_norm': 6.6474151611328125, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [04:19<4:43:52,  2.88s/it]  2%|â–         | 93/6000 [04:22<4:44:28,  2.89s/it]                                                   {'loss': 2.7726, 'grad_norm': 8.148268699645996, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [04:22<4:44:28,  2.89s/it]  2%|â–         | 94/6000 [04:25<4:40:47,  2.85s/it]                                                   {'loss': 2.8027, 'grad_norm': 10.072439193725586, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [04:25<4:40:47,  2.85s/it]  2%|â–         | 95/6000 [04:28<4:38:47,  2.83s/it]                                                   {'loss': 2.7836, 'grad_norm': 6.054105281829834, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [04:28<4:38:47,  2.83s/it]  2%|â–         | 96/6000 [04:30<4:36:31,  2.81s/it]                                                   {'loss': 2.7778, 'grad_norm': 7.453190326690674, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [04:30<4:36:31,  2.81s/it]  2%|â–         | 97/6000 [04:33<4:33:02,  2.78s/it]                                                   {'loss': 2.9352, 'grad_norm': 48.03439712524414, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [04:33<4:33:02,  2.78s/it]  2%|â–         | 98/6000 [04:36<4:32:56,  2.77s/it]                                                   {'loss': 2.781, 'grad_norm': 8.157503128051758, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [04:36<4:32:56,  2.77s/it]  2%|â–         | 99/6000 [04:38<4:30:18,  2.75s/it]                                                   {'loss': 2.7771, 'grad_norm': 12.760002136230469, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [04:38<4:30:18,  2.75s/it]  2%|â–         | 100/6000 [04:41<4:28:14,  2.73s/it]                                                    {'loss': 2.778, 'grad_norm': 8.238170623779297, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [04:41<4:28:14,  2.73s/it]  2%|â–         | 101/6000 [04:45<4:59:53,  3.05s/it]                                                    {'loss': 2.7743, 'grad_norm': 16.771997451782227, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [04:45<4:59:53,  3.05s/it]  2%|â–         | 102/6000 [04:48<4:49:06,  2.94s/it]                                                    {'loss': 2.7984, 'grad_norm': 7.01178503036499, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [04:48<4:49:06,  2.94s/it]  2%|â–         | 103/6000 [04:50<4:46:12,  2.91s/it]                                                    {'loss': 2.7812, 'grad_norm': 10.965394973754883, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [04:50<4:46:12,  2.91s/it]  2%|â–         | 104/6000 [04:53<4:48:13,  2.93s/it]                                                    {'loss': 2.7777, 'grad_norm': 7.915133476257324, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [04:53<4:48:13,  2.93s/it]  2%|â–         | 105/6000 [04:56<4:44:47,  2.90s/it]                                                    {'loss': 2.7927, 'grad_norm': 17.78035545349121, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [04:56<4:44:47,  2.90s/it]  2%|â–         | 106/6000 [04:59<4:44:20,  2.89s/it]                                                    {'loss': 2.7791, 'grad_norm': 4.864046096801758, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [04:59<4:44:20,  2.89s/it]  2%|â–         | 107/6000 [05:02<4:37:26,  2.82s/it]                                                    {'loss': 2.7969, 'grad_norm': 10.808025360107422, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [05:02<4:37:26,  2.82s/it]  2%|â–         | 108/6000 [05:05<4:38:01,  2.83s/it]                                                    {'loss': 2.7785, 'grad_norm': 6.466994285583496, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [05:05<4:38:01,  2.83s/it]  2%|â–         | 109/6000 [05:08<4:47:37,  2.93s/it]                                                    {'loss': 2.8324, 'grad_norm': 9.552123069763184, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [05:08<4:47:37,  2.93s/it]  2%|â–         | 110/6000 [05:11<4:43:00,  2.88s/it]                                                    {'loss': 2.8477, 'grad_norm': 9.544389724731445, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [05:11<4:43:00,  2.88s/it]  2%|â–         | 111/6000 [05:13<4:42:20,  2.88s/it]                                                    {'loss': 2.7828, 'grad_norm': 6.799395561218262, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [05:13<4:42:20,  2.88s/it]  2%|â–         | 112/6000 [05:17<4:54:38,  3.00s/it]                                                    {'loss': 2.7804, 'grad_norm': 13.171011924743652, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [05:17<4:54:38,  3.00s/it]  2%|â–         | 113/6000 [05:19<4:46:13,  2.92s/it]                                                    {'loss': 2.7732, 'grad_norm': 6.482656478881836, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [05:19<4:46:13,  2.92s/it]  2%|â–         | 114/6000 [05:22<4:42:55,  2.88s/it]                                                    {'loss': 2.7733, 'grad_norm': 5.202889442443848, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [05:22<4:42:55,  2.88s/it]  2%|â–         | 115/6000 [05:25<4:39:28,  2.85s/it]                                                    {'loss': 2.8499, 'grad_norm': 16.107757568359375, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [05:25<4:39:28,  2.85s/it]  2%|â–         | 116/6000 [05:28<4:37:06,  2.83s/it]                                                    {'loss': 2.8074, 'grad_norm': 19.39562225341797, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [05:28<4:37:06,  2.83s/it]  2%|â–         | 117/6000 [05:31<4:36:17,  2.82s/it]                                                    {'loss': 2.9109, 'grad_norm': 7.520374774932861, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [05:31<4:36:17,  2.82s/it]  2%|â–         | 118/6000 [05:34<4:54:23,  3.00s/it]                                                    {'loss': 2.8086, 'grad_norm': 18.952136993408203, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [05:34<4:54:23,  3.00s/it]  2%|â–         | 119/6000 [05:37<4:44:51,  2.91s/it]                                                    {'loss': 2.7776, 'grad_norm': 5.199635982513428, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [05:37<4:44:51,  2.91s/it]  2%|â–         | 120/6000 [05:39<4:39:50,  2.86s/it]                                                    {'loss': 2.786, 'grad_norm': 15.989542007446289, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [05:39<4:39:50,  2.86s/it]  2%|â–         | 121/6000 [05:42<4:37:47,  2.84s/it]                                                    {'loss': 2.7766, 'grad_norm': 8.396831512451172, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [05:42<4:37:47,  2.84s/it]  2%|â–         | 122/6000 [05:45<4:41:08,  2.87s/it]                                                    {'loss': 2.782, 'grad_norm': 7.803498268127441, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [05:45<4:41:08,  2.87s/it]  2%|â–         | 123/6000 [05:48<4:36:38,  2.82s/it]                                                    {'loss': 2.7966, 'grad_norm': 11.93399429321289, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [05:48<4:36:38,  2.82s/it]  2%|â–         | 124/6000 [05:51<4:33:16,  2.79s/it]                                                    {'loss': 2.8285, 'grad_norm': 5.504461288452148, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [05:51<4:33:16,  2.79s/it]  2%|â–         | 125/6000 [05:54<4:39:22,  2.85s/it]                                                    {'loss': 2.9381, 'grad_norm': 10.174980163574219, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [05:54<4:39:22,  2.85s/it]  2%|â–         | 126/6000 [05:56<4:39:31,  2.86s/it]                                                    {'loss': 2.7882, 'grad_norm': 10.056753158569336, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [05:56<4:39:31,  2.86s/it]  2%|â–         | 127/6000 [05:59<4:40:59,  2.87s/it]                                                    {'loss': 2.7796, 'grad_norm': 8.390911102294922, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [05:59<4:40:59,  2.87s/it]  2%|â–         | 128/6000 [06:02<4:37:56,  2.84s/it]                                                    {'loss': 2.7962, 'grad_norm': 5.733127117156982, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [06:02<4:37:56,  2.84s/it]  2%|â–         | 129/6000 [06:05<4:35:28,  2.82s/it]                                                    {'loss': 2.8524, 'grad_norm': 5.46159029006958, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [06:05<4:35:28,  2.82s/it]  2%|â–         | 130/6000 [06:08<4:35:57,  2.82s/it]                                                    {'loss': 2.7781, 'grad_norm': 5.180747032165527, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [06:08<4:35:57,  2.82s/it]  2%|â–         | 131/6000 [06:11<4:35:11,  2.81s/it]                                                    {'loss': 2.7779, 'grad_norm': 10.620302200317383, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [06:11<4:35:11,  2.81s/it]  2%|â–         | 132/6000 [06:13<4:31:32,  2.78s/it]                                                    {'loss': 2.7934, 'grad_norm': 7.951864242553711, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [06:13<4:31:32,  2.78s/it]  2%|â–         | 133/6000 [06:16<4:30:26,  2.77s/it]                                                    {'loss': 2.773, 'grad_norm': 7.510024547576904, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [06:16<4:30:26,  2.77s/it]  2%|â–         | 134/6000 [06:19<4:35:53,  2.82s/it]                                                    {'loss': 2.7741, 'grad_norm': 7.534571170806885, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [06:19<4:35:53,  2.82s/it]  2%|â–         | 135/6000 [06:22<4:33:30,  2.80s/it]                                                    {'loss': 2.7906, 'grad_norm': 4.920023441314697, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [06:22<4:33:30,  2.80s/it]  2%|â–         | 136/6000 [06:24<4:31:30,  2.78s/it]                                                    {'loss': 2.7841, 'grad_norm': 5.058201789855957, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [06:24<4:31:30,  2.78s/it]  2%|â–         | 137/6000 [06:28<4:46:04,  2.93s/it]                                                    {'loss': 2.78, 'grad_norm': 9.007956504821777, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [06:28<4:46:04,  2.93s/it]  2%|â–         | 138/6000 [06:30<4:40:08,  2.87s/it]                                                    {'loss': 2.7747, 'grad_norm': 4.825573444366455, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [06:30<4:40:08,  2.87s/it]  2%|â–         | 139/6000 [06:33<4:39:53,  2.87s/it]                                                    {'loss': 2.8285, 'grad_norm': 5.048906326293945, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [06:33<4:39:53,  2.87s/it]  2%|â–         | 140/6000 [06:36<4:47:58,  2.95s/it]                                                    {'loss': 2.7893, 'grad_norm': 8.247395515441895, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [06:36<4:47:58,  2.95s/it]  2%|â–         | 141/6000 [06:39<4:43:36,  2.90s/it]                                                    {'loss': 2.7882, 'grad_norm': 12.050835609436035, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [06:39<4:43:36,  2.90s/it]  2%|â–         | 142/6000 [06:42<4:38:17,  2.85s/it]                                                    {'loss': 2.7719, 'grad_norm': 4.877791404724121, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [06:42<4:38:17,  2.85s/it]  2%|â–         | 143/6000 [06:45<4:35:23,  2.82s/it]                                                    {'loss': 2.768, 'grad_norm': 6.51579475402832, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [06:45<4:35:23,  2.82s/it]  2%|â–         | 144/6000 [06:47<4:34:03,  2.81s/it]                                                    {'loss': 2.7901, 'grad_norm': 6.432374954223633, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [06:47<4:34:03,  2.81s/it]  2%|â–         | 145/6000 [06:50<4:30:39,  2.77s/it]                                                    {'loss': 2.8002, 'grad_norm': 5.0942535400390625, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [06:50<4:30:39,  2.77s/it]  2%|â–         | 146/6000 [06:53<4:31:40,  2.78s/it]                                                    {'loss': 2.807, 'grad_norm': 5.394997596740723, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [06:53<4:31:40,  2.78s/it]  2%|â–         | 147/6000 [06:56<4:30:24,  2.77s/it]                                                    {'loss': 2.7801, 'grad_norm': 9.579854011535645, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [06:56<4:30:24,  2.77s/it]  2%|â–         | 148/6000 [06:58<4:29:48,  2.77s/it]                                                    {'loss': 2.7854, 'grad_norm': 6.944116592407227, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [06:58<4:29:48,  2.77s/it]  2%|â–         | 149/6000 [07:01<4:28:13,  2.75s/it]                                                    {'loss': 2.7737, 'grad_norm': 5.422399044036865, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [07:01<4:28:13,  2.75s/it]  2%|â–Ž         | 150/6000 [07:04<4:27:22,  2.74s/it]                                                    {'loss': 2.7858, 'grad_norm': 5.324855327606201, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [07:04<4:27:22,  2.74s/it]  3%|â–Ž         | 151/6000 [07:07<4:28:50,  2.76s/it]                                                    {'loss': 2.774, 'grad_norm': 6.569355010986328, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [07:07<4:28:50,  2.76s/it]  3%|â–Ž         | 152/6000 [07:09<4:28:27,  2.75s/it]                                                    {'loss': 2.7938, 'grad_norm': 6.455658912658691, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [07:09<4:28:27,  2.75s/it]  3%|â–Ž         | 153/6000 [07:12<4:27:26,  2.74s/it]                                                    {'loss': 2.7779, 'grad_norm': 6.252767562866211, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [07:12<4:27:26,  2.74s/it]  3%|â–Ž         | 154/6000 [07:15<4:37:20,  2.85s/it]                                                    {'loss': 2.7828, 'grad_norm': 8.847806930541992, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [07:15<4:37:20,  2.85s/it]  3%|â–Ž         | 155/6000 [07:18<4:34:59,  2.82s/it]                                                    {'loss': 2.7983, 'grad_norm': 9.272771835327148, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [07:18<4:34:59,  2.82s/it]  3%|â–Ž         | 156/6000 [07:21<4:35:33,  2.83s/it]                                                    {'loss': 2.8392, 'grad_norm': 9.527286529541016, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [07:21<4:35:33,  2.83s/it]  3%|â–Ž         | 157/6000 [07:24<4:47:19,  2.95s/it]                                                    {'loss': 2.7934, 'grad_norm': 8.307581901550293, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [07:24<4:47:19,  2.95s/it]  3%|â–Ž         | 158/6000 [07:27<4:40:46,  2.88s/it]                                                    {'loss': 2.7731, 'grad_norm': 8.164628028869629, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [07:27<4:40:46,  2.88s/it]  3%|â–Ž         | 159/6000 [07:30<4:35:55,  2.83s/it]                                                    {'loss': 2.7886, 'grad_norm': 5.021883964538574, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [07:30<4:35:55,  2.83s/it]  3%|â–Ž         | 160/6000 [07:33<4:52:37,  3.01s/it]                                                    {'loss': 2.7964, 'grad_norm': 11.360126495361328, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [07:33<4:52:37,  3.01s/it]  3%|â–Ž         | 161/6000 [07:36<4:50:23,  2.98s/it]                                                    {'loss': 2.7775, 'grad_norm': 5.7436137199401855, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [07:36<4:50:23,  2.98s/it]  3%|â–Ž         | 162/6000 [07:39<4:42:48,  2.91s/it]                                                    {'loss': 2.7842, 'grad_norm': 9.553457260131836, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [07:39<4:42:48,  2.91s/it]  3%|â–Ž         | 163/6000 [07:42<4:57:53,  3.06s/it]                                                    {'loss': 2.7742, 'grad_norm': 5.612649440765381, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [07:42<4:57:53,  3.06s/it]  3%|â–Ž         | 164/6000 [07:45<4:47:36,  2.96s/it]                                                    {'loss': 2.903, 'grad_norm': 4.33889102935791, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [07:45<4:47:36,  2.96s/it]  3%|â–Ž         | 165/6000 [07:48<4:46:10,  2.94s/it]                                                    {'loss': 2.7872, 'grad_norm': 8.713345527648926, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [07:48<4:46:10,  2.94s/it]  3%|â–Ž         | 166/6000 [07:50<4:39:56,  2.88s/it]                                                    {'loss': 2.7799, 'grad_norm': 8.13367748260498, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [07:50<4:39:56,  2.88s/it]  3%|â–Ž         | 167/6000 [07:53<4:37:22,  2.85s/it]                                                    {'loss': 2.7988, 'grad_norm': 14.475629806518555, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [07:53<4:37:22,  2.85s/it]  3%|â–Ž         | 168/6000 [07:56<4:35:48,  2.84s/it]                                                    {'loss': 2.8114, 'grad_norm': 6.78063440322876, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [07:56<4:35:48,  2.84s/it]  3%|â–Ž         | 169/6000 [07:59<4:45:57,  2.94s/it]                                                    {'loss': 2.8147, 'grad_norm': 7.647359848022461, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [07:59<4:45:57,  2.94s/it]  3%|â–Ž         | 170/6000 [08:02<4:38:26,  2.87s/it]                                                    {'loss': 2.7843, 'grad_norm': 7.246089935302734, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [08:02<4:38:26,  2.87s/it]  3%|â–Ž         | 171/6000 [08:05<4:33:56,  2.82s/it]                                                    {'loss': 2.7697, 'grad_norm': 7.98048210144043, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [08:05<4:33:56,  2.82s/it]  3%|â–Ž         | 172/6000 [08:07<4:36:56,  2.85s/it]                                                    {'loss': 2.8021, 'grad_norm': 7.100424766540527, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [08:07<4:36:56,  2.85s/it]  3%|â–Ž         | 173/6000 [08:10<4:35:09,  2.83s/it]                                                    {'loss': 2.7865, 'grad_norm': 6.399461269378662, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [08:10<4:35:09,  2.83s/it]  3%|â–Ž         | 174/6000 [08:14<4:53:18,  3.02s/it]                                                    {'loss': 2.7814, 'grad_norm': 5.435342311859131, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [08:14<4:53:18,  3.02s/it]  3%|â–Ž         | 175/6000 [08:17<4:48:39,  2.97s/it]                                                    {'loss': 2.7912, 'grad_norm': 8.22487735748291, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [08:17<4:48:39,  2.97s/it]  3%|â–Ž         | 176/6000 [08:20<4:52:14,  3.01s/it]                                                    {'loss': 2.7783, 'grad_norm': 8.055106163024902, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [08:20<4:52:14,  3.01s/it]  3%|â–Ž         | 177/6000 [08:22<4:44:31,  2.93s/it]                                                    {'loss': 2.7937, 'grad_norm': 5.06854772567749, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [08:22<4:44:31,  2.93s/it]  3%|â–Ž         | 178/6000 [08:25<4:40:34,  2.89s/it]                                                    {'loss': 2.7918, 'grad_norm': 6.253116130828857, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [08:25<4:40:34,  2.89s/it]  3%|â–Ž         | 179/6000 [08:28<4:37:08,  2.86s/it]                                                    {'loss': 2.8014, 'grad_norm': 6.690028190612793, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [08:28<4:37:08,  2.86s/it]  3%|â–Ž         | 180/6000 [08:31<4:37:49,  2.86s/it]                                                    {'loss': 2.7881, 'grad_norm': 5.233399868011475, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [08:31<4:37:49,  2.86s/it]  3%|â–Ž         | 181/6000 [08:34<4:35:13,  2.84s/it]                                                    {'loss': 2.8015, 'grad_norm': 6.23856782913208, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [08:34<4:35:13,  2.84s/it]  3%|â–Ž         | 182/6000 [08:36<4:33:24,  2.82s/it]                                                    {'loss': 2.8256, 'grad_norm': 5.404327869415283, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [08:36<4:33:24,  2.82s/it]  3%|â–Ž         | 183/6000 [08:39<4:29:46,  2.78s/it]                                                    {'loss': 2.7754, 'grad_norm': 6.162595748901367, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [08:39<4:29:46,  2.78s/it]  3%|â–Ž         | 184/6000 [08:42<4:28:57,  2.77s/it]                                                    {'loss': 2.7818, 'grad_norm': 7.378725528717041, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [08:42<4:28:57,  2.77s/it]  3%|â–Ž         | 185/6000 [08:45<4:30:03,  2.79s/it]                                                    {'loss': 2.7866, 'grad_norm': 5.110113620758057, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [08:45<4:30:03,  2.79s/it]  3%|â–Ž         | 186/6000 [08:47<4:29:30,  2.78s/it]                                                    {'loss': 2.7785, 'grad_norm': 6.070524215698242, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [08:47<4:29:30,  2.78s/it]  3%|â–Ž         | 187/6000 [08:50<4:28:46,  2.77s/it]                                                    {'loss': 2.7858, 'grad_norm': 7.611653804779053, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [08:50<4:28:46,  2.77s/it]  3%|â–Ž         | 188/6000 [08:53<4:28:22,  2.77s/it]                                                    {'loss': 2.7855, 'grad_norm': 6.416444301605225, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [08:53<4:28:22,  2.77s/it]  3%|â–Ž         | 189/6000 [08:56<4:28:47,  2.78s/it]                                                    {'loss': 2.7775, 'grad_norm': 7.366429328918457, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [08:56<4:28:47,  2.78s/it]  3%|â–Ž         | 190/6000 [08:58<4:25:17,  2.74s/it]                                                    {'loss': 2.7838, 'grad_norm': 5.737556457519531, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [08:58<4:25:17,  2.74s/it]  3%|â–Ž         | 191/6000 [09:01<4:27:38,  2.76s/it]                                                    {'loss': 2.7832, 'grad_norm': 5.594120979309082, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [09:01<4:27:38,  2.76s/it]  3%|â–Ž         | 192/6000 [09:05<4:46:05,  2.96s/it]                                                    {'loss': 2.8727, 'grad_norm': 6.7309041023254395, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [09:05<4:46:05,  2.96s/it]  3%|â–Ž         | 193/6000 [09:08<4:50:04,  3.00s/it]                                                    {'loss': 2.7784, 'grad_norm': 6.9964518547058105, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [09:08<4:50:04,  3.00s/it]  3%|â–Ž         | 194/6000 [09:10<4:41:59,  2.91s/it]                                                    {'loss': 2.7818, 'grad_norm': 5.766421318054199, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [09:10<4:41:59,  2.91s/it]  3%|â–Ž         | 195/6000 [09:13<4:38:24,  2.88s/it]                                                    {'loss': 2.8099, 'grad_norm': 11.380470275878906, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [09:13<4:38:24,  2.88s/it]  3%|â–Ž         | 196/6000 [09:16<4:35:18,  2.85s/it]                                                    {'loss': 2.7883, 'grad_norm': 8.426620483398438, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [09:16<4:35:18,  2.85s/it]  3%|â–Ž         | 197/6000 [09:19<4:37:10,  2.87s/it]                                                    {'loss': 2.7943, 'grad_norm': 4.208181381225586, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [09:19<4:37:10,  2.87s/it]  3%|â–Ž         | 198/6000 [09:22<4:38:22,  2.88s/it]                                                    {'loss': 2.7868, 'grad_norm': 5.56076192855835, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [09:22<4:38:22,  2.88s/it]  3%|â–Ž         | 199/6000 [09:25<4:33:26,  2.83s/it]                                                    {'loss': 2.771, 'grad_norm': 4.902326583862305, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [09:25<4:33:26,  2.83s/it]  3%|â–Ž         | 200/6000 [09:28<4:46:07,  2.96s/it]                                                    {'loss': 2.7771, 'grad_norm': 3.854548215866089, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [09:28<4:46:07,  2.96s/it]  3%|â–Ž         | 201/6000 [09:31<4:41:09,  2.91s/it]                                                    {'loss': 2.7884, 'grad_norm': 7.959254741668701, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [09:31<4:41:09,  2.91s/it]  3%|â–Ž         | 202/6000 [09:33<4:36:28,  2.86s/it]                                                    {'loss': 2.7831, 'grad_norm': 5.968593597412109, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [09:33<4:36:28,  2.86s/it]  3%|â–Ž         | 203/6000 [09:36<4:33:39,  2.83s/it]                                                    {'loss': 2.783, 'grad_norm': 7.248470306396484, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [09:36<4:33:39,  2.83s/it]  3%|â–Ž         | 204/6000 [09:39<4:35:18,  2.85s/it]                                                    {'loss': 2.787, 'grad_norm': 5.17315673828125, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [09:39<4:35:18,  2.85s/it]  3%|â–Ž         | 205/6000 [09:42<4:30:36,  2.80s/it]                                                    {'loss': 2.7767, 'grad_norm': 8.648390769958496, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [09:42<4:30:36,  2.80s/it]  3%|â–Ž         | 206/6000 [09:44<4:26:47,  2.76s/it]                                                    {'loss': 2.8235, 'grad_norm': 6.861793041229248, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [09:44<4:26:47,  2.76s/it]  3%|â–Ž         | 207/6000 [09:47<4:25:36,  2.75s/it]                                                    {'loss': 2.8683, 'grad_norm': 6.961474895477295, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [09:47<4:25:36,  2.75s/it]  3%|â–Ž         | 208/6000 [09:50<4:23:46,  2.73s/it]                                                    {'loss': 2.7912, 'grad_norm': 4.2401957511901855, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [09:50<4:23:46,  2.73s/it]  3%|â–Ž         | 209/6000 [09:53<4:25:28,  2.75s/it]                                                    {'loss': 2.7835, 'grad_norm': 8.810465812683105, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [09:53<4:25:28,  2.75s/it]  4%|â–Ž         | 210/6000 [09:56<4:38:16,  2.88s/it]                                                    {'loss': 2.7773, 'grad_norm': 8.506933212280273, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [09:56<4:38:16,  2.88s/it]  4%|â–Ž         | 211/6000 [09:59<4:39:03,  2.89s/it]                                                    {'loss': 2.7983, 'grad_norm': 9.065832138061523, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [09:59<4:39:03,  2.89s/it]  4%|â–Ž         | 212/6000 [10:02<4:45:55,  2.96s/it]                                                    {'loss': 2.7775, 'grad_norm': 6.059235095977783, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [10:02<4:45:55,  2.96s/it]  4%|â–Ž         | 213/6000 [10:05<4:42:56,  2.93s/it]                                                    {'loss': 2.7839, 'grad_norm': 5.294758319854736, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [10:05<4:42:56,  2.93s/it]  4%|â–Ž         | 214/6000 [10:07<4:37:25,  2.88s/it]                                                    {'loss': 2.7736, 'grad_norm': 10.91677188873291, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [10:07<4:37:25,  2.88s/it]  4%|â–Ž         | 215/6000 [10:10<4:33:31,  2.84s/it]                                                    {'loss': 2.8132, 'grad_norm': 7.215562343597412, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [10:10<4:33:31,  2.84s/it]  4%|â–Ž         | 216/6000 [10:13<4:33:18,  2.84s/it]                                                    {'loss': 2.7859, 'grad_norm': 7.827420234680176, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [10:13<4:33:18,  2.84s/it]  4%|â–Ž         | 217/6000 [10:16<4:35:03,  2.85s/it]                                                    {'loss': 2.8235, 'grad_norm': 7.64764928817749, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [10:16<4:35:03,  2.85s/it]  4%|â–Ž         | 218/6000 [10:19<4:34:55,  2.85s/it]                                                    {'loss': 2.7982, 'grad_norm': 3.510359048843384, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [10:19<4:34:55,  2.85s/it]  4%|â–Ž         | 219/6000 [10:22<4:32:21,  2.83s/it]                                                    {'loss': 2.8303, 'grad_norm': 4.692403793334961, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [10:22<4:32:21,  2.83s/it]  4%|â–Ž         | 220/6000 [10:24<4:29:59,  2.80s/it]                                                    {'loss': 2.7789, 'grad_norm': 4.615904808044434, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [10:24<4:29:59,  2.80s/it]  4%|â–Ž         | 221/6000 [10:27<4:29:14,  2.80s/it]                                                    {'loss': 2.8019, 'grad_norm': 4.453727722167969, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [10:27<4:29:14,  2.80s/it]  4%|â–Ž         | 222/6000 [10:30<4:28:47,  2.79s/it]                                                    {'loss': 2.7817, 'grad_norm': 6.403298854827881, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [10:30<4:28:47,  2.79s/it]  4%|â–Ž         | 223/6000 [10:33<4:28:06,  2.78s/it]                                                    {'loss': 2.7658, 'grad_norm': 3.70119309425354, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [10:33<4:28:06,  2.78s/it]  4%|â–Ž         | 224/6000 [10:35<4:25:13,  2.76s/it]                                                    {'loss': 2.8244, 'grad_norm': 7.098489284515381, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [10:35<4:25:13,  2.76s/it]  4%|â–         | 225/6000 [10:38<4:26:36,  2.77s/it]                                                    {'loss': 2.7723, 'grad_norm': 5.432816028594971, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [10:38<4:26:36,  2.77s/it]  4%|â–         | 226/6000 [10:41<4:40:15,  2.91s/it]                                                    {'loss': 2.7823, 'grad_norm': 6.172773838043213, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [10:41<4:40:15,  2.91s/it]  4%|â–         | 227/6000 [10:44<4:35:07,  2.86s/it]                                                    {'loss': 2.8133, 'grad_norm': 4.016733169555664, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [10:44<4:35:07,  2.86s/it]  4%|â–         | 228/6000 [10:47<4:32:16,  2.83s/it]                                                    {'loss': 2.7869, 'grad_norm': 5.743722438812256, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [10:47<4:32:16,  2.83s/it]  4%|â–         | 229/6000 [10:50<4:28:17,  2.79s/it]                                                    {'loss': 2.7858, 'grad_norm': 3.241140365600586, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [10:50<4:28:17,  2.79s/it]  4%|â–         | 230/6000 [10:52<4:30:02,  2.81s/it]                                                    {'loss': 2.8055, 'grad_norm': 5.661441326141357, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [10:52<4:30:02,  2.81s/it]  4%|â–         | 231/6000 [10:55<4:28:26,  2.79s/it]                                                    {'loss': 2.787, 'grad_norm': 3.6401021480560303, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [10:55<4:28:26,  2.79s/it]  4%|â–         | 232/6000 [10:58<4:29:03,  2.80s/it]                                                    {'loss': 2.8025, 'grad_norm': 22.71535873413086, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [10:58<4:29:03,  2.80s/it]  4%|â–         | 233/6000 [11:01<4:38:40,  2.90s/it]                                                    {'loss': 2.7692, 'grad_norm': 4.696514129638672, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [11:01<4:38:40,  2.90s/it]  4%|â–         | 234/6000 [11:04<4:43:18,  2.95s/it]                                                    {'loss': 2.7689, 'grad_norm': 4.750247478485107, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [11:04<4:43:18,  2.95s/it]  4%|â–         | 235/6000 [11:07<4:35:18,  2.87s/it]                                                    {'loss': 2.8442, 'grad_norm': 2.826110601425171, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [11:07<4:35:18,  2.87s/it]  4%|â–         | 236/6000 [11:10<4:32:55,  2.84s/it]                                                    {'loss': 2.8098, 'grad_norm': 7.40544319152832, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [11:10<4:32:55,  2.84s/it]  4%|â–         | 237/6000 [11:13<4:37:57,  2.89s/it]                                                    {'loss': 2.7772, 'grad_norm': 3.583310127258301, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [11:13<4:37:57,  2.89s/it]  4%|â–         | 238/6000 [11:16<4:49:00,  3.01s/it]                                                    {'loss': 2.8439, 'grad_norm': 4.314332008361816, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [11:16<4:49:00,  3.01s/it]  4%|â–         | 239/6000 [11:19<4:44:41,  2.96s/it]                                                    {'loss': 2.777, 'grad_norm': 3.2210614681243896, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [11:19<4:44:41,  2.96s/it]  4%|â–         | 240/6000 [11:22<4:38:37,  2.90s/it]                                                    {'loss': 2.792, 'grad_norm': 3.9851126670837402, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [11:22<4:38:37,  2.90s/it]  4%|â–         | 241/6000 [11:24<4:34:48,  2.86s/it]                                                    {'loss': 2.7923, 'grad_norm': 9.399885177612305, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [11:24<4:34:48,  2.86s/it]  4%|â–         | 242/6000 [11:27<4:30:00,  2.81s/it]                                                    {'loss': 2.8086, 'grad_norm': 4.08465576171875, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [11:27<4:30:00,  2.81s/it]  4%|â–         | 243/6000 [11:30<4:38:04,  2.90s/it]                                                    {'loss': 2.7731, 'grad_norm': 4.158974647521973, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [11:30<4:38:04,  2.90s/it]  4%|â–         | 244/6000 [11:33<4:34:13,  2.86s/it]                                                    {'loss': 2.7736, 'grad_norm': 3.5569674968719482, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [11:33<4:34:13,  2.86s/it]  4%|â–         | 245/6000 [11:36<4:33:25,  2.85s/it]                                                    {'loss': 2.7802, 'grad_norm': 6.005006313323975, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [11:36<4:33:25,  2.85s/it]  4%|â–         | 246/6000 [11:39<4:35:32,  2.87s/it]                                                    {'loss': 2.7814, 'grad_norm': 4.852730751037598, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [11:39<4:35:32,  2.87s/it]  4%|â–         | 247/6000 [11:42<4:37:38,  2.90s/it]                                                    {'loss': 2.7714, 'grad_norm': 3.9576516151428223, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [11:42<4:37:38,  2.90s/it]  4%|â–         | 248/6000 [11:44<4:37:01,  2.89s/it]                                                    {'loss': 2.7703, 'grad_norm': 5.007737636566162, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [11:44<4:37:01,  2.89s/it]  4%|â–         | 249/6000 [11:47<4:32:08,  2.84s/it]                                                    {'loss': 2.779, 'grad_norm': 4.329067230224609, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [11:47<4:32:08,  2.84s/it]  4%|â–         | 250/6000 [11:50<4:30:02,  2.82s/it]                                                    {'loss': 2.7852, 'grad_norm': 3.8240606784820557, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [11:50<4:30:02,  2.82s/it]  4%|â–         | 251/6000 [11:53<4:27:27,  2.79s/it]                                                    {'loss': 2.7853, 'grad_norm': 4.292458534240723, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [11:53<4:27:27,  2.79s/it]  4%|â–         | 252/6000 [11:56<4:32:16,  2.84s/it]                                                    {'loss': 2.8063, 'grad_norm': 4.387660980224609, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [11:56<4:32:16,  2.84s/it]  4%|â–         | 253/6000 [11:58<4:31:16,  2.83s/it]                                                    {'loss': 2.804, 'grad_norm': 7.9821271896362305, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [11:58<4:31:16,  2.83s/it]  4%|â–         | 254/6000 [12:01<4:32:16,  2.84s/it]                                                    {'loss': 2.77, 'grad_norm': 4.768737316131592, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [12:01<4:32:16,  2.84s/it]  4%|â–         | 255/6000 [12:04<4:26:44,  2.79s/it]                                                    {'loss': 2.762, 'grad_norm': 4.213633060455322, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [12:04<4:26:44,  2.79s/it]  4%|â–         | 256/6000 [12:07<4:38:28,  2.91s/it]                                                    {'loss': 2.7882, 'grad_norm': 6.625791549682617, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [12:07<4:38:28,  2.91s/it]  4%|â–         | 257/6000 [12:10<4:33:41,  2.86s/it]                                                    {'loss': 2.7818, 'grad_norm': 4.2468485832214355, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [12:10<4:33:41,  2.86s/it]  4%|â–         | 258/6000 [12:13<4:30:13,  2.82s/it]                                                    {'loss': 2.7796, 'grad_norm': 4.797693729400635, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [12:13<4:30:13,  2.82s/it]  4%|â–         | 259/6000 [12:15<4:27:48,  2.80s/it]                                                    {'loss': 2.7647, 'grad_norm': 7.4893012046813965, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [12:15<4:27:48,  2.80s/it]  4%|â–         | 260/6000 [12:18<4:26:28,  2.79s/it]                                                    {'loss': 2.7869, 'grad_norm': 6.160087585449219, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [12:18<4:26:28,  2.79s/it]  4%|â–         | 261/6000 [12:21<4:31:11,  2.84s/it]                                                    {'loss': 2.7857, 'grad_norm': 4.236968994140625, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [12:21<4:31:11,  2.84s/it]  4%|â–         | 262/6000 [12:24<4:29:42,  2.82s/it]                                                    {'loss': 2.7753, 'grad_norm': 8.680157661437988, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [12:24<4:29:42,  2.82s/it]  4%|â–         | 263/6000 [12:27<4:38:26,  2.91s/it]                                                    {'loss': 2.8422, 'grad_norm': 3.6672866344451904, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [12:27<4:38:26,  2.91s/it]  4%|â–         | 264/6000 [12:30<4:32:33,  2.85s/it]                                                    {'loss': 2.7782, 'grad_norm': 4.754322528839111, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [12:30<4:32:33,  2.85s/it]  4%|â–         | 265/6000 [12:33<4:35:32,  2.88s/it]                                                    {'loss': 2.7731, 'grad_norm': 6.201951503753662, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [12:33<4:35:32,  2.88s/it]  4%|â–         | 266/6000 [12:36<4:36:17,  2.89s/it]                                                    {'loss': 2.8109, 'grad_norm': 3.606884002685547, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [12:36<4:36:17,  2.89s/it]  4%|â–         | 267/6000 [12:38<4:31:05,  2.84s/it]                                                    {'loss': 2.7688, 'grad_norm': 4.153329849243164, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [12:38<4:31:05,  2.84s/it]  4%|â–         | 268/6000 [12:41<4:29:11,  2.82s/it]                                                    {'loss': 2.8392, 'grad_norm': 4.725314617156982, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [12:41<4:29:11,  2.82s/it]  4%|â–         | 269/6000 [12:44<4:32:15,  2.85s/it]                                                    {'loss': 2.7798, 'grad_norm': 6.5745530128479, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [12:44<4:32:15,  2.85s/it]  4%|â–         | 270/6000 [12:47<4:29:33,  2.82s/it]                                                    {'loss': 2.7568, 'grad_norm': 9.793485641479492, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [12:47<4:29:33,  2.82s/it]  5%|â–         | 271/6000 [12:49<4:26:50,  2.79s/it]                                                    {'loss': 2.8054, 'grad_norm': 5.672661304473877, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [12:49<4:26:50,  2.79s/it]  5%|â–         | 272/6000 [12:52<4:25:56,  2.79s/it]                                                    {'loss': 2.7735, 'grad_norm': 9.890403747558594, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [12:52<4:25:56,  2.79s/it]  5%|â–         | 273/6000 [12:55<4:24:21,  2.77s/it]                                                    {'loss': 2.789, 'grad_norm': 7.974658012390137, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [12:55<4:24:21,  2.77s/it]  5%|â–         | 274/6000 [12:58<4:35:52,  2.89s/it]                                                    {'loss': 2.7739, 'grad_norm': 5.833853244781494, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [12:58<4:35:52,  2.89s/it]  5%|â–         | 275/6000 [13:01<4:33:31,  2.87s/it]                                                    {'loss': 2.8017, 'grad_norm': 10.23381519317627, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [13:01<4:33:31,  2.87s/it]  5%|â–         | 276/6000 [13:04<4:31:50,  2.85s/it]                                                    {'loss': 2.8431, 'grad_norm': 6.369607925415039, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [13:04<4:31:50,  2.85s/it]  5%|â–         | 277/6000 [13:06<4:26:15,  2.79s/it]                                                    {'loss': 2.7649, 'grad_norm': 6.525539875030518, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [13:06<4:26:15,  2.79s/it]  5%|â–         | 278/6000 [13:09<4:25:00,  2.78s/it]                                                    {'loss': 2.7723, 'grad_norm': 8.32983684539795, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [13:09<4:25:00,  2.78s/it]  5%|â–         | 279/6000 [13:12<4:26:38,  2.80s/it]                                                    {'loss': 2.7848, 'grad_norm': 13.920561790466309, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [13:12<4:26:38,  2.80s/it]  5%|â–         | 280/6000 [13:15<4:23:55,  2.77s/it]                                                    {'loss': 2.8072, 'grad_norm': 10.327654838562012, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [13:15<4:23:55,  2.77s/it]  5%|â–         | 281/6000 [13:17<4:23:04,  2.76s/it]                                                    {'loss': 2.7932, 'grad_norm': 8.625899314880371, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [13:17<4:23:04,  2.76s/it]  5%|â–         | 282/6000 [13:21<4:33:24,  2.87s/it]                                                    {'loss': 2.7837, 'grad_norm': 7.285622596740723, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [13:21<4:33:24,  2.87s/it]  5%|â–         | 283/6000 [13:24<4:39:01,  2.93s/it]                                                    {'loss': 2.8049, 'grad_norm': 17.556766510009766, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [13:24<4:39:01,  2.93s/it]  5%|â–         | 284/6000 [13:27<4:54:14,  3.09s/it]                                                    {'loss': 2.7724, 'grad_norm': 5.4386162757873535, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [13:27<4:54:14,  3.09s/it]  5%|â–         | 285/6000 [13:30<4:48:27,  3.03s/it]                                                    {'loss': 2.7815, 'grad_norm': 5.424343109130859, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [13:30<4:48:27,  3.03s/it]  5%|â–         | 286/6000 [13:33<4:37:23,  2.91s/it]                                                    {'loss': 2.8002, 'grad_norm': 9.824044227600098, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [13:33<4:37:23,  2.91s/it]  5%|â–         | 287/6000 [13:35<4:33:35,  2.87s/it]                                                    {'loss': 2.8026, 'grad_norm': 6.733323097229004, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [13:35<4:33:35,  2.87s/it]  5%|â–         | 288/6000 [13:38<4:29:14,  2.83s/it]                                                    {'loss': 2.7653, 'grad_norm': 10.25866985321045, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [13:38<4:29:14,  2.83s/it]  5%|â–         | 289/6000 [13:41<4:32:11,  2.86s/it]                                                    {'loss': 2.7696, 'grad_norm': 9.094305038452148, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [13:41<4:32:11,  2.86s/it]  5%|â–         | 290/6000 [13:44<4:29:37,  2.83s/it]                                                    {'loss': 2.8058, 'grad_norm': 6.630707263946533, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [13:44<4:29:37,  2.83s/it]  5%|â–         | 291/6000 [13:47<4:24:49,  2.78s/it]                                                    {'loss': 2.7747, 'grad_norm': 4.2634124755859375, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [13:47<4:24:49,  2.78s/it]  5%|â–         | 292/6000 [13:49<4:24:03,  2.78s/it]                                                    {'loss': 2.7708, 'grad_norm': 3.7030951976776123, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [13:49<4:24:03,  2.78s/it]  5%|â–         | 293/6000 [13:52<4:22:00,  2.75s/it]                                                    {'loss': 2.8486, 'grad_norm': 4.499835968017578, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [13:52<4:22:00,  2.75s/it]  5%|â–         | 294/6000 [13:55<4:22:21,  2.76s/it]                                                    {'loss': 2.7924, 'grad_norm': 6.39186429977417, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [13:55<4:22:21,  2.76s/it]  5%|â–         | 295/6000 [13:57<4:21:08,  2.75s/it]                                                    {'loss': 2.7795, 'grad_norm': 3.268019199371338, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [13:57<4:21:08,  2.75s/it]  5%|â–         | 296/6000 [14:00<4:25:08,  2.79s/it]                                                    {'loss': 2.7703, 'grad_norm': 4.4100799560546875, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [14:00<4:25:08,  2.79s/it]  5%|â–         | 297/6000 [14:03<4:25:23,  2.79s/it]                                                    {'loss': 2.7937, 'grad_norm': 4.427470684051514, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [14:03<4:25:23,  2.79s/it]  5%|â–         | 298/6000 [14:06<4:30:21,  2.84s/it]                                                    {'loss': 2.7865, 'grad_norm': 4.5095672607421875, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [14:06<4:30:21,  2.84s/it]  5%|â–         | 299/6000 [14:10<4:51:53,  3.07s/it]                                                    {'loss': 2.7899, 'grad_norm': 4.318160057067871, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [14:10<4:51:53,  3.07s/it]  5%|â–Œ         | 300/6000 [14:12<4:42:18,  2.97s/it]                                                    {'loss': 2.7689, 'grad_norm': 3.1719932556152344, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [14:12<4:42:18,  2.97s/it]  5%|â–Œ         | 301/6000 [14:15<4:37:43,  2.92s/it]                                                    {'loss': 2.8078, 'grad_norm': 4.828431129455566, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [14:15<4:37:43,  2.92s/it]  5%|â–Œ         | 302/6000 [14:18<4:33:47,  2.88s/it]                                                    {'loss': 2.7754, 'grad_norm': 5.720090389251709, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [14:18<4:33:47,  2.88s/it]  5%|â–Œ         | 303/6000 [14:21<4:31:33,  2.86s/it]                                                    {'loss': 2.776, 'grad_norm': 5.432626247406006, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [14:21<4:31:33,  2.86s/it]  5%|â–Œ         | 304/6000 [14:24<4:29:44,  2.84s/it]                                                    {'loss': 2.7954, 'grad_norm': 6.400700569152832, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [14:24<4:29:44,  2.84s/it]  5%|â–Œ         | 305/6000 [14:26<4:25:23,  2.80s/it]                                                    {'loss': 2.7863, 'grad_norm': 6.610134601593018, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [14:26<4:25:23,  2.80s/it]  5%|â–Œ         | 306/6000 [14:29<4:23:58,  2.78s/it]                                                    {'loss': 2.7782, 'grad_norm': 3.311701774597168, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [14:29<4:23:58,  2.78s/it]  5%|â–Œ         | 307/6000 [14:32<4:22:44,  2.77s/it]                                                    {'loss': 2.7923, 'grad_norm': 3.5300381183624268, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [14:32<4:22:44,  2.77s/it]  5%|â–Œ         | 308/6000 [14:35<4:21:07,  2.75s/it]                                                    {'loss': 2.7945, 'grad_norm': 4.223882675170898, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [14:35<4:21:07,  2.75s/it]  5%|â–Œ         | 309/6000 [14:37<4:25:05,  2.79s/it]                                                    {'loss': 2.7752, 'grad_norm': 3.6646623611450195, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [14:37<4:25:05,  2.79s/it]  5%|â–Œ         | 310/6000 [14:40<4:23:10,  2.78s/it]                                                    {'loss': 2.8239, 'grad_norm': 3.308704376220703, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [14:40<4:23:10,  2.78s/it]  5%|â–Œ         | 311/6000 [14:43<4:23:26,  2.78s/it]                                                    {'loss': 2.7773, 'grad_norm': 3.0409939289093018, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [14:43<4:23:26,  2.78s/it]  5%|â–Œ         | 312/6000 [14:46<4:22:09,  2.77s/it]                                                    {'loss': 2.7918, 'grad_norm': 3.9032092094421387, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [14:46<4:22:09,  2.77s/it]  5%|â–Œ         | 313/6000 [14:48<4:22:50,  2.77s/it]                                                    {'loss': 2.8416, 'grad_norm': 3.3653037548065186, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [14:48<4:22:50,  2.77s/it]  5%|â–Œ         | 314/6000 [14:51<4:20:56,  2.75s/it]                                                    {'loss': 2.7927, 'grad_norm': 3.8309061527252197, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [14:51<4:20:56,  2.75s/it]  5%|â–Œ         | 315/6000 [14:54<4:19:21,  2.74s/it]                                                    {'loss': 2.7818, 'grad_norm': 2.3856146335601807, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [14:54<4:19:21,  2.74s/it]  5%|â–Œ         | 316/6000 [14:57<4:20:47,  2.75s/it]                                                    {'loss': 2.776, 'grad_norm': 3.4813902378082275, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [14:57<4:20:47,  2.75s/it]  5%|â–Œ         | 317/6000 [15:00<4:25:07,  2.80s/it]                                                    {'loss': 2.8448, 'grad_norm': 2.7456562519073486, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [15:00<4:25:07,  2.80s/it]  5%|â–Œ         | 318/6000 [15:03<4:32:32,  2.88s/it]                                                    {'loss': 2.771, 'grad_norm': 2.5711772441864014, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [15:03<4:32:32,  2.88s/it]  5%|â–Œ         | 319/6000 [15:06<4:36:04,  2.92s/it]                                                    {'loss': 2.7729, 'grad_norm': 3.1405375003814697, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [15:06<4:36:04,  2.92s/it]  5%|â–Œ         | 320/6000 [15:09<4:34:13,  2.90s/it]                                                    {'loss': 2.7657, 'grad_norm': 3.389061212539673, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [15:09<4:34:13,  2.90s/it]  5%|â–Œ         | 321/6000 [15:12<4:41:53,  2.98s/it]                                                    {'loss': 2.7918, 'grad_norm': 3.611358404159546, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [15:12<4:41:53,  2.98s/it]  5%|â–Œ         | 322/6000 [15:14<4:35:39,  2.91s/it]                                                    {'loss': 2.7767, 'grad_norm': 2.973323106765747, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [15:14<4:35:39,  2.91s/it]  5%|â–Œ         | 323/6000 [15:18<4:48:17,  3.05s/it]                                                    {'loss': 2.7766, 'grad_norm': 4.215885162353516, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [15:18<4:48:17,  3.05s/it]  5%|â–Œ         | 324/6000 [15:20<4:37:47,  2.94s/it]                                                    {'loss': 2.8055, 'grad_norm': 2.660311460494995, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [15:20<4:37:47,  2.94s/it]  5%|â–Œ         | 325/6000 [15:23<4:31:14,  2.87s/it]                                                    {'loss': 2.7793, 'grad_norm': 2.8920645713806152, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [15:23<4:31:14,  2.87s/it]  5%|â–Œ         | 326/6000 [15:26<4:27:31,  2.83s/it]                                                    {'loss': 2.784, 'grad_norm': 2.4140615463256836, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [15:26<4:27:31,  2.83s/it]  5%|â–Œ         | 327/6000 [15:29<4:22:36,  2.78s/it]                                                    {'loss': 2.7668, 'grad_norm': 3.3656692504882812, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [15:29<4:22:36,  2.78s/it]  5%|â–Œ         | 328/6000 [15:31<4:21:51,  2.77s/it]                                                    {'loss': 2.7773, 'grad_norm': 3.6194348335266113, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [15:31<4:21:51,  2.77s/it]  5%|â–Œ         | 329/6000 [15:34<4:20:37,  2.76s/it]                                                    {'loss': 2.792, 'grad_norm': 3.4830853939056396, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [15:34<4:20:37,  2.76s/it]  6%|â–Œ         | 330/6000 [15:37<4:19:21,  2.74s/it]                                                    {'loss': 2.7877, 'grad_norm': 4.444991588592529, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [15:37<4:19:21,  2.74s/it]  6%|â–Œ         | 331/6000 [15:40<4:25:28,  2.81s/it]                                                    {'loss': 2.7827, 'grad_norm': 4.802549362182617, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [15:40<4:25:28,  2.81s/it]  6%|â–Œ         | 332/6000 [15:42<4:21:17,  2.77s/it]                                                    {'loss': 2.7735, 'grad_norm': 3.7728285789489746, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [15:42<4:21:17,  2.77s/it]  6%|â–Œ         | 333/6000 [15:45<4:21:50,  2.77s/it]                                                    {'loss': 2.7741, 'grad_norm': 2.7282984256744385, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [15:45<4:21:50,  2.77s/it]  6%|â–Œ         | 334/6000 [15:48<4:21:11,  2.77s/it]                                                    {'loss': 2.7902, 'grad_norm': 6.4934515953063965, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [15:48<4:21:11,  2.77s/it]  6%|â–Œ         | 335/6000 [15:51<4:22:52,  2.78s/it]                                                    {'loss': 2.7889, 'grad_norm': 3.847891330718994, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [15:51<4:22:52,  2.78s/it]  6%|â–Œ         | 336/6000 [15:53<4:19:18,  2.75s/it]                                                    {'loss': 2.7696, 'grad_norm': 3.7019872665405273, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [15:53<4:19:18,  2.75s/it]  6%|â–Œ         | 337/6000 [15:56<4:22:11,  2.78s/it]                                                    {'loss': 2.7878, 'grad_norm': 3.5830607414245605, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [15:56<4:22:11,  2.78s/it]  6%|â–Œ         | 338/6000 [15:59<4:20:46,  2.76s/it]                                                    {'loss': 2.7733, 'grad_norm': 3.7024364471435547, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [15:59<4:20:46,  2.76s/it]  6%|â–Œ         | 339/6000 [16:02<4:20:39,  2.76s/it]                                                    {'loss': 2.775, 'grad_norm': 3.08386492729187, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [16:02<4:20:39,  2.76s/it]  6%|â–Œ         | 340/6000 [16:05<4:28:32,  2.85s/it]                                                    {'loss': 2.7723, 'grad_norm': 3.444244146347046, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [16:05<4:28:32,  2.85s/it]  6%|â–Œ         | 341/6000 [16:07<4:24:00,  2.80s/it]                                                    {'loss': 2.7763, 'grad_norm': 4.7820539474487305, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [16:07<4:24:00,  2.80s/it]  6%|â–Œ         | 342/6000 [16:10<4:21:17,  2.77s/it]                                                    {'loss': 2.8041, 'grad_norm': 4.470468997955322, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [16:10<4:21:17,  2.77s/it]  6%|â–Œ         | 343/6000 [16:14<4:38:44,  2.96s/it]                                                    {'loss': 2.8334, 'grad_norm': 7.599388599395752, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [16:14<4:38:44,  2.96s/it]  6%|â–Œ         | 344/6000 [16:16<4:32:52,  2.89s/it]                                                    {'loss': 2.7904, 'grad_norm': 4.754183769226074, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [16:16<4:32:52,  2.89s/it]  6%|â–Œ         | 345/6000 [16:19<4:28:43,  2.85s/it]                                                    {'loss': 2.8217, 'grad_norm': 2.8630268573760986, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [16:19<4:28:43,  2.85s/it]  6%|â–Œ         | 346/6000 [16:22<4:26:44,  2.83s/it]                                                    {'loss': 2.8191, 'grad_norm': 3.0029845237731934, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [16:22<4:26:44,  2.83s/it]  6%|â–Œ         | 347/6000 [16:25<4:22:51,  2.79s/it]                                                    {'loss': 2.7906, 'grad_norm': 3.1464180946350098, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [16:25<4:22:51,  2.79s/it]  6%|â–Œ         | 348/6000 [16:27<4:23:51,  2.80s/it]                                                    {'loss': 2.7972, 'grad_norm': 5.7308220863342285, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [16:27<4:23:51,  2.80s/it]  6%|â–Œ         | 349/6000 [16:30<4:21:18,  2.77s/it]                                                    {'loss': 2.7907, 'grad_norm': 4.174839019775391, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [16:30<4:21:18,  2.77s/it]  6%|â–Œ         | 350/6000 [16:33<4:22:31,  2.79s/it]                                                    {'loss': 2.7865, 'grad_norm': 4.065009117126465, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [16:33<4:22:31,  2.79s/it]  6%|â–Œ         | 351/6000 [16:36<4:32:25,  2.89s/it]                                                    {'loss': 2.778, 'grad_norm': 4.81569766998291, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [16:36<4:32:25,  2.89s/it]  6%|â–Œ         | 352/6000 [16:39<4:27:05,  2.84s/it]                                                    {'loss': 2.7766, 'grad_norm': 3.8342514038085938, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [16:39<4:27:05,  2.84s/it]  6%|â–Œ         | 353/6000 [16:41<4:21:46,  2.78s/it]                                                    {'loss': 2.8173, 'grad_norm': 4.279848098754883, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [16:41<4:21:46,  2.78s/it]  6%|â–Œ         | 354/6000 [16:44<4:20:47,  2.77s/it]                                                    {'loss': 2.7762, 'grad_norm': 5.3343682289123535, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [16:44<4:20:47,  2.77s/it]  6%|â–Œ         | 355/6000 [16:47<4:20:18,  2.77s/it]                                                    {'loss': 2.8077, 'grad_norm': 3.7737843990325928, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [16:47<4:20:18,  2.77s/it]  6%|â–Œ         | 356/6000 [16:50<4:26:35,  2.83s/it]                                                    {'loss': 2.7862, 'grad_norm': 3.3733201026916504, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [16:50<4:26:35,  2.83s/it]  6%|â–Œ         | 357/6000 [16:53<4:25:10,  2.82s/it]                                                    {'loss': 2.8204, 'grad_norm': 5.2582197189331055, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [16:53<4:25:10,  2.82s/it]  6%|â–Œ         | 358/6000 [16:55<4:21:21,  2.78s/it]                                                    {'loss': 2.7759, 'grad_norm': 2.809354305267334, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [16:55<4:21:21,  2.78s/it]  6%|â–Œ         | 359/6000 [16:58<4:21:41,  2.78s/it]                                                    {'loss': 2.7731, 'grad_norm': 2.937565326690674, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [16:58<4:21:41,  2.78s/it]  6%|â–Œ         | 360/6000 [17:01<4:21:30,  2.78s/it]                                                    {'loss': 2.7891, 'grad_norm': 3.83030366897583, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [17:01<4:21:30,  2.78s/it]  6%|â–Œ         | 361/6000 [17:04<4:20:24,  2.77s/it]                                                    {'loss': 2.7693, 'grad_norm': 6.618751525878906, 'learning_rate': 9.557627118644068e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [17:04<4:20:24,  2.77s/it]  6%|â–Œ         | 362/6000 [17:06<4:19:25,  2.76s/it]                                                    {'loss': 2.78, 'grad_norm': 3.284048080444336, 'learning_rate': 9.555932203389831e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [17:06<4:19:25,  2.76s/it]  6%|â–Œ         | 363/6000 [17:09<4:20:20,  2.77s/it]                                                    {'loss': 2.8078, 'grad_norm': 6.901210784912109, 'learning_rate': 9.554237288135594e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [17:09<4:20:20,  2.77s/it]  6%|â–Œ         | 364/6000 [17:12<4:23:36,  2.81s/it]                                                    {'loss': 2.7873, 'grad_norm': 4.63174295425415, 'learning_rate': 9.552542372881358e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [17:12<4:23:36,  2.81s/it]  6%|â–Œ         | 365/6000 [17:15<4:20:50,  2.78s/it]                                                    {'loss': 2.7802, 'grad_norm': 4.693757057189941, 'learning_rate': 9.55084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [17:15<4:20:50,  2.78s/it]  6%|â–Œ         | 366/6000 [17:18<4:20:17,  2.77s/it]                                                    {'loss': 2.7744, 'grad_norm': 6.0904059410095215, 'learning_rate': 9.549152542372883e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [17:18<4:20:17,  2.77s/it]  6%|â–Œ         | 367/6000 [17:20<4:18:40,  2.76s/it]                                                    {'loss': 2.7767, 'grad_norm': 2.9870264530181885, 'learning_rate': 9.547457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [17:20<4:18:40,  2.76s/it]  6%|â–Œ         | 368/6000 [17:24<4:32:03,  2.90s/it]                                                    {'loss': 2.7758, 'grad_norm': 4.865990161895752, 'learning_rate': 9.545762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [17:24<4:32:03,  2.90s/it]  6%|â–Œ         | 369/6000 [17:26<4:27:10,  2.85s/it]                                                    {'loss': 2.7931, 'grad_norm': 4.015151023864746, 'learning_rate': 9.54406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [17:26<4:27:10,  2.85s/it]  6%|â–Œ         | 370/6000 [17:29<4:24:51,  2.82s/it]                                                    {'loss': 2.7743, 'grad_norm': 2.868521213531494, 'learning_rate': 9.542372881355934e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [17:29<4:24:51,  2.82s/it]  6%|â–Œ         | 371/6000 [17:32<4:21:31,  2.79s/it]                                                    {'loss': 2.7703, 'grad_norm': 3.3114538192749023, 'learning_rate': 9.540677966101696e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [17:32<4:21:31,  2.79s/it]  6%|â–Œ         | 372/6000 [17:34<4:18:44,  2.76s/it]                                                    {'loss': 2.7828, 'grad_norm': 2.6885995864868164, 'learning_rate': 9.538983050847457e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [17:34<4:18:44,  2.76s/it]  6%|â–Œ         | 373/6000 [17:37<4:17:07,  2.74s/it]                                                    {'loss': 2.9049, 'grad_norm': 4.337611675262451, 'learning_rate': 9.53728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [17:37<4:17:07,  2.74s/it]  6%|â–Œ         | 374/6000 [17:40<4:18:08,  2.75s/it]                                                    {'loss': 2.7757, 'grad_norm': 4.942732334136963, 'learning_rate': 9.535593220338984e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [17:40<4:18:08,  2.75s/it]  6%|â–‹         | 375/6000 [17:43<4:17:19,  2.74s/it]                                                    {'loss': 2.8062, 'grad_norm': 4.2695631980896, 'learning_rate': 9.533898305084747e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [17:43<4:17:19,  2.74s/it]  6%|â–‹         | 376/6000 [17:45<4:16:18,  2.73s/it]                                                    {'loss': 2.7754, 'grad_norm': 5.754229545593262, 'learning_rate': 9.532203389830508e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [17:45<4:16:18,  2.73s/it]  6%|â–‹         | 377/6000 [17:48<4:15:55,  2.73s/it]                                                    {'loss': 2.8031, 'grad_norm': 4.369229316711426, 'learning_rate': 9.530508474576272e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [17:48<4:15:55,  2.73s/it]  6%|â–‹         | 378/6000 [17:51<4:19:49,  2.77s/it]                                                    {'loss': 2.7864, 'grad_norm': 3.530128240585327, 'learning_rate': 9.528813559322035e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [17:51<4:19:49,  2.77s/it]  6%|â–‹         | 379/6000 [17:54<4:20:31,  2.78s/it]                                                    {'loss': 2.7698, 'grad_norm': 6.024551868438721, 'learning_rate': 9.527118644067798e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [17:54<4:20:31,  2.78s/it]  6%|â–‹         | 380/6000 [17:57<4:29:29,  2.88s/it]                                                    {'loss': 2.7869, 'grad_norm': 3.418771743774414, 'learning_rate': 9.52542372881356e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [17:57<4:29:29,  2.88s/it]  6%|â–‹         | 381/6000 [18:00<4:25:58,  2.84s/it]                                                    {'loss': 2.7769, 'grad_norm': 4.055296897888184, 'learning_rate': 9.523728813559323e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [18:00<4:25:58,  2.84s/it]  6%|â–‹         | 382/6000 [18:02<4:23:32,  2.81s/it]                                                    {'loss': 2.7791, 'grad_norm': 3.945193290710449, 'learning_rate': 9.522033898305085e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [18:02<4:23:32,  2.81s/it]  6%|â–‹         | 383/6000 [18:05<4:22:01,  2.80s/it]                                                    {'loss': 2.8004, 'grad_norm': 4.246231555938721, 'learning_rate': 9.520338983050848e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [18:05<4:22:01,  2.80s/it]  6%|â–‹         | 384/6000 [18:08<4:20:45,  2.79s/it]                                                    {'loss': 2.7669, 'grad_norm': 3.8901307582855225, 'learning_rate': 9.518644067796611e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [18:08<4:20:45,  2.79s/it]  6%|â–‹         | 385/6000 [18:11<4:30:20,  2.89s/it]                                                    {'loss': 2.7703, 'grad_norm': 3.881814479827881, 'learning_rate': 9.516949152542375e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [18:11<4:30:20,  2.89s/it]  6%|â–‹         | 386/6000 [18:14<4:27:40,  2.86s/it]                                                    {'loss': 2.7943, 'grad_norm': 5.647403240203857, 'learning_rate': 9.515254237288136e-06, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [18:14<4:27:40,  2.86s/it]  6%|â–‹         | 387/6000 [18:17<4:36:47,  2.96s/it]                                                    {'loss': 2.7795, 'grad_norm': 5.965195178985596, 'learning_rate': 9.5135593220339e-06, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [18:17<4:36:47,  2.96s/it]  6%|â–‹         | 388/6000 [18:20<4:35:40,  2.95s/it]                                                    {'loss': 2.8191, 'grad_norm': 5.053308963775635, 'learning_rate': 9.511864406779661e-06, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [18:20<4:35:40,  2.95s/it]  6%|â–‹         | 389/6000 [18:23<4:30:25,  2.89s/it]                                                    {'loss': 2.7752, 'grad_norm': 4.264379501342773, 'learning_rate': 9.510169491525424e-06, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [18:23<4:30:25,  2.89s/it]  6%|â–‹         | 390/6000 [18:25<4:26:39,  2.85s/it]                                                    {'loss': 2.7719, 'grad_norm': 4.109795093536377, 'learning_rate': 9.508474576271188e-06, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [18:25<4:26:39,  2.85s/it]  7%|â–‹         | 391/6000 [18:28<4:21:58,  2.80s/it]                                                    {'loss': 2.7751, 'grad_norm': 4.070371150970459, 'learning_rate': 9.506779661016949e-06, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [18:28<4:21:58,  2.80s/it]  7%|â–‹         | 392/6000 [18:31<4:21:38,  2.80s/it]                                                    {'loss': 2.7759, 'grad_norm': 3.7905263900756836, 'learning_rate': 9.505084745762712e-06, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [18:31<4:21:38,  2.80s/it]  7%|â–‹         | 393/6000 [18:34<4:20:23,  2.79s/it]                                                    {'loss': 2.7821, 'grad_norm': 4.834784507751465, 'learning_rate': 9.503389830508476e-06, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [18:34<4:20:23,  2.79s/it]  7%|â–‹         | 394/6000 [18:36<4:21:25,  2.80s/it]                                                    {'loss': 2.7862, 'grad_norm': 4.511346340179443, 'learning_rate': 9.501694915254239e-06, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [18:36<4:21:25,  2.80s/it]  7%|â–‹         | 395/6000 [18:39<4:19:55,  2.78s/it]                                                    {'loss': 2.7918, 'grad_norm': 6.395314693450928, 'learning_rate': 9.5e-06, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [18:39<4:19:55,  2.78s/it]  7%|â–‹         | 396/6000 [18:42<4:19:27,  2.78s/it]                                                    {'loss': 2.7851, 'grad_norm': 5.081392765045166, 'learning_rate': 9.498305084745764e-06, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [18:42<4:19:27,  2.78s/it]  7%|â–‹         | 397/6000 [18:45<4:29:17,  2.88s/it]                                                    {'loss': 2.7684, 'grad_norm': 9.74113655090332, 'learning_rate': 9.496610169491525e-06, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [18:45<4:29:17,  2.88s/it]  7%|â–‹         | 398/6000 [18:48<4:24:33,  2.83s/it]                                                    {'loss': 2.7954, 'grad_norm': 7.136310577392578, 'learning_rate': 9.494915254237289e-06, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [18:48<4:24:33,  2.83s/it]  7%|â–‹         | 399/6000 [18:51<4:23:25,  2.82s/it]                                                    {'loss': 2.7923, 'grad_norm': 9.30820369720459, 'learning_rate': 9.493220338983052e-06, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [18:51<4:23:25,  2.82s/it]  7%|â–‹         | 400/6000 [18:54<4:26:45,  2.86s/it]                                                    {'loss': 2.7709, 'grad_norm': 3.476299524307251, 'learning_rate': 9.491525423728815e-06, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [18:54<4:26:45,  2.86s/it]  7%|â–‹         | 401/6000 [18:56<4:23:52,  2.83s/it]                                                    {'loss': 2.7757, 'grad_norm': 10.662158012390137, 'learning_rate': 9.489830508474577e-06, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [18:56<4:23:52,  2.83s/it]  7%|â–‹         | 402/6000 [18:59<4:20:23,  2.79s/it]                                                    {'loss': 2.7774, 'grad_norm': 6.088863849639893, 'learning_rate': 9.48813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [18:59<4:20:23,  2.79s/it]  7%|â–‹         | 403/6000 [19:02<4:20:49,  2.80s/it]                                                    {'loss': 2.781, 'grad_norm': 8.435074806213379, 'learning_rate': 9.486440677966102e-06, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [19:02<4:20:49,  2.80s/it]  7%|â–‹         | 404/6000 [19:05<4:24:00,  2.83s/it]                                                    {'loss': 2.7693, 'grad_norm': 9.125956535339355, 'learning_rate': 9.484745762711865e-06, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [19:05<4:24:00,  2.83s/it]  7%|â–‹         | 405/6000 [19:08<4:21:49,  2.81s/it]                                                    {'loss': 2.7775, 'grad_norm': 7.905030250549316, 'learning_rate': 9.483050847457628e-06, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [19:08<4:21:49,  2.81s/it]  7%|â–‹         | 406/6000 [19:10<4:19:58,  2.79s/it]                                                    {'loss': 2.7749, 'grad_norm': 8.366549491882324, 'learning_rate': 9.481355932203391e-06, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [19:10<4:19:58,  2.79s/it]  7%|â–‹         | 407/6000 [19:13<4:21:13,  2.80s/it]                                                    {'loss': 2.7571, 'grad_norm': 10.866390228271484, 'learning_rate': 9.479661016949153e-06, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [19:13<4:21:13,  2.80s/it]  7%|â–‹         | 408/6000 [19:17<4:40:47,  3.01s/it]                                                    {'loss': 2.7785, 'grad_norm': 13.27944278717041, 'learning_rate': 9.477966101694916e-06, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [19:17<4:40:47,  3.01s/it]  7%|â–‹         | 409/6000 [19:19<4:34:23,  2.94s/it]                                                    {'loss': 2.7609, 'grad_norm': 10.616994857788086, 'learning_rate': 9.476271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [19:19<4:34:23,  2.94s/it]  7%|â–‹         | 410/6000 [19:22<4:28:08,  2.88s/it]                                                    {'loss': 2.7942, 'grad_norm': 21.2659912109375, 'learning_rate': 9.474576271186441e-06, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [19:22<4:28:08,  2.88s/it]  7%|â–‹         | 411/6000 [19:26<4:45:11,  3.06s/it]                                                    {'loss': 2.7791, 'grad_norm': 10.824060440063477, 'learning_rate': 9.472881355932204e-06, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [19:26<4:45:11,  3.06s/it]  7%|â–‹         | 412/6000 [19:28<4:34:03,  2.94s/it]                                                    {'loss': 2.803, 'grad_norm': 12.2188720703125, 'learning_rate': 9.471186440677966e-06, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [19:28<4:34:03,  2.94s/it]  7%|â–‹         | 413/6000 [19:31<4:32:46,  2.93s/it]                                                    {'loss': 2.782, 'grad_norm': 10.056615829467773, 'learning_rate': 9.46949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [19:31<4:32:46,  2.93s/it]  7%|â–‹         | 414/6000 [19:34<4:26:57,  2.87s/it]                                                    {'loss': 2.789, 'grad_norm': 6.453975677490234, 'learning_rate': 9.467796610169493e-06, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [19:34<4:26:57,  2.87s/it]  7%|â–‹         | 415/6000 [19:37<4:31:22,  2.92s/it]                                                    {'loss': 2.7569, 'grad_norm': 11.283227920532227, 'learning_rate': 9.466101694915256e-06, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [19:37<4:31:22,  2.92s/it]  7%|â–‹         | 416/6000 [19:40<4:28:19,  2.88s/it]                                                    {'loss': 2.8008, 'grad_norm': 10.693974494934082, 'learning_rate': 9.464406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [19:40<4:28:19,  2.88s/it]  7%|â–‹         | 417/6000 [19:42<4:24:09,  2.84s/it]                                                    {'loss': 2.804, 'grad_norm': 17.355138778686523, 'learning_rate': 9.46271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [19:42<4:24:09,  2.84s/it]  7%|â–‹         | 418/6000 [19:45<4:20:56,  2.80s/it]                                                    {'loss': 2.7889, 'grad_norm': 9.506975173950195, 'learning_rate': 9.461016949152542e-06, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [19:45<4:20:56,  2.80s/it]  7%|â–‹         | 419/6000 [19:48<4:32:26,  2.93s/it]                                                    {'loss': 2.7824, 'grad_norm': 11.064299583435059, 'learning_rate': 9.459322033898306e-06, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [19:48<4:32:26,  2.93s/it]  7%|â–‹         | 420/6000 [19:51<4:29:00,  2.89s/it]                                                    {'loss': 2.852, 'grad_norm': 26.081037521362305, 'learning_rate': 9.457627118644069e-06, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [19:51<4:29:00,  2.89s/it]  7%|â–‹         | 421/6000 [19:54<4:26:31,  2.87s/it]                                                    {'loss': 2.7886, 'grad_norm': 24.851106643676758, 'learning_rate': 9.455932203389832e-06, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [19:54<4:26:31,  2.87s/it]  7%|â–‹         | 422/6000 [19:57<4:22:13,  2.82s/it]                                                    {'loss': 2.7837, 'grad_norm': 15.654955863952637, 'learning_rate': 9.454237288135594e-06, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [19:57<4:22:13,  2.82s/it]  7%|â–‹         | 423/6000 [19:59<4:20:24,  2.80s/it]                                                    {'loss': 2.7789, 'grad_norm': 23.308809280395508, 'learning_rate': 9.452542372881357e-06, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [19:59<4:20:24,  2.80s/it]  7%|â–‹         | 424/6000 [20:02<4:19:27,  2.79s/it]                                                    {'loss': 2.7488, 'grad_norm': 20.815122604370117, 'learning_rate': 9.450847457627119e-06, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [20:02<4:19:27,  2.79s/it]  7%|â–‹         | 425/6000 [20:05<4:16:24,  2.76s/it]                                                    {'loss': 2.7931, 'grad_norm': 12.251605033874512, 'learning_rate': 9.449152542372882e-06, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [20:05<4:16:24,  2.76s/it]  7%|â–‹         | 426/6000 [20:08<4:15:56,  2.76s/it]                                                    {'loss': 2.7693, 'grad_norm': 14.315496444702148, 'learning_rate': 9.447457627118645e-06, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [20:08<4:15:56,  2.76s/it]  7%|â–‹         | 427/6000 [20:10<4:16:51,  2.77s/it]                                                    {'loss': 2.7875, 'grad_norm': 10.224654197692871, 'learning_rate': 9.445762711864408e-06, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [20:10<4:16:51,  2.77s/it]  7%|â–‹         | 428/6000 [20:13<4:16:16,  2.76s/it]                                                    {'loss': 2.796, 'grad_norm': 5.994043350219727, 'learning_rate': 9.44406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [20:13<4:16:16,  2.76s/it]  7%|â–‹         | 429/6000 [20:16<4:14:54,  2.75s/it]                                                    {'loss': 2.7595, 'grad_norm': 9.555903434753418, 'learning_rate': 9.442372881355933e-06, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [20:16<4:14:54,  2.75s/it]  7%|â–‹         | 430/6000 [20:19<4:15:28,  2.75s/it]                                                    {'loss': 2.8206, 'grad_norm': 6.711406230926514, 'learning_rate': 9.440677966101696e-06, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [20:19<4:15:28,  2.75s/it]  7%|â–‹         | 431/6000 [20:21<4:14:04,  2.74s/it]                                                    {'loss': 2.7806, 'grad_norm': 10.279391288757324, 'learning_rate': 9.43898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [20:21<4:14:04,  2.74s/it]  7%|â–‹         | 432/6000 [20:24<4:12:38,  2.72s/it]                                                    {'loss': 2.8023, 'grad_norm': 6.3439507484436035, 'learning_rate': 9.437288135593221e-06, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [20:24<4:12:38,  2.72s/it]  7%|â–‹         | 433/6000 [20:27<4:12:07,  2.72s/it]                                                    {'loss': 2.7747, 'grad_norm': 8.88521957397461, 'learning_rate': 9.435593220338983e-06, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [20:27<4:12:07,  2.72s/it]  7%|â–‹         | 434/6000 [20:30<4:14:31,  2.74s/it]                                                    {'loss': 2.782, 'grad_norm': 9.045158386230469, 'learning_rate': 9.433898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [20:30<4:14:31,  2.74s/it]  7%|â–‹         | 435/6000 [20:32<4:17:05,  2.77s/it]                                                    {'loss': 2.7746, 'grad_norm': 8.782501220703125, 'learning_rate': 9.43220338983051e-06, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [20:32<4:17:05,  2.77s/it]  7%|â–‹         | 436/6000 [20:35<4:17:38,  2.78s/it]                                                    {'loss': 2.8187, 'grad_norm': 7.243849754333496, 'learning_rate': 9.430508474576273e-06, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [20:35<4:17:38,  2.78s/it]  7%|â–‹         | 437/6000 [20:38<4:19:32,  2.80s/it]                                                    {'loss': 2.7997, 'grad_norm': 7.297418117523193, 'learning_rate': 9.428813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [20:38<4:19:32,  2.80s/it]  7%|â–‹         | 438/6000 [20:41<4:21:53,  2.83s/it]                                                    {'loss': 2.7807, 'grad_norm': 5.862771034240723, 'learning_rate': 9.427118644067798e-06, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [20:41<4:21:53,  2.83s/it]  7%|â–‹         | 439/6000 [20:44<4:19:51,  2.80s/it]                                                    {'loss': 2.8044, 'grad_norm': 6.276735305786133, 'learning_rate': 9.425423728813559e-06, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [20:44<4:19:51,  2.80s/it]  7%|â–‹         | 440/6000 [20:46<4:19:04,  2.80s/it]                                                    {'loss': 2.7861, 'grad_norm': 4.490973472595215, 'learning_rate': 9.423728813559322e-06, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [20:46<4:19:04,  2.80s/it]  7%|â–‹         | 441/6000 [20:50<4:27:49,  2.89s/it]                                                    {'loss': 2.7945, 'grad_norm': 5.371031284332275, 'learning_rate': 9.422033898305086e-06, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [20:50<4:27:49,  2.89s/it]  7%|â–‹         | 442/6000 [20:53<4:33:56,  2.96s/it]                                                    {'loss': 2.7753, 'grad_norm': 7.27292013168335, 'learning_rate': 9.420338983050849e-06, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [20:53<4:33:56,  2.96s/it]  7%|â–‹         | 443/6000 [20:56<4:29:46,  2.91s/it]                                                    {'loss': 2.8261, 'grad_norm': 8.845781326293945, 'learning_rate': 9.41864406779661e-06, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [20:56<4:29:46,  2.91s/it]  7%|â–‹         | 444/6000 [20:58<4:25:20,  2.87s/it]                                                    {'loss': 2.83, 'grad_norm': 6.067440986633301, 'learning_rate': 9.416949152542374e-06, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [20:58<4:25:20,  2.87s/it]