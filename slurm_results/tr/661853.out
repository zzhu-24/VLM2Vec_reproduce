==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --plus_one_token True --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/train.log
W1117 10:50:26.335000 137231831750464 torch/distributed/run.py:779] 
W1117 10:50:26.335000 137231831750464 torch/distributed/run.py:779] *****************************************
W1117 10:50:26.335000 137231831750464 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1117 10:50:26.335000 137231831750464 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!DropoutAddRMSNorm of flash_attn is not installed!!!

/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-17 10:50:33,963] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.61it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.78it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wandb: setting up run 1sbyzsm8
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251117_105034-1sbyzsm8
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/1sbyzsm8
[2025-11-17 10:50:35,416] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.97it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.38it/s]
Some weights of Qwen2VLForConditionalGenerationWithTail were not initialized from the model checkpoint at Qwen/Qwen2-VL-2B-Instruct and are newly initialized: ['tail_emb']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[2025-11-17 10:50:35,928] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGenerationWithTail(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-17 10:50:41,912] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-17 10:50:43,108] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-17 10:50:43,109] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-17 10:50:46,492] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-17 10:50:46,492] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-17 10:50:47,306] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-17 10:50:47,306] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-17 10:50:47,306] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-17 10:50:47,307] INFO [src.utils:19] ==================================================
[2025-11-17 10:50:47,307] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-17 10:50:47,308] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-17 10:50:47,308] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-17 10:50:47,309] INFO [src.utils:19] ==================================================
[2025-11-17 10:50:48,920] INFO [src.trainer:350] ***** Running training *****
[2025-11-17 10:50:48,920] INFO [src.trainer:350] ***** Running training *****
[2025-11-17 10:50:48,920] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-17 10:50:48,920] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-17 10:50:48,920] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-17 10:50:48,920] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-17 10:50:48,920] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-17 10:50:48,920] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-17 10:50:48,921] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-17 10:50:48,921] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-17 10:50:48,921] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-17 10:50:48,921] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-17 10:50:48,921] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-17 10:50:48,922] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-17 10:50:48,924] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-17 10:50:48,925] INFO [src.trainer:359]   Number of trainable parameters = 9,205,248
[2025-11-17 10:50:48,926] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-17 10:50:48,928] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.tail_emb', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/6000 [00:03<5:22:07,  3.22s/it]                                                  {'loss': 12.7815, 'grad_norm': 427.91290283203125, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:22:07,  3.22s/it]  0%|          | 2/6000 [00:05<4:03:55,  2.44s/it]                                                  {'loss': 18.2151, 'grad_norm': 186.854248046875, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:03:55,  2.44s/it]  0%|          | 3/6000 [00:07<3:42:53,  2.23s/it]                                                  {'loss': 10.9606, 'grad_norm': 245.5409698486328, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:42:53,  2.23s/it]  0%|          | 4/6000 [00:09<3:33:33,  2.14s/it]                                                  {'loss': 17.0728, 'grad_norm': 270.5247497558594, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:33:33,  2.14s/it]  0%|          | 5/6000 [00:11<3:25:28,  2.06s/it]                                                  {'loss': 16.0796, 'grad_norm': 241.04234313964844, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:25:28,  2.06s/it]  0%|          | 6/6000 [00:12<3:20:30,  2.01s/it]                                                  {'loss': 14.1992, 'grad_norm': 149.11648559570312, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:12<3:20:30,  2.01s/it]  0%|          | 7/6000 [00:14<3:17:55,  1.98s/it]                                                  {'loss': 13.1844, 'grad_norm': 198.77980041503906, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:14<3:17:55,  1.98s/it]  0%|          | 8/6000 [00:16<3:15:18,  1.96s/it]                                                  {'loss': 25.9366, 'grad_norm': 339.0107727050781, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:16<3:15:18,  1.96s/it]  0%|          | 9/6000 [00:18<3:16:30,  1.97s/it]                                                  {'loss': 14.9116, 'grad_norm': 156.2489013671875, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:18<3:16:30,  1.97s/it]  0%|          | 10/6000 [00:20<3:15:54,  1.96s/it]                                                   {'loss': 17.4583, 'grad_norm': 282.2052001953125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:15:54,  1.96s/it]  0%|          | 11/6000 [00:22<3:16:35,  1.97s/it]                                                   {'loss': 12.3597, 'grad_norm': 148.11776733398438, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:16:35,  1.97s/it]  0%|          | 12/6000 [00:24<3:18:11,  1.99s/it]                                                   {'loss': 11.6482, 'grad_norm': 132.65696716308594, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:18:11,  1.99s/it]  0%|          | 13/6000 [00:26<3:17:59,  1.98s/it]                                                   {'loss': 9.4773, 'grad_norm': 113.221435546875, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:17:59,  1.98s/it]  0%|          | 14/6000 [00:28<3:16:52,  1.97s/it]                                                   {'loss': 14.0631, 'grad_norm': 316.2310791015625, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:16:52,  1.97s/it]  0%|          | 15/6000 [00:30<3:16:37,  1.97s/it]                                                   {'loss': 10.7625, 'grad_norm': 112.27668762207031, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:16:37,  1.97s/it]  0%|          | 16/6000 [00:32<3:17:07,  1.98s/it]                                                   {'loss': 9.4613, 'grad_norm': 155.00672912597656, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:17:07,  1.98s/it]  0%|          | 17/6000 [00:34<3:15:12,  1.96s/it]                                                   {'loss': 14.5774, 'grad_norm': 152.333984375, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:15:12,  1.96s/it]  0%|          | 18/6000 [00:36<3:14:57,  1.96s/it]                                                   {'loss': 9.5437, 'grad_norm': 158.66941833496094, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:14:57,  1.96s/it]  0%|          | 19/6000 [00:38<3:13:47,  1.94s/it]                                                   {'loss': 14.7311, 'grad_norm': 198.4029541015625, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:13:47,  1.94s/it]  0%|          | 20/6000 [00:40<3:17:11,  1.98s/it]                                                   {'loss': 9.2507, 'grad_norm': 196.1227569580078, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:17:11,  1.98s/it]  0%|          | 21/6000 [00:42<3:18:04,  1.99s/it]                                                   {'loss': 10.2496, 'grad_norm': 189.11058044433594, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:18:04,  1.99s/it]  0%|          | 22/6000 [00:44<3:16:54,  1.98s/it]                                                   {'loss': 16.4586, 'grad_norm': 144.77906799316406, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:16:54,  1.98s/it]  0%|          | 23/6000 [00:46<3:17:18,  1.98s/it]                                                   {'loss': 19.1628, 'grad_norm': 214.56512451171875, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:17:18,  1.98s/it]  0%|          | 24/6000 [00:48<3:19:44,  2.01s/it]                                                   {'loss': 11.6335, 'grad_norm': 106.84009552001953, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:19:44,  2.01s/it]  0%|          | 25/6000 [00:50<3:18:58,  2.00s/it]                                                   {'loss': 15.9351, 'grad_norm': 189.1713409423828, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:18:58,  2.00s/it]  0%|          | 26/6000 [00:52<3:18:30,  1.99s/it]                                                   {'loss': 9.1608, 'grad_norm': 114.25871276855469, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:18:30,  1.99s/it]  0%|          | 27/6000 [00:54<3:17:37,  1.99s/it]                                                   {'loss': 10.1964, 'grad_norm': 103.02127838134766, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:17:37,  1.99s/it]  0%|          | 28/6000 [00:56<3:26:27,  2.07s/it]                                                   {'loss': 10.5966, 'grad_norm': 98.0772705078125, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:26:27,  2.07s/it]  0%|          | 29/6000 [00:58<3:21:49,  2.03s/it]                                                   {'loss': 18.5515, 'grad_norm': 183.5142364501953, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:21:49,  2.03s/it]  0%|          | 30/6000 [01:00<3:17:56,  1.99s/it]                                                   {'loss': 13.2977, 'grad_norm': 122.14738464355469, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:17:56,  1.99s/it]  1%|          | 31/6000 [01:02<3:16:12,  1.97s/it]                                                   {'loss': 11.9471, 'grad_norm': 128.2501983642578, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:16:12,  1.97s/it]  1%|          | 32/6000 [01:04<3:15:37,  1.97s/it]                                                   {'loss': 16.4321, 'grad_norm': 99.65350341796875, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:15:37,  1.97s/it]  1%|          | 33/6000 [01:06<3:16:12,  1.97s/it]                                                   {'loss': 8.7053, 'grad_norm': 112.09196472167969, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:16:12,  1.97s/it]  1%|          | 34/6000 [01:08<3:15:07,  1.96s/it]                                                   {'loss': 13.4236, 'grad_norm': 105.60090637207031, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:15:07,  1.96s/it]  1%|          | 35/6000 [01:10<3:15:46,  1.97s/it]                                                   {'loss': 6.8277, 'grad_norm': 71.23878479003906, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:15:46,  1.97s/it]  1%|          | 36/6000 [01:12<3:14:15,  1.95s/it]                                                   {'loss': 14.8257, 'grad_norm': 187.32337951660156, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:14:15,  1.95s/it]  1%|          | 37/6000 [01:14<3:13:57,  1.95s/it]                                                   {'loss': 11.639, 'grad_norm': 162.67967224121094, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:13:57,  1.95s/it]  1%|          | 38/6000 [01:16<3:12:02,  1.93s/it]                                                   {'loss': 17.7925, 'grad_norm': 238.6424560546875, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:12:02,  1.93s/it]  1%|          | 39/6000 [01:17<3:11:58,  1.93s/it]                                                   {'loss': 9.6009, 'grad_norm': 139.6044464111328, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:17<3:11:58,  1.93s/it]  1%|          | 40/6000 [01:19<3:12:24,  1.94s/it]                                                   {'loss': 8.4383, 'grad_norm': 223.31326293945312, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:19<3:12:24,  1.94s/it]  1%|          | 41/6000 [01:21<3:13:38,  1.95s/it]                                                   {'loss': 14.8709, 'grad_norm': 347.86175537109375, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:21<3:13:38,  1.95s/it]  1%|          | 42/6000 [01:23<3:13:03,  1.94s/it]                                                   {'loss': 14.0949, 'grad_norm': 266.6495666503906, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:23<3:13:03,  1.94s/it]  1%|          | 43/6000 [01:26<3:25:01,  2.07s/it]                                                   {'loss': 7.7416, 'grad_norm': 150.3404541015625, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:25:01,  2.07s/it]  1%|          | 44/6000 [01:28<3:24:15,  2.06s/it]                                                   {'loss': 7.3973, 'grad_norm': 205.11280822753906, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:24:15,  2.06s/it]  1%|          | 45/6000 [01:30<3:21:09,  2.03s/it]                                                   {'loss': 6.8829, 'grad_norm': 90.95137023925781, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:21:09,  2.03s/it]  1%|          | 46/6000 [01:32<3:19:56,  2.01s/it]                                                   {'loss': 13.4294, 'grad_norm': 322.3768005371094, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:19:56,  2.01s/it]  1%|          | 47/6000 [01:34<3:17:56,  2.00s/it]                                                   {'loss': 6.6346, 'grad_norm': 207.66249084472656, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:17:56,  2.00s/it]  1%|          | 48/6000 [01:36<3:18:06,  2.00s/it]                                                   {'loss': 9.0785, 'grad_norm': 147.64833068847656, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:18:06,  2.00s/it]  1%|          | 49/6000 [01:38<3:16:10,  1.98s/it]                                                   {'loss': 12.7948, 'grad_norm': 251.8748321533203, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:16:10,  1.98s/it]  1%|          | 50/6000 [01:40<3:16:46,  1.98s/it]                                                   {'loss': 12.0075, 'grad_norm': 336.79571533203125, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:16:46,  1.98s/it]  1%|          | 51/6000 [01:41<3:16:11,  1.98s/it]                                                   {'loss': 10.9767, 'grad_norm': 228.55003356933594, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:41<3:16:11,  1.98s/it]  1%|          | 52/6000 [01:43<3:14:41,  1.96s/it]                                                   {'loss': 12.1335, 'grad_norm': 987.2967529296875, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:43<3:14:41,  1.96s/it]  1%|          | 53/6000 [01:45<3:15:28,  1.97s/it]                                                   {'loss': 10.8936, 'grad_norm': 608.2450561523438, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:45<3:15:28,  1.97s/it]  1%|          | 54/6000 [01:47<3:16:13,  1.98s/it]                                                   {'loss': 15.8049, 'grad_norm': 1065.4798583984375, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:47<3:16:13,  1.98s/it]  1%|          | 55/6000 [01:49<3:14:30,  1.96s/it]                                                   {'loss': 13.1807, 'grad_norm': 927.1355590820312, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:49<3:14:30,  1.96s/it]  1%|          | 56/6000 [01:51<3:13:32,  1.95s/it]                                                   {'loss': 13.3396, 'grad_norm': 379.3334655761719, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:51<3:13:32,  1.95s/it]  1%|          | 57/6000 [01:53<3:14:02,  1.96s/it]                                                   {'loss': 13.5684, 'grad_norm': 617.4732055664062, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:53<3:14:02,  1.96s/it]  1%|          | 58/6000 [01:55<3:14:18,  1.96s/it]                                                   {'loss': 8.2253, 'grad_norm': 368.3024597167969, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:55<3:14:18,  1.96s/it]  1%|          | 59/6000 [01:57<3:12:19,  1.94s/it]                                                   {'loss': 11.528, 'grad_norm': 663.3224487304688, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:57<3:12:19,  1.94s/it]  1%|          | 60/6000 [01:59<3:13:52,  1.96s/it]                                                   {'loss': 12.8088, 'grad_norm': 682.0530395507812, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [01:59<3:13:52,  1.96s/it]  1%|          | 61/6000 [02:01<3:11:51,  1.94s/it]                                                   {'loss': 16.1735, 'grad_norm': 854.5738525390625, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:01<3:11:51,  1.94s/it]  1%|          | 62/6000 [02:03<3:11:24,  1.93s/it]                                                   {'loss': 11.0595, 'grad_norm': 624.7246704101562, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:03<3:11:24,  1.93s/it]  1%|          | 63/6000 [02:05<3:11:37,  1.94s/it]                                                   {'loss': 10.3323, 'grad_norm': 511.8174743652344, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:05<3:11:37,  1.94s/it]  1%|          | 64/6000 [02:07<3:10:49,  1.93s/it]                                                   {'loss': 11.2302, 'grad_norm': 247.71823120117188, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:07<3:10:49,  1.93s/it]  1%|          | 65/6000 [02:09<3:10:43,  1.93s/it]                                                   {'loss': 9.0316, 'grad_norm': 540.3217163085938, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:09<3:10:43,  1.93s/it]  1%|          | 66/6000 [02:11<3:14:30,  1.97s/it]                                                   {'loss': 5.5536, 'grad_norm': 341.84222412109375, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:11<3:14:30,  1.97s/it]  1%|          | 67/6000 [02:13<3:14:24,  1.97s/it]                                                   {'loss': 5.8187, 'grad_norm': 227.27426147460938, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:13<3:14:24,  1.97s/it]  1%|          | 68/6000 [02:15<3:12:31,  1.95s/it]                                                   {'loss': 12.4631, 'grad_norm': 1430.451416015625, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:15<3:12:31,  1.95s/it]  1%|          | 69/6000 [02:17<3:12:55,  1.95s/it]                                                   {'loss': 8.1495, 'grad_norm': 1116.42822265625, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:17<3:12:55,  1.95s/it]  1%|          | 70/6000 [02:19<3:14:35,  1.97s/it]                                                   {'loss': 7.2992, 'grad_norm': 876.316650390625, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:19<3:14:35,  1.97s/it]  1%|          | 71/6000 [02:20<3:12:20,  1.95s/it]                                                   {'loss': 7.9362, 'grad_norm': 857.3798828125, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:20<3:12:20,  1.95s/it]  1%|          | 72/6000 [02:22<3:14:19,  1.97s/it]                                                   {'loss': 9.3639, 'grad_norm': 183.85340881347656, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:22<3:14:19,  1.97s/it]  1%|          | 73/6000 [02:24<3:13:53,  1.96s/it]                                                   {'loss': 10.2129, 'grad_norm': 379.5457458496094, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:24<3:13:53,  1.96s/it]  1%|          | 74/6000 [02:26<3:14:31,  1.97s/it]                                                   {'loss': 8.2855, 'grad_norm': 354.74462890625, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:26<3:14:31,  1.97s/it]  1%|â–         | 75/6000 [02:28<3:13:05,  1.96s/it]                                                   {'loss': 11.6484, 'grad_norm': 616.6143188476562, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:28<3:13:05,  1.96s/it]  1%|â–         | 76/6000 [02:31<3:20:59,  2.04s/it]                                                   {'loss': 5.3231, 'grad_norm': 411.8447265625, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:31<3:20:59,  2.04s/it]  1%|â–         | 77/6000 [02:33<3:25:13,  2.08s/it]                                                   {'loss': 6.2407, 'grad_norm': 175.110595703125, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:33<3:25:13,  2.08s/it]  1%|â–         | 78/6000 [02:35<3:23:09,  2.06s/it]                                                   {'loss': 8.0311, 'grad_norm': 662.5186767578125, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:35<3:23:09,  2.06s/it]  1%|â–         | 79/6000 [02:37<3:20:09,  2.03s/it]                                                   {'loss': 5.3261, 'grad_norm': 278.7772521972656, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:37<3:20:09,  2.03s/it]  1%|â–         | 80/6000 [02:39<3:19:39,  2.02s/it]                                                   {'loss': 7.2056, 'grad_norm': 562.7294921875, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:39<3:19:39,  2.02s/it]  1%|â–         | 81/6000 [02:41<3:18:05,  2.01s/it]                                                   {'loss': 6.4863, 'grad_norm': 284.6662292480469, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:41<3:18:05,  2.01s/it]  1%|â–         | 82/6000 [02:43<3:16:53,  2.00s/it]                                                   {'loss': 9.3012, 'grad_norm': 441.88909912109375, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:43<3:16:53,  2.00s/it]  1%|â–         | 83/6000 [02:45<3:16:39,  1.99s/it]                                                   {'loss': 6.8188, 'grad_norm': 610.2111206054688, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:45<3:16:39,  1.99s/it]  1%|â–         | 84/6000 [02:47<3:16:20,  1.99s/it]                                                   {'loss': 4.7895, 'grad_norm': 222.2933349609375, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:47<3:16:20,  1.99s/it]  1%|â–         | 85/6000 [02:49<3:17:46,  2.01s/it]                                                   {'loss': 6.8755, 'grad_norm': 284.43414306640625, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:49<3:17:46,  2.01s/it]  1%|â–         | 86/6000 [02:51<3:17:11,  2.00s/it]                                                   {'loss': 5.0184, 'grad_norm': 515.1201171875, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:51<3:17:11,  2.00s/it]  1%|â–         | 87/6000 [02:53<3:14:51,  1.98s/it]                                                   {'loss': 4.8628, 'grad_norm': 265.0749206542969, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:53<3:14:51,  1.98s/it]  1%|â–         | 88/6000 [02:55<3:12:49,  1.96s/it]                                                   {'loss': 6.3289, 'grad_norm': 487.81439208984375, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:55<3:12:49,  1.96s/it]  1%|â–         | 89/6000 [02:56<3:12:08,  1.95s/it]                                                   {'loss': 4.2617, 'grad_norm': 168.11595153808594, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:56<3:12:08,  1.95s/it]  2%|â–         | 90/6000 [02:58<3:12:10,  1.95s/it]                                                   {'loss': 4.4475, 'grad_norm': 304.2450256347656, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [02:58<3:12:10,  1.95s/it]  2%|â–         | 91/6000 [03:00<3:11:00,  1.94s/it]                                                   {'loss': 4.5258, 'grad_norm': 205.3786163330078, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:00<3:11:00,  1.94s/it]  2%|â–         | 92/6000 [03:02<3:13:27,  1.96s/it]                                                   {'loss': 5.9361, 'grad_norm': 522.1288452148438, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:02<3:13:27,  1.96s/it]  2%|â–         | 93/6000 [03:04<3:14:33,  1.98s/it]                                                   {'loss': 6.2906, 'grad_norm': 297.616943359375, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:04<3:14:33,  1.98s/it]  2%|â–         | 94/6000 [03:06<3:13:33,  1.97s/it]                                                   {'loss': 4.6573, 'grad_norm': 152.02685546875, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:06<3:13:33,  1.97s/it]  2%|â–         | 95/6000 [03:08<3:13:14,  1.96s/it]                                                   {'loss': 4.1549, 'grad_norm': 351.1684875488281, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:08<3:13:14,  1.96s/it]  2%|â–         | 96/6000 [03:10<3:11:59,  1.95s/it]                                                   {'loss': 5.5813, 'grad_norm': 86.25017547607422, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:10<3:11:59,  1.95s/it]  2%|â–         | 97/6000 [03:12<3:10:37,  1.94s/it]                                                   {'loss': 5.6407, 'grad_norm': 93.60783386230469, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:12<3:10:37,  1.94s/it]  2%|â–         | 98/6000 [03:14<3:11:34,  1.95s/it]                                                   {'loss': 6.4207, 'grad_norm': 331.53863525390625, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:14<3:11:34,  1.95s/it]  2%|â–         | 99/6000 [03:16<3:10:53,  1.94s/it]                                                   {'loss': 5.0326, 'grad_norm': 241.09188842773438, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:16<3:10:53,  1.94s/it]  2%|â–         | 100/6000 [03:18<3:10:36,  1.94s/it]                                                    {'loss': 4.5453, 'grad_norm': 109.69578552246094, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:18<3:10:36,  1.94s/it]  2%|â–         | 101/6000 [03:20<3:23:07,  2.07s/it]                                                    {'loss': 4.5806, 'grad_norm': 264.9677734375, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:20<3:23:07,  2.07s/it]  2%|â–         | 102/6000 [03:22<3:19:19,  2.03s/it]                                                    {'loss': 3.7609, 'grad_norm': 69.44852447509766, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:22<3:19:19,  2.03s/it]  2%|â–         | 103/6000 [03:24<3:18:02,  2.01s/it]                                                    {'loss': 4.523, 'grad_norm': 136.8304901123047, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:24<3:18:02,  2.01s/it]  2%|â–         | 104/6000 [03:26<3:18:40,  2.02s/it]                                                    {'loss': 3.8911, 'grad_norm': 78.36754608154297, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:26<3:18:40,  2.02s/it]  2%|â–         | 105/6000 [03:28<3:17:17,  2.01s/it]                                                    {'loss': 4.9038, 'grad_norm': 871.3367309570312, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:28<3:17:17,  2.01s/it]  2%|â–         | 106/6000 [03:30<3:17:24,  2.01s/it]                                                    {'loss': 5.0343, 'grad_norm': 104.88151550292969, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:30<3:17:24,  2.01s/it]  2%|â–         | 107/6000 [03:32<3:15:53,  1.99s/it]                                                    {'loss': 4.5779, 'grad_norm': 347.2270202636719, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:32<3:15:53,  1.99s/it]  2%|â–         | 108/6000 [03:34<3:17:09,  2.01s/it]                                                    {'loss': 4.4774, 'grad_norm': 480.853759765625, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:34<3:17:09,  2.01s/it]  2%|â–         | 109/6000 [03:36<3:19:36,  2.03s/it]                                                    {'loss': 4.0288, 'grad_norm': 86.96292877197266, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:36<3:19:36,  2.03s/it]  2%|â–         | 110/6000 [03:38<3:16:56,  2.01s/it]                                                    {'loss': 3.8379, 'grad_norm': 79.61170959472656, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:38<3:16:56,  2.01s/it]  2%|â–         | 111/6000 [03:40<3:15:58,  2.00s/it]                                                    {'loss': 4.5915, 'grad_norm': 645.4032592773438, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:40<3:15:58,  2.00s/it]  2%|â–         | 112/6000 [03:42<3:20:38,  2.04s/it]                                                    {'loss': 4.0212, 'grad_norm': 994.7672729492188, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:42<3:20:38,  2.04s/it]  2%|â–         | 113/6000 [03:44<3:18:13,  2.02s/it]                                                    {'loss': 3.8685, 'grad_norm': 431.5754089355469, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:44<3:18:13,  2.02s/it]  2%|â–         | 114/6000 [03:46<3:17:15,  2.01s/it]                                                    {'loss': 3.8918, 'grad_norm': 109.92658996582031, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:46<3:17:15,  2.01s/it]  2%|â–         | 115/6000 [03:48<3:17:29,  2.01s/it]                                                    {'loss': 4.1174, 'grad_norm': 395.72052001953125, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:48<3:17:29,  2.01s/it]  2%|â–         | 116/6000 [03:50<3:15:43,  2.00s/it]                                                    {'loss': 3.792, 'grad_norm': 48.772029876708984, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:50<3:15:43,  2.00s/it]  2%|â–         | 117/6000 [03:52<3:13:59,  1.98s/it]                                                    {'loss': 3.9503, 'grad_norm': 132.50950622558594, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:52<3:13:59,  1.98s/it]  2%|â–         | 118/6000 [03:54<3:18:03,  2.02s/it]                                                    {'loss': 3.6585, 'grad_norm': 124.46510314941406, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:54<3:18:03,  2.02s/it]  2%|â–         | 119/6000 [03:56<3:14:56,  1.99s/it]                                                    {'loss': 3.9918, 'grad_norm': 158.58944702148438, 'learning_rate': 9.967796610169493e-06, 'epoch': 0.02}
  2%|â–         | 119/6000 [03:56<3:14:56,  1.99s/it]  2%|â–         | 120/6000 [03:58<3:14:01,  1.98s/it]                                                    {'loss': 3.6994, 'grad_norm': 101.3173599243164, 'learning_rate': 9.966101694915256e-06, 'epoch': 0.02}
  2%|â–         | 120/6000 [03:58<3:14:01,  1.98s/it]  2%|â–         | 121/6000 [04:00<3:14:42,  1.99s/it]                                                    {'loss': 3.8667, 'grad_norm': 122.59785461425781, 'learning_rate': 9.964406779661018e-06, 'epoch': 0.02}
  2%|â–         | 121/6000 [04:00<3:14:42,  1.99s/it]  2%|â–         | 122/6000 [04:02<3:14:19,  1.98s/it]                                                    {'loss': 3.5962, 'grad_norm': 83.47929382324219, 'learning_rate': 9.96271186440678e-06, 'epoch': 0.02}
  2%|â–         | 122/6000 [04:02<3:14:19,  1.98s/it]  2%|â–         | 123/6000 [04:04<3:13:21,  1.97s/it]                                                    {'loss': 3.7263, 'grad_norm': 37.4984245300293, 'learning_rate': 9.961016949152543e-06, 'epoch': 0.02}
  2%|â–         | 123/6000 [04:04<3:13:21,  1.97s/it]  2%|â–         | 124/6000 [04:06<3:13:55,  1.98s/it]                                                    {'loss': 3.7483, 'grad_norm': 393.15045166015625, 'learning_rate': 9.959322033898306e-06, 'epoch': 0.02}
  2%|â–         | 124/6000 [04:06<3:13:55,  1.98s/it]  2%|â–         | 125/6000 [04:08<3:16:11,  2.00s/it]                                                    {'loss': 3.7279, 'grad_norm': 80.06305694580078, 'learning_rate': 9.957627118644069e-06, 'epoch': 0.02}
  2%|â–         | 125/6000 [04:08<3:16:11,  2.00s/it]  2%|â–         | 126/6000 [04:10<3:16:33,  2.01s/it]                                                    {'loss': 3.853, 'grad_norm': 98.67477416992188, 'learning_rate': 9.95593220338983e-06, 'epoch': 0.02}
  2%|â–         | 126/6000 [04:10<3:16:33,  2.01s/it]  2%|â–         | 127/6000 [04:12<3:16:18,  2.01s/it]                                                    {'loss': 3.966, 'grad_norm': 147.13438415527344, 'learning_rate': 9.954237288135594e-06, 'epoch': 0.02}
  2%|â–         | 127/6000 [04:12<3:16:18,  2.01s/it]  2%|â–         | 128/6000 [04:14<3:13:41,  1.98s/it]                                                    {'loss': 3.8875, 'grad_norm': 59.86530685424805, 'learning_rate': 9.952542372881356e-06, 'epoch': 0.02}
  2%|â–         | 128/6000 [04:14<3:13:41,  1.98s/it]  2%|â–         | 129/6000 [04:16<3:14:12,  1.98s/it]                                                    {'loss': 4.0466, 'grad_norm': 118.41114044189453, 'learning_rate': 9.95084745762712e-06, 'epoch': 0.02}
  2%|â–         | 129/6000 [04:16<3:14:12,  1.98s/it]  2%|â–         | 130/6000 [04:18<3:13:26,  1.98s/it]                                                    {'loss': 3.5548, 'grad_norm': 41.99189758300781, 'learning_rate': 9.949152542372882e-06, 'epoch': 0.02}
  2%|â–         | 130/6000 [04:18<3:13:26,  1.98s/it]  2%|â–         | 131/6000 [04:20<3:12:58,  1.97s/it]                                                    {'loss': 3.6531, 'grad_norm': 67.5063247680664, 'learning_rate': 9.947457627118645e-06, 'epoch': 0.02}
  2%|â–         | 131/6000 [04:20<3:12:58,  1.97s/it]  2%|â–         | 132/6000 [04:22<3:12:15,  1.97s/it]                                                    {'loss': 3.8778, 'grad_norm': 180.3008575439453, 'learning_rate': 9.945762711864407e-06, 'epoch': 0.02}
  2%|â–         | 132/6000 [04:22<3:12:15,  1.97s/it]  2%|â–         | 133/6000 [04:24<3:10:37,  1.95s/it]                                                    {'loss': 3.7154, 'grad_norm': 103.96382141113281, 'learning_rate': 9.94406779661017e-06, 'epoch': 0.02}
  2%|â–         | 133/6000 [04:24<3:10:37,  1.95s/it]  2%|â–         | 134/6000 [04:26<3:11:33,  1.96s/it]                                                    {'loss': 3.6962, 'grad_norm': 65.26579284667969, 'learning_rate': 9.942372881355933e-06, 'epoch': 0.02}
  2%|â–         | 134/6000 [04:26<3:11:33,  1.96s/it]  2%|â–         | 135/6000 [04:28<3:10:01,  1.94s/it]                                                    {'loss': 3.6694, 'grad_norm': 45.43143844604492, 'learning_rate': 9.940677966101697e-06, 'epoch': 0.02}
  2%|â–         | 135/6000 [04:28<3:10:01,  1.94s/it]  2%|â–         | 136/6000 [04:30<3:08:51,  1.93s/it]                                                    {'loss': 3.7741, 'grad_norm': 94.25511169433594, 'learning_rate': 9.938983050847458e-06, 'epoch': 0.02}
  2%|â–         | 136/6000 [04:30<3:08:51,  1.93s/it]  2%|â–         | 137/6000 [04:32<3:14:35,  1.99s/it]                                                    {'loss': 3.6473, 'grad_norm': 91.75630950927734, 'learning_rate': 9.937288135593222e-06, 'epoch': 0.02}
  2%|â–         | 137/6000 [04:32<3:14:35,  1.99s/it]  2%|â–         | 138/6000 [04:34<3:12:03,  1.97s/it]                                                    {'loss': 4.0364, 'grad_norm': 109.15287780761719, 'learning_rate': 9.935593220338983e-06, 'epoch': 0.02}
  2%|â–         | 138/6000 [04:34<3:12:03,  1.97s/it]  2%|â–         | 139/6000 [04:36<3:13:18,  1.98s/it]                                                    {'loss': 3.7116, 'grad_norm': 66.6628189086914, 'learning_rate': 9.933898305084746e-06, 'epoch': 0.02}
  2%|â–         | 139/6000 [04:36<3:13:18,  1.98s/it]  2%|â–         | 140/6000 [04:38<3:16:38,  2.01s/it]                                                    {'loss': 3.9639, 'grad_norm': 50.373146057128906, 'learning_rate': 9.93220338983051e-06, 'epoch': 0.02}
  2%|â–         | 140/6000 [04:38<3:16:38,  2.01s/it]  2%|â–         | 141/6000 [04:40<3:14:51,  2.00s/it]                                                    {'loss': 3.7682, 'grad_norm': 46.24123764038086, 'learning_rate': 9.930508474576273e-06, 'epoch': 0.02}
  2%|â–         | 141/6000 [04:40<3:14:51,  2.00s/it]  2%|â–         | 142/6000 [04:42<3:11:37,  1.96s/it]                                                    {'loss': 3.9036, 'grad_norm': 51.06374740600586, 'learning_rate': 9.928813559322035e-06, 'epoch': 0.02}
  2%|â–         | 142/6000 [04:42<3:11:37,  1.96s/it]  2%|â–         | 143/6000 [04:44<3:11:49,  1.97s/it]                                                    {'loss': 3.7291, 'grad_norm': 79.4251937866211, 'learning_rate': 9.927118644067796e-06, 'epoch': 0.02}
  2%|â–         | 143/6000 [04:44<3:11:49,  1.97s/it]  2%|â–         | 144/6000 [04:46<3:12:06,  1.97s/it]                                                    {'loss': 3.5655, 'grad_norm': 191.29452514648438, 'learning_rate': 9.92542372881356e-06, 'epoch': 0.02}
  2%|â–         | 144/6000 [04:46<3:12:06,  1.97s/it]  2%|â–         | 145/6000 [04:48<3:10:15,  1.95s/it]                                                    {'loss': 3.7953, 'grad_norm': 137.01141357421875, 'learning_rate': 9.923728813559323e-06, 'epoch': 0.02}
  2%|â–         | 145/6000 [04:48<3:10:15,  1.95s/it]  2%|â–         | 146/6000 [04:50<3:12:14,  1.97s/it]                                                    {'loss': 3.7176, 'grad_norm': 140.92742919921875, 'learning_rate': 9.922033898305086e-06, 'epoch': 0.02}
  2%|â–         | 146/6000 [04:50<3:12:14,  1.97s/it]  2%|â–         | 147/6000 [04:52<3:11:38,  1.96s/it]                                                    {'loss': 3.67, 'grad_norm': 114.02819061279297, 'learning_rate': 9.920338983050848e-06, 'epoch': 0.02}
  2%|â–         | 147/6000 [04:52<3:11:38,  1.96s/it]  2%|â–         | 148/6000 [04:54<3:12:14,  1.97s/it]                                                    {'loss': 3.5842, 'grad_norm': 48.861751556396484, 'learning_rate': 9.918644067796611e-06, 'epoch': 0.02}
  2%|â–         | 148/6000 [04:54<3:12:14,  1.97s/it]  2%|â–         | 149/6000 [04:55<3:10:49,  1.96s/it]                                                    {'loss': 3.7388, 'grad_norm': 136.27688598632812, 'learning_rate': 9.916949152542374e-06, 'epoch': 0.02}
  2%|â–         | 149/6000 [04:55<3:10:49,  1.96s/it]  2%|â–Ž         | 150/6000 [04:57<3:11:17,  1.96s/it]                                                    {'loss': 3.6614, 'grad_norm': 179.48500061035156, 'learning_rate': 9.915254237288137e-06, 'epoch': 0.03}
  2%|â–Ž         | 150/6000 [04:57<3:11:17,  1.96s/it]  3%|â–Ž         | 151/6000 [04:59<3:11:56,  1.97s/it]                                                    {'loss': 3.6354, 'grad_norm': 301.7174377441406, 'learning_rate': 9.913559322033899e-06, 'epoch': 0.03}
  3%|â–Ž         | 151/6000 [04:59<3:11:56,  1.97s/it]  3%|â–Ž         | 152/6000 [05:01<3:13:19,  1.98s/it]                                                    {'loss': 3.6046, 'grad_norm': 206.82591247558594, 'learning_rate': 9.911864406779662e-06, 'epoch': 0.03}
  3%|â–Ž         | 152/6000 [05:01<3:13:19,  1.98s/it]  3%|â–Ž         | 153/6000 [05:03<3:10:30,  1.95s/it]                                                    {'loss': 3.7234, 'grad_norm': 101.22838592529297, 'learning_rate': 9.910169491525424e-06, 'epoch': 0.03}
  3%|â–Ž         | 153/6000 [05:03<3:10:30,  1.95s/it]  3%|â–Ž         | 154/6000 [05:05<3:10:42,  1.96s/it]                                                    {'loss': 3.5436, 'grad_norm': 103.84098815917969, 'learning_rate': 9.908474576271187e-06, 'epoch': 0.03}
  3%|â–Ž         | 154/6000 [05:05<3:10:42,  1.96s/it]  3%|â–Ž         | 155/6000 [05:07<3:12:13,  1.97s/it]                                                    {'loss': 3.5085, 'grad_norm': 34.16358947753906, 'learning_rate': 9.90677966101695e-06, 'epoch': 0.03}
  3%|â–Ž         | 155/6000 [05:07<3:12:13,  1.97s/it]  3%|â–Ž         | 156/6000 [05:09<3:11:34,  1.97s/it]                                                    {'loss': 3.7132, 'grad_norm': 49.40913391113281, 'learning_rate': 9.905084745762714e-06, 'epoch': 0.03}
  3%|â–Ž         | 156/6000 [05:09<3:11:34,  1.97s/it]  3%|â–Ž         | 157/6000 [05:11<3:15:43,  2.01s/it]                                                    {'loss': 3.5635, 'grad_norm': 233.13299560546875, 'learning_rate': 9.903389830508475e-06, 'epoch': 0.03}
  3%|â–Ž         | 157/6000 [05:11<3:15:43,  2.01s/it]  3%|â–Ž         | 158/6000 [05:13<3:14:19,  2.00s/it]                                                    {'loss': 3.8258, 'grad_norm': 1578.108642578125, 'learning_rate': 9.901694915254239e-06, 'epoch': 0.03}
  3%|â–Ž         | 158/6000 [05:13<3:14:19,  2.00s/it]  3%|â–Ž         | 159/6000 [05:15<3:16:29,  2.02s/it]                                                    {'loss': 3.6223, 'grad_norm': 152.5836181640625, 'learning_rate': 9.9e-06, 'epoch': 0.03}
  3%|â–Ž         | 159/6000 [05:15<3:16:29,  2.02s/it]  3%|â–Ž         | 160/6000 [05:18<3:21:48,  2.07s/it]                                                    {'loss': 3.5291, 'grad_norm': 47.98439407348633, 'learning_rate': 9.898305084745763e-06, 'epoch': 0.03}
  3%|â–Ž         | 160/6000 [05:18<3:21:48,  2.07s/it]  3%|â–Ž         | 161/6000 [05:20<3:23:09,  2.09s/it]                                                    {'loss': 3.5379, 'grad_norm': 17.68199348449707, 'learning_rate': 9.896610169491527e-06, 'epoch': 0.03}
  3%|â–Ž         | 161/6000 [05:20<3:23:09,  2.09s/it]  3%|â–Ž         | 162/6000 [05:22<3:18:58,  2.05s/it]                                                    {'loss': 3.5538, 'grad_norm': 57.35950469970703, 'learning_rate': 9.89491525423729e-06, 'epoch': 0.03}
  3%|â–Ž         | 162/6000 [05:22<3:18:58,  2.05s/it]  3%|â–Ž         | 163/6000 [05:24<3:19:56,  2.06s/it]                                                    {'loss': 3.4821, 'grad_norm': 29.45334243774414, 'learning_rate': 9.893220338983051e-06, 'epoch': 0.03}
  3%|â–Ž         | 163/6000 [05:24<3:19:56,  2.06s/it]  3%|â–Ž         | 164/6000 [05:26<3:17:08,  2.03s/it]                                                    {'loss': 3.6323, 'grad_norm': 263.5810241699219, 'learning_rate': 9.891525423728813e-06, 'epoch': 0.03}
  3%|â–Ž         | 164/6000 [05:26<3:17:08,  2.03s/it]  3%|â–Ž         | 165/6000 [05:28<3:16:21,  2.02s/it]                                                    {'loss': 3.5218, 'grad_norm': 121.9154052734375, 'learning_rate': 9.889830508474576e-06, 'epoch': 0.03}
  3%|â–Ž         | 165/6000 [05:28<3:16:21,  2.02s/it]  3%|â–Ž         | 166/6000 [05:30<3:12:42,  1.98s/it]                                                    {'loss': 3.5781, 'grad_norm': 64.24615478515625, 'learning_rate': 9.88813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 166/6000 [05:30<3:12:42,  1.98s/it]  3%|â–Ž         | 167/6000 [05:32<3:11:55,  1.97s/it]                                                    {'loss': 3.5947, 'grad_norm': 51.386497497558594, 'learning_rate': 9.886440677966103e-06, 'epoch': 0.03}
  3%|â–Ž         | 167/6000 [05:32<3:11:55,  1.97s/it]  3%|â–Ž         | 168/6000 [05:33<3:10:36,  1.96s/it]                                                    {'loss': 3.5521, 'grad_norm': 265.64874267578125, 'learning_rate': 9.884745762711864e-06, 'epoch': 0.03}
  3%|â–Ž         | 168/6000 [05:33<3:10:36,  1.96s/it]  3%|â–Ž         | 169/6000 [05:36<3:15:50,  2.02s/it]                                                    {'loss': 3.5025, 'grad_norm': 223.50392150878906, 'learning_rate': 9.883050847457628e-06, 'epoch': 0.03}
  3%|â–Ž         | 169/6000 [05:36<3:15:50,  2.02s/it]  3%|â–Ž         | 170/6000 [05:38<3:14:04,  2.00s/it]                                                    {'loss': 3.5789, 'grad_norm': 60.558162689208984, 'learning_rate': 9.881355932203391e-06, 'epoch': 0.03}
  3%|â–Ž         | 170/6000 [05:38<3:14:04,  2.00s/it]  3%|â–Ž         | 171/6000 [05:39<3:11:49,  1.97s/it]                                                    {'loss': 3.5646, 'grad_norm': 17.79172134399414, 'learning_rate': 9.879661016949154e-06, 'epoch': 0.03}
  3%|â–Ž         | 171/6000 [05:39<3:11:49,  1.97s/it]  3%|â–Ž         | 172/6000 [05:41<3:11:45,  1.97s/it]                                                    {'loss': 3.7526, 'grad_norm': 26.044416427612305, 'learning_rate': 9.877966101694916e-06, 'epoch': 0.03}
  3%|â–Ž         | 172/6000 [05:41<3:11:45,  1.97s/it]  3%|â–Ž         | 173/6000 [05:43<3:11:09,  1.97s/it]                                                    {'loss': 3.4922, 'grad_norm': 47.715171813964844, 'learning_rate': 9.876271186440679e-06, 'epoch': 0.03}
  3%|â–Ž         | 173/6000 [05:43<3:11:09,  1.97s/it]  3%|â–Ž         | 174/6000 [05:46<3:19:25,  2.05s/it]                                                    {'loss': 3.5175, 'grad_norm': 13.062104225158691, 'learning_rate': 9.87457627118644e-06, 'epoch': 0.03}
  3%|â–Ž         | 174/6000 [05:46<3:19:25,  2.05s/it]  3%|â–Ž         | 175/6000 [05:48<3:17:21,  2.03s/it]                                                    {'loss': 3.4996, 'grad_norm': 39.70358657836914, 'learning_rate': 9.872881355932204e-06, 'epoch': 0.03}
  3%|â–Ž         | 175/6000 [05:48<3:17:21,  2.03s/it]  3%|â–Ž         | 176/6000 [05:50<3:16:17,  2.02s/it]                                                    {'loss': 3.5194, 'grad_norm': 20.728246688842773, 'learning_rate': 9.871186440677967e-06, 'epoch': 0.03}
  3%|â–Ž         | 176/6000 [05:50<3:16:17,  2.02s/it]  3%|â–Ž         | 177/6000 [05:52<3:15:05,  2.01s/it]                                                    {'loss': 3.5125, 'grad_norm': 16.521339416503906, 'learning_rate': 9.86949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 177/6000 [05:52<3:15:05,  2.01s/it]  3%|â–Ž         | 178/6000 [05:54<3:14:17,  2.00s/it]                                                    {'loss': 3.4839, 'grad_norm': 30.786836624145508, 'learning_rate': 9.867796610169492e-06, 'epoch': 0.03}
  3%|â–Ž         | 178/6000 [05:54<3:14:17,  2.00s/it]  3%|â–Ž         | 179/6000 [05:56<3:11:55,  1.98s/it]                                                    {'loss': 3.5056, 'grad_norm': 15.361035346984863, 'learning_rate': 9.866101694915255e-06, 'epoch': 0.03}
  3%|â–Ž         | 179/6000 [05:56<3:11:55,  1.98s/it]  3%|â–Ž         | 180/6000 [05:58<3:12:18,  1.98s/it]                                                    {'loss': 3.57, 'grad_norm': 16.231348037719727, 'learning_rate': 9.864406779661017e-06, 'epoch': 0.03}
  3%|â–Ž         | 180/6000 [05:58<3:12:18,  1.98s/it]  3%|â–Ž         | 181/6000 [06:00<3:13:08,  1.99s/it]                                                    {'loss': 3.5346, 'grad_norm': 32.21653747558594, 'learning_rate': 9.86271186440678e-06, 'epoch': 0.03}
  3%|â–Ž         | 181/6000 [06:00<3:13:08,  1.99s/it]  3%|â–Ž         | 182/6000 [06:01<3:12:12,  1.98s/it]                                                    {'loss': 3.5185, 'grad_norm': 26.51558494567871, 'learning_rate': 9.861016949152544e-06, 'epoch': 0.03}
  3%|â–Ž         | 182/6000 [06:02<3:12:12,  1.98s/it]  3%|â–Ž         | 183/6000 [06:03<3:10:55,  1.97s/it]                                                    {'loss': 3.4846, 'grad_norm': 16.481616973876953, 'learning_rate': 9.859322033898307e-06, 'epoch': 0.03}
  3%|â–Ž         | 183/6000 [06:03<3:10:55,  1.97s/it]  3%|â–Ž         | 184/6000 [06:06<3:13:37,  2.00s/it]                                                    {'loss': 3.4899, 'grad_norm': 18.84120750427246, 'learning_rate': 9.857627118644068e-06, 'epoch': 0.03}
  3%|â–Ž         | 184/6000 [06:06<3:13:37,  2.00s/it]  3%|â–Ž         | 185/6000 [06:07<3:13:14,  1.99s/it]                                                    {'loss': 3.4747, 'grad_norm': 28.00648307800293, 'learning_rate': 9.855932203389832e-06, 'epoch': 0.03}
  3%|â–Ž         | 185/6000 [06:07<3:13:14,  1.99s/it]  3%|â–Ž         | 186/6000 [06:10<3:14:31,  2.01s/it]                                                    {'loss': 3.5082, 'grad_norm': 29.799978256225586, 'learning_rate': 9.854237288135595e-06, 'epoch': 0.03}
  3%|â–Ž         | 186/6000 [06:10<3:14:31,  2.01s/it]  3%|â–Ž         | 187/6000 [06:11<3:12:23,  1.99s/it]                                                    {'loss': 3.4887, 'grad_norm': 14.7357816696167, 'learning_rate': 9.852542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 187/6000 [06:11<3:12:23,  1.99s/it]  3%|â–Ž         | 188/6000 [06:13<3:11:44,  1.98s/it]                                                    {'loss': 3.5009, 'grad_norm': 16.583465576171875, 'learning_rate': 9.85084745762712e-06, 'epoch': 0.03}
  3%|â–Ž         | 188/6000 [06:13<3:11:44,  1.98s/it]  3%|â–Ž         | 189/6000 [06:15<3:10:44,  1.97s/it]                                                    {'loss': 3.521, 'grad_norm': 12.376413345336914, 'learning_rate': 9.849152542372881e-06, 'epoch': 0.03}
  3%|â–Ž         | 189/6000 [06:15<3:10:44,  1.97s/it]  3%|â–Ž         | 190/6000 [06:17<3:10:51,  1.97s/it]                                                    {'loss': 3.513, 'grad_norm': 28.324748992919922, 'learning_rate': 9.847457627118645e-06, 'epoch': 0.03}
  3%|â–Ž         | 190/6000 [06:17<3:10:51,  1.97s/it]  3%|â–Ž         | 191/6000 [06:19<3:10:18,  1.97s/it]                                                    {'loss': 3.515, 'grad_norm': 65.61690521240234, 'learning_rate': 9.845762711864408e-06, 'epoch': 0.03}
  3%|â–Ž         | 191/6000 [06:19<3:10:18,  1.97s/it]  3%|â–Ž         | 192/6000 [06:22<3:17:28,  2.04s/it]                                                    {'loss': 3.5332, 'grad_norm': 22.91518783569336, 'learning_rate': 9.844067796610171e-06, 'epoch': 0.03}
  3%|â–Ž         | 192/6000 [06:22<3:17:28,  2.04s/it]  3%|â–Ž         | 193/6000 [06:24<3:17:12,  2.04s/it]                                                    {'loss': 3.4841, 'grad_norm': 21.819385528564453, 'learning_rate': 9.842372881355933e-06, 'epoch': 0.03}
  3%|â–Ž         | 193/6000 [06:24<3:17:12,  2.04s/it]  3%|â–Ž         | 194/6000 [06:25<3:13:34,  2.00s/it]                                                    {'loss': 3.5447, 'grad_norm': 385.4642333984375, 'learning_rate': 9.840677966101696e-06, 'epoch': 0.03}
  3%|â–Ž         | 194/6000 [06:25<3:13:34,  2.00s/it]  3%|â–Ž         | 195/6000 [06:27<3:12:16,  1.99s/it]                                                    {'loss': 3.5525, 'grad_norm': 586.46728515625, 'learning_rate': 9.838983050847458e-06, 'epoch': 0.03}
  3%|â–Ž         | 195/6000 [06:27<3:12:16,  1.99s/it]  3%|â–Ž         | 196/6000 [06:29<3:11:39,  1.98s/it]                                                    {'loss': 3.5118, 'grad_norm': 30.162878036499023, 'learning_rate': 9.837288135593221e-06, 'epoch': 0.03}
  3%|â–Ž         | 196/6000 [06:29<3:11:39,  1.98s/it]  3%|â–Ž         | 197/6000 [06:31<3:12:06,  1.99s/it]                                                    {'loss': 3.4802, 'grad_norm': 30.755586624145508, 'learning_rate': 9.835593220338984e-06, 'epoch': 0.03}
  3%|â–Ž         | 197/6000 [06:31<3:12:06,  1.99s/it]  3%|â–Ž         | 198/6000 [06:33<3:11:06,  1.98s/it]                                                    {'loss': 3.4776, 'grad_norm': 27.395919799804688, 'learning_rate': 9.833898305084747e-06, 'epoch': 0.03}
  3%|â–Ž         | 198/6000 [06:33<3:11:06,  1.98s/it]  3%|â–Ž         | 199/6000 [06:35<3:09:27,  1.96s/it]                                                    {'loss': 3.4917, 'grad_norm': 72.70919799804688, 'learning_rate': 9.832203389830509e-06, 'epoch': 0.03}
  3%|â–Ž         | 199/6000 [06:35<3:09:27,  1.96s/it]  3%|â–Ž         | 200/6000 [06:37<3:13:28,  2.00s/it]                                                    {'loss': 3.5245, 'grad_norm': 123.42292022705078, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.03}
  3%|â–Ž         | 200/6000 [06:37<3:13:28,  2.00s/it]  3%|â–Ž         | 201/6000 [06:39<3:13:44,  2.00s/it]                                                    {'loss': 3.5064, 'grad_norm': 60.79713821411133, 'learning_rate': 9.828813559322034e-06, 'epoch': 0.03}
  3%|â–Ž         | 201/6000 [06:39<3:13:44,  2.00s/it]  3%|â–Ž         | 202/6000 [06:41<3:11:21,  1.98s/it]                                                    {'loss': 3.4888, 'grad_norm': 20.089040756225586, 'learning_rate': 9.827118644067797e-06, 'epoch': 0.03}
  3%|â–Ž         | 202/6000 [06:41<3:11:21,  1.98s/it]  3%|â–Ž         | 203/6000 [06:43<3:10:56,  1.98s/it]                                                    {'loss': 3.5296, 'grad_norm': 82.98656463623047, 'learning_rate': 9.82542372881356e-06, 'epoch': 0.03}
  3%|â–Ž         | 203/6000 [06:43<3:10:56,  1.98s/it]  3%|â–Ž         | 204/6000 [06:45<3:11:21,  1.98s/it]                                                    {'loss': 3.4928, 'grad_norm': 82.0943603515625, 'learning_rate': 9.823728813559322e-06, 'epoch': 0.03}
  3%|â–Ž         | 204/6000 [06:45<3:11:21,  1.98s/it]  3%|â–Ž         | 205/6000 [06:47<3:09:48,  1.97s/it]                                                    {'loss': 3.4992, 'grad_norm': 22.26544761657715, 'learning_rate': 9.822033898305085e-06, 'epoch': 0.03}
  3%|â–Ž         | 205/6000 [06:47<3:09:48,  1.97s/it]  3%|â–Ž         | 206/6000 [06:49<3:09:35,  1.96s/it]                                                    {'loss': 3.5135, 'grad_norm': 31.514659881591797, 'learning_rate': 9.820338983050849e-06, 'epoch': 0.03}
  3%|â–Ž         | 206/6000 [06:49<3:09:35,  1.96s/it]  3%|â–Ž         | 207/6000 [06:51<3:10:31,  1.97s/it]                                                    {'loss': 3.4979, 'grad_norm': 27.87031364440918, 'learning_rate': 9.818644067796612e-06, 'epoch': 0.03}
  3%|â–Ž         | 207/6000 [06:51<3:10:31,  1.97s/it]  3%|â–Ž         | 208/6000 [06:53<3:08:52,  1.96s/it]                                                    {'loss': 3.5271, 'grad_norm': 38.120811462402344, 'learning_rate': 9.816949152542373e-06, 'epoch': 0.03}
  3%|â–Ž         | 208/6000 [06:53<3:08:52,  1.96s/it]  3%|â–Ž         | 209/6000 [06:55<3:08:38,  1.95s/it]                                                    {'loss': 3.4849, 'grad_norm': 19.732173919677734, 'learning_rate': 9.815254237288137e-06, 'epoch': 0.03}
  3%|â–Ž         | 209/6000 [06:55<3:08:38,  1.95s/it]  4%|â–Ž         | 210/6000 [06:57<3:10:43,  1.98s/it]                                                    {'loss': 3.5088, 'grad_norm': 281.31427001953125, 'learning_rate': 9.813559322033898e-06, 'epoch': 0.04}
  4%|â–Ž         | 210/6000 [06:57<3:10:43,  1.98s/it]  4%|â–Ž         | 211/6000 [06:59<3:10:53,  1.98s/it]                                                    {'loss': 3.4783, 'grad_norm': 23.111515045166016, 'learning_rate': 9.811864406779662e-06, 'epoch': 0.04}
  4%|â–Ž         | 211/6000 [06:59<3:10:53,  1.98s/it]  4%|â–Ž         | 212/6000 [07:01<3:13:25,  2.01s/it]                                                    {'loss': 3.4954, 'grad_norm': 26.301427841186523, 'learning_rate': 9.810169491525425e-06, 'epoch': 0.04}
  4%|â–Ž         | 212/6000 [07:01<3:13:25,  2.01s/it]  4%|â–Ž         | 213/6000 [07:03<3:14:59,  2.02s/it]                                                    {'loss': 3.4673, 'grad_norm': 20.486621856689453, 'learning_rate': 9.808474576271188e-06, 'epoch': 0.04}
  4%|â–Ž         | 213/6000 [07:03<3:14:59,  2.02s/it]  4%|â–Ž         | 214/6000 [07:05<3:13:00,  2.00s/it]                                                    {'loss': 3.5119, 'grad_norm': 17.932939529418945, 'learning_rate': 9.80677966101695e-06, 'epoch': 0.04}
  4%|â–Ž         | 214/6000 [07:05<3:13:00,  2.00s/it]  4%|â–Ž         | 215/6000 [07:07<3:12:14,  1.99s/it]                                                    {'loss': 3.6094, 'grad_norm': 37.20467758178711, 'learning_rate': 9.805084745762713e-06, 'epoch': 0.04}
  4%|â–Ž         | 215/6000 [07:07<3:12:14,  1.99s/it]  4%|â–Ž         | 216/6000 [07:09<3:11:52,  1.99s/it]                                                    {'loss': 3.5129, 'grad_norm': 12.841805458068848, 'learning_rate': 9.803389830508474e-06, 'epoch': 0.04}
  4%|â–Ž         | 216/6000 [07:09<3:11:52,  1.99s/it]  4%|â–Ž         | 217/6000 [07:11<3:12:57,  2.00s/it]                                                    {'loss': 3.4965, 'grad_norm': 10.674739837646484, 'learning_rate': 9.801694915254238e-06, 'epoch': 0.04}
  4%|â–Ž         | 217/6000 [07:11<3:12:57,  2.00s/it]  4%|â–Ž         | 218/6000 [07:13<3:12:34,  2.00s/it]                                                    {'loss': 3.4839, 'grad_norm': 19.869653701782227, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.04}
  4%|â–Ž         | 218/6000 [07:13<3:12:34,  2.00s/it]  4%|â–Ž         | 219/6000 [07:15<3:09:25,  1.97s/it]                                                    {'loss': 3.489, 'grad_norm': 33.47409439086914, 'learning_rate': 9.798305084745764e-06, 'epoch': 0.04}
  4%|â–Ž         | 219/6000 [07:15<3:09:25,  1.97s/it]  4%|â–Ž         | 220/6000 [07:17<3:10:54,  1.98s/it]                                                    {'loss': 3.4927, 'grad_norm': 19.15031623840332, 'learning_rate': 9.796610169491526e-06, 'epoch': 0.04}
  4%|â–Ž         | 220/6000 [07:17<3:10:54,  1.98s/it]  4%|â–Ž         | 221/6000 [07:19<3:10:09,  1.97s/it]                                                    {'loss': 3.5025, 'grad_norm': 17.063119888305664, 'learning_rate': 9.79491525423729e-06, 'epoch': 0.04}
  4%|â–Ž         | 221/6000 [07:19<3:10:09,  1.97s/it]  4%|â–Ž         | 222/6000 [07:21<3:08:10,  1.95s/it]                                                    {'loss': 3.4856, 'grad_norm': 23.220550537109375, 'learning_rate': 9.79322033898305e-06, 'epoch': 0.04}
  4%|â–Ž         | 222/6000 [07:21<3:08:10,  1.95s/it]  4%|â–Ž         | 223/6000 [07:23<3:08:08,  1.95s/it]                                                    {'loss': 3.5181, 'grad_norm': 26.835372924804688, 'learning_rate': 9.791525423728816e-06, 'epoch': 0.04}
  4%|â–Ž         | 223/6000 [07:23<3:08:08,  1.95s/it]  4%|â–Ž         | 224/6000 [07:25<3:07:37,  1.95s/it]                                                    {'loss': 3.4943, 'grad_norm': 57.072547912597656, 'learning_rate': 9.789830508474577e-06, 'epoch': 0.04}
  4%|â–Ž         | 224/6000 [07:25<3:07:37,  1.95s/it]  4%|â–         | 225/6000 [07:27<3:09:52,  1.97s/it]                                                    {'loss': 3.4671, 'grad_norm': 11.467193603515625, 'learning_rate': 9.788135593220339e-06, 'epoch': 0.04}
  4%|â–         | 225/6000 [07:27<3:09:52,  1.97s/it]  4%|â–         | 226/6000 [07:29<3:13:31,  2.01s/it]                                                    {'loss': 3.5769, 'grad_norm': 11.355620384216309, 'learning_rate': 9.786440677966102e-06, 'epoch': 0.04}
  4%|â–         | 226/6000 [07:29<3:13:31,  2.01s/it]  4%|â–         | 227/6000 [07:31<3:09:55,  1.97s/it]                                                    {'loss': 3.4958, 'grad_norm': 34.06612014770508, 'learning_rate': 9.784745762711865e-06, 'epoch': 0.04}
  4%|â–         | 227/6000 [07:31<3:09:55,  1.97s/it]  4%|â–         | 228/6000 [07:33<3:09:49,  1.97s/it]                                                    {'loss': 3.5881, 'grad_norm': 636.7092895507812, 'learning_rate': 9.783050847457629e-06, 'epoch': 0.04}
  4%|â–         | 228/6000 [07:33<3:09:49,  1.97s/it]  4%|â–         | 229/6000 [07:35<3:07:45,  1.95s/it]                                                    {'loss': 3.5729, 'grad_norm': 260.1296081542969, 'learning_rate': 9.78135593220339e-06, 'epoch': 0.04}
  4%|â–         | 229/6000 [07:35<3:07:45,  1.95s/it]  4%|â–         | 230/6000 [07:37<3:10:15,  1.98s/it]                                                    {'loss': 3.4889, 'grad_norm': 22.970800399780273, 'learning_rate': 9.779661016949154e-06, 'epoch': 0.04}
  4%|â–         | 230/6000 [07:37<3:10:15,  1.98s/it]  4%|â–         | 231/6000 [07:39<3:08:42,  1.96s/it]                                                    {'loss': 3.4699, 'grad_norm': 12.203570365905762, 'learning_rate': 9.777966101694915e-06, 'epoch': 0.04}
  4%|â–         | 231/6000 [07:39<3:08:42,  1.96s/it]  4%|â–         | 232/6000 [07:41<3:08:00,  1.96s/it]                                                    {'loss': 3.4895, 'grad_norm': 32.522586822509766, 'learning_rate': 9.776271186440678e-06, 'epoch': 0.04}
  4%|â–         | 232/6000 [07:41<3:08:00,  1.96s/it]  4%|â–         | 233/6000 [07:43<3:10:26,  1.98s/it]                                                    {'loss': 3.4692, 'grad_norm': 29.117734909057617, 'learning_rate': 9.774576271186442e-06, 'epoch': 0.04}
  4%|â–         | 233/6000 [07:43<3:10:26,  1.98s/it]  4%|â–         | 234/6000 [07:45<3:10:10,  1.98s/it]                                                    {'loss': 3.5133, 'grad_norm': 111.80268859863281, 'learning_rate': 9.772881355932205e-06, 'epoch': 0.04}
  4%|â–         | 234/6000 [07:45<3:10:10,  1.98s/it]  4%|â–         | 235/6000 [07:47<3:09:31,  1.97s/it]                                                    {'loss': 3.5683, 'grad_norm': 304.25909423828125, 'learning_rate': 9.771186440677967e-06, 'epoch': 0.04}
  4%|â–         | 235/6000 [07:47<3:09:31,  1.97s/it]  4%|â–         | 236/6000 [07:48<3:09:41,  1.97s/it]                                                    {'loss': 3.5642, 'grad_norm': 275.7974853515625, 'learning_rate': 9.76949152542373e-06, 'epoch': 0.04}
  4%|â–         | 236/6000 [07:48<3:09:41,  1.97s/it]  4%|â–         | 237/6000 [07:50<3:09:04,  1.97s/it]                                                    {'loss': 3.565, 'grad_norm': 263.0221862792969, 'learning_rate': 9.767796610169491e-06, 'epoch': 0.04}
  4%|â–         | 237/6000 [07:50<3:09:04,  1.97s/it]  4%|â–         | 238/6000 [07:53<3:13:32,  2.02s/it]                                                    {'loss': 3.5448, 'grad_norm': 190.0968780517578, 'learning_rate': 9.766101694915255e-06, 'epoch': 0.04}
  4%|â–         | 238/6000 [07:53<3:13:32,  2.02s/it]  4%|â–         | 239/6000 [07:55<3:11:31,  1.99s/it]                                                    {'loss': 3.5239, 'grad_norm': 142.44879150390625, 'learning_rate': 9.764406779661018e-06, 'epoch': 0.04}
  4%|â–         | 239/6000 [07:55<3:11:31,  1.99s/it]  4%|â–         | 240/6000 [07:57<3:13:49,  2.02s/it]                                                    {'loss': 3.509, 'grad_norm': 72.4871597290039, 'learning_rate': 9.762711864406781e-06, 'epoch': 0.04}
  4%|â–         | 240/6000 [07:57<3:13:49,  2.02s/it]  4%|â–         | 241/6000 [07:59<3:15:06,  2.03s/it]                                                    {'loss': 3.4923, 'grad_norm': 11.461844444274902, 'learning_rate': 9.761016949152543e-06, 'epoch': 0.04}
  4%|â–         | 241/6000 [07:59<3:15:06,  2.03s/it]  4%|â–         | 242/6000 [08:01<3:11:40,  2.00s/it]                                                    {'loss': 3.4793, 'grad_norm': 14.406347274780273, 'learning_rate': 9.759322033898306e-06, 'epoch': 0.04}
  4%|â–         | 242/6000 [08:01<3:11:40,  2.00s/it]  4%|â–         | 243/6000 [08:03<3:14:57,  2.03s/it]                                                    {'loss': 3.4839, 'grad_norm': 207.49317932128906, 'learning_rate': 9.75762711864407e-06, 'epoch': 0.04}
  4%|â–         | 243/6000 [08:03<3:14:57,  2.03s/it]  4%|â–         | 244/6000 [08:05<3:17:52,  2.06s/it]                                                    {'loss': 3.5134, 'grad_norm': 323.6859130859375, 'learning_rate': 9.755932203389833e-06, 'epoch': 0.04}
  4%|â–         | 244/6000 [08:05<3:17:52,  2.06s/it]  4%|â–         | 245/6000 [08:07<3:14:41,  2.03s/it]                                                    {'loss': 3.5131, 'grad_norm': 81.4854507446289, 'learning_rate': 9.754237288135594e-06, 'epoch': 0.04}
  4%|â–         | 245/6000 [08:07<3:14:41,  2.03s/it]  4%|â–         | 246/6000 [08:09<3:13:30,  2.02s/it]                                                    {'loss': 3.4749, 'grad_norm': 16.125438690185547, 'learning_rate': 9.752542372881356e-06, 'epoch': 0.04}
  4%|â–         | 246/6000 [08:09<3:13:30,  2.02s/it]  4%|â–         | 247/6000 [08:11<3:12:14,  2.01s/it]                                                    {'loss': 3.5015, 'grad_norm': 20.19004249572754, 'learning_rate': 9.750847457627119e-06, 'epoch': 0.04}
  4%|â–         | 247/6000 [08:11<3:12:14,  2.01s/it]  4%|â–         | 248/6000 [08:13<3:14:03,  2.02s/it]                                                    {'loss': 3.4805, 'grad_norm': 8.777973175048828, 'learning_rate': 9.749152542372882e-06, 'epoch': 0.04}
  4%|â–         | 248/6000 [08:13<3:14:03,  2.02s/it]  4%|â–         | 249/6000 [08:15<3:10:20,  1.99s/it]                                                    {'loss': 3.51, 'grad_norm': 18.695518493652344, 'learning_rate': 9.747457627118646e-06, 'epoch': 0.04}
  4%|â–         | 249/6000 [08:15<3:10:20,  1.99s/it]  4%|â–         | 250/6000 [08:17<3:10:16,  1.99s/it]                                                    {'loss': 3.4854, 'grad_norm': 21.055036544799805, 'learning_rate': 9.745762711864407e-06, 'epoch': 0.04}
  4%|â–         | 250/6000 [08:17<3:10:16,  1.99s/it]  4%|â–         | 251/6000 [08:19<3:07:28,  1.96s/it]                                                    {'loss': 3.4862, 'grad_norm': 15.409140586853027, 'learning_rate': 9.74406779661017e-06, 'epoch': 0.04}
  4%|â–         | 251/6000 [08:19<3:07:28,  1.96s/it]  4%|â–         | 252/6000 [08:21<3:12:51,  2.01s/it]                                                    {'loss': 3.4814, 'grad_norm': 41.49574279785156, 'learning_rate': 9.742372881355932e-06, 'epoch': 0.04}
  4%|â–         | 252/6000 [08:21<3:12:51,  2.01s/it]  4%|â–         | 253/6000 [08:23<3:11:39,  2.00s/it]                                                    {'loss': 3.4939, 'grad_norm': 51.26778030395508, 'learning_rate': 9.740677966101695e-06, 'epoch': 0.04}
  4%|â–         | 253/6000 [08:23<3:11:39,  2.00s/it]  4%|â–         | 254/6000 [08:25<3:10:42,  1.99s/it]                                                    {'loss': 3.477, 'grad_norm': 12.964892387390137, 'learning_rate': 9.738983050847459e-06, 'epoch': 0.04}
  4%|â–         | 254/6000 [08:25<3:10:42,  1.99s/it]  4%|â–         | 255/6000 [08:27<3:11:06,  2.00s/it]                                                    {'loss': 3.5335, 'grad_norm': 12.397608757019043, 'learning_rate': 9.737288135593222e-06, 'epoch': 0.04}
  4%|â–         | 255/6000 [08:27<3:11:06,  2.00s/it]  4%|â–         | 256/6000 [08:29<3:13:06,  2.02s/it]                                                    {'loss': 3.4811, 'grad_norm': 15.074474334716797, 'learning_rate': 9.735593220338983e-06, 'epoch': 0.04}
  4%|â–         | 256/6000 [08:29<3:13:06,  2.02s/it]  4%|â–         | 257/6000 [08:31<3:13:02,  2.02s/it]                                                    {'loss': 3.4704, 'grad_norm': 13.31198787689209, 'learning_rate': 9.733898305084747e-06, 'epoch': 0.04}
  4%|â–         | 257/6000 [08:31<3:13:02,  2.02s/it]  4%|â–         | 258/6000 [08:33<3:12:58,  2.02s/it]                                                    {'loss': 3.4888, 'grad_norm': 21.522550582885742, 'learning_rate': 9.732203389830508e-06, 'epoch': 0.04}
  4%|â–         | 258/6000 [08:33<3:12:58,  2.02s/it]  4%|â–         | 259/6000 [08:35<3:09:51,  1.98s/it]                                                    {'loss': 3.5156, 'grad_norm': 75.78034973144531, 'learning_rate': 9.730508474576272e-06, 'epoch': 0.04}
  4%|â–         | 259/6000 [08:35<3:09:51,  1.98s/it]  4%|â–         | 260/6000 [08:37<3:09:00,  1.98s/it]                                                    {'loss': 3.4971, 'grad_norm': 25.29339599609375, 'learning_rate': 9.728813559322035e-06, 'epoch': 0.04}
  4%|â–         | 260/6000 [08:37<3:09:00,  1.98s/it]  4%|â–         | 261/6000 [08:39<3:10:33,  1.99s/it]                                                    {'loss': 3.4654, 'grad_norm': 22.98469352722168, 'learning_rate': 9.727118644067798e-06, 'epoch': 0.04}
  4%|â–         | 261/6000 [08:39<3:10:33,  1.99s/it]  4%|â–         | 262/6000 [08:41<3:09:36,  1.98s/it]                                                    {'loss': 3.4767, 'grad_norm': 8.195534706115723, 'learning_rate': 9.72542372881356e-06, 'epoch': 0.04}
  4%|â–         | 262/6000 [08:41<3:09:36,  1.98s/it]  4%|â–         | 263/6000 [08:43<3:11:46,  2.01s/it]                                                    {'loss': 3.4851, 'grad_norm': 7.639389991760254, 'learning_rate': 9.723728813559323e-06, 'epoch': 0.04}
  4%|â–         | 263/6000 [08:43<3:11:46,  2.01s/it]  4%|â–         | 264/6000 [08:45<3:08:07,  1.97s/it]                                                    {'loss': 3.4763, 'grad_norm': 12.161009788513184, 'learning_rate': 9.722033898305086e-06, 'epoch': 0.04}
  4%|â–         | 264/6000 [08:45<3:08:07,  1.97s/it]  4%|â–         | 265/6000 [08:47<3:09:38,  1.98s/it]                                                    {'loss': 3.4644, 'grad_norm': 7.558732509613037, 'learning_rate': 9.72033898305085e-06, 'epoch': 0.04}
  4%|â–         | 265/6000 [08:47<3:09:38,  1.98s/it]  4%|â–         | 266/6000 [08:49<3:08:55,  1.98s/it]                                                    {'loss': 3.468, 'grad_norm': 5.593337059020996, 'learning_rate': 9.718644067796611e-06, 'epoch': 0.04}
  4%|â–         | 266/6000 [08:49<3:08:55,  1.98s/it]  4%|â–         | 267/6000 [08:50<3:07:06,  1.96s/it]                                                    {'loss': 3.4803, 'grad_norm': 21.46381187438965, 'learning_rate': 9.716949152542373e-06, 'epoch': 0.04}
  4%|â–         | 267/6000 [08:50<3:07:06,  1.96s/it]  4%|â–         | 268/6000 [08:52<3:07:00,  1.96s/it]                                                    {'loss': 3.4731, 'grad_norm': 23.135408401489258, 'learning_rate': 9.715254237288136e-06, 'epoch': 0.04}
  4%|â–         | 268/6000 [08:52<3:07:00,  1.96s/it]  4%|â–         | 269/6000 [08:54<3:08:32,  1.97s/it]                                                    {'loss': 3.49, 'grad_norm': 53.052040100097656, 'learning_rate': 9.7135593220339e-06, 'epoch': 0.04}
  4%|â–         | 269/6000 [08:54<3:08:32,  1.97s/it]  4%|â–         | 270/6000 [08:56<3:08:33,  1.97s/it]                                                    {'loss': 3.4892, 'grad_norm': 23.173831939697266, 'learning_rate': 9.711864406779662e-06, 'epoch': 0.04}
  4%|â–         | 270/6000 [08:56<3:08:33,  1.97s/it]  5%|â–         | 271/6000 [08:58<3:06:52,  1.96s/it]                                                    {'loss': 3.4591, 'grad_norm': 18.773805618286133, 'learning_rate': 9.710169491525424e-06, 'epoch': 0.05}
  5%|â–         | 271/6000 [08:58<3:06:52,  1.96s/it]  5%|â–         | 272/6000 [09:00<3:08:08,  1.97s/it]                                                    {'loss': 3.4919, 'grad_norm': 10.515124320983887, 'learning_rate': 9.708474576271187e-06, 'epoch': 0.05}
  5%|â–         | 272/6000 [09:00<3:08:08,  1.97s/it]  5%|â–         | 273/6000 [09:02<3:08:00,  1.97s/it]                                                    {'loss': 3.4761, 'grad_norm': 12.574243545532227, 'learning_rate': 9.706779661016949e-06, 'epoch': 0.05}
  5%|â–         | 273/6000 [09:02<3:08:00,  1.97s/it]  5%|â–         | 274/6000 [09:04<3:09:25,  1.98s/it]                                                    {'loss': 3.4731, 'grad_norm': 25.496177673339844, 'learning_rate': 9.705084745762712e-06, 'epoch': 0.05}
  5%|â–         | 274/6000 [09:04<3:09:25,  1.98s/it]  5%|â–         | 275/6000 [09:06<3:10:41,  2.00s/it]                                                    {'loss': 3.4638, 'grad_norm': 15.245054244995117, 'learning_rate': 9.703389830508475e-06, 'epoch': 0.05}
  5%|â–         | 275/6000 [09:06<3:10:41,  2.00s/it]  5%|â–         | 276/6000 [09:08<3:09:20,  1.98s/it]                                                    {'loss': 3.4681, 'grad_norm': 14.680452346801758, 'learning_rate': 9.701694915254239e-06, 'epoch': 0.05}
  5%|â–         | 276/6000 [09:08<3:09:20,  1.98s/it]  5%|â–         | 277/6000 [09:10<3:07:55,  1.97s/it]                                                    {'loss': 3.4834, 'grad_norm': 11.717259407043457, 'learning_rate': 9.7e-06, 'epoch': 0.05}
  5%|â–         | 277/6000 [09:10<3:07:55,  1.97s/it]  5%|â–         | 278/6000 [09:12<3:05:38,  1.95s/it]                                                    {'loss': 3.4657, 'grad_norm': 9.662543296813965, 'learning_rate': 9.698305084745764e-06, 'epoch': 0.05}
  5%|â–         | 278/6000 [09:12<3:05:38,  1.95s/it]  5%|â–         | 279/6000 [09:14<3:07:14,  1.96s/it]                                                    {'loss': 3.4703, 'grad_norm': 25.827791213989258, 'learning_rate': 9.696610169491527e-06, 'epoch': 0.05}
  5%|â–         | 279/6000 [09:14<3:07:14,  1.96s/it]  5%|â–         | 280/6000 [09:16<3:07:29,  1.97s/it]                                                    {'loss': 3.4589, 'grad_norm': 16.661039352416992, 'learning_rate': 9.69491525423729e-06, 'epoch': 0.05}
  5%|â–         | 280/6000 [09:16<3:07:29,  1.97s/it]  5%|â–         | 281/6000 [09:18<3:07:27,  1.97s/it]                                                    {'loss': 3.4788, 'grad_norm': 78.46221160888672, 'learning_rate': 9.693220338983052e-06, 'epoch': 0.05}
  5%|â–         | 281/6000 [09:18<3:07:27,  1.97s/it]  5%|â–         | 282/6000 [09:20<3:10:21,  2.00s/it]                                                    {'loss': 3.485, 'grad_norm': 38.48657989501953, 'learning_rate': 9.691525423728815e-06, 'epoch': 0.05}
  5%|â–         | 282/6000 [09:20<3:10:21,  2.00s/it]  5%|â–         | 283/6000 [09:22<3:10:14,  2.00s/it]                                                    {'loss': 3.51, 'grad_norm': 245.02676391601562, 'learning_rate': 9.689830508474577e-06, 'epoch': 0.05}
  5%|â–         | 283/6000 [09:22<3:10:14,  2.00s/it]  5%|â–         | 284/6000 [09:24<3:16:03,  2.06s/it]                                                    {'loss': 3.4917, 'grad_norm': 13.974727630615234, 'learning_rate': 9.68813559322034e-06, 'epoch': 0.05}
  5%|â–         | 284/6000 [09:24<3:16:03,  2.06s/it]  5%|â–         | 285/6000 [09:26<3:15:24,  2.05s/it]                                                    {'loss': 3.4744, 'grad_norm': 17.03862953186035, 'learning_rate': 9.686440677966103e-06, 'epoch': 0.05}
  5%|â–         | 285/6000 [09:26<3:15:24,  2.05s/it]  5%|â–         | 286/6000 [09:28<3:11:50,  2.01s/it]                                                    {'loss': 3.4878, 'grad_norm': 50.20600891113281, 'learning_rate': 9.684745762711866e-06, 'epoch': 0.05}
  5%|â–         | 286/6000 [09:28<3:11:50,  2.01s/it]  5%|â–         | 287/6000 [09:30<3:11:36,  2.01s/it]                                                    {'loss': 3.4785, 'grad_norm': 30.449634552001953, 'learning_rate': 9.683050847457628e-06, 'epoch': 0.05}
  5%|â–         | 287/6000 [09:30<3:11:36,  2.01s/it]  5%|â–         | 288/6000 [09:32<3:09:10,  1.99s/it]                                                    {'loss': 3.4726, 'grad_norm': 46.95297622680664, 'learning_rate': 9.68135593220339e-06, 'epoch': 0.05}
  5%|â–         | 288/6000 [09:32<3:09:10,  1.99s/it]  5%|â–         | 289/6000 [09:34<3:10:07,  2.00s/it]                                                    {'loss': 3.4738, 'grad_norm': 10.158125877380371, 'learning_rate': 9.679661016949153e-06, 'epoch': 0.05}
  5%|â–         | 289/6000 [09:34<3:10:07,  2.00s/it]  5%|â–         | 290/6000 [09:36<3:09:51,  1.99s/it]                                                    {'loss': 3.4782, 'grad_norm': 21.833215713500977, 'learning_rate': 9.677966101694916e-06, 'epoch': 0.05}
  5%|â–         | 290/6000 [09:36<3:09:51,  1.99s/it]  5%|â–         | 291/6000 [09:38<3:06:45,  1.96s/it]                                                    {'loss': 3.4788, 'grad_norm': 13.878560066223145, 'learning_rate': 9.67627118644068e-06, 'epoch': 0.05}
  5%|â–         | 291/6000 [09:38<3:06:45,  1.96s/it]  5%|â–         | 292/6000 [09:40<3:06:27,  1.96s/it]                                                    {'loss': 3.4685, 'grad_norm': 26.950267791748047, 'learning_rate': 9.674576271186441e-06, 'epoch': 0.05}
  5%|â–         | 292/6000 [09:40<3:06:27,  1.96s/it]  5%|â–         | 293/6000 [09:42<3:07:07,  1.97s/it]                                                    {'loss': 3.4866, 'grad_norm': 9.973494529724121, 'learning_rate': 9.672881355932204e-06, 'epoch': 0.05}
  5%|â–         | 293/6000 [09:42<3:07:07,  1.97s/it]  5%|â–         | 294/6000 [09:44<3:06:32,  1.96s/it]                                                    {'loss': 3.4754, 'grad_norm': 27.219303131103516, 'learning_rate': 9.671186440677966e-06, 'epoch': 0.05}
  5%|â–         | 294/6000 [09:44<3:06:32,  1.96s/it]  5%|â–         | 295/6000 [09:46<3:05:45,  1.95s/it]                                                    {'loss': 3.4705, 'grad_norm': 5.088730812072754, 'learning_rate': 9.669491525423729e-06, 'epoch': 0.05}
  5%|â–         | 295/6000 [09:46<3:05:45,  1.95s/it]  5%|â–         | 296/6000 [09:48<3:10:00,  2.00s/it]                                                    {'loss': 3.4675, 'grad_norm': 10.090088844299316, 'learning_rate': 9.667796610169492e-06, 'epoch': 0.05}
  5%|â–         | 296/6000 [09:48<3:10:00,  2.00s/it]  5%|â–         | 297/6000 [09:50<3:08:53,  1.99s/it]                                                    {'loss': 3.4719, 'grad_norm': 9.541388511657715, 'learning_rate': 9.666101694915256e-06, 'epoch': 0.05}
  5%|â–         | 297/6000 [09:50<3:08:53,  1.99s/it]  5%|â–         | 298/6000 [09:52<3:10:36,  2.01s/it]                                                    {'loss': 3.4653, 'grad_norm': 5.107979774475098, 'learning_rate': 9.664406779661017e-06, 'epoch': 0.05}
  5%|â–         | 298/6000 [09:52<3:10:36,  2.01s/it]  5%|â–         | 299/6000 [09:54<3:20:35,  2.11s/it]                                                    {'loss': 3.4843, 'grad_norm': 7.881491661071777, 'learning_rate': 9.66271186440678e-06, 'epoch': 0.05}
  5%|â–         | 299/6000 [09:54<3:20:35,  2.11s/it]  5%|â–Œ         | 300/6000 [09:56<3:19:24,  2.10s/it]                                                    {'loss': 3.4716, 'grad_norm': 4.020023822784424, 'learning_rate': 9.661016949152544e-06, 'epoch': 0.05}
  5%|â–Œ         | 300/6000 [09:56<3:19:24,  2.10s/it]  5%|â–Œ         | 301/6000 [09:58<3:14:37,  2.05s/it]                                                    {'loss': 3.4706, 'grad_norm': 12.643733978271484, 'learning_rate': 9.659322033898307e-06, 'epoch': 0.05}
  5%|â–Œ         | 301/6000 [09:58<3:14:37,  2.05s/it]  5%|â–Œ         | 302/6000 [10:00<3:12:02,  2.02s/it]                                                    {'loss': 3.4725, 'grad_norm': 3.7309510707855225, 'learning_rate': 9.657627118644069e-06, 'epoch': 0.05}
  5%|â–Œ         | 302/6000 [10:00<3:12:02,  2.02s/it]  5%|â–Œ         | 303/6000 [10:02<3:10:04,  2.00s/it]                                                    {'loss': 3.4707, 'grad_norm': 5.240653038024902, 'learning_rate': 9.655932203389832e-06, 'epoch': 0.05}
  5%|â–Œ         | 303/6000 [10:02<3:10:04,  2.00s/it]  5%|â–Œ         | 304/6000 [10:04<3:08:32,  1.99s/it]                                                    {'loss': 3.4722, 'grad_norm': 8.024779319763184, 'learning_rate': 9.654237288135593e-06, 'epoch': 0.05}
  5%|â–Œ         | 304/6000 [10:04<3:08:32,  1.99s/it]  5%|â–Œ         | 305/6000 [10:06<3:07:14,  1.97s/it]                                                    {'loss': 3.479, 'grad_norm': 10.014092445373535, 'learning_rate': 9.652542372881357e-06, 'epoch': 0.05}
  5%|â–Œ         | 305/6000 [10:06<3:07:14,  1.97s/it]  5%|â–Œ         | 306/6000 [10:08<3:05:54,  1.96s/it]                                                    {'loss': 3.4688, 'grad_norm': 12.142754554748535, 'learning_rate': 9.65084745762712e-06, 'epoch': 0.05}
  5%|â–Œ         | 306/6000 [10:08<3:05:54,  1.96s/it]  5%|â–Œ         | 307/6000 [10:10<3:04:38,  1.95s/it]                                                    {'loss': 3.4764, 'grad_norm': 4.152864456176758, 'learning_rate': 9.649152542372883e-06, 'epoch': 0.05}
  5%|â–Œ         | 307/6000 [10:10<3:04:38,  1.95s/it]  5%|â–Œ         | 308/6000 [10:12<3:04:16,  1.94s/it]                                                    {'loss': 3.4732, 'grad_norm': 6.582392692565918, 'learning_rate': 9.647457627118645e-06, 'epoch': 0.05}
  5%|â–Œ         | 308/6000 [10:12<3:04:16,  1.94s/it]  5%|â–Œ         | 309/6000 [10:14<3:05:47,  1.96s/it]                                                    {'loss': 3.4683, 'grad_norm': 4.240499496459961, 'learning_rate': 9.645762711864406e-06, 'epoch': 0.05}
  5%|â–Œ         | 309/6000 [10:14<3:05:47,  1.96s/it]  5%|â–Œ         | 310/6000 [10:16<3:05:52,  1.96s/it]                                                    {'loss': 3.4712, 'grad_norm': 7.689084529876709, 'learning_rate': 9.64406779661017e-06, 'epoch': 0.05}
  5%|â–Œ         | 310/6000 [10:16<3:05:52,  1.96s/it]  5%|â–Œ         | 311/6000 [10:18<3:05:19,  1.95s/it]                                                    {'loss': 3.4774, 'grad_norm': 10.608698844909668, 'learning_rate': 9.642372881355933e-06, 'epoch': 0.05}
  5%|â–Œ         | 311/6000 [10:18<3:05:19,  1.95s/it]  5%|â–Œ         | 312/6000 [10:20<3:05:54,  1.96s/it]                                                    {'loss': 3.4649, 'grad_norm': 8.226899147033691, 'learning_rate': 9.640677966101696e-06, 'epoch': 0.05}
  5%|â–Œ         | 312/6000 [10:20<3:05:54,  1.96s/it]  5%|â–Œ         | 313/6000 [10:22<3:05:25,  1.96s/it]                                                    {'loss': 3.4651, 'grad_norm': 15.260965347290039, 'learning_rate': 9.638983050847458e-06, 'epoch': 0.05}
  5%|â–Œ         | 313/6000 [10:22<3:05:25,  1.96s/it]  5%|â–Œ         | 314/6000 [10:24<3:05:20,  1.96s/it]                                                    {'loss': 3.4701, 'grad_norm': 28.259462356567383, 'learning_rate': 9.637288135593221e-06, 'epoch': 0.05}
  5%|â–Œ         | 314/6000 [10:24<3:05:20,  1.96s/it]  5%|â–Œ         | 315/6000 [10:26<3:03:07,  1.93s/it]                                                    {'loss': 3.4581, 'grad_norm': 11.874409675598145, 'learning_rate': 9.635593220338983e-06, 'epoch': 0.05}
  5%|â–Œ         | 315/6000 [10:26<3:03:07,  1.93s/it]  5%|â–Œ         | 316/6000 [10:28<3:03:51,  1.94s/it]                                                    {'loss': 3.4708, 'grad_norm': 51.4010124206543, 'learning_rate': 9.633898305084746e-06, 'epoch': 0.05}
  5%|â–Œ         | 316/6000 [10:28<3:03:51,  1.94s/it]  5%|â–Œ         | 317/6000 [10:30<3:05:55,  1.96s/it]                                                    {'loss': 3.4916, 'grad_norm': 41.99425506591797, 'learning_rate': 9.63220338983051e-06, 'epoch': 0.05}
  5%|â–Œ         | 317/6000 [10:30<3:05:55,  1.96s/it]  5%|â–Œ         | 318/6000 [10:32<3:08:17,  1.99s/it]                                                    {'loss': 3.4799, 'grad_norm': 22.99384880065918, 'learning_rate': 9.630508474576272e-06, 'epoch': 0.05}
  5%|â–Œ         | 318/6000 [10:32<3:08:17,  1.99s/it]  5%|â–Œ         | 319/6000 [10:34<3:06:12,  1.97s/it]                                                    {'loss': 3.4866, 'grad_norm': 12.221710205078125, 'learning_rate': 9.628813559322034e-06, 'epoch': 0.05}
  5%|â–Œ         | 319/6000 [10:34<3:06:12,  1.97s/it]  5%|â–Œ         | 320/6000 [10:36<3:06:22,  1.97s/it]                                                    {'loss': 3.4574, 'grad_norm': 24.742630004882812, 'learning_rate': 9.627118644067797e-06, 'epoch': 0.05}
  5%|â–Œ         | 320/6000 [10:36<3:06:22,  1.97s/it]  5%|â–Œ         | 321/6000 [10:38<3:10:11,  2.01s/it]                                                    {'loss': 3.4494, 'grad_norm': 24.676326751708984, 'learning_rate': 9.62542372881356e-06, 'epoch': 0.05}
  5%|â–Œ         | 321/6000 [10:38<3:10:11,  2.01s/it]  5%|â–Œ         | 322/6000 [10:40<3:09:17,  2.00s/it]                                                    {'loss': 3.4766, 'grad_norm': 37.732261657714844, 'learning_rate': 9.623728813559324e-06, 'epoch': 0.05}
  5%|â–Œ         | 322/6000 [10:40<3:09:17,  2.00s/it]  5%|â–Œ         | 323/6000 [10:42<3:14:25,  2.05s/it]                                                    {'loss': 3.5047, 'grad_norm': 111.07691955566406, 'learning_rate': 9.622033898305085e-06, 'epoch': 0.05}
  5%|â–Œ         | 323/6000 [10:42<3:14:25,  2.05s/it]  5%|â–Œ         | 324/6000 [10:44<3:10:41,  2.02s/it]                                                    {'loss': 3.6083, 'grad_norm': 400.7398376464844, 'learning_rate': 9.620338983050849e-06, 'epoch': 0.05}
  5%|â–Œ         | 324/6000 [10:44<3:10:41,  2.02s/it]  5%|â–Œ         | 325/6000 [10:46<3:07:43,  1.98s/it]                                                    {'loss': 3.4808, 'grad_norm': 22.692455291748047, 'learning_rate': 9.61864406779661e-06, 'epoch': 0.05}
  5%|â–Œ         | 325/6000 [10:46<3:07:43,  1.98s/it]  5%|â–Œ         | 326/6000 [10:48<3:07:08,  1.98s/it]                                                    {'loss': 3.4797, 'grad_norm': 8.383370399475098, 'learning_rate': 9.616949152542374e-06, 'epoch': 0.05}
  5%|â–Œ         | 326/6000 [10:48<3:07:08,  1.98s/it]  5%|â–Œ         | 327/6000 [10:50<3:06:09,  1.97s/it]                                                    {'loss': 3.478, 'grad_norm': 6.417734622955322, 'learning_rate': 9.615254237288137e-06, 'epoch': 0.05}
  5%|â–Œ         | 327/6000 [10:50<3:06:09,  1.97s/it]  5%|â–Œ         | 328/6000 [10:52<3:08:49,  2.00s/it]                                                    {'loss': 3.4684, 'grad_norm': 5.078492641448975, 'learning_rate': 9.6135593220339e-06, 'epoch': 0.05}
  5%|â–Œ         | 328/6000 [10:52<3:08:49,  2.00s/it]  5%|â–Œ         | 329/6000 [10:54<3:12:08,  2.03s/it]                                                    {'loss': 3.4734, 'grad_norm': 15.449257850646973, 'learning_rate': 9.611864406779662e-06, 'epoch': 0.05}
  5%|â–Œ         | 329/6000 [10:54<3:12:08,  2.03s/it]  6%|â–Œ         | 330/6000 [10:56<3:10:11,  2.01s/it]                                                    {'loss': 3.4725, 'grad_norm': 10.02253246307373, 'learning_rate': 9.610169491525423e-06, 'epoch': 0.06}
  6%|â–Œ         | 330/6000 [10:56<3:10:11,  2.01s/it]  6%|â–Œ         | 331/6000 [10:58<3:08:07,  1.99s/it]                                                    {'loss': 3.4789, 'grad_norm': 5.266232967376709, 'learning_rate': 9.608474576271187e-06, 'epoch': 0.06}
  6%|â–Œ         | 331/6000 [10:58<3:08:07,  1.99s/it]  6%|â–Œ         | 332/6000 [11:00<3:05:19,  1.96s/it]                                                    {'loss': 3.4758, 'grad_norm': 13.826598167419434, 'learning_rate': 9.60677966101695e-06, 'epoch': 0.06}
  6%|â–Œ         | 332/6000 [11:00<3:05:19,  1.96s/it]  6%|â–Œ         | 333/6000 [11:01<3:04:30,  1.95s/it]                                                    {'loss': 3.4716, 'grad_norm': 9.260209083557129, 'learning_rate': 9.605084745762713e-06, 'epoch': 0.06}
  6%|â–Œ         | 333/6000 [11:01<3:04:30,  1.95s/it]  6%|â–Œ         | 334/6000 [11:03<3:05:44,  1.97s/it]                                                    {'loss': 3.4754, 'grad_norm': 5.094398498535156, 'learning_rate': 9.603389830508475e-06, 'epoch': 0.06}
  6%|â–Œ         | 334/6000 [11:03<3:05:44,  1.97s/it]  6%|â–Œ         | 335/6000 [11:06<3:07:06,  1.98s/it]                                                    {'loss': 3.489, 'grad_norm': 4.171005725860596, 'learning_rate': 9.601694915254238e-06, 'epoch': 0.06}
  6%|â–Œ         | 335/6000 [11:06<3:07:06,  1.98s/it]  6%|â–Œ         | 336/6000 [11:07<3:05:31,  1.97s/it]                                                    {'loss': 3.4755, 'grad_norm': 3.7866761684417725, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.06}
  6%|â–Œ         | 336/6000 [11:07<3:05:31,  1.97s/it]  6%|â–Œ         | 337/6000 [11:09<3:06:17,  1.97s/it]                                                    {'loss': 3.4681, 'grad_norm': 13.219164848327637, 'learning_rate': 9.598305084745765e-06, 'epoch': 0.06}
  6%|â–Œ         | 337/6000 [11:09<3:06:17,  1.97s/it]  6%|â–Œ         | 338/6000 [11:11<3:04:32,  1.96s/it]                                                    {'loss': 3.482, 'grad_norm': 51.21141815185547, 'learning_rate': 9.596610169491526e-06, 'epoch': 0.06}
  6%|â–Œ         | 338/6000 [11:11<3:04:32,  1.96s/it]  6%|â–Œ         | 339/6000 [11:13<3:04:11,  1.95s/it]                                                    {'loss': 3.4416, 'grad_norm': 130.7047882080078, 'learning_rate': 9.59491525423729e-06, 'epoch': 0.06}
  6%|â–Œ         | 339/6000 [11:13<3:04:11,  1.95s/it]  6%|â–Œ         | 340/6000 [11:15<3:06:34,  1.98s/it]                                                    {'loss': 3.4608, 'grad_norm': 112.27005004882812, 'learning_rate': 9.593220338983051e-06, 'epoch': 0.06}
  6%|â–Œ         | 340/6000 [11:15<3:06:34,  1.98s/it]  6%|â–Œ         | 341/6000 [11:17<3:04:41,  1.96s/it]                                                    {'loss': 7.3866, 'grad_norm': 6727.3427734375, 'learning_rate': 9.591525423728814e-06, 'epoch': 0.06}
  6%|â–Œ         | 341/6000 [11:17<3:04:41,  1.96s/it]  6%|â–Œ         | 342/6000 [11:19<3:03:14,  1.94s/it]                                                    {'loss': 3.937, 'grad_norm': 3640.6337890625, 'learning_rate': 9.589830508474578e-06, 'epoch': 0.06}
  6%|â–Œ         | 342/6000 [11:19<3:03:14,  1.94s/it]  6%|â–Œ         | 343/6000 [11:21<3:10:08,  2.02s/it]                                                    {'loss': 4.6759, 'grad_norm': 3928.507080078125, 'learning_rate': 9.58813559322034e-06, 'epoch': 0.06}
  6%|â–Œ         | 343/6000 [11:21<3:10:08,  2.02s/it]  6%|â–Œ         | 344/6000 [11:23<3:09:37,  2.01s/it]                                                    {'loss': 3.4929, 'grad_norm': 182.21315002441406, 'learning_rate': 9.586440677966102e-06, 'epoch': 0.06}
  6%|â–Œ         | 344/6000 [11:23<3:09:37,  2.01s/it]  6%|â–Œ         | 345/6000 [11:25<3:09:17,  2.01s/it]                                                    {'loss': 3.4578, 'grad_norm': 23.22464370727539, 'learning_rate': 9.584745762711866e-06, 'epoch': 0.06}
  6%|â–Œ         | 345/6000 [11:25<3:09:17,  2.01s/it]  6%|â–Œ         | 346/6000 [11:27<3:09:06,  2.01s/it]                                                    {'loss': 3.4684, 'grad_norm': 7.3598833084106445, 'learning_rate': 9.583050847457627e-06, 'epoch': 0.06}
  6%|â–Œ         | 346/6000 [11:27<3:09:06,  2.01s/it]  6%|â–Œ         | 347/6000 [11:29<3:05:48,  1.97s/it]                                                    {'loss': 3.4733, 'grad_norm': 16.163827896118164, 'learning_rate': 9.58135593220339e-06, 'epoch': 0.06}
  6%|â–Œ         | 347/6000 [11:29<3:05:48,  1.97s/it]  6%|â–Œ         | 348/6000 [11:31<3:05:03,  1.96s/it]                                                    {'loss': 3.4673, 'grad_norm': 7.658686637878418, 'learning_rate': 9.579661016949154e-06, 'epoch': 0.06}
  6%|â–Œ         | 348/6000 [11:31<3:05:03,  1.96s/it]  6%|â–Œ         | 349/6000 [11:33<3:05:07,  1.97s/it]                                                    {'loss': 3.4784, 'grad_norm': 8.418303489685059, 'learning_rate': 9.577966101694917e-06, 'epoch': 0.06}
  6%|â–Œ         | 349/6000 [11:33<3:05:07,  1.97s/it]  6%|â–Œ         | 350/6000 [11:35<3:05:19,  1.97s/it]                                                    {'loss': 3.4754, 'grad_norm': 7.628639221191406, 'learning_rate': 9.576271186440679e-06, 'epoch': 0.06}
  6%|â–Œ         | 350/6000 [11:35<3:05:19,  1.97s/it]  6%|â–Œ         | 351/6000 [11:37<3:07:42,  1.99s/it]                                                    {'loss': 3.4563, 'grad_norm': 9.251235961914062, 'learning_rate': 9.57457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 351/6000 [11:37<3:07:42,  1.99s/it]  6%|â–Œ         | 352/6000 [11:39<3:04:20,  1.96s/it]                                                    {'loss': 3.4706, 'grad_norm': 32.735816955566406, 'learning_rate': 9.572881355932203e-06, 'epoch': 0.06}
  6%|â–Œ         | 352/6000 [11:39<3:04:20,  1.96s/it]  6%|â–Œ         | 353/6000 [11:41<3:01:49,  1.93s/it]                                                    {'loss': 3.4832, 'grad_norm': 55.81620407104492, 'learning_rate': 9.571186440677967e-06, 'epoch': 0.06}
  6%|â–Œ         | 353/6000 [11:41<3:01:49,  1.93s/it]  6%|â–Œ         | 354/6000 [11:43<3:03:53,  1.95s/it]                                                    {'loss': 3.474, 'grad_norm': 8.801238059997559, 'learning_rate': 9.56949152542373e-06, 'epoch': 0.06}
  6%|â–Œ         | 354/6000 [11:43<3:03:53,  1.95s/it]  6%|â–Œ         | 355/6000 [11:45<3:02:37,  1.94s/it]                                                    {'loss': 3.4608, 'grad_norm': 11.857768058776855, 'learning_rate': 9.567796610169492e-06, 'epoch': 0.06}
  6%|â–Œ         | 355/6000 [11:45<3:02:37,  1.94s/it]  6%|â–Œ         | 356/6000 [11:47<3:04:43,  1.96s/it]                                                    {'loss': 3.468, 'grad_norm': 9.575262069702148, 'learning_rate': 9.566101694915255e-06, 'epoch': 0.06}
  6%|â–Œ         | 356/6000 [11:47<3:04:43,  1.96s/it]  6%|â–Œ         | 357/6000 [11:49<3:04:12,  1.96s/it]                                                    {'loss': 3.4789, 'grad_norm': 22.718698501586914, 'learning_rate': 9.564406779661018e-06, 'epoch': 0.06}
  6%|â–Œ         | 357/6000 [11:49<3:04:12,  1.96s/it]  6%|â–Œ         | 358/6000 [11:51<3:01:22,  1.93s/it]                                                    {'loss': 3.4721, 'grad_norm': 16.1453800201416, 'learning_rate': 9.562711864406781e-06, 'epoch': 0.06}
  6%|â–Œ         | 358/6000 [11:51<3:01:22,  1.93s/it]  6%|â–Œ         | 359/6000 [11:53<3:03:18,  1.95s/it]                                                    {'loss': 3.4813, 'grad_norm': 22.935901641845703, 'learning_rate': 9.561016949152543e-06, 'epoch': 0.06}
  6%|â–Œ         | 359/6000 [11:53<3:03:18,  1.95s/it]  6%|â–Œ         | 360/6000 [11:55<3:03:15,  1.95s/it]                                                    {'loss': 3.4737, 'grad_norm': 7.696578502655029, 'learning_rate': 9.559322033898306e-06, 'epoch': 0.06}
  6%|â–Œ         | 360/6000 [11:55<3:03:15,  1.95s/it]  6%|â–Œ         | 361/6000 [11:56<3:01:21,  1.93s/it]                                                    {'loss': 3.4714, 'grad_norm': 9.396515846252441, 'learning_rate': 9.557627118644068e-06, 'epoch': 0.06}
  6%|â–Œ         | 361/6000 [11:56<3:01:21,  1.93s/it]  6%|â–Œ         | 362/6000 [11:58<3:00:36,  1.92s/it]                                                    {'loss': 3.4603, 'grad_norm': 9.822465896606445, 'learning_rate': 9.555932203389831e-06, 'epoch': 0.06}
  6%|â–Œ         | 362/6000 [11:58<3:00:36,  1.92s/it]  6%|â–Œ         | 363/6000 [12:00<2:59:53,  1.91s/it]                                                    {'loss': 3.4749, 'grad_norm': 12.05787467956543, 'learning_rate': 9.554237288135594e-06, 'epoch': 0.06}
  6%|â–Œ         | 363/6000 [12:00<2:59:53,  1.91s/it]  6%|â–Œ         | 364/6000 [12:02<3:03:33,  1.95s/it]                                                    {'loss': 3.4729, 'grad_norm': 11.451279640197754, 'learning_rate': 9.552542372881358e-06, 'epoch': 0.06}
  6%|â–Œ         | 364/6000 [12:02<3:03:33,  1.95s/it]  6%|â–Œ         | 365/6000 [12:04<3:05:35,  1.98s/it]                                                    {'loss': 3.4716, 'grad_norm': 5.684235095977783, 'learning_rate': 9.55084745762712e-06, 'epoch': 0.06}
  6%|â–Œ         | 365/6000 [12:04<3:05:35,  1.98s/it]  6%|â–Œ         | 366/6000 [12:06<3:04:46,  1.97s/it]                                                    {'loss': 3.4621, 'grad_norm': 7.1098222732543945, 'learning_rate': 9.549152542372883e-06, 'epoch': 0.06}
  6%|â–Œ         | 366/6000 [12:06<3:04:46,  1.97s/it]  6%|â–Œ         | 367/6000 [12:08<3:03:57,  1.96s/it]                                                    {'loss': 3.4756, 'grad_norm': 8.12402057647705, 'learning_rate': 9.547457627118644e-06, 'epoch': 0.06}
  6%|â–Œ         | 367/6000 [12:08<3:03:57,  1.96s/it]  6%|â–Œ         | 368/6000 [12:10<3:08:41,  2.01s/it]                                                    {'loss': 3.4646, 'grad_norm': 6.996877193450928, 'learning_rate': 9.545762711864407e-06, 'epoch': 0.06}
  6%|â–Œ         | 368/6000 [12:10<3:08:41,  2.01s/it]  6%|â–Œ         | 369/6000 [12:12<3:06:35,  1.99s/it]                                                    {'loss': 3.4714, 'grad_norm': 5.071839332580566, 'learning_rate': 9.54406779661017e-06, 'epoch': 0.06}
  6%|â–Œ         | 369/6000 [12:12<3:06:35,  1.99s/it]  6%|â–Œ         | 370/6000 [12:14<3:06:53,  1.99s/it]                                                    {'loss': 3.4593, 'grad_norm': 8.946934700012207, 'learning_rate': 9.542372881355934e-06, 'epoch': 0.06}
  6%|â–Œ         | 370/6000 [12:14<3:06:53,  1.99s/it]  6%|â–Œ         | 371/6000 [12:16<3:05:29,  1.98s/it]                                                    {'loss': 3.4744, 'grad_norm': 10.460419654846191, 'learning_rate': 9.540677966101696e-06, 'epoch': 0.06}
  6%|â–Œ         | 371/6000 [12:16<3:05:29,  1.98s/it]  6%|â–Œ         | 372/6000 [12:18<3:04:01,  1.96s/it]                                                    {'loss': 3.4738, 'grad_norm': 5.275444984436035, 'learning_rate': 9.538983050847457e-06, 'epoch': 0.06}
  6%|â–Œ         | 372/6000 [12:18<3:04:01,  1.96s/it]  6%|â–Œ         | 373/6000 [12:20<3:03:16,  1.95s/it]                                                    {'loss': 3.4702, 'grad_norm': 3.1367075443267822, 'learning_rate': 9.53728813559322e-06, 'epoch': 0.06}
  6%|â–Œ         | 373/6000 [12:20<3:03:16,  1.95s/it]  6%|â–Œ         | 374/6000 [12:22<3:04:13,  1.96s/it]                                                    {'loss': 3.4718, 'grad_norm': 5.497097492218018, 'learning_rate': 9.535593220338984e-06, 'epoch': 0.06}
  6%|â–Œ         | 374/6000 [12:22<3:04:13,  1.96s/it]  6%|â–‹         | 375/6000 [12:24<3:03:09,  1.95s/it]                                                    {'loss': 3.4662, 'grad_norm': 2.4433093070983887, 'learning_rate': 9.533898305084747e-06, 'epoch': 0.06}
  6%|â–‹         | 375/6000 [12:24<3:03:09,  1.95s/it]  6%|â–‹         | 376/6000 [12:26<3:03:14,  1.96s/it]                                                    {'loss': 3.466, 'grad_norm': 8.89222240447998, 'learning_rate': 9.532203389830508e-06, 'epoch': 0.06}
  6%|â–‹         | 376/6000 [12:26<3:03:14,  1.96s/it]  6%|â–‹         | 377/6000 [12:28<3:02:41,  1.95s/it]                                                    {'loss': 3.4834, 'grad_norm': 17.314857482910156, 'learning_rate': 9.530508474576272e-06, 'epoch': 0.06}
  6%|â–‹         | 377/6000 [12:28<3:02:41,  1.95s/it]  6%|â–‹         | 378/6000 [12:30<3:02:54,  1.95s/it]                                                    {'loss': 3.4855, 'grad_norm': 7.934417724609375, 'learning_rate': 9.528813559322035e-06, 'epoch': 0.06}
  6%|â–‹         | 378/6000 [12:30<3:02:54,  1.95s/it]  6%|â–‹         | 379/6000 [12:32<3:04:37,  1.97s/it]                                                    {'loss': 3.4693, 'grad_norm': 4.13910436630249, 'learning_rate': 9.527118644067798e-06, 'epoch': 0.06}
  6%|â–‹         | 379/6000 [12:32<3:04:37,  1.97s/it]  6%|â–‹         | 380/6000 [12:34<3:06:34,  1.99s/it]                                                    {'loss': 3.4676, 'grad_norm': 3.190843105316162, 'learning_rate': 9.52542372881356e-06, 'epoch': 0.06}
  6%|â–‹         | 380/6000 [12:34<3:06:34,  1.99s/it]  6%|â–‹         | 381/6000 [12:36<3:05:26,  1.98s/it]                                                    {'loss': 3.465, 'grad_norm': 3.659925937652588, 'learning_rate': 9.523728813559323e-06, 'epoch': 0.06}
  6%|â–‹         | 381/6000 [12:36<3:05:26,  1.98s/it]  6%|â–‹         | 382/6000 [12:38<3:03:02,  1.95s/it]                                                    {'loss': 3.4678, 'grad_norm': 5.081336975097656, 'learning_rate': 9.522033898305085e-06, 'epoch': 0.06}
  6%|â–‹         | 382/6000 [12:38<3:03:02,  1.95s/it]  6%|â–‹         | 383/6000 [12:40<3:03:40,  1.96s/it]                                                    {'loss': 3.582, 'grad_norm': 5.580069065093994, 'learning_rate': 9.520338983050848e-06, 'epoch': 0.06}
  6%|â–‹         | 383/6000 [12:40<3:03:40,  1.96s/it]  6%|â–‹         | 384/6000 [12:42<3:03:35,  1.96s/it]                                                    {'loss': 3.4708, 'grad_norm': 3.9600565433502197, 'learning_rate': 9.518644067796611e-06, 'epoch': 0.06}
  6%|â–‹         | 384/6000 [12:42<3:03:35,  1.96s/it]  6%|â–‹         | 385/6000 [12:44<3:05:22,  1.98s/it]                                                    {'loss': 3.4694, 'grad_norm': 31.218931198120117, 'learning_rate': 9.516949152542375e-06, 'epoch': 0.06}
  6%|â–‹         | 385/6000 [12:44<3:05:22,  1.98s/it]  6%|â–‹         | 386/6000 [12:46<3:05:26,  1.98s/it]                                                    {'loss': 3.4699, 'grad_norm': 11.481757164001465, 'learning_rate': 9.515254237288136e-06, 'epoch': 0.06}
  6%|â–‹         | 386/6000 [12:46<3:05:26,  1.98s/it]  6%|â–‹         | 387/6000 [12:48<3:08:16,  2.01s/it]                                                    {'loss': 3.4705, 'grad_norm': 4.265483379364014, 'learning_rate': 9.5135593220339e-06, 'epoch': 0.06}
  6%|â–‹         | 387/6000 [12:48<3:08:16,  2.01s/it]  6%|â–‹         | 388/6000 [12:50<3:09:10,  2.02s/it]                                                    {'loss': 3.4662, 'grad_norm': 4.922942638397217, 'learning_rate': 9.511864406779661e-06, 'epoch': 0.06}
  6%|â–‹         | 388/6000 [12:50<3:09:10,  2.02s/it]  6%|â–‹         | 389/6000 [12:52<3:07:07,  2.00s/it]                                                    {'loss': 3.4729, 'grad_norm': 2.400700807571411, 'learning_rate': 9.510169491525424e-06, 'epoch': 0.06}
  6%|â–‹         | 389/6000 [12:52<3:07:07,  2.00s/it]  6%|â–‹         | 390/6000 [12:54<3:05:38,  1.99s/it]                                                    {'loss': 3.469, 'grad_norm': 2.465500593185425, 'learning_rate': 9.508474576271188e-06, 'epoch': 0.07}
  6%|â–‹         | 390/6000 [12:54<3:05:38,  1.99s/it]  7%|â–‹         | 391/6000 [12:56<3:02:44,  1.95s/it]                                                    {'loss': 3.4636, 'grad_norm': 2.427279233932495, 'learning_rate': 9.506779661016949e-06, 'epoch': 0.07}
  7%|â–‹         | 391/6000 [12:56<3:02:44,  1.95s/it]  7%|â–‹         | 392/6000 [12:58<3:03:31,  1.96s/it]                                                    {'loss': 3.4646, 'grad_norm': 2.6888275146484375, 'learning_rate': 9.505084745762712e-06, 'epoch': 0.07}
  7%|â–‹         | 392/6000 [12:58<3:03:31,  1.96s/it]  7%|â–‹         | 393/6000 [13:00<3:02:40,  1.95s/it]                                                    {'loss': 3.4746, 'grad_norm': 3.03094482421875, 'learning_rate': 9.503389830508476e-06, 'epoch': 0.07}
  7%|â–‹         | 393/6000 [13:00<3:02:40,  1.95s/it]  7%|â–‹         | 394/6000 [13:02<3:03:46,  1.97s/it]                                                    {'loss': 3.4705, 'grad_norm': 5.420895576477051, 'learning_rate': 9.501694915254239e-06, 'epoch': 0.07}
  7%|â–‹         | 394/6000 [13:02<3:03:46,  1.97s/it]  7%|â–‹         | 395/6000 [13:04<3:03:23,  1.96s/it]                                                    {'loss': 3.4707, 'grad_norm': 3.227973699569702, 'learning_rate': 9.5e-06, 'epoch': 0.07}
  7%|â–‹         | 395/6000 [13:04<3:03:23,  1.96s/it]  7%|â–‹         | 396/6000 [13:06<3:04:02,  1.97s/it]                                                    {'loss': 3.469, 'grad_norm': 7.697513103485107, 'learning_rate': 9.498305084745764e-06, 'epoch': 0.07}
  7%|â–‹         | 396/6000 [13:06<3:04:02,  1.97s/it]  7%|â–‹         | 397/6000 [13:08<3:07:21,  2.01s/it]                                                    {'loss': 3.4705, 'grad_norm': 7.577437877655029, 'learning_rate': 9.496610169491525e-06, 'epoch': 0.07}
  7%|â–‹         | 397/6000 [13:08<3:07:21,  2.01s/it]  7%|â–‹         | 398/6000 [13:10<3:04:45,  1.98s/it]                                                    {'loss': 3.4708, 'grad_norm': 4.405614376068115, 'learning_rate': 9.494915254237289e-06, 'epoch': 0.07}
  7%|â–‹         | 398/6000 [13:10<3:04:45,  1.98s/it]  7%|â–‹         | 399/6000 [13:11<3:03:35,  1.97s/it]                                                    {'loss': 3.4665, 'grad_norm': 2.7230396270751953, 'learning_rate': 9.493220338983052e-06, 'epoch': 0.07}
  7%|â–‹         | 399/6000 [13:11<3:03:35,  1.97s/it]  7%|â–‹         | 400/6000 [13:13<3:04:01,  1.97s/it]                                                    {'loss': 3.4697, 'grad_norm': 4.334050178527832, 'learning_rate': 9.491525423728815e-06, 'epoch': 0.07}
  7%|â–‹         | 400/6000 [13:13<3:04:01,  1.97s/it]  7%|â–‹         | 401/6000 [13:15<3:03:08,  1.96s/it]                                                    {'loss': 3.4653, 'grad_norm': 5.6926445960998535, 'learning_rate': 9.489830508474577e-06, 'epoch': 0.07}
  7%|â–‹         | 401/6000 [13:15<3:03:08,  1.96s/it]  7%|â–‹         | 402/6000 [13:17<3:01:38,  1.95s/it]                                                    {'loss': 3.471, 'grad_norm': 2.74487566947937, 'learning_rate': 9.48813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 402/6000 [13:17<3:01:38,  1.95s/it]  7%|â–‹         | 403/6000 [13:19<3:00:51,  1.94s/it]                                                    {'loss': 3.4652, 'grad_norm': 5.281644821166992, 'learning_rate': 9.486440677966102e-06, 'epoch': 0.07}
  7%|â–‹         | 403/6000 [13:19<3:00:51,  1.94s/it]  7%|â–‹         | 404/6000 [13:21<3:01:25,  1.95s/it]                                                    {'loss': 3.4634, 'grad_norm': 3.882094621658325, 'learning_rate': 9.484745762711865e-06, 'epoch': 0.07}
  7%|â–‹         | 404/6000 [13:21<3:01:25,  1.95s/it]  7%|â–‹         | 405/6000 [13:23<3:01:20,  1.94s/it]                                                    {'loss': 3.468, 'grad_norm': 5.71507453918457, 'learning_rate': 9.483050847457628e-06, 'epoch': 0.07}
  7%|â–‹         | 405/6000 [13:23<3:01:20,  1.94s/it]  7%|â–‹         | 406/6000 [13:25<3:00:51,  1.94s/it]                                                    {'loss': 3.4647, 'grad_norm': 3.803162097930908, 'learning_rate': 9.481355932203391e-06, 'epoch': 0.07}
  7%|â–‹         | 406/6000 [13:25<3:00:51,  1.94s/it]  7%|â–‹         | 407/6000 [13:27<3:00:34,  1.94s/it]                                                    {'loss': 3.4718, 'grad_norm': 4.046913146972656, 'learning_rate': 9.479661016949153e-06, 'epoch': 0.07}
  7%|â–‹         | 407/6000 [13:27<3:00:34,  1.94s/it]  7%|â–‹         | 408/6000 [13:29<3:07:27,  2.01s/it]                                                    {'loss': 3.4663, 'grad_norm': 3.962660789489746, 'learning_rate': 9.477966101694916e-06, 'epoch': 0.07}
  7%|â–‹         | 408/6000 [13:29<3:07:27,  2.01s/it]  7%|â–‹         | 409/6000 [13:31<3:06:43,  2.00s/it]                                                    {'loss': 3.4686, 'grad_norm': 5.050837516784668, 'learning_rate': 9.476271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 409/6000 [13:31<3:06:43,  2.00s/it]  7%|â–‹         | 410/6000 [13:33<3:05:49,  1.99s/it]                                                    {'loss': 3.4725, 'grad_norm': 3.8489410877227783, 'learning_rate': 9.474576271186441e-06, 'epoch': 0.07}
  7%|â–‹         | 410/6000 [13:33<3:05:49,  1.99s/it]  7%|â–‹         | 411/6000 [13:35<3:11:56,  2.06s/it]                                                    {'loss': 3.4675, 'grad_norm': 7.0334553718566895, 'learning_rate': 9.472881355932204e-06, 'epoch': 0.07}
  7%|â–‹         | 411/6000 [13:35<3:11:56,  2.06s/it]  7%|â–‹         | 412/6000 [13:37<3:07:10,  2.01s/it]                                                    {'loss': 3.4745, 'grad_norm': 10.029953002929688, 'learning_rate': 9.471186440677966e-06, 'epoch': 0.07}
  7%|â–‹         | 412/6000 [13:37<3:07:10,  2.01s/it]  7%|â–‹         | 413/6000 [13:39<3:07:45,  2.02s/it]                                                    {'loss': 3.4695, 'grad_norm': 5.68899393081665, 'learning_rate': 9.46949152542373e-06, 'epoch': 0.07}
  7%|â–‹         | 413/6000 [13:39<3:07:45,  2.02s/it]  7%|â–‹         | 414/6000 [13:41<3:08:51,  2.03s/it]                                                    {'loss': 3.4648, 'grad_norm': 2.890787363052368, 'learning_rate': 9.467796610169493e-06, 'epoch': 0.07}
  7%|â–‹         | 414/6000 [13:41<3:08:51,  2.03s/it]  7%|â–‹         | 415/6000 [13:43<3:07:31,  2.01s/it]                                                    {'loss': 3.4703, 'grad_norm': 3.1584272384643555, 'learning_rate': 9.466101694915256e-06, 'epoch': 0.07}
  7%|â–‹         | 415/6000 [13:43<3:07:31,  2.01s/it]  7%|â–‹         | 416/6000 [13:45<3:06:54,  2.01s/it]                                                    {'loss': 3.4663, 'grad_norm': 3.414405584335327, 'learning_rate': 9.464406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 416/6000 [13:45<3:06:54,  2.01s/it]  7%|â–‹         | 417/6000 [13:47<3:05:41,  2.00s/it]                                                    {'loss': 3.4685, 'grad_norm': 3.649369955062866, 'learning_rate': 9.46271186440678e-06, 'epoch': 0.07}
  7%|â–‹         | 417/6000 [13:47<3:05:41,  2.00s/it]  7%|â–‹         | 418/6000 [13:49<3:04:07,  1.98s/it]                                                    {'loss': 3.4684, 'grad_norm': 7.898382186889648, 'learning_rate': 9.461016949152542e-06, 'epoch': 0.07}
  7%|â–‹         | 418/6000 [13:49<3:04:07,  1.98s/it]  7%|â–‹         | 419/6000 [13:51<3:06:50,  2.01s/it]                                                    {'loss': 3.4692, 'grad_norm': 5.341527938842773, 'learning_rate': 9.459322033898306e-06, 'epoch': 0.07}
  7%|â–‹         | 419/6000 [13:51<3:06:50,  2.01s/it]  7%|â–‹         | 420/6000 [13:53<3:05:10,  1.99s/it]                                                    {'loss': 3.4678, 'grad_norm': 3.072059392929077, 'learning_rate': 9.457627118644069e-06, 'epoch': 0.07}
  7%|â–‹         | 420/6000 [13:53<3:05:10,  1.99s/it]  7%|â–‹         | 421/6000 [13:55<3:05:07,  1.99s/it]                                                    {'loss': 3.4677, 'grad_norm': 3.4025821685791016, 'learning_rate': 9.455932203389832e-06, 'epoch': 0.07}
  7%|â–‹         | 421/6000 [13:55<3:05:07,  1.99s/it]  7%|â–‹         | 422/6000 [13:57<3:03:37,  1.98s/it]                                                    {'loss': 3.4639, 'grad_norm': 3.200047731399536, 'learning_rate': 9.454237288135594e-06, 'epoch': 0.07}
  7%|â–‹         | 422/6000 [13:57<3:03:37,  1.98s/it]  7%|â–‹         | 423/6000 [13:59<3:04:48,  1.99s/it]                                                    {'loss': 3.4687, 'grad_norm': 5.562178611755371, 'learning_rate': 9.452542372881357e-06, 'epoch': 0.07}
  7%|â–‹         | 423/6000 [13:59<3:04:48,  1.99s/it]  7%|â–‹         | 424/6000 [14:01<3:03:50,  1.98s/it]                                                    {'loss': 3.4734, 'grad_norm': 9.566889762878418, 'learning_rate': 9.450847457627119e-06, 'epoch': 0.07}
  7%|â–‹         | 424/6000 [14:01<3:03:50,  1.98s/it]  7%|â–‹         | 425/6000 [14:03<3:02:02,  1.96s/it]                                                    {'loss': 3.4655, 'grad_norm': 6.850089073181152, 'learning_rate': 9.449152542372882e-06, 'epoch': 0.07}
  7%|â–‹         | 425/6000 [14:03<3:02:02,  1.96s/it]  7%|â–‹         | 426/6000 [14:05<3:02:31,  1.96s/it]                                                    {'loss': 3.4701, 'grad_norm': 5.591611862182617, 'learning_rate': 9.447457627118645e-06, 'epoch': 0.07}
  7%|â–‹         | 426/6000 [14:05<3:02:31,  1.96s/it]  7%|â–‹         | 427/6000 [14:07<3:04:25,  1.99s/it]                                                    {'loss': 3.464, 'grad_norm': 5.937247276306152, 'learning_rate': 9.445762711864408e-06, 'epoch': 0.07}
  7%|â–‹         | 427/6000 [14:07<3:04:25,  1.99s/it]  7%|â–‹         | 428/6000 [14:09<3:03:27,  1.98s/it]                                                    {'loss': 3.4731, 'grad_norm': 12.767379760742188, 'learning_rate': 9.44406779661017e-06, 'epoch': 0.07}
  7%|â–‹         | 428/6000 [14:09<3:03:27,  1.98s/it]  7%|â–‹         | 429/6000 [14:11<3:01:48,  1.96s/it]                                                    {'loss': 3.4916, 'grad_norm': 37.541343688964844, 'learning_rate': 9.442372881355933e-06, 'epoch': 0.07}
  7%|â–‹         | 429/6000 [14:11<3:01:48,  1.96s/it]  7%|â–‹         | 430/6000 [14:13<3:01:08,  1.95s/it]                                                    {'loss': 3.4788, 'grad_norm': 25.111515045166016, 'learning_rate': 9.440677966101696e-06, 'epoch': 0.07}
  7%|â–‹         | 430/6000 [14:13<3:01:08,  1.95s/it]  7%|â–‹         | 431/6000 [14:15<3:01:43,  1.96s/it]                                                    {'loss': 3.4694, 'grad_norm': 9.343905448913574, 'learning_rate': 9.43898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 431/6000 [14:15<3:01:43,  1.96s/it]  7%|â–‹         | 432/6000 [14:17<3:03:57,  1.98s/it]                                                    {'loss': 3.4744, 'grad_norm': 15.190013885498047, 'learning_rate': 9.437288135593221e-06, 'epoch': 0.07}
  7%|â–‹         | 432/6000 [14:17<3:03:57,  1.98s/it]  7%|â–‹         | 433/6000 [14:19<3:01:19,  1.95s/it]                                                    {'loss': 3.4685, 'grad_norm': 2.887249708175659, 'learning_rate': 9.435593220338983e-06, 'epoch': 0.07}
  7%|â–‹         | 433/6000 [14:19<3:01:19,  1.95s/it]  7%|â–‹         | 434/6000 [14:21<3:01:02,  1.95s/it]                                                    {'loss': 3.4681, 'grad_norm': 2.689603328704834, 'learning_rate': 9.433898305084746e-06, 'epoch': 0.07}
  7%|â–‹         | 434/6000 [14:21<3:01:02,  1.95s/it]  7%|â–‹         | 435/6000 [14:23<3:01:38,  1.96s/it]                                                    {'loss': 3.4715, 'grad_norm': 4.622138500213623, 'learning_rate': 9.43220338983051e-06, 'epoch': 0.07}
  7%|â–‹         | 435/6000 [14:23<3:01:38,  1.96s/it]  7%|â–‹         | 436/6000 [14:25<3:02:28,  1.97s/it]                                                    {'loss': 3.4668, 'grad_norm': 3.729811191558838, 'learning_rate': 9.430508474576273e-06, 'epoch': 0.07}
  7%|â–‹         | 436/6000 [14:25<3:02:28,  1.97s/it]  7%|â–‹         | 437/6000 [14:27<3:02:19,  1.97s/it]                                                    {'loss': 3.4675, 'grad_norm': 4.020373344421387, 'learning_rate': 9.428813559322034e-06, 'epoch': 0.07}
  7%|â–‹         | 437/6000 [14:27<3:02:19,  1.97s/it]  7%|â–‹         | 438/6000 [14:29<3:04:33,  1.99s/it]                                                    {'loss': 3.4641, 'grad_norm': 2.5755159854888916, 'learning_rate': 9.427118644067798e-06, 'epoch': 0.07}
  7%|â–‹         | 438/6000 [14:29<3:04:33,  1.99s/it]  7%|â–‹         | 439/6000 [14:31<3:03:37,  1.98s/it]                                                    {'loss': 3.4754, 'grad_norm': 27.1771297454834, 'learning_rate': 9.425423728813559e-06, 'epoch': 0.07}
  7%|â–‹         | 439/6000 [14:31<3:03:37,  1.98s/it]  7%|â–‹         | 440/6000 [14:33<3:04:59,  2.00s/it]                                                    {'loss': 3.4645, 'grad_norm': 2.6141679286956787, 'learning_rate': 9.423728813559322e-06, 'epoch': 0.07}
  7%|â–‹         | 440/6000 [14:33<3:04:59,  2.00s/it]  7%|â–‹         | 441/6000 [14:35<3:07:06,  2.02s/it]                                                    {'loss': 3.4721, 'grad_norm': 3.1963722705841064, 'learning_rate': 9.422033898305086e-06, 'epoch': 0.07}
  7%|â–‹         | 441/6000 [14:35<3:07:06,  2.02s/it]  7%|â–‹         | 442/6000 [14:37<3:05:39,  2.00s/it]                                                    {'loss': 3.4648, 'grad_norm': 9.503445625305176, 'learning_rate': 9.420338983050849e-06, 'epoch': 0.07}
  7%|â–‹         | 442/6000 [14:37<3:05:39,  2.00s/it]  7%|â–‹         | 443/6000 [14:39<3:03:08,  1.98s/it]                                                    {'loss': 3.468, 'grad_norm': 2.9355533123016357, 'learning_rate': 9.41864406779661e-06, 'epoch': 0.07}
  7%|â–‹         | 443/6000 [14:39<3:03:08,  1.98s/it]  7%|â–‹         | 444/6000 [14:41<3:02:15,  1.97s/it]                                                    {'loss': 3.4655, 'grad_norm': 2.650182008743286, 'learning_rate': 9.416949152542374e-06, 'epoch': 0.07}
  7%|â–‹         | 444/6000 [14:41<3:02:15,  1.97s/it]  7%|â–‹         | 445/6000 [14:43<3:01:44,  1.96s/it]                                                    {'loss': 3.4678, 'grad_norm': 7.124088764190674, 'learning_rate': 9.415254237288135e-06, 'epoch': 0.07}
  7%|â–‹         | 445/6000 [14:43<3:01:44,  1.96s/it]  7%|â–‹         | 446/6000 [14:44<3:01:13,  1.96s/it]                                                    {'loss': 3.4666, 'grad_norm': 6.245575428009033, 'learning_rate': 9.413559322033899e-06, 'epoch': 0.07}
  7%|â–‹         | 446/6000 [14:44<3:01:13,  1.96s/it]  7%|â–‹         | 447/6000 [14:46<3:02:12,  1.97s/it]                                                    {'loss': 3.4812, 'grad_norm': 30.69131088256836, 'learning_rate': 9.411864406779662e-06, 'epoch': 0.07}
  7%|â–‹         | 447/6000 [14:46<3:02:12,  1.97s/it]  7%|â–‹         | 448/6000 [14:48<3:00:50,  1.95s/it]                                                    {'loss': 3.4687, 'grad_norm': 9.504861831665039, 'learning_rate': 9.410169491525425e-06, 'epoch': 0.07}
  7%|â–‹         | 448/6000 [14:48<3:00:50,  1.95s/it]  7%|â–‹         | 449/6000 [14:50<3:01:02,  1.96s/it]                                                    {'loss': 3.4752, 'grad_norm': 10.343339920043945, 'learning_rate': 9.408474576271187e-06, 'epoch': 0.07}
  7%|â–‹         | 449/6000 [14:50<3:01:02,  1.96s/it]  8%|â–Š         | 450/6000 [14:52<3:01:19,  1.96s/it]                                                    {'loss': 3.4717, 'grad_norm': 13.539627075195312, 'learning_rate': 9.40677966101695e-06, 'epoch': 0.07}
  8%|â–Š         | 450/6000 [14:52<3:01:19,  1.96s/it]  8%|â–Š         | 451/6000 [14:54<3:00:53,  1.96s/it]                                                    {'loss': 3.4625, 'grad_norm': 5.082148551940918, 'learning_rate': 9.405084745762713e-06, 'epoch': 0.08}
  8%|â–Š         | 451/6000 [14:54<3:00:53,  1.96s/it]  8%|â–Š         | 452/6000 [14:56<3:03:07,  1.98s/it]                                                    {'loss': 3.4714, 'grad_norm': 4.946483612060547, 'learning_rate': 9.403389830508477e-06, 'epoch': 0.08}
  8%|â–Š         | 452/6000 [14:56<3:03:07,  1.98s/it]  8%|â–Š         | 453/6000 [14:58<3:01:42,  1.97s/it]                                                    {'loss': 3.469, 'grad_norm': 5.827859401702881, 'learning_rate': 9.401694915254238e-06, 'epoch': 0.08}
  8%|â–Š         | 453/6000 [14:58<3:01:42,  1.97s/it]  8%|â–Š         | 454/6000 [15:00<3:01:29,  1.96s/it]                                                    {'loss': 3.4671, 'grad_norm': 9.775155067443848, 'learning_rate': 9.4e-06, 'epoch': 0.08}
  8%|â–Š         | 454/6000 [15:00<3:01:29,  1.96s/it]  8%|â–Š         | 455/6000 [15:02<2:59:59,  1.95s/it]                                                    {'loss': 3.4708, 'grad_norm': 5.305819511413574, 'learning_rate': 9.398305084745763e-06, 'epoch': 0.08}
  8%|â–Š         | 455/6000 [15:02<2:59:59,  1.95s/it]  8%|â–Š         | 456/6000 [15:04<2:58:58,  1.94s/it]                                                    {'loss': 3.4693, 'grad_norm': 2.3759970664978027, 'learning_rate': 9.396610169491526e-06, 'epoch': 0.08}
  8%|â–Š         | 456/6000 [15:04<2:58:58,  1.94s/it]  8%|â–Š         | 457/6000 [15:06<2:57:42,  1.92s/it]                                                    {'loss': 3.4736, 'grad_norm': 6.220673561096191, 'learning_rate': 9.39491525423729e-06, 'epoch': 0.08}
  8%|â–Š         | 457/6000 [15:06<2:57:42,  1.92s/it]  8%|â–Š         | 458/6000 [15:08<2:58:13,  1.93s/it]                                                    {'loss': 3.4675, 'grad_norm': 8.112197875976562, 'learning_rate': 9.393220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 458/6000 [15:08<2:58:13,  1.93s/it]  8%|â–Š         | 459/6000 [15:10<2:58:23,  1.93s/it]                                                    {'loss': 3.4694, 'grad_norm': 4.229415416717529, 'learning_rate': 9.391525423728814e-06, 'epoch': 0.08}
  8%|â–Š         | 459/6000 [15:10<2:58:23,  1.93s/it]  8%|â–Š         | 460/6000 [15:12<2:59:23,  1.94s/it]                                                    {'loss': 3.4655, 'grad_norm': 3.189544200897217, 'learning_rate': 9.389830508474576e-06, 'epoch': 0.08}
  8%|â–Š         | 460/6000 [15:12<2:59:23,  1.94s/it]  8%|â–Š         | 461/6000 [15:14<3:00:59,  1.96s/it]                                                    {'loss': 3.4652, 'grad_norm': 2.0124318599700928, 'learning_rate': 9.38813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 461/6000 [15:14<3:00:59,  1.96s/it]  8%|â–Š         | 462/6000 [15:16<2:59:18,  1.94s/it]                                                    {'loss': 3.4672, 'grad_norm': 5.532235145568848, 'learning_rate': 9.386440677966103e-06, 'epoch': 0.08}
  8%|â–Š         | 462/6000 [15:16<2:59:18,  1.94s/it]  8%|â–Š         | 463/6000 [15:18<2:59:39,  1.95s/it]                                                    {'loss': 3.4657, 'grad_norm': 3.100480318069458, 'learning_rate': 9.384745762711866e-06, 'epoch': 0.08}
  8%|â–Š         | 463/6000 [15:18<2:59:39,  1.95s/it]  8%|â–Š         | 464/6000 [15:20<2:58:44,  1.94s/it]                                                    {'loss': 3.4714, 'grad_norm': 4.479148864746094, 'learning_rate': 9.383050847457627e-06, 'epoch': 0.08}
  8%|â–Š         | 464/6000 [15:20<2:58:44,  1.94s/it]  8%|â–Š         | 465/6000 [15:22<3:00:14,  1.95s/it]                                                    {'loss': 3.466, 'grad_norm': 3.911384344100952, 'learning_rate': 9.38135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 465/6000 [15:22<3:00:14,  1.95s/it]  8%|â–Š         | 466/6000 [15:23<3:00:16,  1.95s/it]                                                    {'loss': 3.4678, 'grad_norm': 2.5496723651885986, 'learning_rate': 9.379661016949152e-06, 'epoch': 0.08}
  8%|â–Š         | 466/6000 [15:23<3:00:16,  1.95s/it]  8%|â–Š         | 467/6000 [15:25<3:01:14,  1.97s/it]                                                    {'loss': 3.4675, 'grad_norm': 2.1492104530334473, 'learning_rate': 9.377966101694916e-06, 'epoch': 0.08}
  8%|â–Š         | 467/6000 [15:25<3:01:14,  1.97s/it]  8%|â–Š         | 468/6000 [15:27<3:02:40,  1.98s/it]                                                    {'loss': 3.467, 'grad_norm': 1.5803650617599487, 'learning_rate': 9.376271186440679e-06, 'epoch': 0.08}
  8%|â–Š         | 468/6000 [15:27<3:02:40,  1.98s/it]  8%|â–Š         | 469/6000 [15:29<3:01:31,  1.97s/it]                                                    {'loss': 3.4798, 'grad_norm': 3.8726186752319336, 'learning_rate': 9.374576271186442e-06, 'epoch': 0.08}
  8%|â–Š         | 469/6000 [15:29<3:01:31,  1.97s/it]  8%|â–Š         | 470/6000 [15:31<3:02:36,  1.98s/it]                                                    {'loss': 3.4704, 'grad_norm': 4.607702255249023, 'learning_rate': 9.372881355932204e-06, 'epoch': 0.08}
  8%|â–Š         | 470/6000 [15:31<3:02:36,  1.98s/it]  8%|â–Š         | 471/6000 [15:33<3:00:39,  1.96s/it]                                                    {'loss': 3.4699, 'grad_norm': 2.186973810195923, 'learning_rate': 9.371186440677967e-06, 'epoch': 0.08}
  8%|â–Š         | 471/6000 [15:33<3:00:39,  1.96s/it]  8%|â–Š         | 472/6000 [15:35<2:59:09,  1.94s/it]                                                    {'loss': 3.4703, 'grad_norm': 5.485238552093506, 'learning_rate': 9.36949152542373e-06, 'epoch': 0.08}
  8%|â–Š         | 472/6000 [15:35<2:59:09,  1.94s/it]  8%|â–Š         | 473/6000 [15:37<3:00:38,  1.96s/it]                                                    {'loss': 3.4985, 'grad_norm': 2.2482481002807617, 'learning_rate': 9.367796610169494e-06, 'epoch': 0.08}
  8%|â–Š         | 473/6000 [15:37<3:00:38,  1.96s/it]  8%|â–Š         | 474/6000 [15:39<2:59:06,  1.94s/it]                                                    {'loss': 3.4678, 'grad_norm': 3.5980117321014404, 'learning_rate': 9.366101694915255e-06, 'epoch': 0.08}
  8%|â–Š         | 474/6000 [15:39<2:59:06,  1.94s/it]  8%|â–Š         | 475/6000 [15:41<2:58:50,  1.94s/it]                                                    {'loss': 3.4663, 'grad_norm': 1.659732699394226, 'learning_rate': 9.364406779661017e-06, 'epoch': 0.08}
  8%|â–Š         | 475/6000 [15:41<2:58:50,  1.94s/it]  8%|â–Š         | 476/6000 [15:43<2:58:30,  1.94s/it]                                                    {'loss': 3.4712, 'grad_norm': 2.562204599380493, 'learning_rate': 9.36271186440678e-06, 'epoch': 0.08}
  8%|â–Š         | 476/6000 [15:43<2:58:30,  1.94s/it]  8%|â–Š         | 477/6000 [15:45<2:57:42,  1.93s/it]                                                    {'loss': 3.4679, 'grad_norm': 25.487838745117188, 'learning_rate': 9.361016949152543e-06, 'epoch': 0.08}
  8%|â–Š         | 477/6000 [15:45<2:57:42,  1.93s/it]  8%|â–Š         | 478/6000 [15:47<2:58:13,  1.94s/it]                                                    {'loss': 3.4892, 'grad_norm': 167.0474853515625, 'learning_rate': 9.359322033898306e-06, 'epoch': 0.08}
  8%|â–Š         | 478/6000 [15:47<2:58:13,  1.94s/it]  8%|â–Š         | 479/6000 [15:49<2:57:38,  1.93s/it]                                                    {'loss': 3.4661, 'grad_norm': 27.940221786499023, 'learning_rate': 9.357627118644068e-06, 'epoch': 0.08}
  8%|â–Š         | 479/6000 [15:49<2:57:38,  1.93s/it]  8%|â–Š         | 480/6000 [15:51<2:57:14,  1.93s/it]                                                    {'loss': 3.4755, 'grad_norm': 53.15666961669922, 'learning_rate': 9.355932203389831e-06, 'epoch': 0.08}
  8%|â–Š         | 480/6000 [15:51<2:57:14,  1.93s/it]  8%|â–Š         | 481/6000 [15:53<2:58:23,  1.94s/it]                                                    {'loss': 3.4688, 'grad_norm': 5.748076915740967, 'learning_rate': 9.354237288135593e-06, 'epoch': 0.08}
  8%|â–Š         | 481/6000 [15:53<2:58:23,  1.94s/it]  8%|â–Š         | 482/6000 [15:55<2:57:31,  1.93s/it]                                                    {'loss': 3.4636, 'grad_norm': 4.244926929473877, 'learning_rate': 9.352542372881356e-06, 'epoch': 0.08}
  8%|â–Š         | 482/6000 [15:55<2:57:31,  1.93s/it]  8%|â–Š         | 483/6000 [15:57<2:58:20,  1.94s/it]                                                    {'loss': 3.4696, 'grad_norm': 2.9019129276275635, 'learning_rate': 9.35084745762712e-06, 'epoch': 0.08}
  8%|â–Š         | 483/6000 [15:57<2:58:20,  1.94s/it]  8%|â–Š         | 484/6000 [15:59<3:02:02,  1.98s/it]                                                    {'loss': 3.4663, 'grad_norm': 3.7442901134490967, 'learning_rate': 9.349152542372883e-06, 'epoch': 0.08}
  8%|â–Š         | 484/6000 [15:59<3:02:02,  1.98s/it]  8%|â–Š         | 485/6000 [16:01<3:05:14,  2.02s/it]                                                    {'loss': 3.47, 'grad_norm': 3.5051517486572266, 'learning_rate': 9.347457627118644e-06, 'epoch': 0.08}
  8%|â–Š         | 485/6000 [16:01<3:05:14,  2.02s/it]  8%|â–Š         | 486/6000 [16:03<3:02:38,  1.99s/it]                                                    {'loss': 3.4686, 'grad_norm': 3.7440128326416016, 'learning_rate': 9.345762711864408e-06, 'epoch': 0.08}
  8%|â–Š         | 486/6000 [16:03<3:02:38,  1.99s/it]  8%|â–Š         | 487/6000 [16:05<3:03:02,  1.99s/it]                                                    {'loss': 3.467, 'grad_norm': 2.4437642097473145, 'learning_rate': 9.344067796610171e-06, 'epoch': 0.08}
  8%|â–Š         | 487/6000 [16:05<3:03:02,  1.99s/it]  8%|â–Š         | 488/6000 [16:07<3:03:16,  2.00s/it]                                                    {'loss': 3.4612, 'grad_norm': 3.377023935317993, 'learning_rate': 9.342372881355934e-06, 'epoch': 0.08}
  8%|â–Š         | 488/6000 [16:07<3:03:16,  2.00s/it]  8%|â–Š         | 489/6000 [16:09<3:03:29,  2.00s/it]                                                    {'loss': 3.4654, 'grad_norm': 2.184434413909912, 'learning_rate': 9.340677966101696e-06, 'epoch': 0.08}
  8%|â–Š         | 489/6000 [16:09<3:03:29,  2.00s/it]  8%|â–Š         | 490/6000 [16:11<3:06:05,  2.03s/it]                                                    {'loss': 3.4728, 'grad_norm': 4.268759727478027, 'learning_rate': 9.338983050847459e-06, 'epoch': 0.08}
  8%|â–Š         | 490/6000 [16:11<3:06:05,  2.03s/it]  8%|â–Š         | 491/6000 [16:13<3:05:23,  2.02s/it]                                                    {'loss': 3.467, 'grad_norm': 4.402041435241699, 'learning_rate': 9.33728813559322e-06, 'epoch': 0.08}
  8%|â–Š         | 491/6000 [16:13<3:05:23,  2.02s/it]  8%|â–Š         | 492/6000 [16:15<3:02:45,  1.99s/it]                                                    {'loss': 3.4712, 'grad_norm': 5.569386005401611, 'learning_rate': 9.335593220338984e-06, 'epoch': 0.08}
  8%|â–Š         | 492/6000 [16:15<3:02:45,  1.99s/it]  8%|â–Š         | 493/6000 [16:17<3:02:01,  1.98s/it]                                                    {'loss': 3.4685, 'grad_norm': 3.0570640563964844, 'learning_rate': 9.333898305084747e-06, 'epoch': 0.08}
  8%|â–Š         | 493/6000 [16:17<3:02:01,  1.98s/it]  8%|â–Š         | 494/6000 [16:19<3:01:30,  1.98s/it]                                                    {'loss': 3.4629, 'grad_norm': 4.27972412109375, 'learning_rate': 9.33220338983051e-06, 'epoch': 0.08}
  8%|â–Š         | 494/6000 [16:19<3:01:30,  1.98s/it]  8%|â–Š         | 495/6000 [16:21<3:02:04,  1.98s/it]                                                    {'loss': 3.4643, 'grad_norm': 2.8546359539031982, 'learning_rate': 9.330508474576272e-06, 'epoch': 0.08}
  8%|â–Š         | 495/6000 [16:21<3:02:04,  1.98s/it]  8%|â–Š         | 496/6000 [16:23<3:00:23,  1.97s/it]                                                    {'loss': 3.4643, 'grad_norm': 3.6984565258026123, 'learning_rate': 9.328813559322034e-06, 'epoch': 0.08}
  8%|â–Š         | 496/6000 [16:23<3:00:23,  1.97s/it]  8%|â–Š         | 497/6000 [16:24<2:57:45,  1.94s/it]                                                    {'loss': 3.4671, 'grad_norm': 3.7084431648254395, 'learning_rate': 9.327118644067797e-06, 'epoch': 0.08}
  8%|â–Š         | 497/6000 [16:24<2:57:45,  1.94s/it]  8%|â–Š         | 498/6000 [16:26<2:58:38,  1.95s/it]                                                    {'loss': 3.4758, 'grad_norm': 35.27696990966797, 'learning_rate': 9.32542372881356e-06, 'epoch': 0.08}
  8%|â–Š         | 498/6000 [16:26<2:58:38,  1.95s/it]  8%|â–Š         | 499/6000 [16:28<3:02:01,  1.99s/it]                                                    {'loss': 3.4682, 'grad_norm': 2.5982930660247803, 'learning_rate': 9.323728813559323e-06, 'epoch': 0.08}
  8%|â–Š         | 499/6000 [16:28<3:02:01,  1.99s/it]  8%|â–Š         | 500/6000 [16:30<3:01:56,  1.98s/it]                                                    {'loss': 3.467, 'grad_norm': 8.060447692871094, 'learning_rate': 9.322033898305085e-06, 'epoch': 0.08}
  8%|â–Š         | 500/6000 [16:30<3:01:56,  1.98s/it][2025-11-17 11:07:19,877] INFO [src.utils:19] Saving model to /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/13Nov_AddTail-Qwen/Qwen2-VL-2B-Instruct/checkpoint-500
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.
  8%|â–Š         | 501/6000 [16:34<3:44:06,  2.45s/it]                                                    {'loss': 3.4741, 'grad_norm': 9.695562362670898, 'learning_rate': 9.320338983050848e-06, 'epoch': 0.08}
  8%|â–Š         | 501/6000 [16:34<3:44:06,  2.45s/it]  8%|â–Š         | 502/6000 [16:36<3:31:33,  2.31s/it]                                                    {'loss': 3.4673, 'grad_norm': 5.728082180023193, 'learning_rate': 9.31864406779661e-06, 'epoch': 0.08}
  8%|â–Š         | 502/6000 [16:36<3:31:33,  2.31s/it]  8%|â–Š         | 503/6000 [16:38<3:21:37,  2.20s/it]                                                    {'loss': 3.4625, 'grad_norm': 3.1326193809509277, 'learning_rate': 9.316949152542373e-06, 'epoch': 0.08}
  8%|â–Š         | 503/6000 [16:38<3:21:37,  2.20s/it]  8%|â–Š         | 504/6000 [16:40<3:13:25,  2.11s/it]                                                    {'loss': 3.47, 'grad_norm': 4.985692501068115, 'learning_rate': 9.315254237288136e-06, 'epoch': 0.08}
  8%|â–Š         | 504/6000 [16:40<3:13:25,  2.11s/it]  8%|â–Š         | 505/6000 [16:42<3:08:58,  2.06s/it]                                                    {'loss': 3.466, 'grad_norm': 6.621530532836914, 'learning_rate': 9.3135593220339e-06, 'epoch': 0.08}
  8%|â–Š         | 505/6000 [16:42<3:08:58,  2.06s/it]  8%|â–Š         | 506/6000 [16:44<3:08:46,  2.06s/it]                                                    {'loss': 3.4736, 'grad_norm': 12.474079132080078, 'learning_rate': 9.311864406779661e-06, 'epoch': 0.08}
  8%|â–Š         | 506/6000 [16:44<3:08:46,  2.06s/it]  8%|â–Š         | 507/6000 [16:46<3:05:30,  2.03s/it]                                                    {'loss': 3.4725, 'grad_norm': 8.215097427368164, 'learning_rate': 9.310169491525424e-06, 'epoch': 0.08}
  8%|â–Š         | 507/6000 [16:46<3:05:30,  2.03s/it]  8%|â–Š         | 508/6000 [16:48<3:01:21,  1.98s/it]                                                    {'loss': 3.4922, 'grad_norm': 31.5606746673584, 'learning_rate': 9.308474576271188e-06, 'epoch': 0.08}
  8%|â–Š         | 508/6000 [16:48<3:01:21,  1.98s/it]  8%|â–Š         | 509/6000 [16:50<3:00:30,  1.97s/it]                                                    {'loss': 3.481, 'grad_norm': 12.3263578414917, 'learning_rate': 9.306779661016951e-06, 'epoch': 0.08}
  8%|â–Š         | 509/6000 [16:50<3:00:30,  1.97s/it]  8%|â–Š         | 510/6000 [16:52<3:01:35,  1.98s/it]                                                    {'loss': 3.4705, 'grad_norm': 5.4313130378723145, 'learning_rate': 9.305084745762713e-06, 'epoch': 0.09}
  8%|â–Š         | 510/6000 [16:52<3:01:35,  1.98s/it]  9%|â–Š         | 511/6000 [16:54<3:01:13,  1.98s/it]                                                    {'loss': 3.4734, 'grad_norm': 4.718049049377441, 'learning_rate': 9.303389830508476e-06, 'epoch': 0.09}
  9%|â–Š         | 511/6000 [16:54<3:01:13,  1.98s/it]  9%|â–Š         | 512/6000 [16:55<2:59:24,  1.96s/it]                                                    {'loss': 3.4667, 'grad_norm': 2.332035541534424, 'learning_rate': 9.301694915254237e-06, 'epoch': 0.09}
  9%|â–Š         | 512/6000 [16:55<2:59:24,  1.96s/it]