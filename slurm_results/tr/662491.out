==> Environment
Python: /home/infres/zzhu-24/anaconda3/envs/vlm2vec/bin/python
Version: Python 3.10.18

/home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct
CUDA_VISIBLE_DEVICES=0,1 torchrun --nproc_per_node=2 --master_port=2207 --max_restarts=0 train.py --lora --lora_r 16 --model_name Qwen/Qwen2-VL-2B-Instruct --bf16 --pooling eos --normalize True --temperature 0.02 --dataloader_num_workers 1 --dataset_config /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/train/retrieval.yaml --data_basedir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/data/vlm2vec_train/MMEB-train --run_name 18Nov_1-Qwen/Qwen2-VL-2B-Instruct --output_dir /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct --grad_cache True --per_device_train_batch_size 16 --gc_q_chunk_size 8 --gc_p_chunk_size 8 --interleave_batch_size 0 --lr_scheduler_type linear --learning_rate 1e-5 --max_steps 6000 --warmup_steps 100 --save_steps 500 --logging_steps 1 --save_safetensors True --remove_unused_columns False --resume_from auto --report_to wandb 2>&1 | tee /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/train.log
W1118 09:12:17.841000 131888771508032 torch/distributed/run.py:779] 
W1118 09:12:17.841000 131888771508032 torch/distributed/run.py:779] *****************************************
W1118 09:12:17.841000 131888771508032 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1118 09:12:17.841000 131888771508032 torch/distributed/run.py:779] *****************************************
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).
  warnings.warn(
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
/home/infres/zzhu-24/anaconda3/envs/vlm2vec/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
DropoutAddRMSNorm of flash_attn is not installed!!!
DropoutAddRMSNorm of flash_attn is not installed!!!
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
/home/infres/zzhu-24/PRIM/VLM2Vec/src/model/baseline_backbone/internvideo2/modeling_internvideo2.py:539: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  @torch.cuda.amp.autocast(enabled=False)
Distributed init debug info:
RANK: 0
LOCAL_RANK: 0
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 0
torch.distributed.get_world_size(): 2
[2025-11-18 09:12:25,393] INFO [src.utils:10] rank0: init wandb
Distributed init debug info:
RANK: 1
LOCAL_RANK: 1
WORLD_SIZE: 2
MASTER_ADDR: 127.0.0.1
MASTER_PORT: 2207
torch.distributed.is_initialized: True
torch.distributed.get_rank(): 1
torch.distributed.get_world_size(): 2
wandb: Currently logged in as: zhuzhehua16 (zhuzhehua16-t-l-com-paris) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.93it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 11.38it/s]
wandb: setting up run 5j0ewh33
wandb: Tracking run with wandb version 0.22.1
wandb: Run data is saved locally in /home/infres/zzhu-24/PRIM/VLM2Vec/experiments/public/exps/train/18Nov_1-Qwen/Qwen2-VL-2B-Instruct/wandb/run-20251118_091225-5j0ewh33
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 18Nov_1-Qwen/Qwen2-VL-2B-Instruct
wandb: â­ï¸ View project at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train
wandb: ðŸš€ View run at https://wandb.ai/zhuzhehua16-t-l-com-paris/vlm2vec_train/runs/5j0ewh33
[2025-11-18 09:12:27,021] INFO [src.utils:19] Loading backbone [qwen2_vl] from Qwen/Qwen2-VL-2B-Instruct
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  5.74it/s]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 10.99it/s]
[2025-11-18 09:12:27,550] INFO [src.utils:19] Loading lora adapter from Qwen2VLForConditionalGeneration(
  (visual): Qwen2VisionTransformerPretrainedModel(
    (patch_embed): PatchEmbed(
      (proj): Conv3d(3, 1280, kernel_size=(2, 14, 14), stride=(2, 14, 14), bias=False)
    )
    (rotary_pos_emb): VisionRotaryEmbedding()
    (blocks): ModuleList(
      (0-31): 32 x Qwen2VLVisionBlock(
        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
        (attn): VisionFlashAttention2(
          (qkv): Linear(in_features=1280, out_features=3840, bias=True)
          (proj): Linear(in_features=1280, out_features=1280, bias=True)
        )
        (mlp): VisionMlp(
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (act): QuickGELUActivation()
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
        )
      )
    )
    (merger): PatchMerger(
      (ln_q): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)
      (mlp): Sequential(
        (0): Linear(in_features=5120, out_features=5120, bias=True)
        (1): GELU(approximate='none')
        (2): Linear(in_features=5120, out_features=1536, bias=True)
      )
    )
  )
  (model): Qwen2VLModel(
    (embed_tokens): Embedding(151936, 1536)
    (layers): ModuleList(
      (0-27): 28 x Qwen2VLDecoderLayer(
        (self_attn): Qwen2VLFlashAttention2(
          (q_proj): Linear(in_features=1536, out_features=1536, bias=True)
          (k_proj): Linear(in_features=1536, out_features=256, bias=True)
          (v_proj): Linear(in_features=1536, out_features=256, bias=True)
          (o_proj): Linear(in_features=1536, out_features=1536, bias=False)
          (rotary_emb): Qwen2VLRotaryEmbedding()
        )
        (mlp): Qwen2MLP(
          (gate_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (up_proj): Linear(in_features=1536, out_features=8960, bias=False)
          (down_proj): Linear(in_features=8960, out_features=1536, bias=False)
          (act_fn): SiLU()
        )
        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)
      )
    )
    (norm): Qwen2RMSNorm((1536,), eps=1e-06)
    (rotary_emb): Qwen2VLRotaryEmbedding()
  )
  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)
)
[2025-11-18 09:12:33,721] INFO [src.utils:10] rank1: model_backbone: qwen2_vl
[2025-11-18 09:12:35,218] INFO [src.utils:10] rank0: model_backbone: qwen2_vl
[2025-11-18 09:12:35,219] INFO [src.utils:19] Loading processor from: Qwen/Qwen2-VL-2B-Instruct
[2025-11-18 09:12:38,683] INFO [src.utils:19] Loaded mmeb/MSCOCO_t2i dataset with 100000 samples
[2025-11-18 09:12:38,684] INFO [src.utils:19] 		Dataset#0 (dataset_parser=mmeb): MSCOCO_t2i, num_rows=100000, prob=50.0
[2025-11-18 09:12:39,702] INFO [src.utils:19] Loaded mmeb/MSCOCO_i2t dataset with 113287 samples
[2025-11-18 09:12:39,703] INFO [src.utils:19] 		Dataset#1 (dataset_parser=mmeb): MSCOCO_i2t, num_rows=113287, prob=50.0
[2025-11-18 09:12:39,703] INFO [src.utils:19] 
Initializing interleave datasets:
		world_size=2
		total num rows=213287
		global batch size=32
		estimated num step per epoch=6665.21875
		interleave_batch_size=0.0
[2025-11-18 09:12:39,704] INFO [src.utils:19] ==================================================
[2025-11-18 09:12:39,704] INFO [src.utils:19] Print the features of each dataset, make sure that all datasets have valid features.
[2025-11-18 09:12:39,704] INFO [src.utils:19] 		Dataset 0 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:12:39,705] INFO [src.utils:19] 		Dataset 1 features: ['query_text', 'query_image', 'pos_text', 'pos_image', 'neg_text', 'neg_image', 'global_dataset_name']
[2025-11-18 09:12:39,705] INFO [src.utils:19] ==================================================
[2025-11-18 09:12:41,281] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:12:41,281] INFO [src.trainer:350] ***** Running training *****
[2025-11-18 09:12:41,281] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:12:41,281] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:12:41,281] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:12:41,281] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:12:41,281] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:12:41,281] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:12:41,281] INFO [src.trainer:351]   Num examples = 192,000
[2025-11-18 09:12:41,281] INFO [src.trainer:352]   Num Epochs = 9,223,372,036,854,775,807
[2025-11-18 09:12:41,282] INFO [src.trainer:353]   Instantaneous batch size per device = 16
[2025-11-18 09:12:41,282] INFO [src.trainer:356]   Total train batch size (w. parallel, distributed & accumulation) = 32
[2025-11-18 09:12:41,282] INFO [src.trainer:357]   Gradient Accumulation steps = 1
[2025-11-18 09:12:41,282] INFO [src.trainer:358]   Total optimization steps = 6,000
[2025-11-18 09:12:41,285] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 09:12:41,285] INFO [src.trainer:359]   Number of trainable parameters = 9,203,712
[2025-11-18 09:12:41,287] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
[2025-11-18 09:12:41,288] INFO [src.trainer:360]   Trainable Parameters = ['module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.0.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.1.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.2.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.3.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.4.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.5.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.6.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.7.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.8.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.9.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.10.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.11.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.12.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.13.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.14.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.15.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.16.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.17.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.18.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.19.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.20.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.21.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.22.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.23.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.24.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.25.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.26.mlp.down_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.q_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.k_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.v_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.self_attn.o_proj.lora_magnitude_vector.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'module.encoder.base_model.model.model.layers.27.mlp.down_proj.lora_magnitude_vector.default.weight']
  0%|          | 0/6000 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
[rank1]:[W1118 09:12:43.511637791 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W1118 09:12:43.512598561 reducer.cpp:1400] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
  0%|          | 1/6000 [00:03<5:40:52,  3.41s/it]                                                  {'loss': 20.6075, 'grad_norm': 1352.077392578125, 'learning_rate': 1.0000000000000001e-07, 'epoch': 0.0}
  0%|          | 1/6000 [00:03<5:40:52,  3.41s/it]  0%|          | 2/6000 [00:05<4:13:44,  2.54s/it]                                                  {'loss': 17.7575, 'grad_norm': 1953.517822265625, 'learning_rate': 2.0000000000000002e-07, 'epoch': 0.0}
  0%|          | 2/6000 [00:05<4:13:44,  2.54s/it]  0%|          | 3/6000 [00:07<3:45:02,  2.25s/it]                                                  {'loss': 16.0961, 'grad_norm': 2228.09033203125, 'learning_rate': 3.0000000000000004e-07, 'epoch': 0.0}
  0%|          | 3/6000 [00:07<3:45:02,  2.25s/it]  0%|          | 4/6000 [00:09<3:37:05,  2.17s/it]                                                  {'loss': 16.2326, 'grad_norm': 1966.8333740234375, 'learning_rate': 4.0000000000000003e-07, 'epoch': 0.0}
  0%|          | 4/6000 [00:09<3:37:05,  2.17s/it]  0%|          | 5/6000 [00:11<3:28:33,  2.09s/it]                                                  {'loss': 16.993, 'grad_norm': 1936.4918212890625, 'learning_rate': 5.000000000000001e-07, 'epoch': 0.0}
  0%|          | 5/6000 [00:11<3:28:33,  2.09s/it]  0%|          | 6/6000 [00:13<3:22:44,  2.03s/it]                                                  {'loss': 18.4031, 'grad_norm': 1500.041748046875, 'learning_rate': 6.000000000000001e-07, 'epoch': 0.0}
  0%|          | 6/6000 [00:13<3:22:44,  2.03s/it]  0%|          | 7/6000 [00:15<3:19:40,  2.00s/it]                                                  {'loss': 17.7411, 'grad_norm': 1891.0074462890625, 'learning_rate': 7.000000000000001e-07, 'epoch': 0.0}
  0%|          | 7/6000 [00:15<3:19:40,  2.00s/it]  0%|          | 8/6000 [00:16<3:15:35,  1.96s/it]                                                  {'loss': 19.3024, 'grad_norm': 1741.8067626953125, 'learning_rate': 8.000000000000001e-07, 'epoch': 0.0}
  0%|          | 8/6000 [00:16<3:15:35,  1.96s/it]  0%|          | 9/6000 [00:18<3:16:08,  1.96s/it]                                                  {'loss': 15.1683, 'grad_norm': 1696.142578125, 'learning_rate': 9.000000000000001e-07, 'epoch': 0.0}
  0%|          | 9/6000 [00:18<3:16:08,  1.96s/it]  0%|          | 10/6000 [00:20<3:18:05,  1.98s/it]                                                   {'loss': 18.4847, 'grad_norm': 1309.7032470703125, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.0}
  0%|          | 10/6000 [00:20<3:18:05,  1.98s/it]  0%|          | 11/6000 [00:22<3:17:53,  1.98s/it]                                                   {'loss': 21.0772, 'grad_norm': 1573.7960205078125, 'learning_rate': 1.1e-06, 'epoch': 0.0}
  0%|          | 11/6000 [00:22<3:17:53,  1.98s/it]  0%|          | 12/6000 [00:24<3:18:42,  1.99s/it]                                                   {'loss': 18.256, 'grad_norm': 1771.708984375, 'learning_rate': 1.2000000000000002e-06, 'epoch': 0.0}
  0%|          | 12/6000 [00:24<3:18:42,  1.99s/it]  0%|          | 13/6000 [00:26<3:16:36,  1.97s/it]                                                   {'loss': 18.1087, 'grad_norm': 2265.65234375, 'learning_rate': 1.3e-06, 'epoch': 0.0}
  0%|          | 13/6000 [00:26<3:16:36,  1.97s/it]  0%|          | 14/6000 [00:28<3:18:09,  1.99s/it]                                                   {'loss': 18.0908, 'grad_norm': 2606.451904296875, 'learning_rate': 1.4000000000000001e-06, 'epoch': 0.0}
  0%|          | 14/6000 [00:28<3:18:09,  1.99s/it]  0%|          | 15/6000 [00:30<3:17:04,  1.98s/it]                                                   {'loss': 14.6021, 'grad_norm': 2240.2978515625, 'learning_rate': 1.5e-06, 'epoch': 0.0}
  0%|          | 15/6000 [00:30<3:17:04,  1.98s/it]  0%|          | 16/6000 [00:32<3:18:41,  1.99s/it]                                                   {'loss': 16.4735, 'grad_norm': 1535.4825439453125, 'learning_rate': 1.6000000000000001e-06, 'epoch': 0.0}
  0%|          | 16/6000 [00:32<3:18:41,  1.99s/it]  0%|          | 17/6000 [00:34<3:16:54,  1.97s/it]                                                   {'loss': 15.7288, 'grad_norm': 1375.7965087890625, 'learning_rate': 1.7000000000000002e-06, 'epoch': 0.0}
  0%|          | 17/6000 [00:34<3:16:54,  1.97s/it]  0%|          | 18/6000 [00:36<3:17:33,  1.98s/it]                                                   {'loss': 12.4371, 'grad_norm': 1554.8714599609375, 'learning_rate': 1.8000000000000001e-06, 'epoch': 0.0}
  0%|          | 18/6000 [00:36<3:17:33,  1.98s/it]  0%|          | 19/6000 [00:38<3:14:04,  1.95s/it]                                                   {'loss': 13.3854, 'grad_norm': 2090.417236328125, 'learning_rate': 1.9000000000000002e-06, 'epoch': 0.0}
  0%|          | 19/6000 [00:38<3:14:04,  1.95s/it]  0%|          | 20/6000 [00:40<3:14:37,  1.95s/it]                                                   {'loss': 14.9535, 'grad_norm': 1707.381591796875, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.0}
  0%|          | 20/6000 [00:40<3:14:37,  1.95s/it]  0%|          | 21/6000 [00:42<3:15:12,  1.96s/it]                                                   {'loss': 12.2229, 'grad_norm': 1982.47314453125, 'learning_rate': 2.1000000000000002e-06, 'epoch': 0.0}
  0%|          | 21/6000 [00:42<3:15:12,  1.96s/it]  0%|          | 22/6000 [00:44<3:13:34,  1.94s/it]                                                   {'loss': 11.649, 'grad_norm': 1795.9991455078125, 'learning_rate': 2.2e-06, 'epoch': 0.0}
  0%|          | 22/6000 [00:44<3:13:34,  1.94s/it]  0%|          | 23/6000 [00:46<3:15:19,  1.96s/it]                                                   {'loss': 10.8464, 'grad_norm': 1602.457275390625, 'learning_rate': 2.3000000000000004e-06, 'epoch': 0.0}
  0%|          | 23/6000 [00:46<3:15:19,  1.96s/it]  0%|          | 24/6000 [00:48<3:16:47,  1.98s/it]                                                   {'loss': 10.5783, 'grad_norm': 1706.12548828125, 'learning_rate': 2.4000000000000003e-06, 'epoch': 0.0}
  0%|          | 24/6000 [00:48<3:16:47,  1.98s/it]  0%|          | 25/6000 [00:50<3:14:20,  1.95s/it]                                                   {'loss': 10.9913, 'grad_norm': 2489.26806640625, 'learning_rate': 2.5e-06, 'epoch': 0.0}
  0%|          | 25/6000 [00:50<3:14:20,  1.95s/it]  0%|          | 26/6000 [00:52<3:14:30,  1.95s/it]                                                   {'loss': 9.159, 'grad_norm': 3867.12060546875, 'learning_rate': 2.6e-06, 'epoch': 0.0}
  0%|          | 26/6000 [00:52<3:14:30,  1.95s/it]  0%|          | 27/6000 [00:54<3:15:15,  1.96s/it]                                                   {'loss': 6.9505, 'grad_norm': 1679.4468994140625, 'learning_rate': 2.7000000000000004e-06, 'epoch': 0.0}
  0%|          | 27/6000 [00:54<3:15:15,  1.96s/it]  0%|          | 28/6000 [00:56<3:26:32,  2.08s/it]                                                   {'loss': 7.9888, 'grad_norm': 1300.5037841796875, 'learning_rate': 2.8000000000000003e-06, 'epoch': 0.0}
  0%|          | 28/6000 [00:56<3:26:32,  2.08s/it]  0%|          | 29/6000 [00:58<3:22:51,  2.04s/it]                                                   {'loss': 7.338, 'grad_norm': 2035.5201416015625, 'learning_rate': 2.9e-06, 'epoch': 0.0}
  0%|          | 29/6000 [00:58<3:22:51,  2.04s/it]  0%|          | 30/6000 [01:00<3:21:18,  2.02s/it]                                                   {'loss': 5.9025, 'grad_norm': 1887.088134765625, 'learning_rate': 3e-06, 'epoch': 0.01}
  0%|          | 30/6000 [01:00<3:21:18,  2.02s/it]  1%|          | 31/6000 [01:02<3:18:45,  2.00s/it]                                                   {'loss': 6.27, 'grad_norm': 1467.122314453125, 'learning_rate': 3.1000000000000004e-06, 'epoch': 0.01}
  1%|          | 31/6000 [01:02<3:18:45,  2.00s/it]  1%|          | 32/6000 [01:04<3:19:41,  2.01s/it]                                                   {'loss': 5.6928, 'grad_norm': 2025.9515380859375, 'learning_rate': 3.2000000000000003e-06, 'epoch': 0.01}
  1%|          | 32/6000 [01:04<3:19:41,  2.01s/it]  1%|          | 33/6000 [01:06<3:17:38,  1.99s/it]                                                   {'loss': 3.8864, 'grad_norm': 648.288818359375, 'learning_rate': 3.3000000000000006e-06, 'epoch': 0.01}
  1%|          | 33/6000 [01:06<3:17:38,  1.99s/it]  1%|          | 34/6000 [01:08<3:15:50,  1.97s/it]                                                   {'loss': 4.0772, 'grad_norm': 1040.15869140625, 'learning_rate': 3.4000000000000005e-06, 'epoch': 0.01}
  1%|          | 34/6000 [01:08<3:15:50,  1.97s/it]  1%|          | 35/6000 [01:10<3:15:33,  1.97s/it]                                                   {'loss': 3.9888, 'grad_norm': 739.9497680664062, 'learning_rate': 3.5e-06, 'epoch': 0.01}
  1%|          | 35/6000 [01:10<3:15:33,  1.97s/it]  1%|          | 36/6000 [01:12<3:14:13,  1.95s/it]                                                   {'loss': 4.4464, 'grad_norm': 1063.122802734375, 'learning_rate': 3.6000000000000003e-06, 'epoch': 0.01}
  1%|          | 36/6000 [01:12<3:14:13,  1.95s/it]  1%|          | 37/6000 [01:14<3:11:59,  1.93s/it]                                                   {'loss': 4.3295, 'grad_norm': 952.3096313476562, 'learning_rate': 3.7e-06, 'epoch': 0.01}
  1%|          | 37/6000 [01:14<3:11:59,  1.93s/it]  1%|          | 38/6000 [01:16<3:13:19,  1.95s/it]                                                   {'loss': 3.6935, 'grad_norm': 459.3982849121094, 'learning_rate': 3.8000000000000005e-06, 'epoch': 0.01}
  1%|          | 38/6000 [01:16<3:13:19,  1.95s/it]  1%|          | 39/6000 [01:18<3:12:31,  1.94s/it]                                                   {'loss': 3.9077, 'grad_norm': 514.9793701171875, 'learning_rate': 3.900000000000001e-06, 'epoch': 0.01}
  1%|          | 39/6000 [01:18<3:12:31,  1.94s/it]  1%|          | 40/6000 [01:20<3:12:18,  1.94s/it]                                                   {'loss': 4.2657, 'grad_norm': 384.4261474609375, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.01}
  1%|          | 40/6000 [01:20<3:12:18,  1.94s/it]  1%|          | 41/6000 [01:21<3:10:23,  1.92s/it]                                                   {'loss': 3.4423, 'grad_norm': 594.0098876953125, 'learning_rate': 4.1e-06, 'epoch': 0.01}
  1%|          | 41/6000 [01:21<3:10:23,  1.92s/it]  1%|          | 42/6000 [01:23<3:11:26,  1.93s/it]                                                   {'loss': 3.2833, 'grad_norm': 281.6761779785156, 'learning_rate': 4.2000000000000004e-06, 'epoch': 0.01}
  1%|          | 42/6000 [01:23<3:11:26,  1.93s/it]  1%|          | 43/6000 [01:26<3:23:24,  2.05s/it]                                                   {'loss': 3.7926, 'grad_norm': 543.957275390625, 'learning_rate': 4.3e-06, 'epoch': 0.01}
  1%|          | 43/6000 [01:26<3:23:24,  2.05s/it]  1%|          | 44/6000 [01:28<3:23:22,  2.05s/it]                                                   {'loss': 3.4486, 'grad_norm': 335.1965637207031, 'learning_rate': 4.4e-06, 'epoch': 0.01}
  1%|          | 44/6000 [01:28<3:23:22,  2.05s/it]  1%|          | 45/6000 [01:30<3:21:35,  2.03s/it]                                                   {'loss': 3.2663, 'grad_norm': 292.1610412597656, 'learning_rate': 4.5e-06, 'epoch': 0.01}
  1%|          | 45/6000 [01:30<3:21:35,  2.03s/it]  1%|          | 46/6000 [01:32<3:19:59,  2.02s/it]                                                   {'loss': 3.0249, 'grad_norm': 216.76332092285156, 'learning_rate': 4.600000000000001e-06, 'epoch': 0.01}
  1%|          | 46/6000 [01:32<3:19:59,  2.02s/it]  1%|          | 47/6000 [01:34<3:17:12,  1.99s/it]                                                   {'loss': 3.3341, 'grad_norm': 218.8826904296875, 'learning_rate': 4.7e-06, 'epoch': 0.01}
  1%|          | 47/6000 [01:34<3:17:12,  1.99s/it]  1%|          | 48/6000 [01:36<3:17:50,  1.99s/it]                                                   {'loss': 3.0284, 'grad_norm': 183.14669799804688, 'learning_rate': 4.800000000000001e-06, 'epoch': 0.01}
  1%|          | 48/6000 [01:36<3:17:50,  1.99s/it]  1%|          | 49/6000 [01:38<3:15:20,  1.97s/it]                                                   {'loss': 3.1504, 'grad_norm': 214.52662658691406, 'learning_rate': 4.9000000000000005e-06, 'epoch': 0.01}
  1%|          | 49/6000 [01:38<3:15:20,  1.97s/it]  1%|          | 50/6000 [01:40<3:18:59,  2.01s/it]                                                   {'loss': 3.1091, 'grad_norm': 173.52061462402344, 'learning_rate': 5e-06, 'epoch': 0.01}
  1%|          | 50/6000 [01:40<3:18:59,  2.01s/it]  1%|          | 51/6000 [01:42<3:15:58,  1.98s/it]                                                   {'loss': 3.0595, 'grad_norm': 199.0796356201172, 'learning_rate': 5.1e-06, 'epoch': 0.01}
  1%|          | 51/6000 [01:42<3:15:58,  1.98s/it]  1%|          | 52/6000 [01:43<3:13:24,  1.95s/it]                                                   {'loss': 3.3502, 'grad_norm': 419.45574951171875, 'learning_rate': 5.2e-06, 'epoch': 0.01}
  1%|          | 52/6000 [01:43<3:13:24,  1.95s/it]  1%|          | 53/6000 [01:45<3:13:40,  1.95s/it]                                                   {'loss': 3.9222, 'grad_norm': 442.0193786621094, 'learning_rate': 5.300000000000001e-06, 'epoch': 0.01}
  1%|          | 53/6000 [01:45<3:13:40,  1.95s/it]  1%|          | 54/6000 [01:47<3:10:54,  1.93s/it]                                                   {'loss': 2.9405, 'grad_norm': 190.20384216308594, 'learning_rate': 5.400000000000001e-06, 'epoch': 0.01}
  1%|          | 54/6000 [01:47<3:10:54,  1.93s/it]  1%|          | 55/6000 [01:49<3:10:15,  1.92s/it]                                                   {'loss': 2.9892, 'grad_norm': 175.042236328125, 'learning_rate': 5.500000000000001e-06, 'epoch': 0.01}
  1%|          | 55/6000 [01:49<3:10:15,  1.92s/it]  1%|          | 56/6000 [01:51<3:12:03,  1.94s/it]                                                   {'loss': 3.0981, 'grad_norm': 170.7777862548828, 'learning_rate': 5.600000000000001e-06, 'epoch': 0.01}
  1%|          | 56/6000 [01:51<3:12:03,  1.94s/it]  1%|          | 57/6000 [01:53<3:11:26,  1.93s/it]                                                   {'loss': 2.8464, 'grad_norm': 159.4150390625, 'learning_rate': 5.7e-06, 'epoch': 0.01}
  1%|          | 57/6000 [01:53<3:11:26,  1.93s/it]  1%|          | 58/6000 [01:55<3:10:39,  1.93s/it]                                                   {'loss': 2.8666, 'grad_norm': 151.4586181640625, 'learning_rate': 5.8e-06, 'epoch': 0.01}
  1%|          | 58/6000 [01:55<3:10:39,  1.93s/it]  1%|          | 59/6000 [01:57<3:10:16,  1.92s/it]                                                   {'loss': 2.893, 'grad_norm': 173.8413848876953, 'learning_rate': 5.9e-06, 'epoch': 0.01}
  1%|          | 59/6000 [01:57<3:10:16,  1.92s/it]  1%|          | 60/6000 [01:59<3:09:34,  1.91s/it]                                                   {'loss': 2.8741, 'grad_norm': 152.4127655029297, 'learning_rate': 6e-06, 'epoch': 0.01}
  1%|          | 60/6000 [01:59<3:09:34,  1.91s/it]  1%|          | 61/6000 [02:01<3:10:12,  1.92s/it]                                                   {'loss': 2.9495, 'grad_norm': 159.14627075195312, 'learning_rate': 6.1e-06, 'epoch': 0.01}
  1%|          | 61/6000 [02:01<3:10:12,  1.92s/it]  1%|          | 62/6000 [02:03<3:10:21,  1.92s/it]                                                   {'loss': 3.0059, 'grad_norm': 795.8521118164062, 'learning_rate': 6.200000000000001e-06, 'epoch': 0.01}
  1%|          | 62/6000 [02:03<3:10:21,  1.92s/it]  1%|          | 63/6000 [02:05<3:10:37,  1.93s/it]                                                   {'loss': 2.6316, 'grad_norm': 134.38197326660156, 'learning_rate': 6.300000000000001e-06, 'epoch': 0.01}
  1%|          | 63/6000 [02:05<3:10:37,  1.93s/it]  1%|          | 64/6000 [02:07<3:09:26,  1.91s/it]                                                   {'loss': 2.8373, 'grad_norm': 356.9548645019531, 'learning_rate': 6.4000000000000006e-06, 'epoch': 0.01}
  1%|          | 64/6000 [02:07<3:09:26,  1.91s/it]  1%|          | 65/6000 [02:08<3:09:00,  1.91s/it]                                                   {'loss': 2.931, 'grad_norm': 189.42930603027344, 'learning_rate': 6.5000000000000004e-06, 'epoch': 0.01}
  1%|          | 65/6000 [02:08<3:09:00,  1.91s/it]  1%|          | 66/6000 [02:10<3:12:21,  1.95s/it]                                                   {'loss': 2.766, 'grad_norm': 205.05029296875, 'learning_rate': 6.600000000000001e-06, 'epoch': 0.01}
  1%|          | 66/6000 [02:10<3:12:21,  1.95s/it]  1%|          | 67/6000 [02:12<3:13:19,  1.96s/it]                                                   {'loss': 2.4914, 'grad_norm': 164.4581298828125, 'learning_rate': 6.700000000000001e-06, 'epoch': 0.01}
  1%|          | 67/6000 [02:12<3:13:19,  1.96s/it]  1%|          | 68/6000 [02:14<3:11:36,  1.94s/it]                                                   {'loss': 2.5636, 'grad_norm': 184.1907501220703, 'learning_rate': 6.800000000000001e-06, 'epoch': 0.01}
  1%|          | 68/6000 [02:14<3:11:36,  1.94s/it]  1%|          | 69/6000 [02:16<3:11:30,  1.94s/it]                                                   {'loss': 2.6341, 'grad_norm': 150.6505889892578, 'learning_rate': 6.9e-06, 'epoch': 0.01}
  1%|          | 69/6000 [02:16<3:11:30,  1.94s/it]  1%|          | 70/6000 [02:18<3:13:30,  1.96s/it]                                                   {'loss': 2.7141, 'grad_norm': 209.7253875732422, 'learning_rate': 7e-06, 'epoch': 0.01}
  1%|          | 70/6000 [02:18<3:13:30,  1.96s/it]  1%|          | 71/6000 [02:20<3:11:47,  1.94s/it]                                                   {'loss': 2.812, 'grad_norm': 141.79627990722656, 'learning_rate': 7.100000000000001e-06, 'epoch': 0.01}
  1%|          | 71/6000 [02:20<3:11:47,  1.94s/it]  1%|          | 72/6000 [02:22<3:13:22,  1.96s/it]                                                   {'loss': 2.5965, 'grad_norm': 166.72279357910156, 'learning_rate': 7.2000000000000005e-06, 'epoch': 0.01}
  1%|          | 72/6000 [02:22<3:13:22,  1.96s/it]  1%|          | 73/6000 [02:24<3:13:33,  1.96s/it]                                                   {'loss': 2.4923, 'grad_norm': 175.38377380371094, 'learning_rate': 7.3e-06, 'epoch': 0.01}
  1%|          | 73/6000 [02:24<3:13:33,  1.96s/it]  1%|          | 74/6000 [02:26<3:12:56,  1.95s/it]                                                   {'loss': 2.2796, 'grad_norm': 134.93917846679688, 'learning_rate': 7.4e-06, 'epoch': 0.01}
  1%|          | 74/6000 [02:26<3:12:56,  1.95s/it]  1%|â–         | 75/6000 [02:28<3:11:54,  1.94s/it]                                                   {'loss': 2.2775, 'grad_norm': 131.6774444580078, 'learning_rate': 7.500000000000001e-06, 'epoch': 0.01}
  1%|â–         | 75/6000 [02:28<3:11:54,  1.94s/it]  1%|â–         | 76/6000 [02:30<3:13:38,  1.96s/it]                                                   {'loss': 2.2331, 'grad_norm': 141.7110137939453, 'learning_rate': 7.600000000000001e-06, 'epoch': 0.01}
  1%|â–         | 76/6000 [02:30<3:13:38,  1.96s/it]  1%|â–         | 77/6000 [02:32<3:21:20,  2.04s/it]                                                   {'loss': 2.5301, 'grad_norm': 166.4300537109375, 'learning_rate': 7.7e-06, 'epoch': 0.01}
  1%|â–         | 77/6000 [02:32<3:21:20,  2.04s/it]  1%|â–         | 78/6000 [02:34<3:24:48,  2.08s/it]                                                   {'loss': 2.2554, 'grad_norm': 161.55824279785156, 'learning_rate': 7.800000000000002e-06, 'epoch': 0.01}
  1%|â–         | 78/6000 [02:34<3:24:48,  2.08s/it]  1%|â–         | 79/6000 [02:36<3:22:15,  2.05s/it]                                                   {'loss': 2.1212, 'grad_norm': 135.1311798095703, 'learning_rate': 7.9e-06, 'epoch': 0.01}
  1%|â–         | 79/6000 [02:36<3:22:15,  2.05s/it]  1%|â–         | 80/6000 [02:38<3:21:31,  2.04s/it]                                                   {'loss': 1.8619, 'grad_norm': 168.51890563964844, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.01}
  1%|â–         | 80/6000 [02:38<3:21:31,  2.04s/it]  1%|â–         | 81/6000 [02:40<3:17:46,  2.00s/it]                                                   {'loss': 1.8106, 'grad_norm': 146.16897583007812, 'learning_rate': 8.1e-06, 'epoch': 0.01}
  1%|â–         | 81/6000 [02:40<3:17:46,  2.00s/it]  1%|â–         | 82/6000 [02:42<3:16:25,  1.99s/it]                                                   {'loss': 2.0624, 'grad_norm': 272.92169189453125, 'learning_rate': 8.2e-06, 'epoch': 0.01}
  1%|â–         | 82/6000 [02:42<3:16:25,  1.99s/it]  1%|â–         | 83/6000 [02:44<3:15:14,  1.98s/it]                                                   {'loss': 1.6448, 'grad_norm': 171.3616180419922, 'learning_rate': 8.3e-06, 'epoch': 0.01}
  1%|â–         | 83/6000 [02:44<3:15:14,  1.98s/it]  1%|â–         | 84/6000 [02:46<3:17:11,  2.00s/it]                                                   {'loss': 1.3724, 'grad_norm': 152.02833557128906, 'learning_rate': 8.400000000000001e-06, 'epoch': 0.01}
  1%|â–         | 84/6000 [02:46<3:17:11,  2.00s/it]  1%|â–         | 85/6000 [02:48<3:17:42,  2.01s/it]                                                   {'loss': 1.4508, 'grad_norm': 452.81744384765625, 'learning_rate': 8.5e-06, 'epoch': 0.01}
  1%|â–         | 85/6000 [02:48<3:17:42,  2.01s/it]  1%|â–         | 86/6000 [02:50<3:16:37,  1.99s/it]                                                   {'loss': 1.2415, 'grad_norm': 114.73025512695312, 'learning_rate': 8.6e-06, 'epoch': 0.01}
  1%|â–         | 86/6000 [02:50<3:16:37,  1.99s/it]  1%|â–         | 87/6000 [02:52<3:15:51,  1.99s/it]                                                   {'loss': 0.8718, 'grad_norm': 117.2070541381836, 'learning_rate': 8.700000000000001e-06, 'epoch': 0.01}
  1%|â–         | 87/6000 [02:52<3:15:51,  1.99s/it]  1%|â–         | 88/6000 [02:54<3:13:33,  1.96s/it]                                                   {'loss': 1.357, 'grad_norm': 324.8514099121094, 'learning_rate': 8.8e-06, 'epoch': 0.01}
  1%|â–         | 88/6000 [02:54<3:13:33,  1.96s/it]  1%|â–         | 89/6000 [02:56<3:12:34,  1.95s/it]                                                   {'loss': 0.8741, 'grad_norm': 108.4057846069336, 'learning_rate': 8.900000000000001e-06, 'epoch': 0.01}
  1%|â–         | 89/6000 [02:56<3:12:34,  1.95s/it]  2%|â–         | 90/6000 [02:58<3:13:35,  1.97s/it]                                                   {'loss': 0.5309, 'grad_norm': 70.76673126220703, 'learning_rate': 9e-06, 'epoch': 0.01}
  2%|â–         | 90/6000 [02:58<3:13:35,  1.97s/it]  2%|â–         | 91/6000 [03:00<3:13:51,  1.97s/it]                                                   {'loss': 0.8078, 'grad_norm': 97.7376708984375, 'learning_rate': 9.100000000000001e-06, 'epoch': 0.02}
  2%|â–         | 91/6000 [03:00<3:13:51,  1.97s/it]  2%|â–         | 92/6000 [03:02<3:16:09,  1.99s/it]                                                   {'loss': 0.6258, 'grad_norm': 158.62075805664062, 'learning_rate': 9.200000000000002e-06, 'epoch': 0.02}
  2%|â–         | 92/6000 [03:02<3:16:09,  1.99s/it]  2%|â–         | 93/6000 [03:04<3:15:23,  1.98s/it]                                                   {'loss': 0.7827, 'grad_norm': 972.3282470703125, 'learning_rate': 9.3e-06, 'epoch': 0.02}
  2%|â–         | 93/6000 [03:04<3:15:23,  1.98s/it]  2%|â–         | 94/6000 [03:06<3:15:17,  1.98s/it]                                                   {'loss': 0.6837, 'grad_norm': 499.1214599609375, 'learning_rate': 9.4e-06, 'epoch': 0.02}
  2%|â–         | 94/6000 [03:06<3:15:17,  1.98s/it]  2%|â–         | 95/6000 [03:08<3:13:08,  1.96s/it]                                                   {'loss': 0.4851, 'grad_norm': 88.55589294433594, 'learning_rate': 9.5e-06, 'epoch': 0.02}
  2%|â–         | 95/6000 [03:08<3:13:08,  1.96s/it]  2%|â–         | 96/6000 [03:10<3:12:44,  1.96s/it]                                                   {'loss': 0.3348, 'grad_norm': 49.78880310058594, 'learning_rate': 9.600000000000001e-06, 'epoch': 0.02}
  2%|â–         | 96/6000 [03:10<3:12:44,  1.96s/it]  2%|â–         | 97/6000 [03:12<3:10:53,  1.94s/it]                                                   {'loss': 0.8666, 'grad_norm': 92.77220153808594, 'learning_rate': 9.7e-06, 'epoch': 0.02}
  2%|â–         | 97/6000 [03:12<3:10:53,  1.94s/it]  2%|â–         | 98/6000 [03:14<3:11:33,  1.95s/it]                                                   {'loss': 0.5875, 'grad_norm': 58.02347183227539, 'learning_rate': 9.800000000000001e-06, 'epoch': 0.02}
  2%|â–         | 98/6000 [03:14<3:11:33,  1.95s/it]  2%|â–         | 99/6000 [03:16<3:09:30,  1.93s/it]                                                   {'loss': 0.3893, 'grad_norm': 54.10197448730469, 'learning_rate': 9.9e-06, 'epoch': 0.02}
  2%|â–         | 99/6000 [03:16<3:09:30,  1.93s/it]  2%|â–         | 100/6000 [03:18<3:08:27,  1.92s/it]                                                    {'loss': 0.8368, 'grad_norm': 489.3428649902344, 'learning_rate': 1e-05, 'epoch': 0.02}
  2%|â–         | 100/6000 [03:18<3:08:27,  1.92s/it]  2%|â–         | 101/6000 [03:20<3:20:14,  2.04s/it]                                                    {'loss': 0.3386, 'grad_norm': 37.497684478759766, 'learning_rate': 9.998305084745762e-06, 'epoch': 0.02}
  2%|â–         | 101/6000 [03:20<3:20:14,  2.04s/it]  2%|â–         | 102/6000 [03:22<3:16:07,  2.00s/it]                                                    {'loss': 0.2875, 'grad_norm': 60.332786560058594, 'learning_rate': 9.996610169491526e-06, 'epoch': 0.02}
  2%|â–         | 102/6000 [03:22<3:16:07,  2.00s/it]  2%|â–         | 103/6000 [03:24<3:14:44,  1.98s/it]                                                    {'loss': 0.2494, 'grad_norm': 50.45642852783203, 'learning_rate': 9.994915254237289e-06, 'epoch': 0.02}
  2%|â–         | 103/6000 [03:24<3:14:44,  1.98s/it]  2%|â–         | 104/6000 [03:26<3:15:30,  1.99s/it]                                                    {'loss': 0.4026, 'grad_norm': 38.71998596191406, 'learning_rate': 9.993220338983052e-06, 'epoch': 0.02}
  2%|â–         | 104/6000 [03:26<3:15:30,  1.99s/it]  2%|â–         | 105/6000 [03:28<3:15:40,  1.99s/it]                                                    {'loss': 0.5434, 'grad_norm': 57.14507293701172, 'learning_rate': 9.991525423728814e-06, 'epoch': 0.02}
  2%|â–         | 105/6000 [03:28<3:15:40,  1.99s/it]  2%|â–         | 106/6000 [03:30<3:15:06,  1.99s/it]                                                    {'loss': 0.1661, 'grad_norm': 28.1639461517334, 'learning_rate': 9.989830508474577e-06, 'epoch': 0.02}
  2%|â–         | 106/6000 [03:30<3:15:06,  1.99s/it]  2%|â–         | 107/6000 [03:32<3:13:19,  1.97s/it]                                                    {'loss': 0.4113, 'grad_norm': 39.91103744506836, 'learning_rate': 9.988135593220339e-06, 'epoch': 0.02}
  2%|â–         | 107/6000 [03:32<3:13:19,  1.97s/it]  2%|â–         | 108/6000 [03:34<3:14:33,  1.98s/it]                                                    {'loss': 0.2958, 'grad_norm': 56.74708938598633, 'learning_rate': 9.986440677966102e-06, 'epoch': 0.02}
  2%|â–         | 108/6000 [03:34<3:14:33,  1.98s/it]  2%|â–         | 109/6000 [03:36<3:18:16,  2.02s/it]                                                    {'loss': 0.3779, 'grad_norm': 57.12906265258789, 'learning_rate': 9.984745762711865e-06, 'epoch': 0.02}
  2%|â–         | 109/6000 [03:36<3:18:16,  2.02s/it]  2%|â–         | 110/6000 [03:38<3:17:33,  2.01s/it]                                                    {'loss': 0.2048, 'grad_norm': 25.718048095703125, 'learning_rate': 9.983050847457628e-06, 'epoch': 0.02}
  2%|â–         | 110/6000 [03:38<3:17:33,  2.01s/it]  2%|â–         | 111/6000 [03:40<3:16:53,  2.01s/it]                                                    {'loss': 0.326, 'grad_norm': 37.6483154296875, 'learning_rate': 9.98135593220339e-06, 'epoch': 0.02}
  2%|â–         | 111/6000 [03:40<3:16:53,  2.01s/it]  2%|â–         | 112/6000 [03:42<3:18:11,  2.02s/it]                                                    {'loss': 0.2813, 'grad_norm': 33.39430618286133, 'learning_rate': 9.979661016949153e-06, 'epoch': 0.02}
  2%|â–         | 112/6000 [03:42<3:18:11,  2.02s/it]  2%|â–         | 113/6000 [03:44<3:18:04,  2.02s/it]                                                    {'loss': 0.3086, 'grad_norm': 54.042823791503906, 'learning_rate': 9.977966101694917e-06, 'epoch': 0.02}
  2%|â–         | 113/6000 [03:44<3:18:04,  2.02s/it]  2%|â–         | 114/6000 [03:46<3:18:14,  2.02s/it]                                                    {'loss': 0.267, 'grad_norm': 49.679378509521484, 'learning_rate': 9.97627118644068e-06, 'epoch': 0.02}
  2%|â–         | 114/6000 [03:46<3:18:14,  2.02s/it]  2%|â–         | 115/6000 [03:48<3:15:19,  1.99s/it]                                                    {'loss': 0.3411, 'grad_norm': 58.24168014526367, 'learning_rate': 9.974576271186441e-06, 'epoch': 0.02}
  2%|â–         | 115/6000 [03:48<3:15:19,  1.99s/it]  2%|â–         | 116/6000 [03:50<3:13:16,  1.97s/it]                                                    {'loss': 0.3752, 'grad_norm': 44.194740295410156, 'learning_rate': 9.972881355932205e-06, 'epoch': 0.02}
  2%|â–         | 116/6000 [03:50<3:13:16,  1.97s/it]  2%|â–         | 117/6000 [03:52<3:12:10,  1.96s/it]                                                    {'loss': 0.3695, 'grad_norm': 50.997188568115234, 'learning_rate': 9.971186440677966e-06, 'epoch': 0.02}
  2%|â–         | 117/6000 [03:52<3:12:10,  1.96s/it]  2%|â–         | 118/6000 [03:54<3:17:15,  2.01s/it]                                                    {'loss': 0.461, 'grad_norm': 42.98667526245117, 'learning_rate': 9.96949152542373e-06, 'epoch': 0.02}
  2%|â–         | 118/6000 [03:54<3:17:15,  2.01s/it]